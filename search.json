[
  {
    "objectID": "slides/recent_advances.html#what-are-they",
    "href": "slides/recent_advances.html#what-are-they",
    "title": "Recent Advances",
    "section": "What are they?",
    "text": "What are they?\nFoundation models are large-scale machine learning models trained on vast amounts of data that can be fine-tuned for various downstream tasks. These models have demonstrated remarkable capabilities in natural language processing, computer vision, and other fields.",
    "crumbs": [
      "Slides",
      "Recent Advances"
    ]
  },
  {
    "objectID": "slides/recent_advances.html#characteristics-of-foundation-models",
    "href": "slides/recent_advances.html#characteristics-of-foundation-models",
    "title": "Recent Advances",
    "section": "Characteristics of Foundation Models",
    "text": "Characteristics of Foundation Models\n\nLarge-scale Pre-training: Foundation models are pre-trained on extensive datasets, enabling them to capture a wide range of knowledge.\nTransfer Learning: These models can be fine-tuned on specific tasks with relatively small datasets, making them versatile and efficient.\nMultimodal Capabilities: Some foundation models can process and integrate multiple types of data, such as text and images.\n\n\nExample: CLIP (Contrastive Language-Image Pre-training, Radford et al. (2021)) is a foundation model developed by OpenAI. It is designed to understand images and text jointly, making it capable of tasks like zero-shot image classification.",
    "crumbs": [
      "Slides",
      "Recent Advances"
    ]
  },
  {
    "objectID": "slides/recent_advances.html#how-clip-works",
    "href": "slides/recent_advances.html#how-clip-works",
    "title": "Recent Advances",
    "section": "How CLIP Works",
    "text": "How CLIP Works\n\n\nFigure 1: Illustration of CLIP. From Radford et al. (2021).\nCLIP is pre-trained on a diverse dataset of images and their corresponding textual descriptions. It learns to associate images with their textual descriptions using a contrastive learning approach, which maximizes the similarity between correct image-text pairs and minimizes the similarity between incorrect pairs.",
    "crumbs": [
      "Slides",
      "Recent Advances"
    ]
  },
  {
    "objectID": "slides/recent_advances.html#how-clip-can-be-applied",
    "href": "slides/recent_advances.html#how-clip-can-be-applied",
    "title": "Recent Advances",
    "section": "How CLIP can be applied",
    "text": "How CLIP can be applied\n\n\nFigure 2: Zero-shot learning with CLIP. From Radford et al. (2021).",
    "crumbs": [
      "Slides",
      "Recent Advances"
    ]
  },
  {
    "objectID": "slides/recent_advances.html#applications-of-clip",
    "href": "slides/recent_advances.html#applications-of-clip",
    "title": "Recent Advances",
    "section": "Applications of CLIP",
    "text": "Applications of CLIP\n\nZero-Shot Classification: CLIP can classify images into categories it has not explicitly been trained on by leveraging its understanding of language.\nImage Search: By inputting a textual description, CLIP can retrieve relevant images from a database.\n(near) Duplicate Detection: CLIP can assist in identifying near duplicate images.",
    "crumbs": [
      "Slides",
      "Recent Advances"
    ]
  },
  {
    "objectID": "slides/recent_advances.html#example",
    "href": "slides/recent_advances.html#example",
    "title": "Recent Advances",
    "section": "Example",
    "text": "Example\nHere’s a simple example of using CLIP for zero-shot image classification:\n\nimport requests\nfrom PIL import Image\nfrom transformers import CLIPModel, CLIPProcessor\n\n# 1) Get Models and Data Pre-Processors\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n# 2) Define Input Data: Image and Texts\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprompt_template = \"a photo of a {}\"\nclasses = [\"cat\", \"dog\", \"car\", \"tree\"]\nprompts = [prompt_template.format(class_name) for class_name in classes]\n\n# 3) Pre-Process Inputs\ninputs = processor(\n    text=prompts,\n    images=image,\n    return_tensors=\"pt\",\n    padding=True,\n)\n\n# 4) Forward-Pass\nwith torch.no_grad():\n    outputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n\n# 5) Find most likely label\nprobs = logits_per_image.softmax(dim=1)\nprint(\"Label:\", classes[probs.argmax().item()])",
    "crumbs": [
      "Slides",
      "Recent Advances"
    ]
  },
  {
    "objectID": "slides/recent_advances.html#fine-tuning-clip",
    "href": "slides/recent_advances.html#fine-tuning-clip",
    "title": "Recent Advances",
    "section": "Fine-Tuning CLIP",
    "text": "Fine-Tuning CLIP\nIf CLIP does not perform well with zero-shot learning, it can be fine-tuned with various techniques. This is a powerful mechanism because the features that CLIP has learned are very powerful and might be useful for your target task.",
    "crumbs": [
      "Slides",
      "Recent Advances"
    ]
  },
  {
    "objectID": "slides/recent_advances.html#what-are-they-1",
    "href": "slides/recent_advances.html#what-are-they-1",
    "title": "Recent Advances",
    "section": "What are they?",
    "text": "What are they?\nMulti-modal models are able to process multiple types of data (modes) simultaneously.",
    "crumbs": [
      "Slides",
      "Recent Advances"
    ]
  },
  {
    "objectID": "slides/recent_advances.html#example-visual-question-answering",
    "href": "slides/recent_advances.html#example-visual-question-answering",
    "title": "Recent Advances",
    "section": "Example: Visual Question Answering",
    "text": "Example: Visual Question Answering\nOne task where multi-modality is required, is Visual Question Answering (VQA). Only by jointly understanding the visual input and the question can such tasks be solved.\n\n\nFigure 3: BLIP2 examples from a chat system. Source: Li et al. (2023)",
    "crumbs": [
      "Slides",
      "Recent Advances"
    ]
  },
  {
    "objectID": "slides/recent_advances.html#how-vqa-works",
    "href": "slides/recent_advances.html#how-vqa-works",
    "title": "Recent Advances",
    "section": "How VQA Works",
    "text": "How VQA Works\nVQA models combine visual data (images) with textual data (questions) to generate accurate answers. These models are typically pre-trained on large datasets containing images, questions about those images, and the corresponding answers.\n\n\nFigure 4: Training data examples. Source: Alayrac et al. (2022)",
    "crumbs": [
      "Slides",
      "Recent Advances"
    ]
  },
  {
    "objectID": "slides/recent_advances.html#applications-of-vqa",
    "href": "slides/recent_advances.html#applications-of-vqa",
    "title": "Recent Advances",
    "section": "Applications of VQA",
    "text": "Applications of VQA\n\nAccessibility: VQA can help visually impaired users by answering questions about their surroundings based on images captured by a camera.\nChat Bots with Visual Understanding: VQA systems can be used in chat bots to incorporate visual inputs.\nCustomer Support: VQA can enhance customer support by allowing users to submit images and ask questions about products or services.",
    "crumbs": [
      "Slides",
      "Recent Advances"
    ]
  },
  {
    "objectID": "slides/recent_advances.html#what-are-transformers",
    "href": "slides/recent_advances.html#what-are-transformers",
    "title": "Recent Advances",
    "section": "What Are Transformers?",
    "text": "What Are Transformers?\nTransformers, introduced by Vaswani et al. (2017) use a self-attention mechanism that enables the model to consider relationships between different parts of the input data. This enables transformers to process data in parallel and capture long-range dependencies more effectively than previous architectures.\n\nKey concepts:\n\nSelf-Attention: Enables each input element to focus on all other elements, capturing complex dependencies within the data.\nPositional Encoding: Injects information about the order of the sequence, allowing the transformer to process sequential data despite its parallel structure.\nScalability: The architecture scales well, making it suitable for training on large datasets across various domains.",
    "crumbs": [
      "Slides",
      "Recent Advances"
    ]
  },
  {
    "objectID": "slides/recent_advances.html#vision-transformers-vit",
    "href": "slides/recent_advances.html#vision-transformers-vit",
    "title": "Recent Advances",
    "section": "Vision Transformers (ViT)",
    "text": "Vision Transformers (ViT)\nVision Transformers (ViTs), proposed by Dosovitskiy et al. (2020), apply the transformer architecture to image data by splitting images into fixed-size patches and treating each patch as a token in a sequence. This approach enables ViTs to capture spatial relationships in images without convolutional layers.\n\n\nFigure 5: Illustration of ViTs. Source: Dosovitskiy et al. (2020)",
    "crumbs": [
      "Slides",
      "Recent Advances"
    ]
  },
  {
    "objectID": "slides/recent_advances.html#applications-of-vision-transformers",
    "href": "slides/recent_advances.html#applications-of-vision-transformers",
    "title": "Recent Advances",
    "section": "Applications of Vision Transformers",
    "text": "Applications of Vision Transformers\n\nImage Classification: ViTs have achieved competitive results in image classification, sometimes outperforming CNNs.\nObject Detection and Segmentation: Transformers can be adapted for tasks like object detection by directly predicting bounding boxes.\nMultimodal Models: Transformers enable models to integrate data from different sources, such as text and images, for tasks like image captioning and visual question answering.",
    "crumbs": [
      "Slides",
      "Recent Advances"
    ]
  },
  {
    "objectID": "slides/recent_advances.html#comparison-vision-transformers-and-convolutional-neural-networks",
    "href": "slides/recent_advances.html#comparison-vision-transformers-and-convolutional-neural-networks",
    "title": "Recent Advances",
    "section": "Comparison: Vision Transformers and Convolutional Neural Networks",
    "text": "Comparison: Vision Transformers and Convolutional Neural Networks\n\nArchitectural differences\nScalability and flexibility\nrobustness and transferability",
    "crumbs": [
      "Slides",
      "Recent Advances"
    ]
  },
  {
    "objectID": "slides/recent_advances.html#pros-and-cons-cnns",
    "href": "slides/recent_advances.html#pros-and-cons-cnns",
    "title": "Recent Advances",
    "section": "Pros and Cons CNNS",
    "text": "Pros and Cons CNNS\n\nPros:\n\nEfficiency for Small Datasets: CNNs excel in small to moderate datasets.\nRobustness to Variations: Their design enables resilience to real-world applications.\n\nCons:\n\nLimited Long-Range Dependencies: Due to their focus on local receptive fields, CNNs may struggle with long-range dependencies.",
    "crumbs": [
      "Slides",
      "Recent Advances"
    ]
  },
  {
    "objectID": "slides/recent_advances.html#pros-and-cons-vits",
    "href": "slides/recent_advances.html#pros-and-cons-vits",
    "title": "Recent Advances",
    "section": "Pros and Cons ViTs",
    "text": "Pros and Cons ViTs\n\nPros:\n\nScalability and Flexibility: ViTs perform well with large datasets and benefit from self-supervised pre-training.\nLong-Range Dependency Capture: Self-attention mechanisms allow ViTs to capture long-range dependencies.\n\nCons:\n\nData Efficiency: ViTs often require more data and pre-training.\nComputational Intensity: Self-attention mechanisms can be computationally demanding.",
    "crumbs": [
      "Slides",
      "Recent Advances"
    ]
  },
  {
    "objectID": "slides/recent_advances.html#conclusion",
    "href": "slides/recent_advances.html#conclusion",
    "title": "Recent Advances",
    "section": "Conclusion",
    "text": "Conclusion\nCurrently, transformers and CNNs perform similarly when pre-trained on massive datasets. Hybrid architectures combining convolutional layers with transformer layers are also being explored. Further research will continue to determine which architecture excels in specific contexts.",
    "crumbs": [
      "Slides",
      "Recent Advances"
    ]
  },
  {
    "objectID": "slides/intro.html#species-identification",
    "href": "slides/intro.html#species-identification",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Species Identification",
    "text": "Species Identification\n\nSource: Breitenmoser-Würsten et al. (2024)",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides/intro.html#synthetic-image-detection",
    "href": "slides/intro.html#synthetic-image-detection",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Synthetic Image Detection",
    "text": "Synthetic Image Detection\n\n\n\n\n\n\nSource\n\n\n\n\n\n\n\nSource",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides/intro.html#object-identification-and-translation",
    "href": "slides/intro.html#object-identification-and-translation",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Object Identification and Translation",
    "text": "Object Identification and Translation\n\n\n\n\n\n\nIdentification & Search\n\n\n\n\n\n\n\nTranslation\n\n\n\n\n\nGoogle Lens",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides/intro.html#self-driving",
    "href": "slides/intro.html#self-driving",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Self-Driving",
    "text": "Self-Driving\n\nExample from Waymo.",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides/intro.html#biometric-id",
    "href": "slides/intro.html#biometric-id",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Biometric ID",
    "text": "Biometric ID\n\nExample from Apple Face ID",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides/intro.html#precision-agriculture",
    "href": "slides/intro.html#precision-agriculture",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Precision Agriculture",
    "text": "Precision Agriculture\n\n\nExample from Häni, Roy, and Isler (2020)",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides/intro.html#medical-segmentation",
    "href": "slides/intro.html#medical-segmentation",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Medical Segmentation",
    "text": "Medical Segmentation\n\nExample from Ma et al. (2024).",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides/intro.html#photo-enhancement",
    "href": "slides/intro.html#photo-enhancement",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Photo Enhancement",
    "text": "Photo Enhancement\n\nExample from Google Magic Editor",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides/intro.html#ai-chips",
    "href": "slides/intro.html#ai-chips",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "AI Chips",
    "text": "AI Chips\n\nFrom Link.",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides/intro.html#image-classification",
    "href": "slides/intro.html#image-classification",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Image Classification",
    "text": "Image Classification\n\nMulti-Class Image Classification Beispiel (aus Krizhevsky, Sutskever, and Hinton (2012)).",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides/intro.html#object-detection",
    "href": "slides/intro.html#object-detection",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Object Detection",
    "text": "Object Detection\n\nObject Detection Beispiel (aus (redmon_you_2016?)). Bounding boxes lokalisieren die Objekte, wobei für jedes Objekt die wahrscheinlichste Klasse, sowie deren Konfidenz angegeben ist.",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides/intro.html#segmentation",
    "href": "slides/intro.html#segmentation",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Segmentation",
    "text": "Segmentation\n\n\n\n\n\nObject Segmentation Beispiel (aus He et al. (2018)).",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides/intro.html#image-generation---manipulation",
    "href": "slides/intro.html#image-generation---manipulation",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Image Generation - Manipulation",
    "text": "Image Generation - Manipulation\n\nSource: Link, DragGAN by Pan et al. (2023)",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides/intro.html#image-generation---translation",
    "href": "slides/intro.html#image-generation---translation",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Image Generation - Translation",
    "text": "Image Generation - Translation\n\nImage Generation Beispiel (aus Isola et al. (2018)).",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides/intro.html#image-generation---super-resolution",
    "href": "slides/intro.html#image-generation---super-resolution",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Image Generation - Super Resolution",
    "text": "Image Generation - Super Resolution\n\nNvidia dlss: Link",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides/intro.html#image-generation---colorization",
    "href": "slides/intro.html#image-generation---colorization",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Image Generation - Colorization",
    "text": "Image Generation - Colorization\n\nNorwegian Bride (est late 1890s) aus DeOldify: Link",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides/intro.html#many-tasks",
    "href": "slides/intro.html#many-tasks",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Many tasks",
    "text": "Many tasks\n\n\n\nImage Classification\nObject Detection (and Tracking)\nImage Segmentation\n\nSemantic Segmentation\nInstance Segmentation\n\nOptical Character Recognition (OCR)\nPose Estimation\nFacial Recognition\nAction Recognition\n\n\n\nImage Generation\n\nStyle Transfer\nImage Inpainting\nSuper-Resolution\nText-to-Image (and more)\n\nImage Captioning\n3D Reconstruction\nImage Retrieval\n\n\nList is not exhaustive!",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides/intro.html#semantic-gap",
    "href": "slides/intro.html#semantic-gap",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Semantic Gap",
    "text": "Semantic Gap\n\nIllustration des semantic gap.",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides/intro.html#point-of-view",
    "href": "slides/intro.html#point-of-view",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Point of View",
    "text": "Point of View\n\nSource",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides/intro.html#deformation",
    "href": "slides/intro.html#deformation",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Deformation",
    "text": "Deformation\n\nSource",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides/intro.html#lighting",
    "href": "slides/intro.html#lighting",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Lighting",
    "text": "Lighting\n\nSource",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides/intro.html#background",
    "href": "slides/intro.html#background",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Background",
    "text": "Background\n\nSource",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides/intro.html#occlusion",
    "href": "slides/intro.html#occlusion",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Occlusion",
    "text": "Occlusion\n\nSource",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides/intro.html#intraclass-variation",
    "href": "slides/intro.html#intraclass-variation",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Intraclass Variation",
    "text": "Intraclass Variation\n\nSource",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides/intro.html#context-relevance",
    "href": "slides/intro.html#context-relevance",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Context Relevance",
    "text": "Context Relevance\n\n\n\n\n\n\n\nKontext Source",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides/intro.html#machine-learning-approach",
    "href": "slides/intro.html#machine-learning-approach",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Machine Learning Approach",
    "text": "Machine Learning Approach\nWith Machine Learning, we follow a data-driven approach to solve various tasks:\n\nCollect a dataset of images and their labels.\nUse a machine learning algorithm to train a model (e.g., a classifier).\nEvaluate and apply the model to new data.\n\n\ndef train(images, labels):\n    \"\"\" Train a Model \"\"\"\n    # Fit Model here\n    return model\n\ndef predict(test_images, model):\n    \"\"\" Predict \"\"\"\n    predictions = model(test_images)\n    return predictions",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides/intro.html#question",
    "href": "slides/intro.html#question",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Question",
    "text": "Question\nImage Super Resolution\nHow would you train a model for image super resolution?\nThe task of the model would be to scale low-resolution images to high-resolution images with the best possible quality.",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides/intro.html#machine-learning-pipeline",
    "href": "slides/intro.html#machine-learning-pipeline",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "Machine Learning Pipeline",
    "text": "Machine Learning Pipeline",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides/intro.html#pytorch",
    "href": "slides/intro.html#pytorch",
    "title": "Einführung Computer Vision mit Deep Learning",
    "section": "PyTorch",
    "text": "PyTorch\nIn this class, we use PyTorch. PyTorch has gained immense popularity in recent years, characterized by high flexibility, a clean API, and many open-source resources.\nFundamental Concepts:\n\nTensor: N-dimensional array, like numpy.array\nAutograd: Functionality to create computational graphs and compute gradients.\nModule: Class to define components of neural networks\n\nLet’s check it out! (on images)",
    "crumbs": [
      "Slides",
      "Einführung Computer Vision mit Deep Learning"
    ]
  },
  {
    "objectID": "slides/foundation_models.html#what-are-they",
    "href": "slides/foundation_models.html#what-are-they",
    "title": "Foundation Models",
    "section": "What are they?",
    "text": "What are they?\nFoundation models are large-scale machine learning models trained on vast amounts of data that can be fine-tuned for various downstream tasks. These models have demonstrated remarkable capabilities in natural language processing, computer vision, and other fields."
  },
  {
    "objectID": "slides/foundation_models.html#characteristics-of-foundation-models",
    "href": "slides/foundation_models.html#characteristics-of-foundation-models",
    "title": "Foundation Models",
    "section": "Characteristics of Foundation Models",
    "text": "Characteristics of Foundation Models\n\nLarge-scale Pre-training: Foundation models are pre-trained on extensive datasets, enabling them to capture a wide range of knowledge.\nTransfer Learning: These models can be fine-tuned on specific tasks with relatively small datasets, making them versatile and efficient.\nMultimodal Capabilities: Some foundation models can process and integrate multiple types of data, such as text and images."
  },
  {
    "objectID": "slides/foundation_models.html#clip-a-foundation-model-example",
    "href": "slides/foundation_models.html#clip-a-foundation-model-example",
    "title": "Foundation Models",
    "section": "CLIP: A Foundation Model Example",
    "text": "CLIP: A Foundation Model Example\nCLIP (Contrastive Language-Image Pre-training, Radford et al. (2021)) is a foundation model developed by OpenAI. It is designed to understand images and text jointly, making it capable of tasks like zero-shot image classification."
  },
  {
    "objectID": "slides/foundation_models.html#how-clip-works",
    "href": "slides/foundation_models.html#how-clip-works",
    "title": "Foundation Models",
    "section": "How CLIP Works",
    "text": "How CLIP Works\n\nSource: Radford et al. (2021)CLIP is pre-trained on a diverse dataset of images and their corresponding textual descriptions. It learns to associate images with their textual descriptions using a contrastive learning approach, which maximizes the similarity between correct image-text pairs and minimizes the similarity between incorrect pairs."
  },
  {
    "objectID": "slides/foundation_models.html#how-clip-can-be-applied",
    "href": "slides/foundation_models.html#how-clip-can-be-applied",
    "title": "Foundation Models",
    "section": "How CLIP can be applied",
    "text": "How CLIP can be applied\n\nSource: Radford et al. (2021)"
  },
  {
    "objectID": "slides/foundation_models.html#applications-of-clip",
    "href": "slides/foundation_models.html#applications-of-clip",
    "title": "Foundation Models",
    "section": "Applications of CLIP",
    "text": "Applications of CLIP\n\nZero-Shot Classification: CLIP can classify images into categories it has not explicitly been trained on by leveraging its understanding of language.\nImage Search: By inputting a textual description, CLIP can retrieve relevant images from a database.\n(near) Duplicate Detection: CLIP can assist in identifying near duplicate images."
  },
  {
    "objectID": "slides/foundation_models.html#example",
    "href": "slides/foundation_models.html#example",
    "title": "Foundation Models",
    "section": "Example",
    "text": "Example\nHere’s a simple example of using CLIP for zero-shot image classification:\n\nimport requests\nfrom PIL import Image\nfrom transformers import CLIPModel, CLIPProcessor\n\n# 1) Get Models and Data Pre-Processors\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n# 2) Define Input Data: Image and Texts\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprompt_template = \"a photo of a {}\"\nclasses = [\"cat\", \"dog\", \"car\", \"tree\"]\nprompts = [prompt_template.format(class_name) for class_name in classes]\n\n# 3) Pre-Process Inputs\ninputs = processor(\n    text=prompts,\n    images=image,\n    return_tensors=\"pt\",\n    padding=True,\n)\n\n# 4) Forward-Pass\nwith torch.no_grad():\n    outputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n\n# 5) Find most likely label\nprobs = logits_per_image.softmax(dim=1)\nprint(\"Label:\", classes[probs.argmax().item()])"
  },
  {
    "objectID": "slides/foundation_models.html#fine-tuning-clip",
    "href": "slides/foundation_models.html#fine-tuning-clip",
    "title": "Foundation Models",
    "section": "Fine-Tuning CLIP",
    "text": "Fine-Tuning CLIP\nIf CLIP does not perform well with zero-shot learning, it can be fine-tuned with various techniques. This is a powerful mechanism because the features that CLIP has learned are very powerful and might be useful for your target task."
  },
  {
    "objectID": "slides/foundation_models.html#visual-question-anwering",
    "href": "slides/foundation_models.html#visual-question-anwering",
    "title": "Foundation Models",
    "section": "Visual Question Anwering",
    "text": "Visual Question Anwering\nMulti-modal models extend the capabilities of foundation models by integrating and processing multiple types of data simultaneously. One notable example of a multi-modal model is a Visual Question Answering (VQA) system, which can understand and answer questions about images. Figure 1 shows an example.\n\nSource: Li et al. (2023)"
  },
  {
    "objectID": "slides/foundation_models.html#how-vqa-works",
    "href": "slides/foundation_models.html#how-vqa-works",
    "title": "Foundation Models",
    "section": "How VQA Works",
    "text": "How VQA Works\nVQA models combine visual data (images) with textual data (questions) to generate accurate answers. These models are typically pre-trained on large datasets containing images, questions about those images, and the corresponding answers. Figure 2 shows training data examples that were used in the Flamingo model Alayrac et al. (2022).\n\nSource: Alayrac et al. (2022)"
  },
  {
    "objectID": "slides/foundation_models.html#applications-of-vqa",
    "href": "slides/foundation_models.html#applications-of-vqa",
    "title": "Foundation Models",
    "section": "Applications of VQA",
    "text": "Applications of VQA\n\nAccessibility: VQA can help visually impaired users by answering questions about their surroundings based on images captured by a camera.\nChat Bots with Visual Understanding: VQA systems can be used in chat bots to incorporate visual inputs.\nCustomer Support: VQA can enhance customer support by allowing users to submit images and ask questions about products or services."
  },
  {
    "objectID": "slides/foundation_models.html#conclusion",
    "href": "slides/foundation_models.html#conclusion",
    "title": "Foundation Models",
    "section": "Conclusion",
    "text": "Conclusion\nFoundation models like CLIP and multi-modal models such as VQA represent significant advancements in machine learning, offering powerful capabilities across various tasks. Their ability to learn from large datasets and generalize to new tasks makes them valuable tools in the AI landscape."
  },
  {
    "objectID": "pages/quiz.html",
    "href": "pages/quiz.html",
    "title": "Quiz",
    "section": "",
    "text": "The following questions could be exam questions.",
    "crumbs": [
      "Quiz & Exam Prep"
    ]
  },
  {
    "objectID": "pages/quiz.html#convolutions",
    "href": "pages/quiz.html#convolutions",
    "title": "Quiz",
    "section": "Convolutions",
    "text": "Convolutions\n\nA convolutional layer has 64 input activations (\\(C_{in} = 64\\) and \\(H=16\\), \\(W=16\\)). You want to reduce its spatial dimensionality by half, while doubling the number of channels. How do you parameterize your convolutional layer? Provide an example.\nIn the example above: How many weights do you need to learn?\nYou have very large images (\\(8000 \\times 8000\\) pixels). Your model always crashes with out-of-memory-errors. What options do you have when parameterizing your convolutions?",
    "crumbs": [
      "Quiz & Exam Prep"
    ]
  },
  {
    "objectID": "pages/quiz.html#cnns",
    "href": "pages/quiz.html#cnns",
    "title": "Quiz",
    "section": "CNNs",
    "text": "CNNs\n\nCan CNNs be used to count objects? Take a look at the following figure. You want to count in how many quadrants an objects occurs. Justify your answer.\n\n\n\n\nCan CNNs count objects\n\n\n\nCan CNNs be used to model inputs, e.g. satellite data that are not RGB images, i.e. have more than 3 input channels? Justify your answer.\nYou want to model images which are not square. They have a spatial resolution of 800x400. What is different within a CNN as opposed to if they were square?",
    "crumbs": [
      "Quiz & Exam Prep"
    ]
  },
  {
    "objectID": "pages/quiz.html#image-classification",
    "href": "pages/quiz.html#image-classification",
    "title": "Quiz",
    "section": "Image Classification",
    "text": "Image Classification\n\nYou trained a model to identify synthetic (fake) images. The model is quite good but not perfect. When deploying the model you have the option to send some images for manual verificaiton by an expert. Which do you choose? Justify your answer.",
    "crumbs": [
      "Quiz & Exam Prep"
    ]
  },
  {
    "objectID": "pages/quiz.html#foundation-models",
    "href": "pages/quiz.html#foundation-models",
    "title": "Quiz",
    "section": "Foundation Models",
    "text": "Foundation Models\n\nYou applied CLIP on a dataset to identify synthetic / fake images and used the following prompts:\n\n\n“A synthetic image”.\n“A real image”.\n\nWhen you compare to the ground truth labels that you have collected you see that the model does not perform well. What options do you have to improve the model?",
    "crumbs": [
      "Quiz & Exam Prep"
    ]
  },
  {
    "objectID": "pages/literature.html",
    "href": "pages/literature.html",
    "title": "Books",
    "section": "",
    "text": "Stevens, Eli and Antiga, Luca and Viehmann, Thomas, Deep learning with PyTorch, Manning Publications Co, Stevens, Antiga, and Viehmann (2020)\n\nKann als PDF gratis heruntergeladen werden\nEinführung in PyTorch von Grund auf mit Anwendungsbeispielen",
    "crumbs": [
      "Resources",
      "Books"
    ]
  },
  {
    "objectID": "pages/literature.html#pytorch",
    "href": "pages/literature.html#pytorch",
    "title": "Books",
    "section": "",
    "text": "Stevens, Eli and Antiga, Luca and Viehmann, Thomas, Deep learning with PyTorch, Manning Publications Co, Stevens, Antiga, and Viehmann (2020)\n\nKann als PDF gratis heruntergeladen werden\nEinführung in PyTorch von Grund auf mit Anwendungsbeispielen",
    "crumbs": [
      "Resources",
      "Books"
    ]
  },
  {
    "objectID": "pages/literature.html#deep-learning",
    "href": "pages/literature.html#deep-learning",
    "title": "Books",
    "section": "Deep Learning",
    "text": "Deep Learning\nSimon J.D. Prince, Understanding Deep Learning, MIT Press, Prince (2023)\n\nBrandaktuelles Buch über Deep Learning\nUmfassende Einführung ins Thema mit sehr guten Illustrationen\nOnline verfügbar: Link\n\nGoodfellow, Ian and Bengio, Yoshua and Courville, Aaron, Deep Learning, MIT Press, Goodfellow, Bengio, and Courville (2016)\n\nSehr gute und umfassende Einführung in Deep Learning\nEtwas älter aber immer noch in weiten Teilen aktuell\nOnline verfügbar: Link\n\nChollet, François, Deep Learning with Python, Second Edition, Manning Publications, Chollet (2021)\n\nchapters 8-9 are about computer vision\nfree access with FHNW-Emailadresse in O’Reilly online Mediathek\n\nStevens et al, Deep Learning with PyTorch, Manning Publications, Stevens, Antiga, and Viehmann (2020)",
    "crumbs": [
      "Resources",
      "Books"
    ]
  },
  {
    "objectID": "pages/literature.html#machine-learning",
    "href": "pages/literature.html#machine-learning",
    "title": "Books",
    "section": "Machine Learning",
    "text": "Machine Learning\nGéron A, Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition, O’Reilly 2019\n\nJupyter Notebooks sind öffentlich verfügbar: Link\nEinsteigerfreundliche Einführung in Machine Learning mit Scikit-Learn und TensorFlow\n\nRaschka S, Python Machine Learning, 3rd Edition, PACKT 2019\n\nEinsteigerfreundliche Einführung in Machine Learning mit Scikit-Learn und TensorFlow\n\nKevin P. Murphy, Probabilistic Machine Learning: An Introduction, MIT Press 2022\n\nVorabversion gratis verfügbar: Link\nUmfassende Einführung in Machine Learning mit ausführlichen theoretischen Hintergründen\n\nChollet F, Deep Learning with Python, 2nd Edition, MEAP 2020\n\nEin Klassiker für eine Einführung in Deep Learning (und Keras)\n\nHastie T et al., Elements of Statistical Learning, Springer 2009.\n\nKann als pdf gratis runtergeladen werden: Link\nEnthält Machine Learning Grundlagen und viele Methoden (wenig über Neuronale Netzwerke)\n\nVanderPlas J, Python Data Science Handbook, O’Reilly 2017.\n\nWurde mit Jupyter Notebooks geschrieben.\nDer gesamte Inhalt finden sie auf einer website: Link\nDas Repository kann von github runtergelanden werden: Link",
    "crumbs": [
      "Resources",
      "Books"
    ]
  },
  {
    "objectID": "pages/exercises.html",
    "href": "pages/exercises.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\nExercises\nExercises can be found here: Link",
    "crumbs": [
      "Exercises"
    ]
  },
  {
    "objectID": "pages/about.html",
    "href": "pages/about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "lectures/practical.html",
    "href": "lectures/practical.html",
    "title": "6 - Practical",
    "section": "",
    "text": "How to successfully train Deep Learning models? Below is a step-by-step recipe. This recipe is based in part on Andrej Karpathy’s blog post A Recipe for Training Neural Networks.\nKarpathy observed that training neural networks with modern ML frameworks is highly abstracted, but these abstractions are “leaky”, meaning that without a deep understanding of internal mechanisms such as back-propagation and gradient descent, mistakes can occur. Therefore, one is not completely protected from understanding what happens within the abstraction.\n\nyour_data = # plug your awesome dataset here\nmodel = SuperCrossValidator(SuperDuper.fit, your_data, ResNet50, SGDOptimizer)\n\nMoreover, faulty implementations or sub-optimal model training do not lead to an error message. This means training neural networks fails silently. For example, the learning rate might be too high, causing the model to converge too early and achieve sub-optimal performance. Such issues would not be flagged with an error message but would require hyper-parameter tuning or other diagnostic plots.\nFor these reasons, Karpathy developed a recipe to avoid the most significant pitfalls.\n\n\nIn the first step, the data to be modeled should be thoroughly inspected. Particularly for images, one can examine sample data points, their distribution, patterns, and anomalies. Corrupted images, skewed class distributions, duplicates, etc., may be found. Often, just by looking, one can gauge if modeling will work or be challenging, assess label quality, or determine how much downscaling images can tolerate.\n\n\n\n\n\n\nFigure 1: Examples of images from camera traps. They may still be usable despite being distorted by heat.\n\n\n\nFigure 2 shows typical challenging cases in modeling images from camera traps. Small ROIs suggest that the image resolution should not be excessively reduced. Typical resolutions like 224x224 might be insufficient for certain species.\n\n\n\n\n\n\nFigure 2: Examples of images from camera traps. Source: Beery, Van Horn, and Perona (2018).\n\n\n\nFigure 3 displays the distribution of a typical camera trap dataset. While some species are frequent, others are rare, making modeling difficult and requiring special techniques.\n\n\n\n\n\n\nFigure 3: Species distribution in a typical camera trap dataset. “Deer” comprises about 65% of cases.\n\n\n\nLater in the modeling process, examining misclassified samples is crucial to understand where more data labeling is necessary. Figure 4 shows an example of a rare species that the model misclassified.\n\n\n\n\n\n\nFigure 4: An image of a serval. Model confidences are displayed below.\n\n\n\nSometimes, one may also encounter certain limitations, such as images that are ambiguous or belong to multiple classes. Figure 5 shows an example with a herd of animals containing multiple species (Thomson’s and Grant’s Gazelle). Here, consideration of the modeling approach’s suitability is necessary.\n\n\n\n\n\n\nFigure 5: Example of an image from a camera trap with various species.\n\n\n\n\n\n\nAfter becoming familiar with the data, set up an evaluation pipeline. Define a metric to measure model performance and establish train/validation/test splits or cross-validation procedures. Also, implement or use tools for monitoring and logging training.\n\n\n\n\n\n\nFigure 6: The components of a typical machine learning process. Source: Raschka and Mirjalili (2020).\n\n\n\nNext, use a simple model, which is unlikely to be incorrectly implemented, to test the pipeline. Visualize metrics, losses, and predictions, and test different hypotheses to verify pipeline accuracy.\n\n\n\n\n\n\nFigure 7: Weights and Biases experiment tracking.\n\n\n\nSome Tips for This Step:\nEnsure reproducibility by fixing the random seed so that results remain consistent. See PyTorch Reproducibility.\n\nimport torch\ntorch.manual_seed(0)\n\nAvoid unnecessary techniques and complexities, such as data augmentation, which may introduce bugs that are hard to detect.\nUse a human baseline if possible. For certain tasks, labels may be subjective, with annotators potentially disagreeing on how to label a data point. This implies an upper bound on model performance, which can be estimated by having some data points annotated multiple times. Examples of such images are shown in Figure 8 and Figure 9.\n\n\n\n\n\n\nFigure 8: Example of a challenging species to classify (for amateurs). It could be either Hartebeest or Topi.\n\n\n\n\n\n\n\n\n\nFigure 9: A challenging image to classify from a camera trap dataset. In the Snapshot Serengeti dataset, amateur annotators achieved a 96.6% agreement rate with experts.\n\n\n\nTrain an input-independent baseline, setting all pixel values to the same value, such as zero. This helps determine if a simple model is learning anything at all.\nAnother option is to overfit the model on a batch of data. If everything is functioning correctly, one should achieve perfect train metrics/losses.\nIt is also helpful to visualize what goes into the model. This can help detect errors in data augmentation or data preprocessing. This should be done just before executing this line of code:\n\ny_hat = model(x)\n\nObserving predictions from a fixed sample throughout training can provide insight into the dynamics of the process. In object detection, one might display bounding boxes and classification scores. Figure 10 shows an example of a segmentation problem: the input image is on the left, and the model’s segmentation is on the right.\n\n\n\n\n\n\nFigure 10: Example of a segmentation problem: left is the input, right is the output.\n\n\n\n\n\n\nAt this point, you should have a good understanding of the dataset, confidence in the evaluation pipeline, and initial baselines for simple models. Additionally, you should have a sense of how well a human could perform on this task (if relevant). In this step, focus on achieving good performance on the training set, accepting overfitting temporarily. Next, focus on the validation set.\nFor model architecture, follow the principle “Don’t be a hero.” Use existing, well-established architectures, such as ResNet-50 for image classification.\nThe choice of optimizer should also be conservative. The adam optimizer is often effective and less sensitive to hyper-parameters like the learning rate.\n\n\n\nAt this point, you should have achieved good performance on the training set. Now, shift focus to the validation set, as it is critical for estimating expected performance on new data. This often involves a reduction in training performance due to regularization.\nMore Training Data\nThe simplest way to improve performance (and reduce overfitting) is to collect more training data. If possible, this approach is more effective than other methods and helps a model generalize better. The benefit of additional data can be estimated by training several models on different training set sizes and observing performance changes (learning curve).\n\n\n\n\n\n\nFigure 11: Example of a learning curve. X-axis is performance, Y-axis is the number of training samples. Left panel: Gaussian Naive Bayes; right panel: Support Vector Classifier.\n\n\n\nData Augmentation\nAnother approach is data augmentation, generating new data points from existing ones by making random alterations. Data points are typically augmented on-the-fly rather than in advance. ML frameworks like PyTorch provide built-in transformations, and additional packages offer more complex augmentations.\nFor example, AugLy, see Figure 12.\n\n\n\n\n\n\nFigure 12: AugLy\n\n\n\nAnother option is Albumentations, see Figure 13.\n\n\n\n\n\n\nFigure 13: Albumentations\n\n\n\nKornia provides even more complex augmentations, see Figure 14.\n\n\n\n\n\n\nFigure 14: Kornia\n\n\n\nFigure 15 shows an example of data augmentation with color changes, cropping, and rotations.\n\n\n\n\n\n\nFigure 15: Data augmentation example.\n\n\n\nSynthetic data can also be added to the training process, which can be successful. Figure 16 shows an example where 3D models of animals were created using a game engine and placed either in synthetic landscapes or spliced into real camera trap images.\n\n\n\n\n\n\nFigure 16: From Beery et al. (2020). Synthetic and semi-synthetic data.\n\n\n\nEarly Stopping\nEarly stopping involves training a model and evaluating it on a validation set periodically (e.g., after each epoch). Training is halted if no significant improvement is observed after a set number of cycles, preventing overfitting.\n\n\n\n\n\n\nFigure 17: Source: Link\n\n\n\nFrameworks like pytorch-lightning provide Callbacks to implement early stopping.\n\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\n\nclass LitModel(LightningModule):\n    def validation_step(self, batch, batch_idx):\n        loss = ...\n        self.log(\"val_loss\", loss)\n\nmodel = LitModel()\ntrainer = Trainer(callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\")])\ntrainer.fit(model)\n\nWeight Decay\nWeight decay is another common regularization technique. In gradient descent, weights are reduced by a factor \\(\\lambda\\) during each update. The modified update step is:\n[ _{t+1} = _t (1 - ) - J() ]\nwhere \\(t\\) is the iteration, \\(\\theta\\) are the model parameters, and \\(\\eta\\) is the learning rate. This can be applied directly in PyTorch torch.optim.SGD.\nTransfer Learning\nTransfer learning also has a regularizing effect and can reduce model training time. This approach typically involves using a pre-trained model on a large dataset (e.g., ImageNet), removing the last layer, and replacing it with a new one suited to the target task. Various layers can be frozen or fine-tuned.\n\n\n\n\n\n\nFigure 18: Source: Johnson and Fouhey (2021)\n\n\n\nThe following example code (PyTorch-Link) shows how to freeze the parameters of a pre-trained model by setting requires_grad = False.\n\ndef set_parameter_requires_grad(model, feature_extracting):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False\n\n\n\n\nThis step involves systematically evaluating various hyper-parameters and architectures. Techniques include grid-search or random-search, with random-search generally preferred.\nComplete packages are available for this, such as Ray Tune.\nHyper-parameter tuning requires configurable architecture implementations. Below is an example of configurable fully connected layers.\n\nclass Net(nn.Module):\n    def __init__(self, l1=120, l2=84):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, l1)\n        self.fc2 = nn.Linear(l1, l2)\n        self.fc3 = nn.Linear(l2, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nFor domain-specific problems, consult relevant literature for ideas on possible hyper-parameter variations and architectures (e.g., medical imaging or satellite images).\n\n\n\nOnce the best architectures and hyper-parameters are identified, there are additional ways to maximize performance.\nOne is ensembling: training multiple models and combining their predictions often yields slightly better performance than a single model. However, this increases training and inference costs.\nSurprising effects sometimes occur in deep learning, like double descent (Nakkiran et al. (2019)) or grokking (Power et al. (2022)). For instance, using highly complex models (an overfitting risk) can work well, and after a plateau, validation loss might improve if training continues. So it may be worthwhile to train a model longer (without early stopping).\n\n\n\n\n\n\nFigure 19: Source: Nakkiran et al. (2019). The X-axis represents model complexity, and the Y-axis shows test and train error. Test error decreases, increases (overfitting), and then decreases again (double descent).\n\n\n\nAdditional techniques often yield small improvements. For example, Liu et al. (2022) reports that the following techniques improved accuracy by about 2.7%:\n\nLonger training\nSpecialized optimizer (AdamW)\nComplex data augmentation techniques (Mixup, Cutmix, RandAugment)\nRegularization techniques (Stochastic Depth)\nLabel smoothing",
    "crumbs": [
      "Lectures",
      "6 - Practical"
    ]
  },
  {
    "objectID": "lectures/practical.html#a-recipe",
    "href": "lectures/practical.html#a-recipe",
    "title": "6 - Practical",
    "section": "",
    "text": "How to successfully train Deep Learning models? Below is a step-by-step recipe. This recipe is based in part on Andrej Karpathy’s blog post A Recipe for Training Neural Networks.\nKarpathy observed that training neural networks with modern ML frameworks is highly abstracted, but these abstractions are “leaky”, meaning that without a deep understanding of internal mechanisms such as back-propagation and gradient descent, mistakes can occur. Therefore, one is not completely protected from understanding what happens within the abstraction.\n\nyour_data = # plug your awesome dataset here\nmodel = SuperCrossValidator(SuperDuper.fit, your_data, ResNet50, SGDOptimizer)\n\nMoreover, faulty implementations or sub-optimal model training do not lead to an error message. This means training neural networks fails silently. For example, the learning rate might be too high, causing the model to converge too early and achieve sub-optimal performance. Such issues would not be flagged with an error message but would require hyper-parameter tuning or other diagnostic plots.\nFor these reasons, Karpathy developed a recipe to avoid the most significant pitfalls.\n\n\nIn the first step, the data to be modeled should be thoroughly inspected. Particularly for images, one can examine sample data points, their distribution, patterns, and anomalies. Corrupted images, skewed class distributions, duplicates, etc., may be found. Often, just by looking, one can gauge if modeling will work or be challenging, assess label quality, or determine how much downscaling images can tolerate.\n\n\n\n\n\n\nFigure 1: Examples of images from camera traps. They may still be usable despite being distorted by heat.\n\n\n\nFigure 2 shows typical challenging cases in modeling images from camera traps. Small ROIs suggest that the image resolution should not be excessively reduced. Typical resolutions like 224x224 might be insufficient for certain species.\n\n\n\n\n\n\nFigure 2: Examples of images from camera traps. Source: Beery, Van Horn, and Perona (2018).\n\n\n\nFigure 3 displays the distribution of a typical camera trap dataset. While some species are frequent, others are rare, making modeling difficult and requiring special techniques.\n\n\n\n\n\n\nFigure 3: Species distribution in a typical camera trap dataset. “Deer” comprises about 65% of cases.\n\n\n\nLater in the modeling process, examining misclassified samples is crucial to understand where more data labeling is necessary. Figure 4 shows an example of a rare species that the model misclassified.\n\n\n\n\n\n\nFigure 4: An image of a serval. Model confidences are displayed below.\n\n\n\nSometimes, one may also encounter certain limitations, such as images that are ambiguous or belong to multiple classes. Figure 5 shows an example with a herd of animals containing multiple species (Thomson’s and Grant’s Gazelle). Here, consideration of the modeling approach’s suitability is necessary.\n\n\n\n\n\n\nFigure 5: Example of an image from a camera trap with various species.\n\n\n\n\n\n\nAfter becoming familiar with the data, set up an evaluation pipeline. Define a metric to measure model performance and establish train/validation/test splits or cross-validation procedures. Also, implement or use tools for monitoring and logging training.\n\n\n\n\n\n\nFigure 6: The components of a typical machine learning process. Source: Raschka and Mirjalili (2020).\n\n\n\nNext, use a simple model, which is unlikely to be incorrectly implemented, to test the pipeline. Visualize metrics, losses, and predictions, and test different hypotheses to verify pipeline accuracy.\n\n\n\n\n\n\nFigure 7: Weights and Biases experiment tracking.\n\n\n\nSome Tips for This Step:\nEnsure reproducibility by fixing the random seed so that results remain consistent. See PyTorch Reproducibility.\n\nimport torch\ntorch.manual_seed(0)\n\nAvoid unnecessary techniques and complexities, such as data augmentation, which may introduce bugs that are hard to detect.\nUse a human baseline if possible. For certain tasks, labels may be subjective, with annotators potentially disagreeing on how to label a data point. This implies an upper bound on model performance, which can be estimated by having some data points annotated multiple times. Examples of such images are shown in Figure 8 and Figure 9.\n\n\n\n\n\n\nFigure 8: Example of a challenging species to classify (for amateurs). It could be either Hartebeest or Topi.\n\n\n\n\n\n\n\n\n\nFigure 9: A challenging image to classify from a camera trap dataset. In the Snapshot Serengeti dataset, amateur annotators achieved a 96.6% agreement rate with experts.\n\n\n\nTrain an input-independent baseline, setting all pixel values to the same value, such as zero. This helps determine if a simple model is learning anything at all.\nAnother option is to overfit the model on a batch of data. If everything is functioning correctly, one should achieve perfect train metrics/losses.\nIt is also helpful to visualize what goes into the model. This can help detect errors in data augmentation or data preprocessing. This should be done just before executing this line of code:\n\ny_hat = model(x)\n\nObserving predictions from a fixed sample throughout training can provide insight into the dynamics of the process. In object detection, one might display bounding boxes and classification scores. Figure 10 shows an example of a segmentation problem: the input image is on the left, and the model’s segmentation is on the right.\n\n\n\n\n\n\nFigure 10: Example of a segmentation problem: left is the input, right is the output.\n\n\n\n\n\n\nAt this point, you should have a good understanding of the dataset, confidence in the evaluation pipeline, and initial baselines for simple models. Additionally, you should have a sense of how well a human could perform on this task (if relevant). In this step, focus on achieving good performance on the training set, accepting overfitting temporarily. Next, focus on the validation set.\nFor model architecture, follow the principle “Don’t be a hero.” Use existing, well-established architectures, such as ResNet-50 for image classification.\nThe choice of optimizer should also be conservative. The adam optimizer is often effective and less sensitive to hyper-parameters like the learning rate.\n\n\n\nAt this point, you should have achieved good performance on the training set. Now, shift focus to the validation set, as it is critical for estimating expected performance on new data. This often involves a reduction in training performance due to regularization.\nMore Training Data\nThe simplest way to improve performance (and reduce overfitting) is to collect more training data. If possible, this approach is more effective than other methods and helps a model generalize better. The benefit of additional data can be estimated by training several models on different training set sizes and observing performance changes (learning curve).\n\n\n\n\n\n\nFigure 11: Example of a learning curve. X-axis is performance, Y-axis is the number of training samples. Left panel: Gaussian Naive Bayes; right panel: Support Vector Classifier.\n\n\n\nData Augmentation\nAnother approach is data augmentation, generating new data points from existing ones by making random alterations. Data points are typically augmented on-the-fly rather than in advance. ML frameworks like PyTorch provide built-in transformations, and additional packages offer more complex augmentations.\nFor example, AugLy, see Figure 12.\n\n\n\n\n\n\nFigure 12: AugLy\n\n\n\nAnother option is Albumentations, see Figure 13.\n\n\n\n\n\n\nFigure 13: Albumentations\n\n\n\nKornia provides even more complex augmentations, see Figure 14.\n\n\n\n\n\n\nFigure 14: Kornia\n\n\n\nFigure 15 shows an example of data augmentation with color changes, cropping, and rotations.\n\n\n\n\n\n\nFigure 15: Data augmentation example.\n\n\n\nSynthetic data can also be added to the training process, which can be successful. Figure 16 shows an example where 3D models of animals were created using a game engine and placed either in synthetic landscapes or spliced into real camera trap images.\n\n\n\n\n\n\nFigure 16: From Beery et al. (2020). Synthetic and semi-synthetic data.\n\n\n\nEarly Stopping\nEarly stopping involves training a model and evaluating it on a validation set periodically (e.g., after each epoch). Training is halted if no significant improvement is observed after a set number of cycles, preventing overfitting.\n\n\n\n\n\n\nFigure 17: Source: Link\n\n\n\nFrameworks like pytorch-lightning provide Callbacks to implement early stopping.\n\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\n\nclass LitModel(LightningModule):\n    def validation_step(self, batch, batch_idx):\n        loss = ...\n        self.log(\"val_loss\", loss)\n\nmodel = LitModel()\ntrainer = Trainer(callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\")])\ntrainer.fit(model)\n\nWeight Decay\nWeight decay is another common regularization technique. In gradient descent, weights are reduced by a factor \\(\\lambda\\) during each update. The modified update step is:\n[ _{t+1} = _t (1 - ) - J() ]\nwhere \\(t\\) is the iteration, \\(\\theta\\) are the model parameters, and \\(\\eta\\) is the learning rate. This can be applied directly in PyTorch torch.optim.SGD.\nTransfer Learning\nTransfer learning also has a regularizing effect and can reduce model training time. This approach typically involves using a pre-trained model on a large dataset (e.g., ImageNet), removing the last layer, and replacing it with a new one suited to the target task. Various layers can be frozen or fine-tuned.\n\n\n\n\n\n\nFigure 18: Source: Johnson and Fouhey (2021)\n\n\n\nThe following example code (PyTorch-Link) shows how to freeze the parameters of a pre-trained model by setting requires_grad = False.\n\ndef set_parameter_requires_grad(model, feature_extracting):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False\n\n\n\n\nThis step involves systematically evaluating various hyper-parameters and architectures. Techniques include grid-search or random-search, with random-search generally preferred.\nComplete packages are available for this, such as Ray Tune.\nHyper-parameter tuning requires configurable architecture implementations. Below is an example of configurable fully connected layers.\n\nclass Net(nn.Module):\n    def __init__(self, l1=120, l2=84):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, l1)\n        self.fc2 = nn.Linear(l1, l2)\n        self.fc3 = nn.Linear(l2, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nFor domain-specific problems, consult relevant literature for ideas on possible hyper-parameter variations and architectures (e.g., medical imaging or satellite images).\n\n\n\nOnce the best architectures and hyper-parameters are identified, there are additional ways to maximize performance.\nOne is ensembling: training multiple models and combining their predictions often yields slightly better performance than a single model. However, this increases training and inference costs.\nSurprising effects sometimes occur in deep learning, like double descent (Nakkiran et al. (2019)) or grokking (Power et al. (2022)). For instance, using highly complex models (an overfitting risk) can work well, and after a plateau, validation loss might improve if training continues. So it may be worthwhile to train a model longer (without early stopping).\n\n\n\n\n\n\nFigure 19: Source: Nakkiran et al. (2019). The X-axis represents model complexity, and the Y-axis shows test and train error. Test error decreases, increases (overfitting), and then decreases again (double descent).\n\n\n\nAdditional techniques often yield small improvements. For example, Liu et al. (2022) reports that the following techniques improved accuracy by about 2.7%:\n\nLonger training\nSpecialized optimizer (AdamW)\nComplex data augmentation techniques (Mixup, Cutmix, RandAugment)\nRegularization techniques (Stochastic Depth)\nLabel smoothing",
    "crumbs": [
      "Lectures",
      "6 - Practical"
    ]
  },
  {
    "objectID": "lectures/practical.html#model-zoos-and-frameworks",
    "href": "lectures/practical.html#model-zoos-and-frameworks",
    "title": "6 - Practical",
    "section": "Model Zoos and Frameworks",
    "text": "Model Zoos and Frameworks\n\nHugging-Face\nHuggingFace provides a wide range of models, datasets, and APIs. It offers easy-to-use Python APIs to integrate, use, and adapt models for PyTorch.\n\n\ntimm\nPyTorch Image Models (timm) is a library with SOTA models, layers, and functions for using, reproducing, and adapting modern image models.",
    "crumbs": [
      "Lectures",
      "6 - Practical"
    ]
  },
  {
    "objectID": "lectures/practical.html#code-setup-configuration",
    "href": "lectures/practical.html#code-setup-configuration",
    "title": "6 - Practical",
    "section": "Code Setup / Configuration",
    "text": "Code Setup / Configuration\nSetting up a well-organized repository structure can significantly aid in model development. Key aspects include reproducibility, configurability, tracking, and logging experiments.\nSuggested repository structures:\n\nDS-cookie cutter: DrivenData\nPyTorch-Lightning example: Lightning Template\nConfigurations: Hydra\nExperiment tracking: Weights & Biases, Neptune AI\nVersion Control for ML projects: DVC",
    "crumbs": [
      "Lectures",
      "6 - Practical"
    ]
  },
  {
    "objectID": "lectures/practical.html#references",
    "href": "lectures/practical.html#references",
    "title": "6 - Practical",
    "section": "References",
    "text": "References\n\n\nBeery, Sara, Yang Liu, Dan Morris, Jim Piavis, Ashish Kapoor, Markus Meister, Neel Joshi, and Pietro Perona. 2020. “Synthetic Examples Improve Generalization for Rare Classes.” Proceedings - 2020 IEEE Winter Conference on Applications of Computer Vision, WACV 2020, 852–62. https://doi.org/10.1109/WACV45572.2020.9093570.\n\n\nBeery, Sara, Grant Van Horn, and Pietro Perona. 2018. “Recognition in Terra Incognita.” In Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 11220 LNCS:472–89. https://doi.org/10.1007/978-3-030-01270-0_28.\n\n\nJohnson, Justin, and David Fouhey. 2021. “EECS 442: Computer Vision.” Lecture {Notes} / {Slides}. https://web.eecs.umich.edu/~justincj/teaching/eecs442/WI2021/.\n\n\nLiu, Zhuang, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. 2022. “A ConvNet for the 2020s.” arXiv. http://arxiv.org/abs/2201.03545.\n\n\nNakkiran, Preetum, Boaz Barak, Gal Kaplun, Ilya Sutskever, Yamini Bansal, and Tristan Yang. 2019. “Deep Double Descent: Where Bigger Models and More Data Hurt.” arXiv, 1–24.\n\n\nPower, Alethea, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. 2022. “Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets.” arXiv. http://arxiv.org/abs/2201.02177.\n\n\nRaschka, Sebastian, and Vahid Mirjalili. 2020. Python Machine Learning: Machine Learning and Deep Learning with Python, Scikit-Learn, and TensorFlow. Second edition, fourth release,[fully revised and updated]. Expert Insight. Birmingham Mumbai: Packt Publishing.",
    "crumbs": [
      "Lectures",
      "6 - Practical"
    ]
  },
  {
    "objectID": "lectures/intro.html",
    "href": "lectures/intro.html",
    "title": "1 - Introduction",
    "section": "",
    "text": "Deep Learning models have played a transformative role in Computer Vision over the last decade, significantly enhancing and expanding the capabilities to process visual data.\nThe integration of Deep Learning in Computer Vision has massively improved the accuracy and efficiency of visual recognition, classification, and analysis. This has opened up new possibilities in applications such as automated driving, facial recognition, and medical image analysis. These models are now embedded in a wide variety of products and services.\nBiologists might need to analyse images from camera traps for species identification, see Figure 1.\n\n\n\n\n\n\nSource: Breitenmoser-Würsten et al. (2024)\n\n\n\n\nFigure 1: Image from a camera trap.\n\n\n\nRecent advances in generative deep learning enables anybody to create realistic looking synthetic images (see Figure 2). This can be used for many applications, positive and negative. Such images might be used to create misinformation which is one reason researchers are trying to build models to detect them.\n\n\n\n\n\n\n\n\nSource\n\n\n\n\n\n\n\nSource\n\n\n\n\n\n\nFigure 2: Examples of synthetic images.\n\n\n\n\nFigure 3 shows an application where one can take a photo of an object of interest and let a model identify it, as well as search additional information about it. Photos can also be used for translation tasks whereby models detect letters (optical character recognition) and then translate it (using language models).\n\n\n\n\n\n\n\n\nIdentification & Search\n\n\n\n\n\n\n\nTranslation\n\n\n\n\n\n\nFigure 3\n\n\n\nGoogle Lens\nOne area with high commercial interest and high competition is the development of self-driving capabilities. Figure 4 shows a demonstration of the capabilities required towards self-driving, such as understanding a scene and identifying any objects in it.\n\n\n\n\n\n\nFigure 4: Example from Waymo.\n\n\n\nFace recognition rechnology, such as Apple’s Face ID (Figure 5), has become ubiquitious. In this case depth information is derived from infrared light.\n\n\n\n\n\n\nFigure 5: Example from Apple Face ID\n\n\n\nComputer vision has also been very useful in agricultur. Figure 6 shows an application where apples can be counted from image data. Furthermore, drones might be used to estimate crop yield, type and health.\n\n\n\n\n\n\nExample from Häni, Roy, and Isler (2020)\n\n\n\n\nFigure 6\n\n\n\nDeep learning has also been applied extensively in the medical area. In particular, many imaging technologies, such as MRI or CT, can be analysed with computer vision techniques. Figure 7 illustrates a generic segmentation model which can be used to identify organs and other anatomical structures on pixel-level.\n\n\n\n\n\n\nExample from Ma et al. (2024).\n\n\n\n\nFigure 7\n\n\n\nPhoto-editing has been brought to new levels of sophistication with AI-based capabilities. Figure 8 shows an impressive example from the Google Magic Editor. Maybe even a bit too good?\n\n\n\n\n\n\nFigure 8: Example from Google Magic Editor\n\n\n\n\n\n\n\n\n\nQuestion\nWhat steps do you think the model in Figure 8 performs?\n\n\n\nThe increasingly better models and the ability to run them quickly and resource-efficiently on mobile devices have enabled such applications. Figure 9 shows that special chips have been developed to process images with Deep Learning models quickly.\n\n\n\n\n\n\nFigure 9: From Link.",
    "crumbs": [
      "Lectures",
      "1 - Introduction"
    ]
  },
  {
    "objectID": "lectures/intro.html#motivation",
    "href": "lectures/intro.html#motivation",
    "title": "1 - Introduction",
    "section": "",
    "text": "Deep Learning models have played a transformative role in Computer Vision over the last decade, significantly enhancing and expanding the capabilities to process visual data.\nThe integration of Deep Learning in Computer Vision has massively improved the accuracy and efficiency of visual recognition, classification, and analysis. This has opened up new possibilities in applications such as automated driving, facial recognition, and medical image analysis. These models are now embedded in a wide variety of products and services.\nBiologists might need to analyse images from camera traps for species identification, see Figure 1.\n\n\n\n\n\n\nSource: Breitenmoser-Würsten et al. (2024)\n\n\n\n\nFigure 1: Image from a camera trap.\n\n\n\nRecent advances in generative deep learning enables anybody to create realistic looking synthetic images (see Figure 2). This can be used for many applications, positive and negative. Such images might be used to create misinformation which is one reason researchers are trying to build models to detect them.\n\n\n\n\n\n\n\n\nSource\n\n\n\n\n\n\n\nSource\n\n\n\n\n\n\nFigure 2: Examples of synthetic images.\n\n\n\n\nFigure 3 shows an application where one can take a photo of an object of interest and let a model identify it, as well as search additional information about it. Photos can also be used for translation tasks whereby models detect letters (optical character recognition) and then translate it (using language models).\n\n\n\n\n\n\n\n\nIdentification & Search\n\n\n\n\n\n\n\nTranslation\n\n\n\n\n\n\nFigure 3\n\n\n\nGoogle Lens\nOne area with high commercial interest and high competition is the development of self-driving capabilities. Figure 4 shows a demonstration of the capabilities required towards self-driving, such as understanding a scene and identifying any objects in it.\n\n\n\n\n\n\nFigure 4: Example from Waymo.\n\n\n\nFace recognition rechnology, such as Apple’s Face ID (Figure 5), has become ubiquitious. In this case depth information is derived from infrared light.\n\n\n\n\n\n\nFigure 5: Example from Apple Face ID\n\n\n\nComputer vision has also been very useful in agricultur. Figure 6 shows an application where apples can be counted from image data. Furthermore, drones might be used to estimate crop yield, type and health.\n\n\n\n\n\n\nExample from Häni, Roy, and Isler (2020)\n\n\n\n\nFigure 6\n\n\n\nDeep learning has also been applied extensively in the medical area. In particular, many imaging technologies, such as MRI or CT, can be analysed with computer vision techniques. Figure 7 illustrates a generic segmentation model which can be used to identify organs and other anatomical structures on pixel-level.\n\n\n\n\n\n\nExample from Ma et al. (2024).\n\n\n\n\nFigure 7\n\n\n\nPhoto-editing has been brought to new levels of sophistication with AI-based capabilities. Figure 8 shows an impressive example from the Google Magic Editor. Maybe even a bit too good?\n\n\n\n\n\n\nFigure 8: Example from Google Magic Editor\n\n\n\n\n\n\n\n\n\nQuestion\nWhat steps do you think the model in Figure 8 performs?\n\n\n\nThe increasingly better models and the ability to run them quickly and resource-efficiently on mobile devices have enabled such applications. Figure 9 shows that special chips have been developed to process images with Deep Learning models quickly.\n\n\n\n\n\n\nFigure 9: From Link.",
    "crumbs": [
      "Lectures",
      "1 - Introduction"
    ]
  },
  {
    "objectID": "lectures/intro.html#computer-vision-tasks",
    "href": "lectures/intro.html#computer-vision-tasks",
    "title": "1 - Introduction",
    "section": "Computer Vision Tasks",
    "text": "Computer Vision Tasks\n\nImage Classification\nIn image classification, an image is assigned to a predefined set of classes. In multi-class classification, there are \\(\\gt 2\\) classes; in binary classification, there are 2 classes; and in multi-label classification, there are multiple sets of classes. Figure 10 shows an example from the paper by Krizhevsky, Sutskever, and Hinton (2012) (multi-class classification), which achieved the best results in the ImageNet competition in 2012 and demonstrated the effectiveness of CNNs.\n\n\n\n\n\n\nFigure 10: Multi-Class Image Classification Beispiel (aus Krizhevsky, Sutskever, and Hinton (2012)).\n\n\n\n\n\nObject Detection\nObject detection involves locating and recognizing (multiple) objects in an image. Figure 11 shows an example from the paper by Redmon et al. (2016). Each object is localized with a bounding box and assigned an object class.\n\n\n\n\n\n\nFigure 11: Object Detection example (from Redmon et al. (2016)). Bounding boxes localize the objects, indicating the most likely class and confidence for each object.\n\n\n\n\n\nSegmentation\nIn segmentation, individual pixels are assigned to specific objects (instance segmentation) or classes (semantic segmentation). Figure 12 shows an example of object segmentation (instance segmentation) from the paper by He et al. (2018), where individual objects are detected and precisely localized (segmented) at the pixel level.\n\n\n\n\n\n\nFigure 12: Object Segmentation example (from He et al. (2018)).\n\n\n\nThe following video shows an example of semantic segmentation:\n\n\n\n\nKeypoint Detection\nIn keypoint detection, keypoints of people are localized. People must be detected and their keypoints (joints) localized. Figure 13 shows an example of keypoint detection from the paper by He et al. (2018), where\nindividual people are detected and their joints localized. This can be used to transfer movements from a person to an avatar or to recognize activities of people (action recognition).\n\n\n\n\n\n\nFigure 13: Keypoint Detection example (from He et al. (2018)).\n\n\n\n\n\nImage Generation\nThere are various applications where models transform input images into specific output images (image-to-image or image translation) or generate completely new images (image generation). Below some example applications.\n\nImage Manipulation\n\n\n\n\n\n\nFigure 14: Source: Link, DragGAN by Pan et al. (2023)\n\n\n\n\n\nImage Translation\nFigure 15 shows an example of image generation from the paper by Isola et al. (2018), where images are generated conditioned on image inputs (translated). This can be used, for example, to convert a sketch of an object into a photorealistic depiction of it.\n\n\n\n\n\n\nFigure 15: Image Generation example (from Isola et al. (2018)).\n\n\n\n\n\nImage Super Resolution\nIn the gaming industry, Deep Learning is used to generate high-resolution images, scaling low-resolution images efficiently (image super resolution), as shown in Figure 16. This allows for higher frame rates.\n\n\n\n\n\n\nFigure 16: Nvidia DLSS: Link\n\n\n\n\n\nImage Colorization\nImage colorization can be learned with Deep Learning. This involves transforming a black-and-white image into an RGB image. Figure 17 shows an example.\n\n\n\n\n\n\nFigure 17: Norwegian Bride (est late 1890s) from DeOldify: Link\n\n\n\n\n\nView Synthesis\nIn view synthesis, views of certain scenes are generated from models. Neural Radiance Fields (NeRFs) are simple models that can generate new views from known viewpoints and their images. Figure 18 shows the data on which such a model is trained and what can be generated with it.\n\n\n\n\n\n\nFigure 18: Neural Radiance Fields - example (from Mildenhall et al. (2020)).\n\n\n\n\n\nUnconditional Generation\nIn unconditional image generation, data (images) are generated that resemble those in the training data. Here, you have no direct control over the model’s output. However, you can often make changes to generated images or interpolate between data points. Figure 19 shows generated images from a model trained on portrait images of people.\n\n\n\n\n\n\nFigure 19: StyleGan3 (from Karras et al. (2021)).\n\n\n\n\n\nText to Image\nFigure 20 shows an example of image generation from the paper by Rombach et al. (2022), where images are generated conditioned on text inputs.\n\n\n\n\n\n\nFigure 20: Image Generation example (from Rombach et al. (2022).)\n\n\n\nOn civitai, there are numerous examples and models to admire or download.\n\n\n\nFurther tasks\nThere are many more computer vision tasks. The following list is not exhaustive:\n\n\n\nImage Classification\nObject Detection (and Tracking)\nImage Segmentation\n\nSemantic Segmentation\nInstance Segmentation\n\nOptical Character Recognition (OCR)\nPose Estimation\nFacial Recognition\nAction Recognition\n\n\n\nImage Generation\n\nStyle Transfer\nImage Inpainting\nSuper-Resolution\nText-to-Image (and more)\n\nImage Captioning\n3D Reconstruction\nImage Retrieval",
    "crumbs": [
      "Lectures",
      "1 - Introduction"
    ]
  },
  {
    "objectID": "lectures/intro.html#challenges",
    "href": "lectures/intro.html#challenges",
    "title": "1 - Introduction",
    "section": "Challenges",
    "text": "Challenges\nWe will now explore some challenges that must be overcome when analyzing images with machine learning models.\n\nSemantic Gap\nThe semantic gap refers to the discrepancy between low-level information that can be extracted from an image and the interpretation of an image by a viewer. Simply put: an image often consists of millions of pixels whose information must be condensed to ultimately derive semantically meaningful information. This is an extremely complex task.\n\n\n\n\n\n\nFigure 21: Illustration of the semantic gap.\n\n\n\n\n\nViewpoint\nThe meaning of the image does not change with the viewpoint, but the pixels do.\n\n\n\n\n\n\nFigure 22: Source\n\n\n\n\n\nDeformation\nObjects are often flexible and appear in different shapes and poses.\n\n\n\n\n\n\nFigure 23: Source\n\n\n\n\n\nIllumination\nChanges in illumination affect pixel values and the visibility of objects.\n\n\n\n\n\n\nFigure 24: Source\n\n\n\n\n\nBackground\nBackground pixels can resemble objects and make their exact delineation or visibility more difficult.\n\n\n\n\n\n\nFigure 25: Source\n\n\n\n\n\nOcclusion\nObjects are not always fully visible, which can make their detection more difficult.\n\n\n\n\n\n\nFigure 26: Source\n\n\n\n\n\nIntra-Class Variation\nObjects of the same class can exhibit large intra-class variability.\n\n\n\n\n\n\nFigure 27: Source\n\n\n\n\n\nContext Dependence\nFigure 28 shows that context information can be important to correctly classify an object.\n\n\n\n\n\n\nFigure 28: Context Source",
    "crumbs": [
      "Lectures",
      "1 - Introduction"
    ]
  },
  {
    "objectID": "lectures/intro.html#software",
    "href": "lectures/intro.html#software",
    "title": "1 - Introduction",
    "section": "Software",
    "text": "Software\n\nPipelines\nVarious models can also be combined into pipelines. One example is Grounded-Segment-Anything, shown in Figure 29. Object detection models that process text queries are used to detect objects. These detections are used by a segmentation model to segment the target object. This segmentation is then used in a text-to-image model to make the desired change at the correct location.\n\n\n\n\n\n\nFigure 29: Example from Grounded-Segment-Anything Link\n\n\n\n\n\nImage Analysis Libraries\nThere are numerous open-source libraries that provide pre-trained models to handle the tasks mentioned above. Therefore, you do not always have to train a model yourself. Figure 30 shows the capabilities of Detectron 2, an object detection library from Facebook, which can also be used for other tasks like segmentation.\n\n\n\n\n\n\nFigure 30: Example from Facebook’s Detectron Library Link\n\n\n\nHugging Face is also well-known. It offers numerous models and datasets for various computer vision questions like object detection, segmentation, and classification.",
    "crumbs": [
      "Lectures",
      "1 - Introduction"
    ]
  },
  {
    "objectID": "lectures/intro.html#machine-learning",
    "href": "lectures/intro.html#machine-learning",
    "title": "1 - Introduction",
    "section": "Machine Learning",
    "text": "Machine Learning\nWe follow a data-driven approach in machine learning to solve various tasks. Typically, the process involves:\n\nCollecting a dataset of images and their labels.\nUsing a machine learning algorithm to train a model that learns to associate images with labels.\nEvaluating/applying the model on new data.\n\n\ndef train(images, labels):\n \"\"\" Train a Model \"\"\"\n # Fit Model here\n return model\n\ndef predict(test_images, model):\n \"\"\" Predict \"\"\"\n predictions = model(test_images)\n return predictions\n\n\n\n\n\n\n\nQuestion\nHow would you train a model for super resolution? The task of the model would be to scale low-resolution images to high-resolution images with the best possible quality.\n\n\n\n\nMachine Learning Process\nWhen modeling data, one often follows certain process steps: acquiring data, preparing it, training multiple models, selecting the most suitable model, estimating its future performance, and finally deploying it in production. Figure 31 illustrates this process graphically.\n\n\n\n\n\n\nFigure 31: Machine Learning Pipeline (Source: Raschka and Mirjalili (2020))\n\n\n\nAt the core of a machine learning application is typically a mathematical model, which is fitted to a dataset so that it can then be used for prediction (in supervised learning). We often refer to ‘models’, meaning the mathematical description of the dataset.\n\n\nModels\nA model is typically described as a function of a data point, generating an output \\(\\hat{y}\\):\n\\[\\begin{align*}\nf(\\mathbf{x}^{(i)}) = \\hat{y}^{(i)}\n\\end{align*}\\]\nMost models have parameters or coefficients that describe the model. The entirety of all parameters is denoted by \\(\\theta\\).\n\\[\\begin{align*}\nf_{\\theta}(\\mathbf{x}^{(i)}) \\text{ or } f(\\theta, \\mathbf{x}^{(i)})\n\\end{align*}\\]\nFor simplicity, we often omit \\(\\theta\\): \\(f(\\mathbf{x}^{(i)})\\)\n\n\nOptimization\nThe coefficients are fitted to a training dataset through an optimization procedure.\nThe optimization procedure can often be influenced by additional factors, called hyperparameters (\\(\\alpha, \\lambda, \\dots\\)). These cannot be directly optimized.\nThe function/quantity to be optimized is usually called the cost function, i.e., cost function (other terms include objective function, loss function, etc.). We use \\(J(\\cdot)\\) to denote the cost function. Often, the cost function is also referred to as the loss function \\(L(\\cdot)\\). We use \\(l(\\cdot)\\) for the per-sample loss, i.e., the computation of the cost function on a single sample.\nOur goal is to find a model (and its parameters) that minimizes the cost function:\n\\[\\begin{equation*}\n\\mathsf{argmin}_{\\theta, \\lambda} J\\Big(f_{\\theta, \\lambda}(\\mathbf{X}), \\mathbf{y}\\Big)\n\\end{equation*}\\]\nUsually, preprocessing of variables precedes the learning of\nthe coefficients. Forms of preprocessing include standardizing, normalizing, feature encoding, dimensionality reduction, and more. This preprocessing also affects the optimization procedure and can be considered hyperparameters.\n\n\nModel Selection\nModel selection is one of the most important and complex components of the machine learning process. This step involves comparing multiple models and selecting the “best” model for the task to be modeled. Which model is the “best” must be defined based on a metric that measures the model’s performance.\nIf we calculate the value of the metric on the training dataset, our model is usually too optimistic about its general performance. This is because the data points in the training dataset were directly used to optimize the cost function, and the model coefficients are thus optimally adjusted to them. New data points, for which predictions are to be made, could not have been used for optimization. Therefore, a dataset is usually divided into a training set and a test set. The model is trained with the training set and its performance is measured on the test set. When comparing many models, it is advisable to compare them on a separate validation set (see Figure 32) and evaluate only the best model on the test set. This makes the estimate on the test set more accurate.\n\n\n\n\n\n\nFigure 32: Train-Test Split to select and evaluate models.\n\n\n\n\n\nML with Classical Computer Vision\nImages typically have very high dimensionality. For example, an RGB image with a resolution of \\(800 \\times 600\\) has a dimensionality of \\(800 \\times 600 \\times 3 = 1,440,000\\). Classical machine learning algorithms often struggle with such high dimensionalities:\n\nThey are very slow or require a lot of memory.\nThey cannot exploit the 2-D structure of images.\nThey are very sensitive to slight changes in images (e.g., rotations).\nThey can easily overfit, as the number of features is close to the number of observations (training set).\n\nWhen modeling images with (classical) machine learning algorithms, methods from (classical) computer vision are often used. With the help of such methods, features can be extracted from images, and the algorithms can learn on these features. This avoids modeling high-dimensional raw data. Figure 33, Figure 34, and Figure 35 show various feature extraction methods.\nFigure 33 shows that, for example, the distribution over the color spectrum can be extracted from an image. This could be an important feature.\n\n\n\n\n\n\nFigure 33: Color Histograms as Features (Source: Johnson (2022))\n\n\n\nFigure 34 shows that techniques like Histogram of Oriented Gradients (HOG) Dalal and Triggs (2005) can be used to extract structures from images. Such features were successfully used for pedestrian detection Dalal and Triggs (2005).\n\n\n\n\n\n\nFigure 34: HOG as Features (Source: Johnson (2022))\n\n\n\nFigure 35 shows another feature variant. Visual patches can be extracted from a dataset, clustered, and then used as descriptors.\n\n\n\n\n\n\nFigure 35: Bag of (visual) words Features (Source: Johnson (2022))\n\n\n\nFinally, all features can be combined, often more is better, as shown in Figure 36.\n\n\n\n\n\n\nFigure 36: Image Features (Source: Johnson (2022))\n\n\n\nDepending on the parameterization, HOG descriptors, for example, can be very large for images. The resulting feature vector from Figure 36 can still be reduced in dimensionality, e.g., with Principal Component Analysis.\nFigure 37 shows the CIFAR10 dataset. A well-known dataset for testing models, consisting of 10 classes with 6,000 images each.\n\n\n\n\n\n\nFigure 37: CIFAR10 Dataset Source",
    "crumbs": [
      "Lectures",
      "1 - Introduction"
    ]
  },
  {
    "objectID": "lectures/intro.html#deep-learning",
    "href": "lectures/intro.html#deep-learning",
    "title": "1 - Introduction",
    "section": "Deep Learning",
    "text": "Deep Learning\nTo accelerate research in computer vision and make progress more measurable, the PASCAL VOC Challenges were introduced Everingham et al. (2007). These involved various tasks, such as detecting objects in photographs (Figure 38).\n\n\n\n\n\n\nFigure 38: Images/illustrations from Link and Johnson (2022). On the left, you see object annotations in images, and on the right, you see the development of Mean Average Precision over the years.\n\n\n\nThe easy availability of images on the internet has made it possible to collect increasingly larger datasets. ImageNet is such a very large, hierarchically annotated image dataset Deng et al. (2009) with over 1.4 million images, categorized into 1,000 object classes. Figure 39 illustrates the dataset.\n\n\n\n\n\n\nFigure 39: ImageNet, Image Source, details in Deng et al. (2009)\n\n\n\nSince 2010, challenges have been regularly conducted on the ImageNet dataset Russakovsky et al. (2015), such as image classification and object detection. Figure 40 shows the development of the error rate over time.\n\n\n\n\n\n\nFigure 40: Source: Johnson (2022)\n\n\n\nIn 2011, a team won Perronnin et al. (2010) by combining various (classical) feature extraction methods with machine learning. They used, among other things, SIFT features to train SVMs.\nIn 2012, a drastic reduction in the error rate was achieved in the ImageNet competition. This development marked the end of classical computer vision methods in many areas. Krizhevsky et al. Krizhevsky, Sutskever, and Hinton (2012) impressively demonstrated the potential of neural networks in 2012. They implemented a convolutional neural network (CNN) with multiple layers, the so-called AlexNet architecture, as shown in Figure 41.\n\n\n\n\n\n\nFigure 41: Alexnet Krizhevsky, Sutskever, and Hinton (2012).\n\n\n\nWhile classical computer vision trains a machine learning model on features extracted with hand-crafted algorithms Figure 42, the development is increasingly moving towards end-to-end learning. In this approach, one avoids as much as possible static/hand-designed components and learns everything, including feature extraction, with machine learning Figure 43.\n\n\n\n\n\n\nFigure 42: Illustration from Johnson (2022)\n\n\n\n\n\n\n\n\n\nFigure 43: Illustration from Johnson (2022)\n\n\n\nDeep learning-based approaches have several advantages over classical machine learning methods:\n\nAutomatic feature extraction: no manual feature extraction procedures are needed.\nHierarchical features: these are particularly valuable for processing and understanding visual data.\nGeneralization: with more training data, deep learning methods generalize better.\nEnd-to-end learning: this approach allows many problems to be modeled similarly.\nRobustness to variability: certain models are naturally invariant to\n\ntransformations like translations, scalings, etc. - Adaptability and transferability: deep learning models can often be easily adapted (transfer learning) and can create good models even with little data.\n\nDeep Learning History\nWe will now explore the most important milestones in deep learning for image analysis.\n\n\nHubel and Wiesel\nHubel and Wiesel (1959) showed in experiments on cats that there are complex cells/neurons in the visual cortex that only respond to certain patterns. In addition to these complex cells, there are also simple cells that only respond to lines at a certain angle.\n\n\n\n\n\n\nFigure 44: Illustration Source\n\n\n\n\n\nNeocognitron\nFukushima (1980) defined a model of a neural network that can recognize visual patterns. It already has the hierarchical structure of a modern convolutional neural network and is inspired by biological neural networks, particularly from the insights of Hubel and Wiesel (1959).\n\n\n\n\n\n\nFigure 45: The Neocognitron Fukushima (1980).\n\n\n\n\n\nBackpropagation\nRumelhart, Hinton, and Williams (1986) introduced the backpropagation algorithm in the context of neural networks. This algorithm is used to train modern neural networks: it finds the parameters of an artificial neural network (ANN) to solve a specific task. Backpropagation is based on the chain rule from calculus and is also important for other machine learning models.\n\n\n\n\n\n\nFigure 46: Backpropagation in neural networks Rumelhart, Hinton, and Williams (1986).\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nA good video on backpropagation: 3Blue1Brown Backpropagation Calculus\n\n\n\n\nConvolutional Neural Networks (CNNs)\nLecun et al. (1998) implemented convolutional neural networks (CNNs) to recognize handwritten digits. It is specialized for the 2-D structure of the input data. They trained a model very similar to modern CNNs, as shown in Figure 47.\n\n\n\n\n\n\nFigure 47: Modern CNN Lecun et al. (1998).\n\n\n\nCNNs became extremely popular after winning the ImageNet competition. Krizhevsky, Sutskever, and Hinton (2012) implemented a CNN with multiple layers, the so-called AlexNet architecture, as shown in Figure 48.\n\n\n\n\n\n\nFigure 48: Alexnet Krizhevsky, Sutskever, and Hinton (2012).\n\n\n\nSince the breakthrough in 2012, CNNs have been used for increasingly complex tasks and further developed. Well-known are, for example, the COCO Challenges, with various tasks.\n\n\nOther Architectures in Computer Vision\nCNNs are still in use today (2024). Meanwhile, there are alternative architectures, such as transformer-based models (Dosovitskiy et al. (2020)), which are extremely successful in language modeling, or multilayer perceptron-based architectures (Liu et al. (2021)). However, it has been shown that CNNs are still competitive and sometimes superior to alternative architectures (see Woo et al. (2023)). The question of which architecture type will prevail is open. Currently, it seems that CNNs and transformer-based models perform similarily (Smith et al. (2023)).",
    "crumbs": [
      "Lectures",
      "1 - Introduction"
    ]
  },
  {
    "objectID": "lectures/intro.html#references",
    "href": "lectures/intro.html#references",
    "title": "1 - Introduction",
    "section": "References",
    "text": "References\n\n\nBreitenmoser-Würsten, Christine, Fridolin Zimmermann, Kristina Vogt, Manuela von Arx, Sven Signe, and Christian Stauffer. 2024. “Das Projekt LUNO - Abschlussbericht.” 121. Stiftung KORA. https://portal.issn.org/#.\n\n\nDalal, N., and B. Triggs. 2005. “Histograms of Oriented Gradients for Human Detection.” In 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05), 1:886–93. San Diego, CA, USA: IEEE. https://doi.org/10.1109/CVPR.2005.177.\n\n\nDeng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. “ImageNet: A Large-Scale Hierarchical Image Database.” In 2009 IEEE Conference on Computer Vision and Pattern Recognition, 248–55. Miami, FL: IEEE. https://doi.org/10.1109/CVPR.2009.5206848.\n\n\nDosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2020. “An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.” arXiv:2010.11929 [Cs], October. http://arxiv.org/abs/2010.11929.\n\n\nEveringham, M., L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. 2007. “The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results.” http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html.\n\n\nFukushima, Kunihiko. 1980. “Neocognitron: A Self-Organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position.” Biological Cybernetics 36 (4): 193–202. https://doi.org/10.1007/BF00344251.\n\n\nHäni, Nicolai, Pravakar Roy, and Volkan Isler. 2020. “MinneApple: A Benchmark Dataset for Apple Detection and Segmentation.” IEEE Robotics and Automation Letters 5 (2): 852–58. https://doi.org/10.1109/LRA.2020.2965061.\n\n\nHe, Kaiming, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. 2018. “Mask R-CNN.” arXiv. http://arxiv.org/abs/1703.06870.\n\n\nHubel, D. H., and T. N. Wiesel. 1959. “Receptive Fields of Single Neurones in the Cat’s Striate Cortex.” The Journal of Physiology 148 (3): 574–91. https://doi.org/10.1113/jphysiol.1959.sp006308.\n\n\nIsola, Phillip, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. 2018. “Image-to-Image Translation with Conditional Adversarial Networks.” arXiv. http://arxiv.org/abs/1611.07004.\n\n\nJohnson, Justin. 2022. “EECS 498.008 / 598.008 Deep Learning for Computer Vision.” Lecture {Notes} / {Slides}. https://web.eecs.umich.edu/~justincj/teaching/eecs498/WI2022/.\n\n\nKarras, Tero, Miika Aittala, Samuli Laine, Erik Härkönen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. 2021. “Alias-Free Generative Adversarial Networks.” arXiv. http://arxiv.org/abs/2106.12423.\n\n\nKrizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. “ImageNet Classification with Deep Convolutional Neural Networks.” In Advances in Neural Information Processing Systems, edited by F. Pereira, C. J. Burges, L. Bottou, and K. Q. Weinberger. Vol. 25. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf.\n\n\nLecun, Y., L. Bottou, Y. Bengio, and P. Haffner. 1998. “Gradient-Based Learning Applied to Document Recognition.” Proceedings of the IEEE 86 (11): 2278–2324. https://doi.org/10.1109/5.726791.\n\n\nLiu, Hanxiao, Zihang Dai, David R. So, and Quoc V. Le. 2021. “Pay Attention to MLPs.” arXiv:2105.08050 [Cs], June. http://arxiv.org/abs/2105.08050.\n\n\nMa, Jun, Yuting He, Feifei Li, Lin Han, Chenyu You, and Bo Wang. 2024. “Segment Anything in Medical Images.” Nature Communications 15 (1): 654. https://doi.org/10.1038/s41467-024-44824-z.\n\n\nMildenhall, Ben, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. 2020. “NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis.” arXiv. http://arxiv.org/abs/2003.08934.\n\n\nPan, Xingang, Ayush Tewari, Thomas Leimkühler, Lingjie Liu, Abhimitra Meka, and Christian Theobalt. 2023. “Drag Your GAN: Interactive Point-Based Manipulation on the Generative Image Manifold.” arXiv. http://arxiv.org/abs/2305.10973.\n\n\nPerronnin, Florent, Yan Liu, Jorge Sanchez, and Herve Poirier. 2010. “Large-Scale Image Retrieval with Compressed Fisher Vectors.” In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 3384–91. San Francisco, CA, USA: IEEE. https://doi.org/10.1109/CVPR.2010.5540009.\n\n\nRaschka, Sebastian, and Vahid Mirjalili. 2020. Python Machine Learning: Machine Learning and Deep Learning with Python, Scikit-Learn, and TensorFlow. Second edition, fourth release,[fully revised and updated]. Expert Insight. Birmingham Mumbai: Packt Publishing.\n\n\nRedmon, Joseph, Santosh Divvala, Ross Girshick, and Ali Farhadi. 2016. “You Only Look Once: Unified, Real-Time Object Detection.” Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition 2016-Decem: 779–88. https://doi.org/10.1109/CVPR.2016.91.\n\n\nRombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. “High-Resolution Image Synthesis with Latent Diffusion Models.” arXiv. http://arxiv.org/abs/2112.10752.\n\n\nRumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. 1986. “Learning Representations by Back-Propagating Errors.” Nature 323 (6088): 533–36. https://doi.org/10.1038/323533a0.\n\n\nRussakovsky, Olga, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, et al. 2015. “ImageNet Large Scale Visual Recognition Challenge.” arXiv. http://arxiv.org/abs/1409.0575.\n\n\nSmith, Samuel L., Andrew Brock, Leonard Berrada, and Soham De. 2023. “ConvNets Match Vision Transformers at Scale.” arXiv. http://arxiv.org/abs/2310.16764.\n\n\nWoo, Sanghyun, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, and Saining Xie. 2023. “ConvNeXt V2: Co-Designing and Scaling ConvNets with Masked Autoencoders.” arXiv. http://arxiv.org/abs/2301.00808.",
    "crumbs": [
      "Lectures",
      "1 - Introduction"
    ]
  },
  {
    "objectID": "lectures/cnns.html",
    "href": "lectures/cnns.html",
    "title": "4 - Convolutional Neural Networks",
    "section": "",
    "text": "Images have the following relevant properties when modelling them:\n\nHigh-Dimensional: An RGB image of size \\(224 \\times 224\\) (height, width) has = \\(150'528\\) values.\nLocality: Nearby pixels are statistically related\nStability under transformations: Interpretation of an image does not change under many geomoetric transformations.\n\n\n\n\n\n\n\nFigure 1: Image Source\n\n\n\nWe will see that CNNs are much better suited to model images than MLPs. Unlike MLPs, CNNs explicitly assume that inputs (e.g., pixels) that are close together need to be considered together and that information is locally correlated. This allows certain properties to be embedded in the architecture of CNNs (inductive biases) to define models much more efficiently (with fewer parameters).\n\n\nThe input to an MLP is a vector \\(\\mathbf{x}^{(i)}\\), which is transformed through multiple hidden layers to the output layer. Each hidden layer has a certain number of neurons, each connected to all neurons in the previous layer (fully-connected layers). The fully connected layers can only process 1-D vectors. Therefore, images \\(\\in \\mathbb{R}^{H \\times W \\times C}\\) must be flattened into 1-D vectors \\(\\in \\mathbb{R}^p\\). Here, \\(p= H \\times W \\times C\\). This causes MLPs to become very large (having many learnable parameters) when applied to high-dimensional inputs such as images. See Figure 2 for an illustration.\n\n\n\n\n\n\nFigure 2: Illustration of an MLP applied to an image.\n\n\n\nIn the CIFAR-10 dataset, which consists of very small images of 32x32x3 (height, width, colors), a single neuron in the first hidden layer has 32 * 32 * 3 = 3,072 weights to learn (see Figure 3).\n\n\n\n\n\n\nFigure 3: Source: Johnson (2019)\n\n\n\n\nFor larger images, which are often encountered in practice, the number of weights is correspondingly much larger. Many neurons are also used, further increasing the number of parameters, leading to overfitting, and making learning the weights more difficult.\nThe dimensionality of weight matrix \\(\\mathbf{W}\\) scales with input size. \\(\\mathbf{W} \\in \\mathbb{R}^{d \\times k}\\), while \\(d\\) the dimensionality of the inputs, and \\(k\\) the number of neurons in the first hidden layer. The columns of the weight matrix \\(\\mathbf{W}\\) (in the first hidden layer) can be visualized to learn what kind pattern a specific neuron has learned. See Figure 4.\n\n\n\n\n\n\nFigure 4: Illustration of weight matrix \\(\\mathbf{W}\\) and how the weights of a single neuron can be reshaped into the input image dimensionality.\n\n\n\nOften the patterns we want to learn are not stationary. Ideally we want to recognize them at all positions, particularly under translations of the inputs, e.g. \\(g(\\mathbf{x})\\) where \\(g()\\) is a spatial translation. Figure Figure 6 illustrated the problem.\n\n\n\n\n\n\nFigure 5: Illustration of different inputs with the same pattern but shifted spatially.\n\n\n\n\n\n\n\n\n\nQuestion\nHow do MLPs deal with such pattern shifts?\n\n\n\n\n\n\n\n\n\nFigure 6: Inputs transformed with a permuation matrix.\n\n\n\n\n\n\n\n\n\nQuestion\nCan an MLP successfully learn patterns in images \\(\\mathbf{x}\\) that are permuted with a permutation matrix \\(\\mathbf{P}\\), i.e. \\(f(P(\\mathbf{x}))\\)?\n\n\n\nTo summarize MLPs on images:\n\nHigh-Dimensional: The size of an MLP scales with the input dimensionality which might blow beyond memory and compute budgets.\nLocality: MLPs have no notion of locality and thus can’t exploit this inherent bias in natural images.\nStability under transformations: MLPs need to learn position-dependent patterns which is very inefficient.\n\n\n\n\n\n\n\nDemonstration\nhttps://adamharley.com/nn_vis/mlp/2d.html\n\n\n\nA single neuron in a CNN is only connected to a small portion (local connectivity) of the image (see Figure 7). As a result, the neurons have far fewer parameters than in an MLP. The 2-D structure of the image is also preserved, meaning they do not need to be flattened as in an MLP. This exploits the property of images that certain features, such as edges and corners, are relevant throughout the image. By convolving the neurons across the entire input, the same feature can be detected by a neuron throughout the image. In an MLP, a specific feature would need to be relearned at each position.\n\n\n\n\n\n\nFigure 7: Source: Johnson (2019)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCNNs are used not only for image data but also for data with spatial dependencies/local structures. This includes not only images but also time series, videos, audio, and text. The key is that signals that are spatially close together should be interpreted together.\n\n\nWhat properties do we want?\n\n\n\nFor many tasks small variations in the input should either not change the model output (invariance) or should change the output in tandem with the input changes (equivariance).\nA function \\(f(\\mathbf{x})\\) is invariant to a transformation \\(g(\\mathbf{x})\\) if:\n\\[\\begin{align}\nf(g(\\mathbf{x})) = f(\\mathbf{x})\n\\end{align}\\]\nA function \\(f(\\mathbf{x})\\) (such as a layer in a neural network) of an image \\(\\mathbf{x}\\) is equivariant with respect to a transformation \\(g(\\mathbf{\\mathbf{x}})\\) if:\n\\[\\begin{align}\nf(g(\\mathbf{x})) = g(f(\\mathbf{x}))\n\\end{align}\\]\nExample where invariance is required:\n\n\n\n\n\n\nFigure 8: When objects in the input translate spatially, the output (in this case the classification of the images) does not change. The model \\(f(\\mathbf{x})\\) is thus invariant to spatial translations.\n\n\n\nExample where equivariance is required:\n\n\n\n\n\n\nFigure 9: When objects in the input (first row) translate spatially, the detections (bounding boxes) change accordingly (bottom row). The model \\(f(\\mathbf{x})\\) that produces the bounding boxes is thus equivariant with respect to spatial translations.",
    "crumbs": [
      "Lectures",
      "4 - Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "lectures/cnns.html#introduction-motivation",
    "href": "lectures/cnns.html#introduction-motivation",
    "title": "4 - Convolutional Neural Networks",
    "section": "",
    "text": "Images have the following relevant properties when modelling them:\n\nHigh-Dimensional: An RGB image of size \\(224 \\times 224\\) (height, width) has = \\(150'528\\) values.\nLocality: Nearby pixels are statistically related\nStability under transformations: Interpretation of an image does not change under many geomoetric transformations.\n\n\n\n\n\n\n\nFigure 1: Image Source\n\n\n\nWe will see that CNNs are much better suited to model images than MLPs. Unlike MLPs, CNNs explicitly assume that inputs (e.g., pixels) that are close together need to be considered together and that information is locally correlated. This allows certain properties to be embedded in the architecture of CNNs (inductive biases) to define models much more efficiently (with fewer parameters).\n\n\nThe input to an MLP is a vector \\(\\mathbf{x}^{(i)}\\), which is transformed through multiple hidden layers to the output layer. Each hidden layer has a certain number of neurons, each connected to all neurons in the previous layer (fully-connected layers). The fully connected layers can only process 1-D vectors. Therefore, images \\(\\in \\mathbb{R}^{H \\times W \\times C}\\) must be flattened into 1-D vectors \\(\\in \\mathbb{R}^p\\). Here, \\(p= H \\times W \\times C\\). This causes MLPs to become very large (having many learnable parameters) when applied to high-dimensional inputs such as images. See Figure 2 for an illustration.\n\n\n\n\n\n\nFigure 2: Illustration of an MLP applied to an image.\n\n\n\nIn the CIFAR-10 dataset, which consists of very small images of 32x32x3 (height, width, colors), a single neuron in the first hidden layer has 32 * 32 * 3 = 3,072 weights to learn (see Figure 3).\n\n\n\n\n\n\nFigure 3: Source: Johnson (2019)\n\n\n\n\nFor larger images, which are often encountered in practice, the number of weights is correspondingly much larger. Many neurons are also used, further increasing the number of parameters, leading to overfitting, and making learning the weights more difficult.\nThe dimensionality of weight matrix \\(\\mathbf{W}\\) scales with input size. \\(\\mathbf{W} \\in \\mathbb{R}^{d \\times k}\\), while \\(d\\) the dimensionality of the inputs, and \\(k\\) the number of neurons in the first hidden layer. The columns of the weight matrix \\(\\mathbf{W}\\) (in the first hidden layer) can be visualized to learn what kind pattern a specific neuron has learned. See Figure 4.\n\n\n\n\n\n\nFigure 4: Illustration of weight matrix \\(\\mathbf{W}\\) and how the weights of a single neuron can be reshaped into the input image dimensionality.\n\n\n\nOften the patterns we want to learn are not stationary. Ideally we want to recognize them at all positions, particularly under translations of the inputs, e.g. \\(g(\\mathbf{x})\\) where \\(g()\\) is a spatial translation. Figure Figure 6 illustrated the problem.\n\n\n\n\n\n\nFigure 5: Illustration of different inputs with the same pattern but shifted spatially.\n\n\n\n\n\n\n\n\n\nQuestion\nHow do MLPs deal with such pattern shifts?\n\n\n\n\n\n\n\n\n\nFigure 6: Inputs transformed with a permuation matrix.\n\n\n\n\n\n\n\n\n\nQuestion\nCan an MLP successfully learn patterns in images \\(\\mathbf{x}\\) that are permuted with a permutation matrix \\(\\mathbf{P}\\), i.e. \\(f(P(\\mathbf{x}))\\)?\n\n\n\nTo summarize MLPs on images:\n\nHigh-Dimensional: The size of an MLP scales with the input dimensionality which might blow beyond memory and compute budgets.\nLocality: MLPs have no notion of locality and thus can’t exploit this inherent bias in natural images.\nStability under transformations: MLPs need to learn position-dependent patterns which is very inefficient.\n\n\n\n\n\n\n\nDemonstration\nhttps://adamharley.com/nn_vis/mlp/2d.html\n\n\n\nA single neuron in a CNN is only connected to a small portion (local connectivity) of the image (see Figure 7). As a result, the neurons have far fewer parameters than in an MLP. The 2-D structure of the image is also preserved, meaning they do not need to be flattened as in an MLP. This exploits the property of images that certain features, such as edges and corners, are relevant throughout the image. By convolving the neurons across the entire input, the same feature can be detected by a neuron throughout the image. In an MLP, a specific feature would need to be relearned at each position.\n\n\n\n\n\n\nFigure 7: Source: Johnson (2019)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCNNs are used not only for image data but also for data with spatial dependencies/local structures. This includes not only images but also time series, videos, audio, and text. The key is that signals that are spatially close together should be interpreted together.\n\n\nWhat properties do we want?\n\n\n\nFor many tasks small variations in the input should either not change the model output (invariance) or should change the output in tandem with the input changes (equivariance).\nA function \\(f(\\mathbf{x})\\) is invariant to a transformation \\(g(\\mathbf{x})\\) if:\n\\[\\begin{align}\nf(g(\\mathbf{x})) = f(\\mathbf{x})\n\\end{align}\\]\nA function \\(f(\\mathbf{x})\\) (such as a layer in a neural network) of an image \\(\\mathbf{x}\\) is equivariant with respect to a transformation \\(g(\\mathbf{\\mathbf{x}})\\) if:\n\\[\\begin{align}\nf(g(\\mathbf{x})) = g(f(\\mathbf{x}))\n\\end{align}\\]\nExample where invariance is required:\n\n\n\n\n\n\nFigure 8: When objects in the input translate spatially, the output (in this case the classification of the images) does not change. The model \\(f(\\mathbf{x})\\) is thus invariant to spatial translations.\n\n\n\nExample where equivariance is required:\n\n\n\n\n\n\nFigure 9: When objects in the input (first row) translate spatially, the detections (bounding boxes) change accordingly (bottom row). The model \\(f(\\mathbf{x})\\) that produces the bounding boxes is thus equivariant with respect to spatial translations.",
    "crumbs": [
      "Lectures",
      "4 - Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "lectures/cnns.html#convolutional-layers",
    "href": "lectures/cnns.html#convolutional-layers",
    "title": "4 - Convolutional Neural Networks",
    "section": "Convolutional Layers",
    "text": "Convolutional Layers\n\nConvolutional Neural Networks\nCNNs consist of a sequence of different layers. Each layer transforms activations from the previous layer into new activations through a differentiable operation. Below we look at the main layer types: convolutional layers, pooling layers, activation layers, and fully connected layers. Arranged in a specific sequence, this is referred to as the architecture of the model.\nFigure 10 shows an example architecture. The activation maps of the various layers are shown, representing the corresponding outputs of the layers.\n\n\n\n\n\n\nFigure 10: The activations of a ConvNet architecture are shown. The input image is on the left and the predictions on the right. Source: Li (2022).\n\n\n\nSometimes different layers are combined and referred to as a block. For example, the combination of a convolutional layer followed by an activation layer and a pooling layer is often used. This would be a CONV-ACT-POOL block.\n\n\nConvolution Operation\nConvolutional layers are the main layers in CNNs responsible for extracting visual features. The weights of a convolutional layer consist of a set of learnable filters. Each filter is typically small along the spatial dimensions (height, width) relative to the input but extends over the entire input depth. A typical filter in the first layer, for example, has the dimension \\(7 \\times 7 \\times 3\\) (7 pixels along height/width and 3 along the color channels). During the forward pass, the filters are convolved along height/width over the input. At each position, the dot product (when considering the input and filter as 1-D vectors) between the filter and input is calculated. This produces a 2-D activation map representing the filter’s expression at each position in the input. Intuitively, the CNN learns filters corresponding to typical visual patterns, such as edges and colors. A set of \\(K\\) filters produces activation maps with a depth of \\(K\\).\n\n\n\n\n\n\nNote\n\n\n\nFilter and kernel are sometimes used synonymously. Here, we differentiate by considering a filter as 3-dimensional (CxHxW) and a kernel as 2-dimensional (HxW). A filter consists of C kernels.\n\n\n\n\n\n\n\n\nNote\n\n\n\nConvolution in deep learning is typically implemented as cross-correlation.\n\\[\\begin{equation}\nS(i, j) = (K * I)(i, j) =\\sum_m\\sum_n I(i + m, j + n)K(m, n)\n\\end{equation}\\]\n\\(I\\) is the input (for example an image), \\(K\\) is the kernel (typically smaller than \\(I\\)) and \\(b\\) is a bias term which is being added to the weighted sum.\nIf \\(I\\) is an RGB image (in the first layer of a CNN for example), the kernel \\(K\\) would have dimensionality \\(3 \\times K \\times K\\) (assuming a square kernel). More generally we learn kernels of the dimensionality \\(C_{in} \\times K \\times K\\).\nMultiple kernels, let’s say \\(C_o\\) kernels, can be grouped together: \\(C_o \\times C_{in} \\times K \\times K\\).\nWe often refer to such tensors as filters or filter banks.\n\n\nConvolution on RGB images is nicely illusrated in Figure 11. We can see that the filter is multiplied element-wise with the corresponding values in the image, the result is summed top produce one single value in the output.\n\n\n\n\n\n\nSource: Prince (2023)\n\n\n\n\nFigure 11\n\n\n\nThe following figures show further step-by-step illustration of applying convolutions on input activations that produce multiple output activation maps.\n\n\n\n\n\n\nFigure 12: Source: Johnson (2019)\n\n\n\n\n\n\n\n\n\nFigure 13: Source: Johnson (2019)\n\n\n\n\n\n\n\n\n\nFigure 14: Source: Johnson (2019)\n\n\n\n\n\n\n\n\n\nFigure 15: Source: Johnson (2019)\n\n\n\nThe data is processed in mini-batches, i.e., multiple images at once, as shown in Figure 16.\n\n\n\n\n\n\nFigure 16: Source: Johnson (2019)\n\n\n\n\n\nPadding, Stride, Kernel Size, Dilation\nTo define a convolution, various hyperparameters need to be set. Some of the most important ones are:\n\nPadding: How much padding is added to the input?\nStride: What is the step size (stride) of the kernel?\nKernel-Size: What is the kernel size?\nDilation: What is the dilation rate?\n\nStride determines how the filters are convolved over the input activations, essentially the step size. If the stride is 1, the filter moves one pixel at a time to compute the next activation (see Figure 17). If the stride is greater, e.g., 2, it moves two pixels at a time, making the activation maps smaller in width and height (see Figure 18).\n\n\n\n\n\n\nFigure 17\n\n\n\n\n\n\n\n\n\nFigure 18\n\n\n\nPadding refers to adding (typically) zeros to the border of the input activations before performing the convolution. This can be useful to ensure, for example, that the spatial dimensions of the activation maps are identical to those of the input activations. This is essential for segmentation tasks. Figure 19 illustrates how padding helps maintain the spatial resolution when padding the input activations.\n\n\n\n\n\n\nFigure 19: Left: Input (Yellow) with Zero-Padding (white border), Middle: Filter, Right: Output.\n\n\n\nKernel-size determines the spatial extend from which a value in the output activation map migh aggregate values. It is the receptive field of an output value with respect to the input values. Figure 20 and Figure 21 show convolutions with different kernel sizes. Note that the amount of padding increases with kernel-size. Kernels usually have an odd number of pixels in height and width to make sure they have a center pixel. Also, the number of parameters increases with kernel-size \\((C_{in} \\times k \\times k) + C_{out}\\), which is often an important consideration.\n\n\n\n\n\n\nFigure 20: Convolution with zero-padding and \\(3 \\times 3\\) kernel that produces activations maps with unchanged spatial resolution.\n\n\n\n\n\n\n\n\n\nFigure 21: Convolution with zero-padding and \\(5 \\times 5\\) kernel that produces activations maps with unchanged spatial resolution.\n\n\n\n\n\n\n\n\n\nQuestion\nWhat is the difference between using 3x3 vs 5x5 kernels? What is the same?\n\n\n\nDilation specifies how much the kernel is spread out and thus directly influences the receptive field. Figure 22 shows how it works.\n\n\n\n\n\n\nFigure 22: Illustration of how a kernel with dilation=2 (middle) is applied to the input activations (left) to produce activations (right). Note the reduced spatial dimensionality of the output activations.\n\n\n\n\n\n\n\n\n\nQuestion\nWhy would we use dilation \\(d \\gt 1\\)?\n\n\n\n\n\n\n\n\n\nDumoulin and Visin (2016) has created some animations for better understanding of convolutions and published them here: https://github.com/vdumoulin/conv_arithmetic.\n\n\n\n\n\n\nFigure 23: Convolving a 3x3 kernel over a 5x5 input without padding and with stride 1x1. Source Dumoulin and Visin (2016)\n\n\n\n\n\n\n\n\nCalculations: Spatial Dimensionality of Activation Maps\nYou can calculate the dimensionality of the activation maps with the following formulas:\n\n\\(i\\): Side length of the input activations (assumption: square inputs)\n\\(k\\): Kernel size (assumption: square kernel)\n\\(o\\): Side length of the output activation maps\n\\(s\\): Stride (assumption: same stride along spatial dimensions)\n\\(p\\): Number of paddings on each side (assumption: same number of paddings along spatial dimensions)\n\\(d\\): Dilation rate.\n\n\\[\\begin{equation}\no = \\left\\lfloor \\frac{i + 2p - k-(k-1)(d-1)}{s} \\right\\rfloor + 1\n\\end{equation}\\]\nNote: If dilation is 1 \\((k-1)(d-1)\\) equals 0.\n\n\nCalculations: Number of Weights in a Convolutional Layer\nYou can calculate the total number of weights in a convolutional layer with the following formula:\n\n\\(k\\): Kernel size (assumption: square kernel, so kernel has dimensions \\(k \\times k\\))\n\\(C_{in}\\): Number of input channels\n\\(C_{out}\\): Number of output channels\n\n\\[\\begin{equation}\n\\text{Total Weights} = (k \\times k) \\times C_{in} \\times C_{out}\n\\end{equation}\\]\nEach filter has a size of \\(k \\times k \\times C_{in}\\), and there are \\(C_{out}\\) filters in total, resulting in \\((k \\times k \\times C_{in}) \\times C_{out}\\) weights.\nBiases: If each output channel has a bias term, add an additional \\(C_{out}\\) weights for the biases.\n\\[\\begin{equation}\n\\text{Total Parameters} = (k \\times k \\times C_{in} \\times C_{out}) + C_{out}\n\\end{equation}\\]\n\n\n\n\n\n\nQuestion\nScenario:\n\nInput activations: \\(3 \\times 32 \\times 32\\) (C, H, W)\nConvolution: 10 filters with: \\(k=5  \\times 5\\), \\(s=1\\), \\(p=2\\), \\(d=1\\)\n\nQuestions:\n\nWhat is the size of the activation map \\(C_{out}, H_{out}, W_{out}\\) ?\nHow many weights / parameters defines this operation?",
    "crumbs": [
      "Lectures",
      "4 - Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "lectures/cnns.html#properties",
    "href": "lectures/cnns.html#properties",
    "title": "4 - Convolutional Neural Networks",
    "section": "Properties",
    "text": "Properties\n\nLocal (Sparse) Connectivity & Parameter Sharing\nFully connected layers are, as discussed, impractical when working with high-dimensional inputs like images. If all neurons in a layer were connected to all previous neurons, the number of parameters to be estimated would increase massively, which is inefficient and leads to overfitting. Each neuron is therefore only connected to a local region of the input volume. The spatial extent of this region is a hyperparameter and is called the receptive field of a neuron (also kernel size) on the input volume. The connections along the depth (C) extend over the entire depth of the input volume. The connections are therefore local along the spatial dimensions (width and height) but complete along the depth.\nParameter sharing in convolutional layers is used to reduce the number of parameters. Since the filters are convolved over the inputs, the individual weights of the filters are identical over the spatial extent of the input volume. One of the main assumptions behind CNNs is the following: If it is useful to learn a specific (visual) feature at a certain position, then it is probably useful at other positions as well. In other words: If I learn filters that detect edges, corners, or cats, then it is a reasonable assumption that I want to do this throughout the image.\n\n\n\n\n\n\nNote\n\n\n\nSometimes parameter sharing does not make sense. This can be the case, for example, if we have centered structures in the images. Then you might want to learn position-dependent features. An example is images of faces that have been centered, where you might want to learn filters that detect the mouth only in the lower middle area (locally connected layers).\n\n\nThe following output shows the number of parameters in an MLP and a CNN (each with two hidden layers) on the CIFAR10 dataset.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchinfo\n\nclass MLP(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.hidden_layer1 = nn.Linear(3 * 32 * 32, 64)\n        self.hidden_layer2 = nn.Linear(64, 32)\n        self.output_layer = nn.Linear(32, 10)\n\n    def forward(self, x):\n        x = self.flatten(x)\n        x = torch.relu(self.hidden_layer1(x))\n        x = torch.relu(self.hidden_layer2(x))\n        x = self.output_layer(x)\n        return x\n\nnet = MLP()\nprint(torchinfo.summary(net, input_size=(1, 3, 32, 32)))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nMLP                                      [1, 10]                   --\n├─Flatten: 1-1                           [1, 3072]                 --\n├─Linear: 1-2                            [1, 64]                   196,672\n├─Linear: 1-3                            [1, 32]                   2,080\n├─Linear: 1-4                            [1, 10]                   330\n==========================================================================================\nTotal params: 199,082\nTrainable params: 199,082\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 0.20\n==========================================================================================\nInput size (MB): 0.01\nForward/backward pass size (MB): 0.00\nParams size (MB): 0.80\nEstimated Total Size (MB): 0.81\n==========================================================================================\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchinfo\n\nclass CNN(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 16, 7, stride=2, padding=3)\n        self.conv2 = nn.Conv2d(16, 16, 3, stride=2, padding=1)\n        self.flatten = nn.Flatten()\n        self.output_layer = nn.Linear(16 * 8 * 8 , 10)\n\n    def forward(self, x):\n        x = torch.relu(self.conv1(x))\n        x = torch.relu(self.conv2(x))\n        x = self.flatten(x)\n        x = self.output_layer(x)\n        return x\n\ncnn = CNN()\nprint(torchinfo.summary(cnn, input_size=(1, 3, 32, 32)))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nCNN                                      [1, 10]                   --\n├─Conv2d: 1-1                            [1, 16, 16, 16]           2,368\n├─Conv2d: 1-2                            [1, 16, 8, 8]             2,320\n├─Flatten: 1-3                           [1, 1024]                 --\n├─Linear: 1-4                            [1, 10]                   10,250\n==========================================================================================\nTotal params: 14,938\nTrainable params: 14,938\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 0.76\n==========================================================================================\nInput size (MB): 0.01\nForward/backward pass size (MB): 0.04\nParams size (MB): 0.06\nEstimated Total Size (MB): 0.11\n==========================================================================================\n\n\n\n\n\n\n\n\nFigure 24: Input in 2-D (top left), the flattened version of it (bottom left), expected output (right), and unknown transformation (middle).\n\n\n\n\n\n\n\n\n\nQuestion\nHow should the linear transformation be defined to obtain the desired result? How many parameters are needed? How could this be done with a convolution?\n\n\n\n\n\nTranslation Invariance / Equivariance\nTranslation invariant is a function that produces the same value under translations \\(g()\\) of the input \\(x\\):\n\\[\\begin{equation}\nf(g(x))=f(x)\n\\end{equation}\\]\nTranslation equivariant is a function that produces the same value under translations \\(g()\\) of the input \\(x\\), provided that it is also shifted by \\(g()\\):\n\\[\\begin{equation}\nf(g(x))=g(f(x))\n\\end{equation}\\]\nConvolutions are translation equivariant, as illustrated well in the following example:\n\n\n\n\nStacking & Receptive Field\nMultiple convolutions can be executed in sequence (stacking). Each convolution is performed on the activation maps of another previous convolution. Figure 25 illustrates the result.\n\n\n\n\n\n\nFigure 25: Source: Johnson (2019)\n\n\n\nA convolution is therefore not only performed directly on the input (e.g., images) but is generally defined on inputs of dimensionality \\(H \\times W \\times C\\)! (There are also variants in higher dimensions.)\nHowever, non-linear activation functions must be used between the convolutions. Otherwise, the stacked convolution can be expressed with a simple convolution (similar to an MLP, which can be expressed with a linear transformation without activation functions).\nThe receptive field defines which inputs influence the activations of a neuron. Figure 26 to Figure 29 illustrate the concept.\n\n\n\n\n\n\nFigure 26: Receptive field with a 5x5 kernel.\n\n\n\n\n\n\n\n\n\nFigure 27: Receptive field with a 5x5 kernel and two layers.\n\n\n\n\n\n\n\n\n\nFigure 28: Receptive field with a 5x5 kernel and two layers.\n\n\n\n\n\n\n\n\n\nFigure 29: Receptive field with a 5x5 kernel and two layers. The output activation at the center (right) aggregates values over the union of all blue boxes in the left figure and has thus a receptive field of \\(9 \\times 9\\) with respect to the input activations.\n\n\n\nThe receptive field \\(r_0\\) of a convolutional neural network can be calculated as follows:\n\nDefine the recurrence relation:\n\n\\[\\begin{equation}\nr_{l-1} = s_l \\cdot r_l + (k_l - s_l)\n\\end{equation}\\]\n\n\\(r_l\\): Receptive field at layer \\(l\\).\n\\(s_l\\): Stride of layer \\(l\\).\n\\(k_l\\): Kernel size of layer \\(l\\).\n\n\nSolving this recurrence relation, we get the closed-form solution for the receptive field at the input layer \\(r_0\\):\n\n\\[\\begin{equation}\nr_0 = \\sum_{l=1}^L \\left( (k_l - 1) \\prod_{i=1}^{l-1} s_i \\right) + 1\n\\end{equation}\\]\n\n\\(L\\): Total number of layers.\n\\(k_l\\): Kernel size at layer \\(l\\).\n\\(s_i\\): Stride at layer \\(i\\), from layer 1 up to \\(l-1\\).\n\nAraujo, Norris, and Sim (2019) have written a nice article about receptive fields and how to calculate them.",
    "crumbs": [
      "Lectures",
      "4 - Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "lectures/cnns.html#more-layers-operations",
    "href": "lectures/cnns.html#more-layers-operations",
    "title": "4 - Convolutional Neural Networks",
    "section": "More Layers & Operations",
    "text": "More Layers & Operations\n\nDownsampling\nSpatial downsampling is often a desired property of convolutional layers. We can achieve that by choosing a stride \\(\\gt 1\\). However, there are other options, such as pooling layers.\n\n\n\n\n\nSub-Sampling, b) Max Pooling, c) Average Pooling . Source: Prince (2023)\n\n\n\nFigure 30\n\n\n\n\n\nUpsampling\nSometimes we need to increase the spatial dimensionality of our activation maps.\n\n\n\n\n\nDuplicate, b) Max-Unpooling, c) Bilinear Interpolation . Source: Prince (2023)\n\n\n\nFigure 31\n\n\n\nUpsampling can also be learned with Transposed Convolutions.\n\n\n\n\n\n\nFigure 32\n\n\n\n\n\nChanging the number of channels\n1x1 convolutions have a kernel size of 1x1 and thus no spatial extent. These layers are often used in CNNs to change the number (\\(C\\)) of activation maps with few parameters. For example, activation maps of dimensionality (\\(C \\times H \\times W\\)) can be changed to a volume of (\\(C2 \\times H \\times W\\)) with \\(C2 * (C + 1)\\). This can be useful, for example, to save parameters before more complex layers or at the end of the CNN to adjust the size of the activation maps to the number of classes to be modeled (for classification problems) or to reduce to 3 color channels (\\(C2=3\\)) for image generation models. Figure 33 shows an example.\n\n\n\n\n\n\nFigure 33: Source: Prince (2023)\n\n\n\n\n\nGlobal Average Pooling\nGlobal Average Pooling is often an important component. It computes the average of the activations along the depth dimension, reducing activation maps from (C x H x W) to (C x 1 x 1). This is useful for directly modeling logits in a classification problem with C classes, enabling architectures that completely eliminate fully-connected layers.\n\n\n\n\n\n\nFigure 34: Global Average pooling, input (left) and output (right).\n\n\n\n\n\nOther Layers\nThere are many other types of layers, also there are more flavours of convolutional layers. Here are some honorable mentions\n\nDepthwise Separable Convolutions\nDepthwise separable convolutions are a way to further reduce the number of parameters in convolutional layers. Instead of extending filters over the entire depth of the input activations, a separate filter (kernel) is used for each input channel, with the dimensionality (\\(1 \\times K \\times K\\)). Figure 35 shows an example. Subsequently, 1x1 convolutions are used to combine information across the input channels. See Figure 36 for a comparison of ‘normal’ convolutions and depthwise separable convolutions. Since 1x1 convolutions require fewer parameters, activation maps can be generated with fewer parameters.\n\n\n\n\n\n\nFigure 35: Source: https://paperswithcode.com/method/depthwise-convolution\n\n\n\n\n\n\n\n\n\nFigure 36: Source: Yu and Koltun (2016)",
    "crumbs": [
      "Lectures",
      "4 - Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "lectures/cnns.html#pytorch-examples",
    "href": "lectures/cnns.html#pytorch-examples",
    "title": "4 - Convolutional Neural Networks",
    "section": "PyTorch Examples",
    "text": "PyTorch Examples\n\nimport numpy as np\nimport torch\nfrom torch.nn import functional as F\nimport torchshow as ts\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\n\n\n#img = Image.open({{&lt; meta params.images_path &gt;}}'cat.jpg')\nimage_path = \"../assets/images/cnns/cat.jpg\"\nimg = Image.open(image_path)\nimg\n\n\n\n\n\n\n\n\n\nfilter = torch.tensor(\n    [   [[1, 0, -1], [1, 0, -1], [1, 0, -1]], # R\n        [[1, 0, -1], [1, 0, -1], [1, 0, -1]], # G\n        [[1, 0, -1], [1, 0, -1], [1, 0, -1]], # B\n    ]).unsqueeze(0).float()\nts.show(filter, show_axis=False)\n\n\n\n\n\n\n\n\n\ninput = torch.tensor(np.array(img)).unsqueeze(0).permute(0, 3, 1, 2).float() # (N, C, H, W)\ninput /= 255.0\ninput -= 1.0\nresult = F.conv2d(input, filter, stride=1, padding=0, dilation=1, groups=1)\n\n\nts.show(result)\n\n/opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages/torchshow/visualization.py:385: UserWarning:\n\nOriginal input range is not 0-1 when using grayscale mode. Auto-rescaling it to 0-1 by default.\n\n\n\n\n\n\n\n\n\n\n2D-Convolution:\n\nresult = F.conv2d(input, filter, stride=6, padding=0, dilation=1, groups=1)\nts.show(result)\n\n\n\n\n\n\n\n\nTransposed convolution:\n\nresult = F.conv2d(input, filter, stride=6, padding=0, dilation=1, groups=1)\nresult = F.conv_transpose2d(result, weight=torch.ones_like(filter))\nts.show(result)\n\n\n\n\n\n\n\n\nMax-Pooling:\n\nresult = F.max_pool2d(input, kernel_size=8, stride=8)\nts.show(result)",
    "crumbs": [
      "Lectures",
      "4 - Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "lectures/cnns.html#references",
    "href": "lectures/cnns.html#references",
    "title": "4 - Convolutional Neural Networks",
    "section": "References",
    "text": "References\n\n\nAraujo, Andr&eacute;, Wade Norris, and Jack Sim. 2019. “Computing Receptive Fields of Convolutional Neural Networks.” Distill 4 (11): 10.23915/distill.00021. https://doi.org/10.23915/distill.00021.\n\n\nDumoulin, Vincent, and Francesco Visin. 2016. “A Guide to Convolution Arithmetic for Deep Learning.” ArXiv e-Prints, March.\n\n\nJohnson, Justin. 2019. “EECS 498-007 / 598-005: Deep Learning for Computer Vision.” Lecture {Notes} / {Slides}. https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2019/.\n\n\nLi, Fei-Fei. 2022. “CS231n Convolutional Neural Networks for Visual Recognition.” Lecture {Notes}. https://cs231n.github.io.\n\n\nPrince, Simon J. D. 2023. Understanding Deep Learning. MIT Press. https://udlbook.github.io/udlbook/.\n\n\nYu, Fisher, and Vladlen Koltun. 2016. “Multi-Scale Context Aggregation by Dilated Convolutions.” arXiv. http://arxiv.org/abs/1511.07122.",
    "crumbs": [
      "Lectures",
      "4 - Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Willkommen zum Modul Computer Vision mit Deep Learning",
    "section": "",
    "text": "Herzlich willkommen zum Modul Computer Vision mit Deep Learning (1. Teil)!\nHier finden Sie Unterlagen und aktuelle Informationen zum Modul.\nModul Computer Vision mit DL\nCAS Page",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#lernziele",
    "href": "index.html#lernziele",
    "title": "Willkommen zum Modul Computer Vision mit Deep Learning",
    "section": "Lernziele",
    "text": "Lernziele\n\nDie fundamentalen Herausforderungen bei Modellieren von Bilddaten verstehen\nConvolutional Neural Networks verstehen, implementieren und trainieren können\nBildklassifikation verstehen und systematisch umsetzen können\nMit Deep Learning Frameworks umgehen und Libraries verwenden können\nPraktische Anwendung umsetzen, vorstellen und erklären können",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#inhalte",
    "href": "index.html#inhalte",
    "title": "Willkommen zum Modul Computer Vision mit Deep Learning",
    "section": "Inhalte",
    "text": "Inhalte\n\nTag 1 - Grundlagen Convolutional Neural Networks\n\n\n\nZeit\nThema\n\n\n\n\n8:45 - 9:30\nEinführung Computer Vision mit Deep Learning\n\n\n9:30 - 10:30\nÜbung: Deep Learning mit PyTorch und Bilder\n\n\n10:30 - 10:45\nPause\n\n\n10:45 - 12:00\nConvolutional Neural Networks\n\n\n12:00 - 13:00\nMittagspause\n\n\n13:00 - 16:30\nÜbung: CNNs für Bildklassifikation\n\n\n\n\n\nTag 2 - Bildklassifikation und Anwendungen\n\n\n\nZeit\nThema\n\n\n\n\n8:45 - 9:30\nTheorie: Bildklassifikation & Recap\n\n\n9:30 - 10:30\nTheorie: Praktische Überlegungen\n\n\n10:30 - 10:45\nPause\n\n\n10:45 - 12:00\nÜbung: Bildklassifikation\n\n\n12:00 - 13:00\nMittagspause\n\n\n13:00 - 14:30\nÜbung: Bildklassifikation\n\n\n14:30 - 14:45\nPause\n\n\n14:45 - 15:15\nTheorie: Recent Advances\n\n\n15:15 - 16:30\nÜbung: Recent Advances",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "lectures/classification.html",
    "href": "lectures/classification.html",
    "title": "5 - Image Classification",
    "section": "",
    "text": "Image classification is a core task of Computer Vision. In image classification, an image is assigned to a predefined set of classes. In multi-class classification, there are ≥ 2 classes; in binary classification, there are 2 classes; and in multi-label classification, there are several sets of classes. Figure 1 illustrates the problem in multi-class classification.\n\n\n\n\n\n\nFigure 1: Image Classification example.\n\n\n\nFigure 2 shows an example from the paper by Krizhevsky, Sutskever, and Hinton (2012), which achieved the best results in the 2012 ImageNet competition, demonstrating how well CNNs work.\n\n\n\n\n\n\nFigure 2: Image Classification example (from Krizhevsky, Sutskever, and Hinton (2012)).\n\n\n\nFigure 3 illustrates the challenge with images taken by camera traps, which need to be classified along animal species.\n\n\n\n\n\n\nFigure 3: Example images from camera traps.",
    "crumbs": [
      "Lectures",
      "5 - Image Classification"
    ]
  },
  {
    "objectID": "lectures/classification.html#introduction",
    "href": "lectures/classification.html#introduction",
    "title": "5 - Image Classification",
    "section": "",
    "text": "Image classification is a core task of Computer Vision. In image classification, an image is assigned to a predefined set of classes. In multi-class classification, there are ≥ 2 classes; in binary classification, there are 2 classes; and in multi-label classification, there are several sets of classes. Figure 1 illustrates the problem in multi-class classification.\n\n\n\n\n\n\nFigure 1: Image Classification example.\n\n\n\nFigure 2 shows an example from the paper by Krizhevsky, Sutskever, and Hinton (2012), which achieved the best results in the 2012 ImageNet competition, demonstrating how well CNNs work.\n\n\n\n\n\n\nFigure 2: Image Classification example (from Krizhevsky, Sutskever, and Hinton (2012)).\n\n\n\nFigure 3 illustrates the challenge with images taken by camera traps, which need to be classified along animal species.\n\n\n\n\n\n\nFigure 3: Example images from camera traps.",
    "crumbs": [
      "Lectures",
      "5 - Image Classification"
    ]
  },
  {
    "objectID": "lectures/classification.html#parametric-approach",
    "href": "lectures/classification.html#parametric-approach",
    "title": "5 - Image Classification",
    "section": "Parametric Approach",
    "text": "Parametric Approach\nIn a parametric approach, we seek a model of the following form:\n\\[\n\\hat{y}^{(i)} = f(\\theta, \\mathbf{x}^{(i)})\n\\]\nWe want to find model parameters \\(\\theta\\) that output a score/prediction \\(\\hat{y}^{(i)}\\) for any data points \\(\\mathbf{x}^{(i)}\\) for each class \\(k \\in K\\). We then want to assess how good this score is with the help of a loss function.\n\nSoftmax Classifier\nWith a Softmax Classifier, we interpret model predictions/scores as probabilities of class memberships: \\(P(Y=\\mathbf{y}^{(i)}| X = \\mathbf{x}^{(i)})\\). We interpret the output as a Categorical Distribution over all possible classes.\nTo obtain a valid probability distribution, the untransformed outputs \\(\\mathbf{z}\\), also called logits, of a model are transformed with the softmax function \\(\\sigma(\\mathbf{z})\\):\n\\[\nP(Y = k| X = \\mathbf{x}^{(i)}) = \\sigma(\\mathbf{z})_k = \\frac{e^{z_k}}{\\sum_i^K e^{z_i}}\n\\]\nFigure 4 shows an example of the effect of the softmax transformation.\n\n\n\n\n\n\n\n\nFigure 4: Logits (left) to probabilities with the Softmax function (right).",
    "crumbs": [
      "Lectures",
      "5 - Image Classification"
    ]
  },
  {
    "objectID": "lectures/classification.html#loss-function",
    "href": "lectures/classification.html#loss-function",
    "title": "5 - Image Classification",
    "section": "Loss Function",
    "text": "Loss Function\nNow we look at how to define the loss function \\(L(\\mathbf{X}, \\mathbf{y}, \\theta)\\) for a multi-class classification problem.\n\nLikelihood\nThe likelihood of a data point \\((\\mathbf{x}^{(i)}, y^{(i)})\\) is the probability of observing/realizing a data point, given a model with certain parameters:\n\\[\nP(Y=y^{(i)}| X = \\mathbf{x}^{(i)}) = f(\\theta, \\mathbf{x}^{(i)})\n\\]\nThis means we formulate a model with a probabilistic interpretation of predictions: \\(f(\\theta, \\mathbf{x}^{(i)}): \\mathbb{R}^{n} \\mapsto [0, 1]\\)\nFor a multi-class classification, the label vector is one-hot encoded \\(\\mathbf{y}^{(i)} \\in \\{0, 1\\}^K\\), where the true class is coded with 1 and the rest with 0. The likelihood of a data point is defined as:\n\\[\n\\prod_{j=1}^K  P(Y = j| X = \\mathbf{x}^{(i)})^{y^{(i)}_j}\n\\]\nSince only one entry in \\(\\mathbf{y}^{(i)}\\) is 1, the likelihood is simply the prediction for the true class \\(P(Y = y^{(i)}| X = \\mathbf{x}^{(i)})\\).\nMore Info\nGiven \\(\\mathbf{y} = [0, 1, 0, 1, 1]\\) and the following \\(\\hat{\\mathbf{y}}\\), calculate the likelihood.\nFor \\(\\hat{\\mathbf{y}} = [0.1, 0.8, 0.2, 0.7, 0.9]\\):\n\n\n\n\n\n\nNoteClick for result\n\n\n\n\n\n\ny_pred = np.array([0.1, 0.8, 0.2, 0.7, 0.9])\ndisplay_likelihood(y_true, y_pred)\n\nTotal Likelihood: 0.362880\n\n\n\n\n\nDoes it get larger for \\(\\hat{\\mathbf{y}} = [0.1, 0.9, 0.2, 0.7, 0.9]\\)?\n\n\n\n\n\n\nNoteClick for result\n\n\n\n\n\n\ny_pred = np.array([0.1, 0.9, 0.2, 0.7, 0.9])\ndisplay_likelihood(y_true, y_pred)\n\nTotal Likelihood: 0.408240\n\n\n\n\n\nWhat happens if we increase the dataset by copying the vector 10 times? \\(\\hat{\\mathbf{y}} = [0.1, 0.8, 0.2, 0.7, 0.9, 0.1, 0.8, ...]\\)?\n\n\n\n\n\n\nNoteClick for result\n\n\n\n\n\n\ny_pred = np.array([0.1, 0.8, 0.2, 0.7, 0.9])\n\ny_true = np.repeat(y_true, 10, axis=0)\ny_pred = np.repeat(y_pred, 10, axis=0)\n\ndisplay_likelihood(y_true, y_pred)\n\nTotal Likelihood: 0.000040\n\n\n\n\n\n\n\nMaximum Likelihood\nThe likelihood \\(P(\\mathbf{y} | \\theta, \\mathbf{X})\\) of observing our entire dataset \\((\\mathbf{X}, \\mathbf{y})\\), given the parameters \\(\\theta\\) and assuming that the data points \\((\\mathbf{x}^{(i)}, y^{(i)})\\) are independent and identically distributed, can be calculated as:\n\\[\\begin{equation}\n\\prod_{i=1}^N \\prod_{j=1}^K  P(Y = j| X = \\mathbf{x}^{(i)})^{y^{(i)}_j}\n\\end{equation}\\]\nUnder the Maximum Likelihood approach, we seek the parameters \\(\\theta\\) that maximize the likelihood of observing the dataset.\n\n\nNegative Log-Likelihood\nWith Maximum Likelihood, we aim to choose the parameters \\(\\theta\\) such that \\(p(\\mathbf{y} | \\theta, \\mathbf{X})\\) is maximized. However, this function can be complex to handle, and we can use some mathematical tricks. We take the logarithm of the likelihood so that the product of probabilities becomes a sum. Since the logarithm is a monotonically increasing function, we can maximize its logarithm instead of the function \\(p(\\mathbf{y} | \\theta, \\mathbf{X})\\). Finally, we take the negative of the function, allowing us to minimize it.\n\\[\\begin{equation}\nL(\\mathbf{X}, \\mathbf{y}, \\theta) = - \\log \\prod_{i=1}^N \\prod_{j=1}^K P(Y = j| X = \\mathbf{x}^{(i)})^{y^{(i)}_j} \\\\\nL(\\mathbf{X}, \\mathbf{y}, \\theta) = -\\sum_{i=1}^N \\sum_{j=1}^K y^{(i)}_j \\log  P(Y = j| X = \\mathbf{x}^{(i)})\n\\end{equation}\\]\n\n\nCross-Entropy\nThe loss function, derived with Maximum Likelihood, can also be viewed through the lens of cross-entropy between two discrete probability functions. Specifically, we can calculate and minimize the cross-entropy between the true distribution \\(\\mathbf{y}^{(i)}\\) and the predicted \\(\\mathbf{\\hat{y}}^{(i)}\\). Cross-entropy comes from information theory and measures how many bits/nats on average are needed to describe an event of a probability distribution \\(p(x)\\) when using the approximation \\(q(x)\\).\n\\[\\begin{equation}\nCE = - \\sum_{x \\in X} p(x) \\log q(x) \\\\\nCE = - \\sum_{i=1}^N \\sum_{j=1}^K y_j^{(i)} \\log \\hat{y}_j^{(i)}\n\\end{equation}\\]\nIt is evident that cross-entropy is identical to the negative log-likelihood.\n\n\n\n\n\n\n\n\nFigure 5: True Distribution (left) and Predicted Distribution (right).\n\n\n\n\n\nFigure 5 shows an example with a cross-entropy value of: 0.266.",
    "crumbs": [
      "Lectures",
      "5 - Image Classification"
    ]
  },
  {
    "objectID": "lectures/classification.html#architectures",
    "href": "lectures/classification.html#architectures",
    "title": "5 - Image Classification",
    "section": "Architectures",
    "text": "Architectures\nArchitecture refers to the complete definition of an untrained (deep-learning) model, i.e., the sequence and configuration of layers. The following are some important architectures that have been successfully used for image classification.\n\nAlexNet\nCNNs became extremely popular after winning the ImageNet Competition. Krizhevsky, Sutskever, and Hinton (2012) implemented a CNN with multiple layers, known as the AlexNet architecture, as shown in Figure 6. ImageNet is a large, hierarchical image dataset Deng et al. (2009), which enabled efficient training of CNNs for the first time.\nAlexNet consists of 5 convolutional layers and 3 fully-connected layers. The last layer is a 1000-way softmax output to model the classes in ImageNet.\nThe model was trained with two GPUs (GTX 580) with 3GB memory each. Since 3GB was insufficient to train the model, the architecture was split across the GPUs. Some layers were split between the GPUs, allowing a larger network to be trained.\nFigure 6 shows the detailed architecture, including kernel sizes, the number of filters per layer, activation map dimensions, and pooling layers. ReLU was used as the activation function. This representation is from the original paper and shows how the model was split across two GPUs.\n\n\n\n\n\n\nFigure 6: AlexNet Krizhevsky, Sutskever, and Hinton (2012).\n\n\n\nSince the split across two GPUs is no longer necessary, the architecture is somewhat simplified. Modern implementations are shown in Figure 7.\n\n\n\n\n\n\nFigure 7: AlexNet Prince (2023)\n\n\n\nFigure 8 presents the operations in AlexNet in tabular form.\n\n\n\n\n\n\nFigure 8: Source: Johnson (2019).\n\n\n\nWe can also easily load AlexNet via torchvision.\n\nimport torch\nimport torchvision.models as models\nimport torchinfo\n\nalexnet = models.alexnet()\nx = torch.zeros(1, 3, 224, 224, dtype=torch.float, requires_grad=False)\nyhat = alexnet(x)\n\nprint(torchinfo.summary(alexnet, input_size=(1, 3, 224, 224)))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nAlexNet                                  [1, 1000]                 --\n├─Sequential: 1-1                        [1, 256, 6, 6]            --\n│    └─Conv2d: 2-1                       [1, 64, 55, 55]           23,296\n│    └─ReLU: 2-2                         [1, 64, 55, 55]           --\n│    └─MaxPool2d: 2-3                    [1, 64, 27, 27]           --\n│    └─Conv2d: 2-4                       [1, 192, 27, 27]          307,392\n│    └─ReLU: 2-5                         [1, 192, 27, 27]          --\n│    └─MaxPool2d: 2-6                    [1, 192, 13, 13]          --\n│    └─Conv2d: 2-7                       [1, 384, 13, 13]          663,936\n│    └─ReLU: 2-8                         [1, 384, 13, 13]          --\n│    └─Conv2d: 2-9                       [1, 256, 13, 13]          884,992\n│    └─ReLU: 2-10                        [1, 256, 13, 13]          --\n│    └─Conv2d: 2-11                      [1, 256, 13, 13]          590,080\n│    └─ReLU: 2-12                        [1, 256, 13, 13]          --\n│    └─MaxPool2d: 2-13                   [1, 256, 6, 6]            --\n├─AdaptiveAvgPool2d: 1-2                 [1, 256, 6, 6]            --\n├─Sequential: 1-3                        [1, 1000]                 --\n│    └─Dropout: 2-14                     [1, 9216]                 --\n│    └─Linear: 2-15                      [1, 4096]                 37,752,832\n│    └─ReLU: 2-16                        [1, 4096]                 --\n│    └─Dropout: 2-17                     [1, 4096]                 --\n│    └─Linear: 2-18                      [1, 4096]                 16,781,312\n│    └─ReLU: 2-19                        [1, 4096]                 --\n│    └─Linear: 2-20                      [1, 1000]                 4,097,000\n==========================================================================================\nTotal params: 61,100,840\nTrainable params: 61,100,840\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 714.68\n==========================================================================================\nInput size (MB): 0.60\nForward/backward pass size (MB): 3.95\nParams size (MB): 244.40\nEstimated Total Size (MB): 248.96\n==========================================================================================\n\n\n\n\nVGG\nSimonyan and Zisserman (2015) won the ImageNet Challenge in 2014 with their VGG architecture. They showed that smaller 3x3 kernels work significantly better and that deeper networks with 16-19 layers can be trained. Figure 9 shows the architecture as presented in the original paper. Figure 10 visualizes the architecture.\n\n\n\n\n\n\nFigure 9: VGG Simonyan and Zisserman (2015).\n\n\n\n\n\n\n\n\n\nFigure 10: VGG Prince (2023)\n\n\n\nVGG introduced a popular design element: A layer has the same number of filters as the previous layer unless the activation map dimensions are halved, in which case the number of filters is doubled (see also Figure 10). This was done to maintain the time complexity of the layers. VGG does not use normalization layers.\nFigure 11 compares VGG with AlexNet.\n\n\n\n\n\n\nFigure 11: Source: Johnson (2019).\n\n\n\n\n\nResNet\nHe et al. (2016) wondered whether CNNs could be improved simply by making them deeper, i.e., adding more layers. Their experiments showed that adding layers eventually stops being beneficial, and performance saturates and then rapidly degrades (see Figure 12). They noticed that the performance did not degrade due to overfitting (see Figure 13, which shows that training error is also poor), but because back-propagation becomes less effective and the weights do not optimize well. He et al. (2016) hypothesized that deeper networks could not perform worse than shallower ones because the additional layers could simply pass on activations unchanged.\n\n\n\n\n\n\nFigure 12: Source: He et al. (2016)\n\n\n\n\n\n\n\n\n\nFigure 13: Source: He et al. (2016)\n\n\n\nThey then hypothesized that passing on activations unchanged as a default behavior could avoid this problem. They introduced identity mappings. Figure 14 shows such a connection. These connections are also called residual connections because the network only needs to learn the change in activations from layer \\(i\\) to layer \\(i+1\\), the residue.\n\n\n\n\n\n\nFigure 14: ResNet He et al. (2016) (Graphic from Johnson (2019).)\n\n\n\nResNet was otherwise inspired by VGG16. The convolutional layers consist of 3x3 kernels (except the first one), and the number of filters is doubled when the activation map dimensions are halved. ResNet uses convolutions with stride 2 for down-sampling and no max pooling. At the end, ResNet uses a global average pooling layer followed by a fully-connected layer with the number of classes. Variants of ResNet have 18, 34, 50, and over 100 layers. ResNet also uses batch normalization. Figure 15 shows the architecture.\n\n\n\n\n\n\nFigure 15: Source: He et al. (2016).\n\n\n\nResNet is very popular and is still widely used today (there are now more modern variants). Recent studies of identity mappings (or more generally skip connections) have shown that the gradient surface of the loss function becomes smoother, allowing the network to be optimized better. This is impressively illustrated in Figure 16.\n\n\n\n\n\n\nFigure 16: Source: Li et al. (2018).\n\n\n\n\n\nConvNext\nOne of the most modern CNN architectures was described in Liu et al. (2022). This architecture uses tricks and implementation ideas accumulated over decades from various architectures. Figure 17 shows, starting from a modern version of ResNet, what has been adjusted to define this state-of-the-art architecture. Examples include: larger kernels, different activation functions, layer normalization instead of batch normalization, and depthwise separable convolutions.\n\n\n\n\n\n\nFigure 17: Convnext Liu et al. (2022).\n\n\n\nThere is already a new version of this architecture Woo et al. (2023).\n\n\nImageNet Performance\nFigure 18 shows the development of ImageNet performance. As of 2023, we are much closer to 100%, see Link.\n\n\n\n\n\n\nFigure 18: Graphic from Johnson (2019)\n\n\n\n\n\nWhich Architecture?\nWhich architecture should be chosen for a specific problem? A common tip is: Don’t be a hero.\nOne should rely on off-the-shelf architectures and not implement their own without a good reason.\nTypically, ResNet-50 or ResNet-101 are good choices. However, there are also models that require significantly fewer parameters, such as Efficient Nets.\nImportant considerations are also the requirements regarding accuracy, performance (FLOPs), and model size (memory).",
    "crumbs": [
      "Lectures",
      "5 - Image Classification"
    ]
  },
  {
    "objectID": "lectures/classification.html#further-variations",
    "href": "lectures/classification.html#further-variations",
    "title": "5 - Image Classification",
    "section": "Further Variations",
    "text": "Further Variations\nThere are countless other architectures and especially specific layers or layer blocks that can be used in various architectures. The following are Squeeze/Excite Networks and the important class of normalization layers, which are often used and can significantly improve models or accelerate the optimization process.\n\nSqueeze/Excite Networks\nSqueeze-and-Excite Networks (SE-Networks) were introduced in 2019 Hu et al. (2019). These include so-called Squeeze and Excite blocks (SE blocks), which allow the scaling of activation maps of a layer. This scaling is learnable through a few additional parameters. In practice, significant performance gains have been observed. Figure 19 shows an illustration.\n\n\n\n\n\n\nFigure 19: Source: Hu et al. (2019).\n\n\n\nThese SE blocks can be easily applied to arbitrary activation maps. Figure 19 shows an input feature map \\(\\mathbf{\\mathsf{X}} \\in \\mathbb{R}^{H' \\times W' \\times C'}\\) that is transformed with \\(F_{tr}\\) (e.g., with a convolutional layer). This results in the activation maps \\(\\mathbf{\\mathsf{U}} \\in \\mathbb{R}^{H \\times W \\times C}\\).\nThe transformation \\(F_{sq}(\\cdot)\\) applies the squeeze operation, a global average pooling, to generate a description of each channel by aggregating spatial information of the activation map \\(\\mathbf{\\mathsf{U}}\\). This results in a vector \\(z \\in \\mathbb{R}^{1 \\times 1 \\times C}\\).\nThe excitation operation \\(F_{ex}(\\cdot, W)\\) uses a gating mechanism with parameters \\(W\\), implemented with two fully-connected layers and activation functions. The result is \\(s \\in \\mathbb{R}^{1 \\times 1 \\times C}\\), the channel weights. It is called gating because the weights range from \\([0, 1]\\) and thus control how much information of a channel flows through (gatekeeping).\nFinally, \\(F_{scale}(\\cdot)\\) scales the activation maps \\(\\mathbf{\\mathsf{U}}\\) with the channel weights.\nThe operation can be described as follows:\n\\[\\begin{equation}\nU = F_{tr}(X) \\\\\nz = F_{sq}(U) = \\text{GlobalAvgPool}(U) \\\\\ns = F_{ex}(z, W) = \\sigma(\\mathbf{W}_2 g(\\mathbf{W}_1 z)) \\\\\n\\hat{X} = F_{scale}(U, s) = U \\odot s\n\\end{equation}\\]\nwhere \\(g()\\) represents the ReLU function, and \\(\\sigma\\) represents the sigmoid function.\n\n\nNormalization Layers\nNormalization layers normalize activation maps to improve parameter learning. There are many variations, as shown in Figure 20. In modern architectures, normalization layers are typically used. Popular is, for example, layer normalization (see Ba, Kiros, and Hinton (2016)). The general form of normalization is given in equation Equation 1. The parameters \\(\\gamma\\) and \\(\\beta\\) are learned, while the means \\(E[x]\\) and variances \\(\\sigma^2[x]\\) are estimated from the activations. Why normalization layers work and which ones to prefer are still subjects of research and are often empirically tested in practice (considered as hyper-parameters). It is believed that the cost function becomes smoother overall, allowing the network to train faster and better Santurkar et al. (2019).\n\\[\ny = \\frac{x - E[x]}{\\sqrt{\\sigma^2[x] + \\epsilon}} * \\gamma + \\beta\n\\tag{1}\\]\n\n\n\n\n\n\nFigure 20: Source: Qiao et al. (2020).",
    "crumbs": [
      "Lectures",
      "5 - Image Classification"
    ]
  },
  {
    "objectID": "lectures/classification.html#pre-processing",
    "href": "lectures/classification.html#pre-processing",
    "title": "5 - Image Classification",
    "section": "Pre-Processing",
    "text": "Pre-Processing\nTypically, images are pre-processed before being processed by the CNN. This is done to make the optimization process converge faster. Common steps include:\n\nResizing/cropping to a fixed size, e.g., 224x224x3. This is necessary for the images to be processed in a batch (they must have the same dimensions).\nScaling: Images are scaled from the interval [0, 255] to the interval [0, 1].\nNormalization: Normalization is often done along the color channels so that the mean value of the pixel values is 0.\n\nIn torchvision, the following pre-processing steps are used: Link",
    "crumbs": [
      "Lectures",
      "5 - Image Classification"
    ]
  },
  {
    "objectID": "lectures/classification.html#transfer-learning",
    "href": "lectures/classification.html#transfer-learning",
    "title": "5 - Image Classification",
    "section": "Transfer Learning",
    "text": "Transfer Learning\nTransfer Learning refers to the process of adapting a trained model for Task A to Task B. Adapting pre-trained models often leads to better results and also reduces the number of training iterations. Deep learning frameworks often provide easy access to such models (also known as Model Zoo). For example, from PyTorch: https://pytorch.org/vision/stable/models.html.\n\nfrom torchvision.models import resnet50, ResNet50_Weights\nresnet50(weights=ResNet50_Weights.IMAGENET1K_V2)",
    "crumbs": [
      "Lectures",
      "5 - Image Classification"
    ]
  },
  {
    "objectID": "lectures/classification.html#pytorch-example",
    "href": "lectures/classification.html#pytorch-example",
    "title": "5 - Image Classification",
    "section": "PyTorch Example",
    "text": "PyTorch Example\n\nArchitecture\nExample of a CNN architecture.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nnet = Net()\n\n\n\nLoss Function\n\nimport torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)",
    "crumbs": [
      "Lectures",
      "5 - Image Classification"
    ]
  },
  {
    "objectID": "lectures/classification.html#references",
    "href": "lectures/classification.html#references",
    "title": "5 - Image Classification",
    "section": "References",
    "text": "References\n\n\nBa, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. “Layer Normalization.” arXiv. http://arxiv.org/abs/1607.06450.\n\n\nDeng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. “ImageNet: A Large-Scale Hierarchical Image Database.” In 2009 IEEE Conference on Computer Vision and Pattern Recognition, 248–55. Miami, FL: IEEE. https://doi.org/10.1109/CVPR.2009.5206848.\n\n\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. “Deep Residual Learning for Image Recognition.” In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 770–78. IEEE. https://doi.org/10.1109/CVPR.2016.90.\n\n\nHu, Jie, Li Shen, Samuel Albanie, Gang Sun, and Enhua Wu. 2019. “Squeeze-and-Excitation Networks.” arXiv. http://arxiv.org/abs/1709.01507.\n\n\nJohnson, Justin. 2019. “EECS 498-007 / 598-005: Deep Learning for Computer Vision.” Lecture {Notes} / {Slides}. https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2019/.\n\n\nKrizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. “ImageNet Classification with Deep Convolutional Neural Networks.” In Advances in Neural Information Processing Systems, edited by F. Pereira, C. J. Burges, L. Bottou, and K. Q. Weinberger. Vol. 25. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf.\n\n\nLi, Hao, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. 2018. “Visualizing the Loss Landscape of Neural Nets.” arXiv. http://arxiv.org/abs/1712.09913.\n\n\nLiu, Zhuang, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. 2022. “A ConvNet for the 2020s.” arXiv. http://arxiv.org/abs/2201.03545.\n\n\nPrince, Simon J. D. 2023. Understanding Deep Learning. MIT Press. https://udlbook.github.io/udlbook/.\n\n\nQiao, Siyuan, Huiyu Wang, Chenxi Liu, Wei Shen, and Alan Yuille. 2020. “Micro-Batch Training with Batch-Channel Normalization and Weight Standardization.” arXiv. http://arxiv.org/abs/1903.10520.\n\n\nSanturkar, Shibani, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. 2019. “How Does Batch Normalization Help Optimization?” arXiv. http://arxiv.org/abs/1805.11604.\n\n\nSimonyan, Karen, and Andrew Zisserman. 2015. “Very Deep Convolutional Networks for Large-Scale Image Recognition.” arXiv. http://arxiv.org/abs/1409.1556.\n\n\nWoo, Sanghyun, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, and Saining Xie. 2023. “ConvNeXt V2: Co-Designing and Scaling ConvNets with Masked Autoencoders.” arXiv. http://arxiv.org/abs/2301.00808.",
    "crumbs": [
      "Lectures",
      "5 - Image Classification"
    ]
  },
  {
    "objectID": "lectures/frameworks.html",
    "href": "lectures/frameworks.html",
    "title": "2 - Software & Hardware for Deep Learning",
    "section": "",
    "text": "There are a variety of Deep Learning frameworks. These frameworks allow for easy configuration, training, and deploying of neural networks. They are often developed via Python API. Figure 1 shows some frameworks.\n\n\n\n\n\n\nFigure 1: Frameworks (from Li (2022)).\n\n\n\nKey features of such frameworks are:\n\nFast development and testing of neural networks\nAutomatic differentiation of operations\nEfficient execution on diverse hardware\n\n\n\nAt the core of neural networks is the Computational Graph. It automatically embeds dependent operations in a directed acyclic graph (DAG). Gradients are tracked as needed, allowing variables to be efficiently updated/trained.\nThe following shows an example in Numpy where we define computations and manually calculate derivatives. The graph is shown in Figure 2.\n\\[\\begin{equation}\n    f(\\mathbf{A}, \\mathbf{B}, \\mathbf{C}) =  \\sum_{ij} \\big((\\mathbf{A} \\odot \\mathbf{B}) + \\mathbf{C}\\big)_{ij}\n\\end{equation}\\]\n\n\n\n\n\n\nFigure 2: Computational Graph.\n\n\n\n\nimport numpy as np\n\nnp.random.seed(123)\n\nH, W = 2, 3\n\na = np.random.random(size=(H, W))\nb = np.random.random(size=(H, W))\nc = np.random.random(size=(H, W))\n\nd = a * b\ne = d + c\nf = e.sum()\n\ndf_de = 1.0\nde_dd = 1.0\nde_dc = c\ndd_da = b\n\ndf_da = df_de * de_dd * dd_da\n\nprint(df_da)\n\n[[0.9807642  0.68482974 0.4809319 ]\n [0.39211752 0.34317802 0.72904971]]\n\n\nHere’s the same example in PyTorch. Using x.backward(), gradients with respect to x are computed for variables connected to x.\n\nimport torch\n\nnp.random.seed(123)\n\nH, W = 2, 3\n\na = torch.tensor(a, requires_grad=True)\nb = torch.tensor(b, requires_grad=True)\nc = torch.tensor(c, requires_grad=True)\n\nd = a * b\ne = d + c\nf = e.sum()\n\nf.backward()\nprint(a.grad)\n\ntensor([[0.9808, 0.6848, 0.4809],\n        [0.3921, 0.3432, 0.7290]], dtype=torch.float64)\n\n\nHere are the nodes of the computational graph.\n\nfrom torchviz import make_dot\nmake_dot(f, params={'a': a, 'b': b, 'c': c, 'f':f , 'd': d, 'e':e })\n\n\n\n\n\n\n\n\nTo perform the computation on a GPU, a simple instruction is enough:\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device} device\")\n\na = a.to(device=device)\nb = b.to(device=device)\nc = c.to(device=device)\n\nUsing cpu device",
    "crumbs": [
      "Lectures",
      "2 - Software & Hardware for Deep Learning"
    ]
  },
  {
    "objectID": "lectures/frameworks.html#computational-graph-autograd",
    "href": "lectures/frameworks.html#computational-graph-autograd",
    "title": "2 - Software & Hardware for Deep Learning",
    "section": "",
    "text": "At the core of neural networks is the Computational Graph. It automatically embeds dependent operations in a directed acyclic graph (DAG). Gradients are tracked as needed, allowing variables to be efficiently updated/trained.\nThe following shows an example in Numpy where we define computations and manually calculate derivatives. The graph is shown in Figure 2.\n\\[\\begin{equation}\n    f(\\mathbf{A}, \\mathbf{B}, \\mathbf{C}) =  \\sum_{ij} \\big((\\mathbf{A} \\odot \\mathbf{B}) + \\mathbf{C}\\big)_{ij}\n\\end{equation}\\]\n\n\n\n\n\n\nFigure 2: Computational Graph.\n\n\n\n\nimport numpy as np\n\nnp.random.seed(123)\n\nH, W = 2, 3\n\na = np.random.random(size=(H, W))\nb = np.random.random(size=(H, W))\nc = np.random.random(size=(H, W))\n\nd = a * b\ne = d + c\nf = e.sum()\n\ndf_de = 1.0\nde_dd = 1.0\nde_dc = c\ndd_da = b\n\ndf_da = df_de * de_dd * dd_da\n\nprint(df_da)\n\n[[0.9807642  0.68482974 0.4809319 ]\n [0.39211752 0.34317802 0.72904971]]\n\n\nHere’s the same example in PyTorch. Using x.backward(), gradients with respect to x are computed for variables connected to x.\n\nimport torch\n\nnp.random.seed(123)\n\nH, W = 2, 3\n\na = torch.tensor(a, requires_grad=True)\nb = torch.tensor(b, requires_grad=True)\nc = torch.tensor(c, requires_grad=True)\n\nd = a * b\ne = d + c\nf = e.sum()\n\nf.backward()\nprint(a.grad)\n\ntensor([[0.9808, 0.6848, 0.4809],\n        [0.3921, 0.3432, 0.7290]], dtype=torch.float64)\n\n\nHere are the nodes of the computational graph.\n\nfrom torchviz import make_dot\nmake_dot(f, params={'a': a, 'b': b, 'c': c, 'f':f , 'd': d, 'e':e })\n\n\n\n\n\n\n\n\nTo perform the computation on a GPU, a simple instruction is enough:\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device} device\")\n\na = a.to(device=device)\nb = b.to(device=device)\nc = c.to(device=device)\n\nUsing cpu device",
    "crumbs": [
      "Lectures",
      "2 - Software & Hardware for Deep Learning"
    ]
  },
  {
    "objectID": "lectures/frameworks.html#fundamental-concepts",
    "href": "lectures/frameworks.html#fundamental-concepts",
    "title": "2 - Software & Hardware for Deep Learning",
    "section": "Fundamental Concepts",
    "text": "Fundamental Concepts\n\nTensor: N-dimensional array, similar to numpy.array\nAutograd: Functionality to create computational graphs and compute gradients.\nModule: Class to define components of neural networks",
    "crumbs": [
      "Lectures",
      "2 - Software & Hardware for Deep Learning"
    ]
  },
  {
    "objectID": "lectures/frameworks.html#tensors",
    "href": "lectures/frameworks.html#tensors",
    "title": "2 - Software & Hardware for Deep Learning",
    "section": "Tensors",
    "text": "Tensors\ntorch.Tensor is the central data structure in PyTorch. Essentially very similar to numpy.array, it can be easily loaded onto GPUs.\nTensors can be created in various ways. For example, from lists:\n\ndata = [[1, 2],[3, 4]]\nx_data = torch.tensor(data)\nprint(x_data)\n\ntensor([[1, 2],\n        [3, 4]])\n\n\nOr from numpy.ndarray:\n\nnp_array = np.array(data)\nx_np = torch.from_numpy(np_array)\nprint(x_np)\n\ntensor([[1, 2],\n        [3, 4]])\n\n\nOr from other tensors:\n\nx_ones = torch.ones_like(x_data) # retains the properties of x_data\nprint(f\"Ones Tensor: \\n {x_ones} \\n\")\n\nx_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\nprint(f\"Random Tensor: \\n {x_rand} \\n\")\n\nOnes Tensor: \n tensor([[1, 1],\n        [1, 1]]) \n\nRandom Tensor: \n tensor([[0.2475, 0.2807],\n        [0.1904, 0.3354]]) \n\n\n\nOr with randomly generated numbers or constants:\n\nshape = (2,3,)\nrand_tensor = torch.rand(shape)\nones_tensor = torch.ones(shape)\nzeros_tensor = torch.zeros(shape)\n\nprint(f\"Random Tensor: \\n {rand_tensor} \\n\")\nprint(f\"Ones Tensor: \\n {ones_tensor} \\n\")\nprint(f\"Zeros Tensor: \\n {zeros_tensor}\")\n\nRandom Tensor: \n tensor([[0.5710, 0.9007, 0.2138],\n        [0.8606, 0.9655, 0.6405]]) \n\nOnes Tensor: \n tensor([[1., 1., 1.],\n        [1., 1., 1.]]) \n\nZeros Tensor: \n tensor([[0., 0., 0.],\n        [0., 0., 0.]])\n\n\nTensor attributes:\n\ntensor = torch.rand(3,4)\n\nprint(f\"Shape of tensor: {tensor.shape}\")\nprint(f\"Datatype of tensor: {tensor.dtype}\")\nprint(f\"Device tensor is stored on: {tensor.device}\")\n\nShape of tensor: torch.Size([3, 4])\nDatatype of tensor: torch.float32\nDevice tensor is stored on: cpu\n\n\nThere are over 100 operations that can be performed on a tensor. The full list is available here.\nIndexing and Slicing:\n\ntensor = torch.ones(4, 4)\nprint(f\"First row: {tensor[0]}\")\nprint(f\"First column: {tensor[:, 0]}\")\nprint(f\"Last column: {tensor[:, -1]}\")\ntensor[:,1] = 0\nprint(tensor)\n\nFirst row: tensor([1., 1., 1., 1.])\nFirst column: tensor([1., 1., 1., 1.])\nLast column: tensor([1., 1., 1., 1.])\ntensor([[1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.]])\n\n\nJoining tensors:\n\nt1 = torch.cat([tensor, tensor, tensor], dim=1)\nprint(t1)\n\ntensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n\n\nArithmetic operations:\n\n# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\ny1 = tensor @ tensor.T\ny2 = tensor.matmul(tensor.T)\n\ny3 = torch.rand_like(y1)\ntorch.matmul(tensor, tensor.T, out=y3)\n\n\n# This computes the element-wise product. z1, z2, z3 will have the same value\nz1 = tensor * tensor\nz2 = tensor.mul(tensor)\n\nz3 = torch.rand_like(tensor)\ntorch.mul(tensor, tensor, out=z3)\n\ntensor([[1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.]])",
    "crumbs": [
      "Lectures",
      "2 - Software & Hardware for Deep Learning"
    ]
  },
  {
    "objectID": "lectures/frameworks.html#autograd",
    "href": "lectures/frameworks.html#autograd",
    "title": "2 - Software & Hardware for Deep Learning",
    "section": "Autograd",
    "text": "Autograd\nTo train neural networks, backpropagation is typically used. This calculates the gradient of the loss function with respect to the model parameters. To compute these gradients, PyTorch provides an auto-diff functionality: torch.autograd. This can automatically compute gradients for a computational graph.\nThe following is an example using a 1-layer neural network (see Figure 3 ):\n\n\n\n\n\n\nFigure 3: Source: PyTorch\n\n\n\nHere is the definition of the network in PyTorch:\n\nimport torch\n\nx = torch.ones(5)  # input tensor\ny = torch.zeros(3)  # expected output\nw = torch.randn(5\n\n, 3, requires_grad=True)\nb = torch.randn(3, requires_grad=True)\nz = torch.matmul(x, w)+b\nloss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n\nWe can now use Autograd to compute the gradient:\n\nloss.backward()\nprint(w.grad)\nprint(b.grad)\n\ntensor([[0.2729, 0.3011, 0.0536],\n        [0.2729, 0.3011, 0.0536],\n        [0.2729, 0.3011, 0.0536],\n        [0.2729, 0.3011, 0.0536],\n        [0.2729, 0.3011, 0.0536]])\ntensor([0.2729, 0.3011, 0.0536])",
    "crumbs": [
      "Lectures",
      "2 - Software & Hardware for Deep Learning"
    ]
  },
  {
    "objectID": "lectures/frameworks.html#torch.nn",
    "href": "lectures/frameworks.html#torch.nn",
    "title": "2 - Software & Hardware for Deep Learning",
    "section": "torch.nn",
    "text": "torch.nn\nPyTorch provides various building blocks for creating neural networks. These are available in torch.nn. Additionally, you can define any compositions of such building blocks that inherit from torch.nn.Module. A neural network is typically a torch.nn.Module. Each module implements the forward() method to define how data is processed.\nHere is an example:\n\nfrom torch import nn\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\nYou can also visualize the model:\n\nmodel = NeuralNetwork()\nprint(model)\n\nNeuralNetwork(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear_relu_stack): Sequential(\n    (0): Linear(in_features=784, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=512, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=512, out_features=10, bias=True)\n  )\n)\n\n\nTo use the model, you can pass input data. This will execute the forward() method, along with background operations.\n\nX = torch.rand(1, 28, 28)\nlogits = model(X)\npred_probab = nn.Softmax(dim=1)(logits)\ny_pred = pred_probab.argmax(1)\nprint(f\"Predicted class: {y_pred}\")\n\nPredicted class: tensor([4])\n\n\nThe executed operations will look like this:\n\nfrom torchviz import make_dot\nmake_dot(logits)",
    "crumbs": [
      "Lectures",
      "2 - Software & Hardware for Deep Learning"
    ]
  },
  {
    "objectID": "lectures/frameworks.html#torch.optim",
    "href": "lectures/frameworks.html#torch.optim",
    "title": "2 - Software & Hardware for Deep Learning",
    "section": "torch.optim",
    "text": "torch.optim\nTo optimize the parameters of a model, you need an optimization algorithm. torch.optim implements various algorithms, such as Stochastic Gradient Descent or the often used Adam Optimizer.\n\nfrom torch import optim\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n\nYou can then use the optimizer to adjust the parameters, you just need to define a loss function:\n\nloss_fn = torch.nn.CrossEntropyLoss()\nfor i in range(0, 3):\n    input, target = torch.rand(1, 28, 28), torch.randint(low=0, high=10, size=(1, ))\n    optimizer.zero_grad()\n    output = model(input)\n    loss = loss_fn(output, target)\n    loss.backward()\n    optimizer.step()\n\n\n\n\n\n\n\nNote\n\n\n\nNote optimizer.zero_grad() which resets the accumulated gradients of the variables to 0.",
    "crumbs": [
      "Lectures",
      "2 - Software & Hardware for Deep Learning"
    ]
  },
  {
    "objectID": "lectures/frameworks.html#training-loops",
    "href": "lectures/frameworks.html#training-loops",
    "title": "2 - Software & Hardware for Deep Learning",
    "section": "Training Loops",
    "text": "Training Loops\nTypically, you put together a training loop to train a model. A training loop iterates over batches of data and optimizes the model parameters with each iteration.\n\ndef train_loop(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    for batch, (X, y) in enumerate(dataloader):\n        # Compute prediction and loss\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:&gt;7f}  [{current:&gt;5d}/{size:&gt;5d}]\")\n\n\ndef test_loop(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    test_loss, correct = 0, 0\n\n    with torch.no_grad():\n        for X, y in dataloader:\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss: {test_loss:&gt;8f} \\n\")\n\n\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\nepochs = 10\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train_loop(train_dataloader, model, loss_fn, optimizer)\n    test_loop(test_dataloader, model, loss_fn)\nprint(\"Done!\")\n\n\n\n\n\n\n\nNote\n\n\n\nHigh-level APIs such as Lightning and Keras provide many functionalities to simplify managing training loops. It is highly recommended to use such libraries to reduce boiler-plate code. However, it depends on the individual complexitiy of a project to what degree such libraries are useful.",
    "crumbs": [
      "Lectures",
      "2 - Software & Hardware for Deep Learning"
    ]
  },
  {
    "objectID": "lectures/frameworks.html#pre-trained-models",
    "href": "lectures/frameworks.html#pre-trained-models",
    "title": "2 - Software & Hardware for Deep Learning",
    "section": "Pre-trained models",
    "text": "Pre-trained models\nSince training models can be time-consuming and expensive, pre-trained models are often used. They allow models to be adapted to a specific task more quickly and cost-effectively. In many areas, particularly NLP and computer vision, using pre-trained models is standard. PyTorch provides torchvision for computer vision applications. torchvision provides functionalities useful for modeling image data. Pre-trained models can also be easily integrated, as shown in the following example:\n\nfrom torchvision.models import resnet50, ResNet50_Weights\n\nweights = ResNet50_Weights.IMAGENET1K_V2\nmodel = resnet50(weights=weights)",
    "crumbs": [
      "Lectures",
      "2 - Software & Hardware for Deep Learning"
    ]
  },
  {
    "objectID": "lectures/frameworks.html#tensorflow",
    "href": "lectures/frameworks.html#tensorflow",
    "title": "2 - Software & Hardware for Deep Learning",
    "section": "TensorFlow",
    "text": "TensorFlow\nFor a long time, PyTorch and TensorFlow have been the biggest deep learning frameworks. TensorFlow stands out with a clean high-level API with Keras, which allows for easy implementation of complex models. Traditionally, TensorFlow is well established in the industry, while PyTorch is widely used in academia.",
    "crumbs": [
      "Lectures",
      "2 - Software & Hardware for Deep Learning"
    ]
  },
  {
    "objectID": "lectures/frameworks.html#jax",
    "href": "lectures/frameworks.html#jax",
    "title": "2 - Software & Hardware for Deep Learning",
    "section": "Jax",
    "text": "Jax\nJax has gained much popularity in recent years. It has been developed by people working for Google and is being used primarily in research settings. It is aking to numpy, however, with automatic-differentiation.",
    "crumbs": [
      "Lectures",
      "2 - Software & Hardware for Deep Learning"
    ]
  },
  {
    "objectID": "lectures/frameworks.html#scikit-learn",
    "href": "lectures/frameworks.html#scikit-learn",
    "title": "2 - Software & Hardware for Deep Learning",
    "section": "Scikit-Learn",
    "text": "Scikit-Learn\nScikit-Learn is THE machine learning framework in Python. However, Scikit-Learn never covered the area of neural networks and lacks auto-diff functionality. Therefore, Scikit-Learn is irrelevant when training neural networks. However, Scikit-Learn functionalities are often used to carry out the machine learning process, such as splitting datasets into train, validation, and test sets. Also, visualizations, such as the confusion matrix or calculating metrics, can be done via Scikit-Learn.",
    "crumbs": [
      "Lectures",
      "2 - Software & Hardware for Deep Learning"
    ]
  },
  {
    "objectID": "lectures/frameworks.html#onnx",
    "href": "lectures/frameworks.html#onnx",
    "title": "2 - Software & Hardware for Deep Learning",
    "section": "ONNX",
    "text": "ONNX\nONNX is an open format to represent machine learning models. It allows models trained in one framework to be transferred to another. Trained models can also be deployed on various platforms.",
    "crumbs": [
      "Lectures",
      "2 - Software & Hardware for Deep Learning"
    ]
  },
  {
    "objectID": "lectures/frameworks.html#monitoring",
    "href": "lectures/frameworks.html#monitoring",
    "title": "2 - Software & Hardware for Deep Learning",
    "section": "Monitoring",
    "text": "Monitoring\nWhen training models, monitoring the training process, debugging, and logging hyperparameters, metrics, etc., is very important. Various tools enable these functionalities. Well-known examples are TensorBoard and Weights & Biases.",
    "crumbs": [
      "Lectures",
      "2 - Software & Hardware for Deep Learning"
    ]
  },
  {
    "objectID": "lectures/frameworks.html#tensor-operations",
    "href": "lectures/frameworks.html#tensor-operations",
    "title": "2 - Software & Hardware for Deep Learning",
    "section": "Tensor Operations",
    "text": "Tensor Operations\nIn neural networks, there are many tensor operations. Tensors are essentially multi-dimensional arrays, such as a scalar \\(x\\), a vector \\(\\mathbf{x}\\), or a matrix \\(\\mathbf{X}\\).\nFigure 4 illustrates a matrix multiplication, a typical representative of a tensor operation. As you can see, the calculations (entries of the matrix \\(\\mathbf{A}\\mathbf{C}\\)) are independent of each other and can be fully parallelized.\n\n\n\n\n\n\nFigure 4: Matrix Multiplication (from Li (2022)).",
    "crumbs": [
      "Lectures",
      "2 - Software & Hardware for Deep Learning"
    ]
  },
  {
    "objectID": "lectures/frameworks.html#graphics-processing-units-gpus",
    "href": "lectures/frameworks.html#graphics-processing-units-gpus",
    "title": "2 - Software & Hardware for Deep Learning",
    "section": "Graphics Processing Units (GPUs)",
    "text": "Graphics Processing Units (GPUs)\nGPUs have made deep learning possible in the first place. With their parallel structure, they can efficiently compute parallelizable tasks such as tensor operations.\nCPUs have far fewer cores than GPUs, but they are faster and can handle more complex tasks. CPUs are therefore ideal for sequential tasks. GPUs have many more cores, which are less complex and slower. Therefore, GPUs are excellent for parallel tasks. Figure 5 illustrates the differences.\n\n\n\n\n\n\nFigure 5: CPU vs GPU example (from Li (2022)).",
    "crumbs": [
      "Lectures",
      "2 - Software & Hardware for Deep Learning"
    ]
  },
  {
    "objectID": "lectures/frameworks.html#cuda-cudnn",
    "href": "lectures/frameworks.html#cuda-cudnn",
    "title": "2 - Software & Hardware for Deep Learning",
    "section": "CUDA & cuDNN",
    "text": "CUDA & cuDNN\nCUDA is an API by Nvidia to perform computations on the GPU. It allows parallelizable tasks to be implemented efficiently. cuDNN is a library that efficiently executes certain operations, such as convolutions, in neural networks on the GPU. cuDNN is based on CUDA and significantly accelerates the training of neural networks. Figure 6 illustrates speed differences when training various neural networks with CPU, GPU, and optimized cuDNN.\n\n\n\n\n\n\nFigure 6: Speed comparison (from Li (2022), data from Link)",
    "crumbs": [
      "Lectures",
      "2 - Software & Hardware for Deep Learning"
    ]
  },
  {
    "objectID": "lectures/frameworks.html#data-loading",
    "href": "lectures/frameworks.html#data-loading",
    "title": "2 - Software & Hardware for Deep Learning",
    "section": "Data Loading",
    "text": "Data Loading\nA crucial bottleneck in practice is the transfer of data (such as images) from the disk to the GPU. If this transfer is not fast enough, it is referred to as GPU starvation. There are several approaches to solve this problem:\n\nRead the data into RAM (not feasible for larger datasets)\nUse fast disks, such as SSDs\nUtilize multiple CPU threads to read data in parallel and keep it in RAM (pre-fetching)\n\nFigure 7 shows the various components.\n\n\n\n\n\n\nFigure 7: Source: Li (2022)\n\n\n\nDeep learning frameworks like PyTorch implement special classes that allow data to be prepared in multiple threads. Sometimes a certain number of CPU cores is needed to supply a GPU with enough data. Figure 8 shows a starved GPU: You can clearly see that the utilization repeatedly drops to 0 because the GPU has to wait for data.\n\n\n\n\n\n\nFigure 8: The Y-axis shows the GPU utilization in percentage, while the X-axis represents time. Source",
    "crumbs": [
      "Lectures",
      "2 - Software & Hardware for Deep Learning"
    ]
  },
  {
    "objectID": "lectures/frameworks.html#gpu-parallelism",
    "href": "lectures/frameworks.html#gpu-parallelism",
    "title": "2 - Software & Hardware for Deep Learning",
    "section": "GPU Parallelism",
    "text": "GPU Parallelism\nModels can also be trained on multiple GPUs. There are two main paradigms: data parallelism and model parallelism (see Figure 9 ). With data parallelism, each GPU has a copy of the model, and each GPU is trained on different data batches. With model parallelism, the model is split across multiple GPUs. Models can be trained on a server with multiple GPUs or even over the network (distributed). ML frameworks provide functionalities to handle these.\n\n\n\n\n\n\nFigure 9: Data and Model Parallelism (from Li (2022)).",
    "crumbs": [
      "Lectures",
      "2 - Software & Hardware for Deep Learning"
    ]
  },
  {
    "objectID": "lectures/neural_networks.html",
    "href": "lectures/neural_networks.html",
    "title": "3 - Neural Networks",
    "section": "",
    "text": "A biological neural network is a part of the nervous system and consists of interconnected neurons. A neuron is connected to other neurons via dendrites (these are “weighted” input signals) and via the axon (output signal) (see Figure 1). If the input signals exceed a certain threshold, the neuron “fires” and sends a signal through the axon, which then serves as an input signal for other neurons. Humans have about 86 billion neurons, each connected to about 1000 others on average (source).\n\n\n\n\n\n\nFigure 1: Schematic representation of connected neurons. Phillips (2015)\n\n\n\nOptical signals are processed, among other things, in the visual cortex (Figure 2). Signals are processed hierarchically, with the first layers recognizing simple patterns and later layers recognizing shapes and objects. See also the work of Hubel and Wiesel Hubel and Wiesel (1959).\n\n\n\n\n\n\nFigure 2: Representation of transformations in the visual cortex. Kubilius (2017)\n\n\n\nThe question that arises is:\nCan we create artificial neural networks and reproduce the performance of biological neural networks?",
    "crumbs": [
      "Lectures",
      "3 - Neural Networks"
    ]
  },
  {
    "objectID": "lectures/neural_networks.html#biological-neural-networks",
    "href": "lectures/neural_networks.html#biological-neural-networks",
    "title": "3 - Neural Networks",
    "section": "",
    "text": "A biological neural network is a part of the nervous system and consists of interconnected neurons. A neuron is connected to other neurons via dendrites (these are “weighted” input signals) and via the axon (output signal) (see Figure 1). If the input signals exceed a certain threshold, the neuron “fires” and sends a signal through the axon, which then serves as an input signal for other neurons. Humans have about 86 billion neurons, each connected to about 1000 others on average (source).\n\n\n\n\n\n\nFigure 1: Schematic representation of connected neurons. Phillips (2015)\n\n\n\nOptical signals are processed, among other things, in the visual cortex (Figure 2). Signals are processed hierarchically, with the first layers recognizing simple patterns and later layers recognizing shapes and objects. See also the work of Hubel and Wiesel Hubel and Wiesel (1959).\n\n\n\n\n\n\nFigure 2: Representation of transformations in the visual cortex. Kubilius (2017)\n\n\n\nThe question that arises is:\nCan we create artificial neural networks and reproduce the performance of biological neural networks?",
    "crumbs": [
      "Lectures",
      "3 - Neural Networks"
    ]
  },
  {
    "objectID": "lectures/neural_networks.html#artificial-neural-networks",
    "href": "lectures/neural_networks.html#artificial-neural-networks",
    "title": "3 - Neural Networks",
    "section": "Artificial Neural Networks",
    "text": "Artificial Neural Networks\nArtificial neural networks are models of biological networks. Such models were created and implemented technically as early as the 1940s.\nNeural networks are often represented with a graph. The nodes are individual neurons in the network, and the edges are connections between the neurons (see Figure 3). The neurons are arranged in layers, with each neuron in a layer connected to every neuron in adjacent layers. The input layer represents the data, the output layer the (observable) outputs, and the hidden layers are within the network. The connections between the neurons are weighted.\n\n\n\n\n\n\nFigure 3: A neural network with two hidden layers. The lines show connections between neurons. Source: Li (2022).",
    "crumbs": [
      "Lectures",
      "3 - Neural Networks"
    ]
  },
  {
    "objectID": "lectures/neural_networks.html#from-linear-models-to-neural-networks",
    "href": "lectures/neural_networks.html#from-linear-models-to-neural-networks",
    "title": "3 - Neural Networks",
    "section": "From Linear Models to Neural Networks",
    "text": "From Linear Models to Neural Networks\nIn the following, we will describe neural networks mathematically. We start with a linear model. A linear model has the following form:\n\\[\\begin{equation}\n   f(\\mathbf{x}^{(i)}) = \\mathbf{W} \\mathbf{x}^{(i)}  +  \\mathbf{b}\n\\end{equation}\\]\nThe data point \\(i\\) is:\n\\[\\begin{equation}\n    \\mathbf{x}^{(i)} \\in \\mathbb{R}^{p \\times 1}\n\\end{equation}\\]\nAnd model weights:\n\\[\\begin{equation}\n   \\mathbf{W} \\in \\mathbb{R}^{k \\times p}\n\\end{equation}\\]\nAs well as a bias term:\n\\[\\begin{equation}\n   \\mathbf{b} \\in \\mathbb{R}^{k \\times 1}\n\\end{equation}\\]\nThe following parameters must be learned from data using an optimization method: \\(\\mathbf{W}\\) and \\(\\mathbf{b}\\).\nIn neural networks, the linear model is extended with additional layers. The following equation defines a neural network with two layers.\n\\[\\begin{equation}\n   f(\\mathbf{x}^{(i)}) = \\mathbf{W}^{(2)} g\\big(\\mathbf{W}^{(1)} \\mathbf{x}^{(i)}  +  \\mathbf{b}^{(1)} \\big)  +  \\mathbf{b}^{(2)}\n\\end{equation}\\]\nWhere \\(g()\\) is a so-called activation function, such as the ReLU function:\n\\[\\begin{equation}\n\\text{ReLU}(x) = \\begin{cases}\nx, & \\text{if } x \\geq 0 \\\\\n0, & \\text{if } x &lt; 0\n\\end{cases}\n\\end{equation}\\]\nOnly due to the activation function are non-linear relationships modelable. Without the activation function, the model collapses to a simple linear model.\nEvery layer between the input and output layer is called a hidden layer.\nTypically, not just one data point is processed but a mini-batch of data, or even the entire dataset. The data points are arranged in a data matrix \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times p}\\). The linear model is then defined as follows:\n\\[\\begin{equation}\n   f( \\mathbf{X}) = \\mathbf{X} \\mathbf{W}^T  +  \\mathbf{b}^T\n\\end{equation}\\]\nThe bias term \\(\\mathbf{b}\\) is broadcasted (details here Link), so it can be added.\n\nActivation Functions\nThe following code shows how activation functions are important for modeling non-linear relationships. The model has a hidden layer with several neurons but (left) no activation function and (right) with ReLU activation.\n\n\n\n\n\n\n\n\nFigure 4: Linear (left) vs non-linear (right) activation function.\n\n\n\n\n\nFigure 5 now shows a neural network including activation functions (ReLU). Sometimes the biases are also shown as nodes that feed into the next layer.\n\n\n\n\n\n\nFigure 5: A neural network with a hidden layer. The lines show connections between neurons and their weights \\(w_{i,j}\\).\n\n\n\n\n\nUniversal Approximation Theorem\nWith a shallow neural network, any continuous function can be modeled with arbitrary accuracy (Universal Approximation Theorem). The following graphic illustrates that as the number of linear functions (and thus piecewise linear regions) increases, the approximation of the underlying function becomes more accurate.\n\n\n\n\n\n\nFigure 6: Approximation of a 1-D function with piecewise linear regions. The more regions, the more accurate the approximation. Source: Prince (2023)\n\n\n\nNeural networks are therefore a particularly powerful class of models!",
    "crumbs": [
      "Lectures",
      "3 - Neural Networks"
    ]
  },
  {
    "objectID": "lectures/neural_networks.html#deep-learning",
    "href": "lectures/neural_networks.html#deep-learning",
    "title": "3 - Neural Networks",
    "section": "Deep Learning",
    "text": "Deep Learning\nWhen there are multiple hidden layers, it is called deep learning. Fig. Figure 7 illustrates such a model with 5 hidden layers.\n\n\n\n\n\n\nFigure 7: Illustration of a deep learning model with 5 hidden layers, from Johnson (2022)\n\n\n\nSuch a model is also called a multilayer perceptron (MLP). It consists of linear layers and activation functions.\nDeep neural networks have more than one hidden layer. Although shallow neural networks can theoretically model arbitrarily complex functions, the number of neurons required is often impractical. It can be shown that adding hidden layers (increasing the depth of a network) is much more efficient (requires fewer neurons) for modeling complex functions.\nIn practice, it has been shown that the first layers in a network learn simple features. These are combined in deeper layers to learn and detect increasingly abstract concepts. This is well illustrated with images. When a neural network is applied to images, the first layers learn to detect simple features, such as edges and\ncolors. Further layers then detect shapes and objects, leading to specific items or people (see Fig. Figure 8).\n\n\n\n\n\n\nFigure 8: Hierarchical features, from Lee et al. (2011)\n\n\n\n\n\n\n\n\n\nInfo\nTo directly observe the functioning of a neural network and try out the influence of various configurations, you can train networks directly in the browser using the following link: TensorFlow Playground.",
    "crumbs": [
      "Lectures",
      "3 - Neural Networks"
    ]
  },
  {
    "objectID": "lectures/neural_networks.html#implementation-in-pytorch",
    "href": "lectures/neural_networks.html#implementation-in-pytorch",
    "title": "3 - Neural Networks",
    "section": "Implementation in PyTorch",
    "text": "Implementation in PyTorch\nThe following code shows how we can implement a configurable neural network.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MLP(nn.Module):\n    def __init__(self, input_size, hidden_layer_sizes, num_outputs):\n        super().__init__()\n\n        # Initialize submodules of your module - typically layers\n        # that your module needs - these can then be used in the\n        # forward pass\n\n        self.flatten = nn.Flatten()\n\n        # a ModuleList allows you to flexibly chain submodules\n        # in a list - depending e.g. on configuration parameters\n\n        self.hidden_layers = nn.ModuleList()\n        last_size = input_size\n        for size in hidden_layer_sizes:\n            self.hidden_layers.append(nn.Linear(last_size, size))\n            last_size = size\n\n        self.output_layer = nn.Linear(last_size, num_outputs)\n\n    def forward(self, x):\n        \"\"\"The forward pass of your module.\"\"\"\n        x = self.flatten(x)\n        for layer in self.hidden_layers:\n            x = F.relu(layer(x))\n        x = self.output_layer(x)\n        return x\n\n# Example usage:\n# Initialize the model with input size 784, one hidden layer of size 128, and 10 output units.\nmodel = MLP(784, [128], 10)\n\n# Example input vector (batch size of 32, input size of 784)\nx = torch.randn(32, 784)\n\n# Forward pass\noutput = model(x)\n\nWe can easily display the architecture details with torchinfo\n\n# Display the model architecture\nfrom torchinfo import summary\nsummary(model, input_size=(32, 784))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nMLP                                      [32, 10]                  --\n├─Flatten: 1-1                           [32, 784]                 --\n├─ModuleList: 1-2                        --                        --\n│    └─Linear: 2-1                       [32, 128]                 100,480\n├─Linear: 1-3                            [32, 10]                  1,290\n==========================================================================================\nTotal params: 101,770\nTrainable params: 101,770\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 3.26\n==========================================================================================\nInput size (MB): 0.10\nForward/backward pass size (MB): 0.04\nParams size (MB): 0.41\nEstimated Total Size (MB): 0.54\n==========================================================================================\n\n\n\n\n\n\n\n\nQuestion\nHow many parameters does this neural network have, and why?\n\n\n\n\n\n\n\n\n\nInfo\nTo refresh your knowledge on neural networks, it is worth watching the following videos: 3Blue1Brown - Neural Networks",
    "crumbs": [
      "Lectures",
      "3 - Neural Networks"
    ]
  },
  {
    "objectID": "lectures/neural_networks.html#references",
    "href": "lectures/neural_networks.html#references",
    "title": "3 - Neural Networks",
    "section": "References",
    "text": "References\n\n\nHubel, D. H., and T. N. Wiesel. 1959. “Receptive Fields of Single Neurones in the Cat’s Striate Cortex.” The Journal of Physiology 148 (3): 574–91. https://doi.org/10.1113/jphysiol.1959.sp006308.\n\n\nJohnson, Justin. 2022. “EECS 498.008 / 598.008 Deep Learning for Computer Vision.” Lecture {Notes} / {Slides}. https://web.eecs.umich.edu/~justincj/teaching/eecs498/WI2022/.\n\n\nKubilius, Jonas. 2017. “Ventral Visual Stream.” https://figshare.com/articles/figure/Ventral_visual_stream/106794.\n\n\nLee, Honglak, Roger Grosse, Rajesh Ranganath, and Andrew Y. Ng. 2011. “Unsupervised Learning of Hierarchical Representations with Convolutional Deep Belief Networks.” Communications of the ACM 54 (10): 95–103. https://doi.org/10.1145/2001269.2001295.\n\n\nLi, Fei-Fei. 2022. “CS231n Convolutional Neural Networks for Visual Recognition.” Lecture {Notes}. https://cs231n.github.io.\n\n\nPhillips, Devin K. 2015. “Speed of the Human Brain.” Ask A Biologist, May. https://askabiologist.asu.edu/plosable/speed-human-brain.\n\n\nPrince, Simon J. D. 2023. Understanding Deep Learning. MIT Press. https://udlbook.github.io/udlbook/.",
    "crumbs": [
      "Lectures",
      "3 - Neural Networks"
    ]
  },
  {
    "objectID": "lectures/recent_advances.html",
    "href": "lectures/recent_advances.html",
    "title": "7 - Recent Advances",
    "section": "",
    "text": "Foundation models are large-scale machine learning models trained on vast amounts of data that can be fine-tuned for various downstream tasks. These models have demonstrated remarkable capabilities in natural language processing, computer vision, and other fields.\n\n\n\nLarge-scale Pre-training: Foundation models are pre-trained on extensive datasets, enabling them to capture a wide range of knowledge.\nTransfer Learning: These models can be fine-tuned on specific tasks with relatively small datasets, making them versatile and efficient.\nMultimodal Capabilities: Some foundation models can process and integrate multiple types of data, such as text and images.\n\n\n\n\nCLIP (Contrastive Language-Image Pre-training, Radford et al. (2021)) is a foundation model developed by OpenAI. It is designed to understand images and text jointly, making it capable of tasks like zero-shot image classification.\n\n\n\n\n\n\n\n\nFigure 1: Illustration of CLIP. From Radford et al. (2021).\n\n\n\nCLIP is pre-trained on a diverse dataset of images and their corresponding textual descriptions. It learns to associate images with their textual descriptions using a contrastive learning approach, which maximizes the similarity between correct image-text pairs and minimizes the similarity between incorrect pairs.\n\n\n\n\n\n\n\n\n\nFigure 2: Zero-shot learning with CLIP. From Radford et al. (2021).\n\n\n\n\n\n\n\nZero-Shot Classification: CLIP can classify images into categories it has not explicitly been trained on by leveraging its understanding of language.\nImage Search: By inputting a textual description, CLIP can retrieve relevant images from a database.\n(near) Duplicate Detection: CLIP can assist in identifying near duplicate images.\n\n\n\n\n\nHere’s a simple example of using CLIP for zero-shot image classification:\n\nimport requests\nfrom PIL import Image\nfrom transformers import CLIPModel, CLIPProcessor\n\n# 1) Get Models and Data Pre-Processors\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n# 2) Define Input Data: Image and Texts\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprompt_template = \"a photo of a {}\"\nclasses = [\"cat\", \"dog\", \"car\", \"tree\"]\nprompts = [prompt_template.format(class_name) for class_name in classes]\n\n# 3) Pre-Process Inputs\ninputs = processor(\n    text=prompts,\n    images=image,\n    return_tensors=\"pt\",\n    padding=True,\n)\n\n# 4) Forward-Pass\nwith torch.no_grad():\n    outputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n\n# 5) Find most likely label\nprobs = logits_per_image.softmax(dim=1)\nprint(\"Label:\", classes[probs.argmax().item()])\n\n\n\nIf CLIP does not perform well with zero-shot learning, it can be fine-tuned with various techniques. This is a powerful mechanism because the features that CLIP has learned are very powerful and might be useful for your target task.",
    "crumbs": [
      "Lectures",
      "7 - Recent Advances"
    ]
  },
  {
    "objectID": "lectures/recent_advances.html#foundation-models",
    "href": "lectures/recent_advances.html#foundation-models",
    "title": "7 - Recent Advances",
    "section": "",
    "text": "Foundation models are large-scale machine learning models trained on vast amounts of data that can be fine-tuned for various downstream tasks. These models have demonstrated remarkable capabilities in natural language processing, computer vision, and other fields.\n\n\n\nLarge-scale Pre-training: Foundation models are pre-trained on extensive datasets, enabling them to capture a wide range of knowledge.\nTransfer Learning: These models can be fine-tuned on specific tasks with relatively small datasets, making them versatile and efficient.\nMultimodal Capabilities: Some foundation models can process and integrate multiple types of data, such as text and images.\n\n\n\n\nCLIP (Contrastive Language-Image Pre-training, Radford et al. (2021)) is a foundation model developed by OpenAI. It is designed to understand images and text jointly, making it capable of tasks like zero-shot image classification.\n\n\n\n\n\n\n\n\nFigure 1: Illustration of CLIP. From Radford et al. (2021).\n\n\n\nCLIP is pre-trained on a diverse dataset of images and their corresponding textual descriptions. It learns to associate images with their textual descriptions using a contrastive learning approach, which maximizes the similarity between correct image-text pairs and minimizes the similarity between incorrect pairs.\n\n\n\n\n\n\n\n\n\nFigure 2: Zero-shot learning with CLIP. From Radford et al. (2021).\n\n\n\n\n\n\n\nZero-Shot Classification: CLIP can classify images into categories it has not explicitly been trained on by leveraging its understanding of language.\nImage Search: By inputting a textual description, CLIP can retrieve relevant images from a database.\n(near) Duplicate Detection: CLIP can assist in identifying near duplicate images.\n\n\n\n\n\nHere’s a simple example of using CLIP for zero-shot image classification:\n\nimport requests\nfrom PIL import Image\nfrom transformers import CLIPModel, CLIPProcessor\n\n# 1) Get Models and Data Pre-Processors\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n# 2) Define Input Data: Image and Texts\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprompt_template = \"a photo of a {}\"\nclasses = [\"cat\", \"dog\", \"car\", \"tree\"]\nprompts = [prompt_template.format(class_name) for class_name in classes]\n\n# 3) Pre-Process Inputs\ninputs = processor(\n    text=prompts,\n    images=image,\n    return_tensors=\"pt\",\n    padding=True,\n)\n\n# 4) Forward-Pass\nwith torch.no_grad():\n    outputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n\n# 5) Find most likely label\nprobs = logits_per_image.softmax(dim=1)\nprint(\"Label:\", classes[probs.argmax().item()])\n\n\n\nIf CLIP does not perform well with zero-shot learning, it can be fine-tuned with various techniques. This is a powerful mechanism because the features that CLIP has learned are very powerful and might be useful for your target task.",
    "crumbs": [
      "Lectures",
      "7 - Recent Advances"
    ]
  },
  {
    "objectID": "lectures/recent_advances.html#multi-modal-models",
    "href": "lectures/recent_advances.html#multi-modal-models",
    "title": "7 - Recent Advances",
    "section": "Multi-Modal Models",
    "text": "Multi-Modal Models\nMulti-modal models are able to process mutliple types of data (modes) simultaneously.\n\nVisual Question Anwering\nOne task where multi-modality is required, is Visual Question Answering (VQA). Only by jointly understanding the visual input and the question such tasks can be solved. See Figure 3 for an example from a chat system.\n\n\n\n\n\n\nFigure 3: BLIP2 examples from a chat system. Source: Li et al. (2023)\n\n\n\n\n\nHow VQA Works\nVQA models combine visual data (images) with textual data (questions) to generate accurate answers. These models are typically pre-trained on large datasets containing images, questions about those images, and the corresponding answers. Figure 4 shows training data examples that were used in the Flamingo model Alayrac et al. (2022). That particular model, and many other multi-modal models, is based on pre-trained foundation language and visual models and subsequently fine-tuned to learn how to combine their features for multi-modal tasks.\n\n\n\n\n\n\nFigure 4: Training data examples. Source: Alayrac et al. (2022)\n\n\n\n\n\nApplications of VQA\n\nAccessibility: VQA can help visually impaired users by answering questions about their surroundings based on images captured by a camera.\nChat Bots with Visual Understanding: VQA systems can be used in chat bots to incorporate visual inputs.\nCustomer Support: VQA can enhance customer support by allowing users to submit images and ask questions about products or services.",
    "crumbs": [
      "Lectures",
      "7 - Recent Advances"
    ]
  },
  {
    "objectID": "lectures/recent_advances.html#vision-transfomers",
    "href": "lectures/recent_advances.html#vision-transfomers",
    "title": "7 - Recent Advances",
    "section": "Vision Transfomers",
    "text": "Vision Transfomers\nTransformers have emerged as foundational architectures in modern machine learning, significantly impacting NLP, computer vision, and multimodal tasks.\n\nWhat Are Transformers?\nTransformers, introduced by Vaswani et al. (2017), use a self-attention mechanism that enables the model to consider relationships between different parts of the input data. This enables transformers to process data in parallel and capture long-range dependencies more effectively than previous architectures (such as recurrent neural networks).\nKey Concepts in Transformers:\n\nSelf-Attention: Enables each input element to focus on all other elements, capturing complex dependencies within the data.\nPositional Encoding: Injects information about the order of the sequence, allowing the transformer to process sequential data despite its parallel structure.\nScalability: The architecture scales well, making it suitable for training on large datasets across various domains.\n\n\n\nVision Transformers (ViT)\nVision Transformers (ViTs), proposed by Dosovitskiy et al. (2020), apply the transformer architecture to image data by splitting images into fixed-size patches and treating each patch as a token in a sequence. This approach enables ViTs to capture spatial relationships in images without convolutional layers.\n\n\n\n\n\n\nFigure 5: An image is split into fixed size patches. Each patch is linearly transformed (patch embedding). According to the patch position in the image, a specific position embedding is added. The sequence of embeddings is then processed with a standard transfomer. Source: Dosovitskiy et al. (2020)\n\n\n\nApplications of Vision Transformers\n\nImage Classification: ViTs have achieved competitive results in image classification, sometimes outperforming CNNs.\nObject Detection and Segmentation: Transformers can be adapted for tasks like object detection by directly predicting bounding boxes, as shown in models like DETR (Carion et al. (2020)).\nMultimodal Models: Transformers enable models to integrate data from different sources, such as text and images, for tasks like image captioning and visual question answering.\n\n\n\nComparison: Vision Transformers and Convolutional Neural Networks\nViTs and CNNs are both prominent architectures in computer vision, each with unique strengths and weaknesses.\n\nArchitectural Differences\nCNNs are designed with built-in spatial inductive biases. Using convolutions assumes that neighbouring pixels are related and that local patterns are relevant (restricted receptive field) and should be detected everywhere in the input (weight sharing). This makes CNNs efficient with limited data and effective at capturing local image features. Local features can be aggregatred hierarchically to capture global patterns.\nViTs process images by dividing them into fixed-sized patches, treating each patch as a “token” akin to words in natural language processing. They then use self-attention mechanisms to capture relationships between all patches simultaneously, allowing the model to learn global dependencies without focusing exclusively on local information already in the first layer (no restrictions in the receptive field).\n\n\nScalability and Flexibility\nGenerally: inductive biases are beneficial because they do not need to be learned from data. However, inductive biases also restrict a models flexibility since such biases might not always be perfectly aligned with the true data distribution. While CNNs excel in low-data regimes and with resource-limited applications, ViTs are scalable and perform well with self-supervised learning on large datasets. ViTs have shown excellent performance on massive datasets (10M+ images), even surpassing CNNs when pretrained with sufficient data and augmented with techniques like self-supervised learning.\n\n\nRobustness and Transferability\nRobustness to noise and corruptions (see Figure 6), as well as the ability to transfer learned features to downstream tasks is highly relevant in practical settings. Some studies indicate that ViTs can handle image corruptions and data shifts more robustly than CNN (Naseer et al. (2021)), however, is contradicted by newer studies (Pinto, Torr, and Dokania (2022)).\n\n\n\n\n\n\nFigure 6: An image with different distortions to evaluate the robustness of methods. Source: Naseer et al. (2021).\n\n\n\n\n\nPros and Cons\nCNNs:\n\nPros:\n\nEfficiency for Small Datasets: CNNs excel in small to moderate datasets, as they are highly effective in learning spatial features with fewer parameters.\nRobustness to Variations: Their design enables in- and equivariance to small geometrical transformations, making them resilient in many real-world applications.\n\nCons:\n\nLimited Long-Range Dependencies: Due to their focus on local receptive fields, CNNs may struggle with capturing long-range dependencies unless deeper architectures are used.\n\n\nVision Transformers (ViTs):\n\nPros:\n\nScalability and Flexibility: ViTs perform well with large datasets and can be scaled efficiently by increasing model size, often benefiting from self-supervised pre-training on massive image corpora.\nLong-Range Dependency Capture: Self-attention mechanisms allow ViTs to capture long-range dependencies between patches, which can be beneficial in tasks requiring global context.\n\nCons:\n\nData Efficiency: ViTs often require significantly more data and pre-training to achieve competitive performance, as they lack the inherent spatial inductive bias of CNNs.\nComputational Intensity: Self-attention mechanisms, especially for high-resolution images, can be computationally demanding and require substantial memory, limiting their efficiency.\n\n\nIn summary, while CNNs remain highly effective for many computer vision tasks, especially when data or compute resources are limited, ViTs provide a scalable alternative that excels with larger datasets and tasks requiring a global view of image data.\n\n\n\nConclusion\nCurrently, transformers and CNNs perform similarily when pre-trained on massive datasets. There has been various researach comparing both architectures, none seems to be able to conclusively judge which architecture is superior (e.g. Smith et al. (2023)). The inductive biases of CNNs are often valuable, as is the ability of transformers to consider global relations early. This has led to the development of hybrid architectures that combine convolutional layers with transformer layers. It remains to be seen where this research leads.\n\n\n\n\n\n\nHere is an interesting blog post on the topic of which architecture to choose: Werff (2024).",
    "crumbs": [
      "Lectures",
      "7 - Recent Advances"
    ]
  },
  {
    "objectID": "lectures/recent_advances.html#references",
    "href": "lectures/recent_advances.html#references",
    "title": "7 - Recent Advances",
    "section": "References",
    "text": "References\n\n\nAlayrac, Jean-Baptiste, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, et al. 2022. “Flamingo: A Visual Language Model for Few-Shot Learning.” arXiv. http://arxiv.org/abs/2204.14198.\n\n\nCarion, Nicolas, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. 2020. “End-to-End Object Detection with Transformers.” arXiv. http://arxiv.org/abs/2005.12872.\n\n\nDosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2020. “An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.” arXiv:2010.11929 [Cs], October. http://arxiv.org/abs/2010.11929.\n\n\nLi, Junnan, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. “BLIP-2: Bootstrapping Language-Image Pre-Training with Frozen Image Encoders and Large Language Models.” arXiv. http://arxiv.org/abs/2301.12597.\n\n\nNaseer, Muzammal, Kanchana Ranasinghe, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. 2021. “Intriguing Properties of Vision Transformers.” arXiv. http://arxiv.org/abs/2105.10497.\n\n\nPinto, Francesco, Philip H. S. Torr, and Puneet K. Dokania. 2022. “An Impartial Take to the CNN Vs Transformer Robustness Contest.” arXiv. http://arxiv.org/abs/2207.11347.\n\n\nRadford, Alec, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, et al. 2021. “Learning Transferable Visual Models From Natural Language Supervision.” arXiv:2103.00020 [Cs], February. http://arxiv.org/abs/2103.00020.\n\n\nSmith, Samuel L., Andrew Brock, Leonard Berrada, and Soham De. 2023. “ConvNets Match Vision Transformers at Scale.” arXiv. http://arxiv.org/abs/2310.16764.\n\n\nVaswani, Ashish, Google Brain, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” Advances in Neural Information Processing Systems, no. Nips: 5998–6008. https://arxiv.org/abs/1706.03762.\n\n\nWerff, Tobias van der. 2024. “CNN Vs. Vision Transformer: A Practitioner’s Guide to Selecting the Right Model.” https://tobiasvanderwerff.github.io/2024/05/15/cnn-vs-vit.html.",
    "crumbs": [
      "Lectures",
      "7 - Recent Advances"
    ]
  },
  {
    "objectID": "pages/demos.html",
    "href": "pages/demos.html",
    "title": "Demos",
    "section": "",
    "text": "CNN Filters Visualization\nVisualize filters and their effect of a pre-trained ResNet-18.\n\n  \n\n\n\nCLIP Demo\nThis model can calculate similarities between and among images and texts. It can be used for zero-shot (no labels) image classification.\n\n  \n\n\n\nVisual Question Answering\nTest a model that can answer questions given an image. The notebook contains a small model which can be run on cpu and a much larger model which ideally is run on GPU."
  },
  {
    "objectID": "pages/links.html",
    "href": "pages/links.html",
    "title": "Helpful Links & Resources",
    "section": "",
    "text": "Links and ressources to different topics related to Machine Learning, Deep Learning, and Images.",
    "crumbs": [
      "Resources",
      "Helpful Links & Resources"
    ]
  },
  {
    "objectID": "pages/links.html#theory",
    "href": "pages/links.html#theory",
    "title": "Helpful Links & Resources",
    "section": "Theory",
    "text": "Theory\n\nPyTorch\nPyTorch internals - Blog Post\n\n\nDeep Learning and Computer Vision\nUniversity of Michigan - Deep Learning for Computer Vision\n\nSehr gute Vorlesung zum Thema\n\nUniversity of California, Berkeley - Modern Computer Vision and Deep Learning\n\nSehr gute Vorlesung zum Thema\n\n\n\nNeuronale Netzwerke - Basics\nPerceptron Learning Rule S. Raschka\nCS229 Stanford MLP Backpropagation\nNotes on Backpropagation\n3Blue1Brown Gradient Descent\n3Blue1Brown Backpropagation Calculus\nAndrew Ng Backprop\nAndrej Karpathy - Backpropagation from the ground up\n\n\nModel Selection\nPaper von S.Raschka: “Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning”",
    "crumbs": [
      "Resources",
      "Helpful Links & Resources"
    ]
  },
  {
    "objectID": "pages/links.html#practical",
    "href": "pages/links.html#practical",
    "title": "Helpful Links & Resources",
    "section": "Practical",
    "text": "Practical\nAndrej Karpathy - A Recipe for Training Neural Networks\n\nML Best Practices Videos\nMartin Zinkevich - Best Practices for ML Engineering\nAndrew Ng - Advice For Applying Machine Learning | Deciding What To Try Next\nAndrew Ng - Advice For Applying Machine Learning | Learning Curves\nAndrew Ng - Advice For Applying Machine Learning | Deciding What To Do Next (Revisited)\nAndrew Ng - Machine Learning System Design | Prioritizing What To Work On\nAndrew Ng - Machine Learning System Design | Error Analysis\nAndrew Ng - Machine Learning System Design | Data For Machine Learning",
    "crumbs": [
      "Resources",
      "Helpful Links & Resources"
    ]
  },
  {
    "objectID": "pages/links.html#tools",
    "href": "pages/links.html#tools",
    "title": "Helpful Links & Resources",
    "section": "Tools",
    "text": "Tools\n\nData Science Repository\nBuild a Reproducible and Maintainable Data Science Project\n\ngreat jupyter book to learen about how to structure a repository and more\n\nLightning-Hydra-Template\n\ntemplate to strcuture a repository based on experiment configuration with Hydra and Pytorch-Lightning\n\n\n\nData Handling\ndatasets\n\nGreat package to create and manage (large) image datasets\n\nimg2dataset\n\nPackage to download large image datasets from urls\n\nDVC\n\nPackage for data version control\n\n\n\nPyTorch\nLightning\n\nboilerplate code to easily train models and use gpu, etc.",
    "crumbs": [
      "Resources",
      "Helpful Links & Resources"
    ]
  },
  {
    "objectID": "pages/notation.html",
    "href": "pages/notation.html",
    "title": "Mathematical Notation",
    "section": "",
    "text": "Syntax\nDescription\n\n\n\n\n\\(a\\)\nA scalar (integer or real)\n\n\n\\(\\mathbf{a}\\)\nA vector\n\n\n\\(\\mathbf{A}\\)\nA matrix\n\n\n\\(\\mathbf{\\mathsf{A}}\\)\nA tensor\n\n\n\\(\\mathbf{I}_n\\)\nIdentity matrix with \\(n\\) rows and \\(n\\) columns\n\n\n\\(\\mathbf{I}\\)\nIdentity matrix with dimensionality implied by context\n\n\n\\(\\mathbf{e}^{(i)}\\)\nStandard basis vector \\([0,\\dots,0,1,0,\\dots,0]\\) with a 1 at position \\(i\\)\n\n\n\\(\\text{diag}(\\mathbf{a})\\)\nA square, diagonal matrix with diagonal entries given by \\(\\mathbf{a}\\)\n\n\n\\(\\textnormal{a}\\)\nA scalar random variable\n\n\n\\(\\mathbf{a}\\)\nA vector-valued random variable\n\n\n\\(\\mathbf{A}\\)\nA matrix-valued random variable\n\n\n\\(\\theta\\)\nParameters of a model\n\n\n\\(f(\\theta, \\mathbf{x})\\)\nA function (model) with paramters \\(\\theta\\) and data \\(\\mathbf{x}\\)\n\n\n\\(\\mathbf{A} \\odot \\mathbf{B}\\)\nElement-wise (Hadamard) product of \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\)",
    "crumbs": [
      "Resources",
      "Mathematical Notation"
    ]
  },
  {
    "objectID": "pages/notation.html#numbers-and-arrays",
    "href": "pages/notation.html#numbers-and-arrays",
    "title": "Mathematical Notation",
    "section": "",
    "text": "Syntax\nDescription\n\n\n\n\n\\(a\\)\nA scalar (integer or real)\n\n\n\\(\\mathbf{a}\\)\nA vector\n\n\n\\(\\mathbf{A}\\)\nA matrix\n\n\n\\(\\mathbf{\\mathsf{A}}\\)\nA tensor\n\n\n\\(\\mathbf{I}_n\\)\nIdentity matrix with \\(n\\) rows and \\(n\\) columns\n\n\n\\(\\mathbf{I}\\)\nIdentity matrix with dimensionality implied by context\n\n\n\\(\\mathbf{e}^{(i)}\\)\nStandard basis vector \\([0,\\dots,0,1,0,\\dots,0]\\) with a 1 at position \\(i\\)\n\n\n\\(\\text{diag}(\\mathbf{a})\\)\nA square, diagonal matrix with diagonal entries given by \\(\\mathbf{a}\\)\n\n\n\\(\\textnormal{a}\\)\nA scalar random variable\n\n\n\\(\\mathbf{a}\\)\nA vector-valued random variable\n\n\n\\(\\mathbf{A}\\)\nA matrix-valued random variable\n\n\n\\(\\theta\\)\nParameters of a model\n\n\n\\(f(\\theta, \\mathbf{x})\\)\nA function (model) with paramters \\(\\theta\\) and data \\(\\mathbf{x}\\)\n\n\n\\(\\mathbf{A} \\odot \\mathbf{B}\\)\nElement-wise (Hadamard) product of \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\)",
    "crumbs": [
      "Resources",
      "Mathematical Notation"
    ]
  },
  {
    "objectID": "pages/notation.html#indexing",
    "href": "pages/notation.html#indexing",
    "title": "Mathematical Notation",
    "section": "Indexing",
    "text": "Indexing\n\n\n\n\n\n\n\nSyntax\nDescription\n\n\n\n\n\\(a_i\\)\nElement \\(i\\) of vector \\(\\mathbf{a}\\), with indexing starting at 1\n\n\n\\(A_{i,j}\\)\nElement \\(i, j\\) of matrix \\(\\mathbf{A}\\)",
    "crumbs": [
      "Resources",
      "Mathematical Notation"
    ]
  },
  {
    "objectID": "pages/notation.html#datasets-and-distributions",
    "href": "pages/notation.html#datasets-and-distributions",
    "title": "Mathematical Notation",
    "section": "Datasets and Distributions",
    "text": "Datasets and Distributions\n\n\n\n\n\n\n\nSyntax\nDescription\n\n\n\n\n\\(\\mathbf{X}\\)\nThe design matrix with dimensionality \\(nxp\\) with \\(n\\) samples with \\(p\\) features.\n\n\n\\(\\mathbf{x}^{(i)}\\)\nThe i-th training example.\n\n\n\\(\\mathbf{y}^{(i)}\\)\nThe label-vector for the i-th training example.\n\n\n\\(y^{(i)}\\)\nThe label for the i-th training example.",
    "crumbs": [
      "Resources",
      "Mathematical Notation"
    ]
  },
  {
    "objectID": "pages/notation.html#probability-theory",
    "href": "pages/notation.html#probability-theory",
    "title": "Mathematical Notation",
    "section": "Probability Theory",
    "text": "Probability Theory\n\n\n\n\n\n\n\nSyntax\nDescription\n\n\n\n\n\\(P(x)\\)\nA probability distribution over a discrete variable.\n\n\n\\(p(x)\\)\nA probability distribution over a contiuous variable or over a variable whose type has not been specified.\n\n\n\\(\\mathbb{E}_{x \\sim P} [ f(x) ]\\text{ or } \\mathbb{E} f(x)\\)\nExpectation of \\(f(x)\\) with respect to \\(P(x)\\)\n\n\n\\(\\mathcal{N} ( \\mathbf{x} ; \\mu , \\Sigma)\\)\nGaussian distribution over \\(\\mathbf{x}\\) with mean \\(\\mu\\) and covariance \\(\\Sigma\\)\n\n\n\\(x \\sim \\mathcal{N} (\\mu , \\sigma)\\)\nGaussian distribution over \\(x\\) with mean \\(\\mu\\) and variance \\(\\sigma\\)",
    "crumbs": [
      "Resources",
      "Mathematical Notation"
    ]
  },
  {
    "objectID": "pages/notation.html#calculus",
    "href": "pages/notation.html#calculus",
    "title": "Mathematical Notation",
    "section": "Calculus",
    "text": "Calculus\n\n\n\n\n\n\n\nSyntax\nDescription\n\n\n\n\n\\(\\nabla_{\\mathbf{w}} J\\)\nGradient of \\(J\\) with respect to \\(\\mathbf{w}\\)\n\n\n\\(\\frac{\\partial J}{\\partial w}\\)\nPartial derivative of \\(J\\) with respect to \\(w\\)",
    "crumbs": [
      "Resources",
      "Mathematical Notation"
    ]
  },
  {
    "objectID": "pages/notation.html#functions",
    "href": "pages/notation.html#functions",
    "title": "Mathematical Notation",
    "section": "Functions",
    "text": "Functions\n\n\n\nSyntax\nDescription\n\n\n\n\n\\(\\log x\\)\nThe natural logarithm of \\(x\\).\n\n\n\\(\\lVert \\mathbf{x} \\rVert_p\\)\n\\(L^p\\) norm of \\(\\mathbf{x}\\)\n\n\n\\(\\lVert \\mathbf{x} \\rVert\\)\n\\(L^2\\) norm of \\(\\mathbf{x}\\)",
    "crumbs": [
      "Resources",
      "Mathematical Notation"
    ]
  },
  {
    "objectID": "pages/notation.html#deep-learning",
    "href": "pages/notation.html#deep-learning",
    "title": "Mathematical Notation",
    "section": "Deep Learning",
    "text": "Deep Learning\n\n\n\n\n\n\n\nSyntax\nDescription\n\n\n\n\nNCHW\nThe input format of images and activations in PyTorch. N: number of images (batch size), C: number of channels, H: height, W: width",
    "crumbs": [
      "Resources",
      "Mathematical Notation"
    ]
  },
  {
    "objectID": "slides/cnns.html#overview",
    "href": "slides/cnns.html#overview",
    "title": "Convolutional Neural Networks",
    "section": "Overview",
    "text": "Overview\n\nIntroduction & Motivation\nConvolutional Layers\nProperties\nVariants and Layers\nVisualizations and Architectures",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#properties-of-image-data",
    "href": "slides/cnns.html#properties-of-image-data",
    "title": "Convolutional Neural Networks",
    "section": "Properties of Image Data",
    "text": "Properties of Image Data\n\nHigh-Dimensional: An RGB image of size \\(224 \\times 224\\) (height, width) has = \\(150'528\\) values.\nLocality: Nearby pixels are statistically related\nStability under transformations: Interpretation of an image does not change under many geomoetric transformations.\n\n\nImage Source",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#multilayer-perceptron-and-images",
    "href": "slides/cnns.html#multilayer-perceptron-and-images",
    "title": "Convolutional Neural Networks",
    "section": "Multilayer-Perceptron and Images",
    "text": "Multilayer-Perceptron and Images\n\n\nMLPs deal with flat inputs: The spatial structure gets lost.",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#multilayer-perceptrons-and-high-dimensional-inputs",
    "href": "slides/cnns.html#multilayer-perceptrons-and-high-dimensional-inputs",
    "title": "Convolutional Neural Networks",
    "section": "Multilayer-Perceptrons and High-Dimensional Inputs",
    "text": "Multilayer-Perceptrons and High-Dimensional Inputs\n\nDimensionality of weight matrix \\(\\mathbf{W}\\) scales with input size. \\(\\mathbf{W} \\in \\mathbb{R}^{d \\times k}\\), while \\(d\\) the dimensionality of the inputs, and \\(k\\) the number of neurons in the first hidden layer. The columns of the weight matrix \\(\\mathbf{W}\\) (in the first hidden layer) can be visualized to learn what kind pattern a specific neuron has learned.",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#multilayer-perceptrons-and-translations",
    "href": "slides/cnns.html#multilayer-perceptrons-and-translations",
    "title": "Convolutional Neural Networks",
    "section": "Multilayer-Perceptrons and Translations",
    "text": "Multilayer-Perceptrons and Translations\n\nMLP and pattern shiftsOften the patterns we want to learn are not stationary. Ideally we want to recognize them at all positions, particularly under translations of the inputs, e.g. \\(g(\\mathbf{x})\\) where \\(g()\\) is a spatial translation.\n\nHow do MLPs deal with this?",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#multilayer-perceptrons-and-pattern-learning",
    "href": "slides/cnns.html#multilayer-perceptrons-and-pattern-learning",
    "title": "Convolutional Neural Networks",
    "section": "Multilayer-Perceptrons and Pattern Learning",
    "text": "Multilayer-Perceptrons and Pattern Learning\n\nMLP on example image.\nCan an MLP successfully learn patterns in images \\(\\mathbf{x}\\) that are permuted with a permutation matrix \\(\\mathbf{P}\\), i.e. \\(f(P(\\mathbf{x}))\\)?",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#mlps-and-images",
    "href": "slides/cnns.html#mlps-and-images",
    "title": "Convolutional Neural Networks",
    "section": "MLPs and Images",
    "text": "MLPs and Images\n\nHigh-Dimensional: The size of an MLP scales with the input dimensionality which might blow beyond memory and compute budgets.\nLocality: MLPs have no notion of locality and thus can’t exploit this inherent bias in natural images.\nStability under transformations: MLPs need to learn position-dependent patterns which is very inefficient.\n\n\nDemonstration: https://adamharley.com/nn_vis/mlp/2d.html\n\n\nWhat do we need?",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#invariance-and-equivariance",
    "href": "slides/cnns.html#invariance-and-equivariance",
    "title": "Convolutional Neural Networks",
    "section": "Invariance and Equivariance",
    "text": "Invariance and Equivariance\nFor many tasks small variations in the input should either not change the model output (invariance) or should change the output in tandem with the input changes (equivariance).\n\nA function \\(f(\\mathbf{x})\\) is invariant to a transformation \\(g(\\mathbf{x})\\) if:\n\\[\\begin{align}\nf(g(\\mathbf{x})) = f(\\mathbf{x})\n\\end{align}\\]\n\n\nA function \\(f(\\mathbf{x})\\) (such as a layer in a neural network) of an image \\(\\mathbf{x}\\) is equivariant with respect to a transformation \\(g(\\mathbf{\\mathbf{x}})\\) if:\n\\[\\begin{align}\nf(g(\\mathbf{x})) = g(f(\\mathbf{x}))\n\\end{align}\\]",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#example-invariance",
    "href": "slides/cnns.html#example-invariance",
    "title": "Convolutional Neural Networks",
    "section": "Example Invariance",
    "text": "Example Invariance\nExample where invariance is required:\n\nWhen objects in the input translate spatially, the output (in this case the classification of the images) does not change. The model \\(f(\\mathbf{x})\\) is thus invariant to spatial translations.",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#example-equivariance",
    "href": "slides/cnns.html#example-equivariance",
    "title": "Convolutional Neural Networks",
    "section": "Example Equivariance",
    "text": "Example Equivariance\nExample where equivariance is required:\n\nWhen objects in the input (first row) translate spatially, the detections (bounding boxes) change accordingly (bottom row). The model \\(f(\\mathbf{x})\\) that produces the bounding boxes is thus equivariant with respect to spatial translations.",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#history-experiments-on-cats",
    "href": "slides/cnns.html#history-experiments-on-cats",
    "title": "Convolutional Neural Networks",
    "section": "History: Experiments on Cats",
    "text": "History: Experiments on Cats\n\nIllustration Source",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#visual-cortex",
    "href": "slides/cnns.html#visual-cortex",
    "title": "Convolutional Neural Networks",
    "section": "Visual Cortex",
    "text": "Visual Cortex\n\nRepresentation of transformations in the visual cortex. Source: Kubilius (2017)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#convolutional-neural-networks",
    "href": "slides/cnns.html#convolutional-neural-networks",
    "title": "Convolutional Neural Networks",
    "section": "Convolutional Neural Networks",
    "text": "Convolutional Neural Networks\n\nThe activations of a ConvNet architecture. The input image is on the left, and the predictions are on the right. Source: Li (2022)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#convolution-operation",
    "href": "slides/cnns.html#convolution-operation",
    "title": "Convolutional Neural Networks",
    "section": "Convolution Operation",
    "text": "Convolution Operation\nConvolution in Deep Learning is typically implemented as cross-correlation.\n\\[\\begin{equation}\nS(i, j) = (K * I)(i, j) = b + \\sum_m \\sum_n I(i + m, j + n) K(m, n)\n\\end{equation}\\]\n\\(I\\) is the input (for example an image), \\(K\\) is the kernel (typically smaller than \\(I\\)) and \\(b\\) is a bias term which is being added to the weighted sum.\nIf \\(I\\) is an RGB image (in the first layer of a CNN for example), the kernel \\(K\\) would have dimensionality \\(3 \\times K \\times K\\) (assuming a square kernel). More generally we learn kernels of the dimensionality \\(C_{in} \\times K \\times K\\).\nMultiple kernels, let’s say \\(C_o\\) kernels, can be grouped together: \\(C_o \\times C_{in} \\times K \\times K\\).\nWe often refer to such tensors as filters or filter banks.",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#convolution-on-rgb-images",
    "href": "slides/cnns.html#convolution-on-rgb-images",
    "title": "Convolutional Neural Networks",
    "section": "Convolution on RGB Images",
    "text": "Convolution on RGB Images\n\nSource: Prince (2023)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#convolutional-layers-1",
    "href": "slides/cnns.html#convolutional-layers-1",
    "title": "Convolutional Neural Networks",
    "section": "Convolutional Layers",
    "text": "Convolutional Layers\n\nSource: Johnson (2019)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#convolutional-layers-2",
    "href": "slides/cnns.html#convolutional-layers-2",
    "title": "Convolutional Neural Networks",
    "section": "Convolutional Layers",
    "text": "Convolutional Layers\n\nSource: Johnson (2019)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#convolutional-layers-3",
    "href": "slides/cnns.html#convolutional-layers-3",
    "title": "Convolutional Neural Networks",
    "section": "Convolutional Layers",
    "text": "Convolutional Layers\n\nSource: Johnson (2019)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#convolutional-layers-4",
    "href": "slides/cnns.html#convolutional-layers-4",
    "title": "Convolutional Neural Networks",
    "section": "Convolutional Layers",
    "text": "Convolutional Layers\n\nSource: Johnson (2019)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#convolutional-layers-5",
    "href": "slides/cnns.html#convolutional-layers-5",
    "title": "Convolutional Neural Networks",
    "section": "Convolutional Layers",
    "text": "Convolutional Layers\n\n\nSource: Johnson (2019)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#padding-stride-kernel-size-dilation",
    "href": "slides/cnns.html#padding-stride-kernel-size-dilation",
    "title": "Convolutional Neural Networks",
    "section": "Padding, Stride, Kernel Size, Dilation",
    "text": "Padding, Stride, Kernel Size, Dilation\nThere are several options to parameterise a convolution.\n\nPadding: How much padding is added to the input?\nStride: What is the step size (stride) of the kernel?\nKernel-Size: What is the kernel size?\nDilation: What is the dilation rate?",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#padding",
    "href": "slides/cnns.html#padding",
    "title": "Convolutional Neural Networks",
    "section": "Padding",
    "text": "Padding\nWhat is the output dimensionality when convolving a 3x3 kernel?",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#padding-1",
    "href": "slides/cnns.html#padding-1",
    "title": "Convolutional Neural Networks",
    "section": "Padding",
    "text": "Padding\n\nLeft: Input (Yellow) with Zero-Padding (white border), Middle: Filter, Right: Output.",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#stride",
    "href": "slides/cnns.html#stride",
    "title": "Convolutional Neural Networks",
    "section": "Stride",
    "text": "Stride\n\nConvolution with stride (1, 1)The kernel is shifted by 1 pixel before each new computation.",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#stride-1",
    "href": "slides/cnns.html#stride-1",
    "title": "Convolutional Neural Networks",
    "section": "Stride",
    "text": "Stride\n\nConvolution with stride (1, 1)The kernel is shifted by 2 pixel before each new computation.",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#kernel-size",
    "href": "slides/cnns.html#kernel-size",
    "title": "Convolutional Neural Networks",
    "section": "Kernel-Size",
    "text": "Kernel-Size",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#kernel-size-1",
    "href": "slides/cnns.html#kernel-size-1",
    "title": "Convolutional Neural Networks",
    "section": "Kernel-Size",
    "text": "Kernel-Size\n\n\nWhat is the difference between using 3x3 vs 5x5 kernels? What is the same?",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#dilation",
    "href": "slides/cnns.html#dilation",
    "title": "Convolutional Neural Networks",
    "section": "Dilation",
    "text": "Dilation\n\n\nWhy would we use dilation \\(d \\gt 1\\)?",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#calculations-spatial-dimensionality-of-activation-maps",
    "href": "slides/cnns.html#calculations-spatial-dimensionality-of-activation-maps",
    "title": "Convolutional Neural Networks",
    "section": "Calculations: Spatial Dimensionality of Activation Maps",
    "text": "Calculations: Spatial Dimensionality of Activation Maps\nYou can calculate the dimensionality of the activation maps with the following formulas:\n\n\\(i\\): Side length of the input activations (assumption: square inputs)\n\\(k\\): Kernel size (assumption: square kernel)\n\\(o\\): Side length of the output activation maps\n\\(s\\): Stride (assumption: same stride along spatial dimensions)\n\\(p\\): Number of paddings on each side (assumption: same number of paddings along spatial dimensions)\n\\(d\\): Dilation rate.\n\n\n\\[\\begin{equation}\no = \\left\\lfloor \\frac{i + 2p - k-(k-1)(d-1)}{s} \\right\\rfloor + 1\n\\end{equation}\\]\nNote: If dilation is 1 \\((k-1)(d-1)\\) equals 0.",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#calculations-number-of-weights-in-a-convolutional-layer",
    "href": "slides/cnns.html#calculations-number-of-weights-in-a-convolutional-layer",
    "title": "Convolutional Neural Networks",
    "section": "Calculations: Number of Weights in a Convolutional Layer",
    "text": "Calculations: Number of Weights in a Convolutional Layer\nYou can calculate the total number of weights in a convolutional layer with the following formula:\n\n\\(k\\): Kernel size (assumption: square kernel, so kernel has dimensions \\(k \\times k\\))\n\\(C_{in}\\): Number of input channels\n\\(C_{out}\\): Number of output channels\n\n\n\\[\\begin{equation}\n\\text{Total Weights} = (k \\times k) \\times C_{in} \\times C_{out}\n\\end{equation}\\]\n\n\nEach filter has a size of \\(k \\times k \\times C_{in}\\), and there are \\(C_{out}\\) filters in total, resulting in \\((k \\times k \\times C_{in}) \\times C_{out}\\) weights.\n\n\nBiases: If each output channel has a bias term, add an additional \\(C_{out}\\) weights for the biases.\n\\[\\begin{equation}\n\\text{Total Parameters} = (k \\times k \\times C_{in} \\times C_{out}) + C_{out}\n\\end{equation}\\]",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#quiz",
    "href": "slides/cnns.html#quiz",
    "title": "Convolutional Neural Networks",
    "section": "Quiz",
    "text": "Quiz\nScenario:\n\nInput activations: \\(3 \\times 32 \\times 32\\) (C, H, W)\nConvolution: 10 filters with \\(5  \\times 5\\) kernel size, stride=1, pad=2\n\nQuestions:\n\nWhat is the size of the activation map?\nHow many weights / parameters defines this operation?\n\nFormulas:\n\n\\[\\begin{equation}\no = \\left\\lfloor \\frac{i + 2p - k-(k-1)(d-1)}{s} \\right\\rfloor + 1\n\\end{equation}\\]\n\\[\\begin{equation}\n\\text{Total Parameters} = (k \\times k \\times C_{in} \\times C_{out}) + C_{out}\n\\end{equation}\\]",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#sparse-connectivity-and-parameter-sharing",
    "href": "slides/cnns.html#sparse-connectivity-and-parameter-sharing",
    "title": "Convolutional Neural Networks",
    "section": "Sparse Connectivity and Parameter Sharing",
    "text": "Sparse Connectivity and Parameter Sharing\nLocal (Sparse) Connectivity: Neurons are only locally connected.\nParameter Sharing: Weights of a neuron are applied locally but are the same across the entire input.\n\nCan we recognize cats with a one-layer CNN?\nCan we recognize cats anywhere in an image with a CNN?\nIs parameter sharing always useful?",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#translation-invariance-equivariance",
    "href": "slides/cnns.html#translation-invariance-equivariance",
    "title": "Convolutional Neural Networks",
    "section": "Translation Invariance / Equivariance",
    "text": "Translation Invariance / Equivariance\nGiven a translation \\(g()\\), which spatially shifts inputs:\n\nTranslation invariance: \\(f(g(x))=f(x)\\)\nTranslation equivariance: \\(f(g(x))=g(f(x))\\)\n\n\nAre convolutions translation invariant, equivariant, or both?",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#stacking-convolutions",
    "href": "slides/cnns.html#stacking-convolutions",
    "title": "Convolutional Neural Networks",
    "section": "Stacking Convolutions",
    "text": "Stacking Convolutions\n\n\n\nSource: Johnson (2019)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#receptive-field",
    "href": "slides/cnns.html#receptive-field",
    "title": "Convolutional Neural Networks",
    "section": "Receptive Field",
    "text": "Receptive Field\n\nUsing a 5x5 kernel",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#receptive-field-1",
    "href": "slides/cnns.html#receptive-field-1",
    "title": "Convolutional Neural Networks",
    "section": "Receptive Field",
    "text": "Receptive Field\n\nUsing a 5x5 kernel and two layers",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#receptive-field-2",
    "href": "slides/cnns.html#receptive-field-2",
    "title": "Convolutional Neural Networks",
    "section": "Receptive Field",
    "text": "Receptive Field",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#receptive-field-3",
    "href": "slides/cnns.html#receptive-field-3",
    "title": "Convolutional Neural Networks",
    "section": "Receptive Field",
    "text": "Receptive Field",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#receptive-field-4",
    "href": "slides/cnns.html#receptive-field-4",
    "title": "Convolutional Neural Networks",
    "section": "Receptive Field",
    "text": "Receptive Field\nThere is a nice article about how to calculate the receptive field of a CNN by Araujo, Norris, and Sim (2019) (see also for more details about the notation).\nThe receptive field of a CNN \\(r_0\\):\n\\[\\begin{equation}\nr_0 = \\sum_{l=1}^L \\left( (k_l - 1) \\prod_{i=1}^{l-1} s_i \\right) + 1\n\\end{equation}\\]\n\n\\(L\\): Total number of layers.\n\\(k_l\\): Kernel size at layer \\(l\\).\n\\(s_i\\): Stride at layer \\(i\\), from layer 1 up to \\(l-1\\).\n\nIf all strides \\(s_i=1\\) the receptive field is the sum \\(k_l−1\\) over all layers plus 1. If all kernels are of size 1, the receptie field will be one.",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#architecture",
    "href": "slides/cnns.html#architecture",
    "title": "Convolutional Neural Networks",
    "section": "Architecture",
    "text": "Architecture\nSequence of layers and their hyper-parameters defines an architecture.\n\n\n\n\nVGG - Source: Prince (2023)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#learned-filters",
    "href": "slides/cnns.html#learned-filters",
    "title": "Convolutional Neural Networks",
    "section": "Learned Filters",
    "text": "Learned Filters\n\nSource: Krizhevsky, Sutskever, and Hinton (2012)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#lets-test-it",
    "href": "slides/cnns.html#lets-test-it",
    "title": "Convolutional Neural Networks",
    "section": "Let’s test it!",
    "text": "Let’s test it!\nLet’s train a CNN.\nHere is an interesting demo: https://poloclub.github.io/cnn-explainer/",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#downsampling",
    "href": "slides/cnns.html#downsampling",
    "title": "Convolutional Neural Networks",
    "section": "Downsampling",
    "text": "Downsampling\nSpatial downsampling is often a desired property of convolutional layers.\nWe can achieve that by choosing a stride \\(\\gt 1\\).\nHowever, there are other options, such as pooling layers.\n\na) Sub-Sampling, b) Max Pooling, c) Average Pooling . Source: Prince (2023)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#upsampling",
    "href": "slides/cnns.html#upsampling",
    "title": "Convolutional Neural Networks",
    "section": "Upsampling",
    "text": "Upsampling\nSometimes we need to increase the spatial dimensionality of our activation maps.\n\nWhat would be an example when we want to do that?\n\n\n\n\n\na) Duplicate, b) Max-Unpooling, c) Bilinear Interpolation . Source: Prince (2023)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#upsampling-with-transposed-convolutions",
    "href": "slides/cnns.html#upsampling-with-transposed-convolutions",
    "title": "Convolutional Neural Networks",
    "section": "Upsampling with Transposed Convolutions",
    "text": "Upsampling with Transposed Convolutions\n\nMore details in image segmentation!",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#changing-the-number-of-channels",
    "href": "slides/cnns.html#changing-the-number-of-channels",
    "title": "Convolutional Neural Networks",
    "section": "Changing the number of channels",
    "text": "Changing the number of channels\nSometimes one needs to just change the number of channels without spatial mixing. This can be achieved with 1x1 convolutions. \n\nSource: Prince (2023)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#global-average-pooling",
    "href": "slides/cnns.html#global-average-pooling",
    "title": "Convolutional Neural Networks",
    "section": "Global Average Pooling",
    "text": "Global Average Pooling\nGlobal Average Pooling is often an important component. It computes the average of the activations along the depth dimension, reducing activation maps from (C x H x W) to (C x 1 x 1). This is useful for directly modeling logits in a classification problem with C classes, enabling architectures that completely eliminate fully-connected layers.\n\nGlobal Average pooling, input (left) and output (right).",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/cnns.html#other-layers",
    "href": "slides/cnns.html#other-layers",
    "title": "Convolutional Neural Networks",
    "section": "Other Layers",
    "text": "Other Layers\nThere are many other types of layers, also there are more flavours of convolutional layers.",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "slides/image_classification.html#overview",
    "href": "slides/image_classification.html#overview",
    "title": "Image Classification",
    "section": "Overview",
    "text": "Overview\n\nIntroduction\nModeling\nLoss Function\nArchitectures",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides/image_classification.html#adversarial-panda",
    "href": "slides/image_classification.html#adversarial-panda",
    "title": "Image Classification",
    "section": "Adversarial Panda",
    "text": "Adversarial Panda\n\nSource: Goodfellow, Shlens, and Szegedy (2015)",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides/image_classification.html#adversarial-panda-1",
    "href": "slides/image_classification.html#adversarial-panda-1",
    "title": "Image Classification",
    "section": "Adversarial Panda",
    "text": "Adversarial Panda\n\nSource: Goodfellow, Shlens, and Szegedy (2015)",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides/image_classification.html#image-classification",
    "href": "slides/image_classification.html#image-classification",
    "title": "Image Classification",
    "section": "Image Classification",
    "text": "Image Classification\n\n\n\nExample of Image Classification.",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides/image_classification.html#image-classification-example",
    "href": "slides/image_classification.html#image-classification-example",
    "title": "Image Classification",
    "section": "Image Classification: Example",
    "text": "Image Classification: Example\n\n\n\nExample of Image Classification (from Krizhevsky, Sutskever, and Hinton (2012)).",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides/image_classification.html#image-classification-camera-traps",
    "href": "slides/image_classification.html#image-classification-camera-traps",
    "title": "Image Classification",
    "section": "Image Classification: Camera Traps",
    "text": "Image Classification: Camera Traps\n\n\n\nExample images from camera traps.",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides/image_classification.html#parametric-approach",
    "href": "slides/image_classification.html#parametric-approach",
    "title": "Image Classification",
    "section": "Parametric Approach",
    "text": "Parametric Approach\nWith a parametric approach, we seek a model of the following form:\n\\[\\begin{equation}\n    \\hat{y}^{(i)} = f(\\theta, \\mathbf{x}^{(i)})\n\\end{equation}\\]\nThe model parameters \\(\\theta\\) define our model and must be learned with an algorithm.",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides/image_classification.html#softmax-classifier",
    "href": "slides/image_classification.html#softmax-classifier",
    "title": "Image Classification",
    "section": "Softmax Classifier",
    "text": "Softmax Classifier\nWe want to model the following probability:\n\\[\\begin{equation}\n    P(Y=\\mathbf{y}^{(i)}| X = \\mathbf{x}^{(i)})\n\\end{equation}\\]\nTo obtain a valid probability distribution, the untransformed outputs \\(\\mathbf{z}\\), also called logits, of a model are transformed with the Softmax function \\(\\sigma(\\mathbf{z})\\).",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides/image_classification.html#softmax-transformation",
    "href": "slides/image_classification.html#softmax-transformation",
    "title": "Image Classification",
    "section": "Softmax Transformation",
    "text": "Softmax Transformation\n\\[\\begin{equation}\n    P(Y = k| X = \\mathbf{x}^{(i)}) = \\sigma(\\mathbf{z})_k = \\frac{e^{z_k}}{\\sum_i^K e^{z_i}}\n\\end{equation}\\]",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides/image_classification.html#logits-to-probabilities",
    "href": "slides/image_classification.html#logits-to-probabilities",
    "title": "Image Classification",
    "section": "Logits to Probabilities",
    "text": "Logits to Probabilities\n\n\n\nLogits (left) to probabilities with the Softmax function.",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides/image_classification.html#probabilities",
    "href": "slides/image_classification.html#probabilities",
    "title": "Image Classification",
    "section": "Probabilities",
    "text": "Probabilities\n\nImage classifier with confidences.",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides/image_classification.html#likelihood",
    "href": "slides/image_classification.html#likelihood",
    "title": "Image Classification",
    "section": "Likelihood",
    "text": "Likelihood\nThe likelihood of a data point:\n\\[\\begin{equation}\n    P(Y=y^{(i)}| X = \\mathbf{x}^{(i)}) = f(\\theta, \\mathbf{x}^{(i)})\n\\end{equation}\\]\nThis is the modeled probability for the actually observed class \\(y^{(i)}\\).",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides/image_classification.html#likelihood-for-multi-class-classification",
    "href": "slides/image_classification.html#likelihood-for-multi-class-classification",
    "title": "Image Classification",
    "section": "Likelihood for Multi-Class Classification",
    "text": "Likelihood for Multi-Class Classification\nThe likelihood of a data point for multi-class classification:\n\\[\\begin{equation}\n    \\prod_{j=1}^K P(Y = j| X = \\mathbf{x}^{(i)})^{y^{(i)}_j}\n\\end{equation}\\]\nWhere \\(y^{(i)} \\in \\mathbb{R}^{K}\\) is a one-hot encoded vector, with the \\(1\\) at the true class.",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides/image_classification.html#maximum-likelihood",
    "href": "slides/image_classification.html#maximum-likelihood",
    "title": "Image Classification",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\nThe likelihood of an entire dataset:\n\\[\\begin{equation}\n    \\prod_{i=1}^N \\prod_{j=1}^K P(Y = j| X = \\mathbf{x}^{(i)})^{y^{(i)}_j}\n\\end{equation}\\]\nUnder the maximum likelihood approach, we seek the parameters \\(\\theta\\) that maximize the likelihood of observing the dataset.",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides/image_classification.html#negative-log-likelihood",
    "href": "slides/image_classification.html#negative-log-likelihood",
    "title": "Image Classification",
    "section": "Negative Log-Likelihood",
    "text": "Negative Log-Likelihood\nEquivalently, we can minimize the negative log likelihood:\n\\[\\begin{align}\n    L(\\mathbf{X}, \\mathbf{y}, \\theta) =& - \\log \\prod_{i=1}^N \\prod_{j=1}^K  P(Y = j| X = \\mathbf{x}^{(i)})^{y^{(i)}_j} \\\\\n    L(\\mathbf{X}, \\mathbf{y}, \\theta) =& -\\sum_{i=1}^N \\sum_{j=1}^K y^{(i)}_j \\log  P(Y = j| X = \\mathbf{x}^{(i)})\n\\end{align}\\]",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides/image_classification.html#cross-entropy",
    "href": "slides/image_classification.html#cross-entropy",
    "title": "Image Classification",
    "section": "Cross-Entropy",
    "text": "Cross-Entropy\nThe loss function derived with maximum likelihood can also be viewed from the perspective of cross-entropy between two discrete probability distributions.\n\\[\\begin{align}\n    CE = - \\sum_{x \\in X} p(x) \\log q(x) \\\\\n    CE = - \\sum_{i=1}^N \\sum_{j=1}^K y_j^{(i)} \\log \\hat{y}_j^{(i)}\n\\end{align}\\]",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides/image_classification.html#cross-entropy-1",
    "href": "slides/image_classification.html#cross-entropy-1",
    "title": "Image Classification",
    "section": "Cross-Entropy",
    "text": "Cross-Entropy\n\nTrue distribution (left) and predicted distribution (right).",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides/image_classification.html#alexnet",
    "href": "slides/image_classification.html#alexnet",
    "title": "Image Classification",
    "section": "AlexNet",
    "text": "AlexNet\n\nAlexNet Krizhevsky, Sutskever, and Hinton (2012)",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides/image_classification.html#alexnet-1",
    "href": "slides/image_classification.html#alexnet-1",
    "title": "Image Classification",
    "section": "AlexNet",
    "text": "AlexNet\n\nAlexNet Prince (2023)",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides/image_classification.html#alexnet-table",
    "href": "slides/image_classification.html#alexnet-table",
    "title": "Image Classification",
    "section": "AlexNet: Table",
    "text": "AlexNet: Table\n\nSource: Johnson (2019)",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides/image_classification.html#vgg",
    "href": "slides/image_classification.html#vgg",
    "title": "Image Classification",
    "section": "VGG",
    "text": "VGG\n\nVGG Simonyan and Zisserman (2015)",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides/image_classification.html#vgg-1",
    "href": "slides/image_classification.html#vgg-1",
    "title": "Image Classification",
    "section": "VGG",
    "text": "VGG\n\n\nVGG Prince (2023)",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides/image_classification.html#vgg-2",
    "href": "slides/image_classification.html#vgg-2",
    "title": "Image Classification",
    "section": "VGG",
    "text": "VGG\n\n\n\nSource: Johnson (2019)",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides/image_classification.html#resnet",
    "href": "slides/image_classification.html#resnet",
    "title": "Image Classification",
    "section": "ResNet",
    "text": "ResNet\n\nSource: He et al. (2016)\nTest error for deeper model is larger. Overfitting?",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides/image_classification.html#resnet-1",
    "href": "slides/image_classification.html#resnet-1",
    "title": "Image Classification",
    "section": "ResNet",
    "text": "ResNet\n\nSource: He et al. (2016)\nTraining error for deeper model is also larger? What is going on?!",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides/image_classification.html#resnet-2",
    "href": "slides/image_classification.html#resnet-2",
    "title": "Image Classification",
    "section": "ResNet",
    "text": "ResNet\n\n\n\nResNet He et al. (2016) (Image from Johnson (2019))",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides/image_classification.html#resnet-3",
    "href": "slides/image_classification.html#resnet-3",
    "title": "Image Classification",
    "section": "ResNet",
    "text": "ResNet\n\n\n\nFrom He et al. (2016)",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides/image_classification.html#resnet-4",
    "href": "slides/image_classification.html#resnet-4",
    "title": "Image Classification",
    "section": "ResNet",
    "text": "ResNet\n\n\n\nFrom Li et al. (2018)",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides/image_classification.html#convnext",
    "href": "slides/image_classification.html#convnext",
    "title": "Image Classification",
    "section": "ConvNext",
    "text": "ConvNext\n\n\n\n\n\n\n\n\n\n\nFigures from Liu et al. (2022).",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides/image_classification.html#imagenet-performance",
    "href": "slides/image_classification.html#imagenet-performance",
    "title": "Image Classification",
    "section": "ImageNet Performance",
    "text": "ImageNet Performance\n\nImage from Prince (2023)",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides/image_classification.html#choosing-the-architecture",
    "href": "slides/image_classification.html#choosing-the-architecture",
    "title": "Image Classification",
    "section": "Choosing the Architecture",
    "text": "Choosing the Architecture\n\nDon’t be a hero!\n\nTypically, ResNet-50 or ResNet-101 are good choices. However, there are also models that require significantly fewer parameters, such as Efficient Nets.",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides/image_classification.html#pre-processing",
    "href": "slides/image_classification.html#pre-processing",
    "title": "Image Classification",
    "section": "Pre-Processing",
    "text": "Pre-Processing\n\nResizing / Cropping to a fixed size\nScaling: from the range [0, 255] to the range [0, 1]\nNormalization: Often normalized along the color channels\n\nPyTorch Examples",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides/image_classification.html#transfer-learning",
    "href": "slides/image_classification.html#transfer-learning",
    "title": "Image Classification",
    "section": "Transfer Learning",
    "text": "Transfer Learning\nTransfer learning refers to the process of adapting a trained model that models Task A to Task B. Adapting pre-trained models often leads to better results and also reduces the number of training iterations.",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "slides/practical.html#leaky-abstraction",
    "href": "slides/practical.html#leaky-abstraction",
    "title": "Practical Considerations",
    "section": "Leaky Abstraction",
    "text": "Leaky Abstraction\n\nyour_data = # plug your awesome dataset here\nmodel = SuperCrossValidator(SuperDuper.fit, your_data, ResNet50, SGDOptimizer)",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#silent-failure",
    "href": "slides/practical.html#silent-failure",
    "title": "Practical Considerations",
    "section": "Silent Failure",
    "text": "Silent Failure\nTraining neural networks fails silently!",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#get-to-know-the-data",
    "href": "slides/practical.html#get-to-know-the-data",
    "title": "Practical Considerations",
    "section": "1) Get to Know the Data",
    "text": "1) Get to Know the Data\nThoroughly inspect the data!",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#camera-traps-errors",
    "href": "slides/practical.html#camera-traps-errors",
    "title": "Practical Considerations",
    "section": "Camera Traps: Errors",
    "text": "Camera Traps: Errors\n\n\n\nExamples of images from camera traps.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#camera-traps-difficulties",
    "href": "slides/practical.html#camera-traps-difficulties",
    "title": "Practical Considerations",
    "section": "Camera Traps: Difficulties",
    "text": "Camera Traps: Difficulties\n\n\n\nExamples of images from camera traps. Source: Beery, Van Horn, and Perona (2018)",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#rare-classes",
    "href": "slides/practical.html#rare-classes",
    "title": "Practical Considerations",
    "section": "Rare Classes",
    "text": "Rare Classes\n\n\n\nAn image of a serval. Below are the model confidences.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#multiple-classes",
    "href": "slides/practical.html#multiple-classes",
    "title": "Practical Considerations",
    "section": "Multiple Classes",
    "text": "Multiple Classes\n\nExamples of an image from a camera trap with different species.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#baselines-1",
    "href": "slides/practical.html#baselines-1",
    "title": "Practical Considerations",
    "section": "2) Baselines",
    "text": "2) Baselines\nEvaluation pipeline, metrics, experiment tracking, and baseline model.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#ml-process",
    "href": "slides/practical.html#ml-process",
    "title": "Practical Considerations",
    "section": "ML Process",
    "text": "ML Process\n\n\n\nThe components of a typical machine learning process. Source: Raschka and Mirjalili (2020)",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#experiment-tracking",
    "href": "slides/practical.html#experiment-tracking",
    "title": "Practical Considerations",
    "section": "Experiment Tracking",
    "text": "Experiment Tracking\n\n\n\nWeights and Biases experiment tracking.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#baselines-2",
    "href": "slides/practical.html#baselines-2",
    "title": "Practical Considerations",
    "section": "2) Baselines",
    "text": "2) Baselines\nEnsure reproducibility.\nimport torch\ntorch.manual_seed(0)",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#baselines-3",
    "href": "slides/practical.html#baselines-3",
    "title": "Practical Considerations",
    "section": "2) Baselines",
    "text": "2) Baselines\nAvoid unnecessary techniques and complexities. Reduce error susceptibility.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#baselines-4",
    "href": "slides/practical.html#baselines-4",
    "title": "Practical Considerations",
    "section": "2) Baselines",
    "text": "2) Baselines\nIf possible, use a human baseline. How good can the model be?",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#difficult-cases",
    "href": "slides/practical.html#difficult-cases",
    "title": "Practical Considerations",
    "section": "Difficult Cases",
    "text": "Difficult Cases\n\nAn image from a camera trap that is difficult to classify. Here, annotators had a 96.6% agreement with experts.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#baselines-5",
    "href": "slides/practical.html#baselines-5",
    "title": "Practical Considerations",
    "section": "2) Baselines",
    "text": "2) Baselines\nTrain an input-independent baseline. Is the model learning anything at all?",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#baselines-6",
    "href": "slides/practical.html#baselines-6",
    "title": "Practical Considerations",
    "section": "2) Baselines",
    "text": "2) Baselines\nOverfit the model on a batch of data. Does the optimization work?",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#baselines-7",
    "href": "slides/practical.html#baselines-7",
    "title": "Practical Considerations",
    "section": "2) Baselines",
    "text": "2) Baselines\nVisualize what goes into the model. Is my preprocessing working?\ny_hat = model(x)",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#fixed-sample-segmentation-example",
    "href": "slides/practical.html#fixed-sample-segmentation-example",
    "title": "Practical Considerations",
    "section": "Fixed Sample: Segmentation Example",
    "text": "Fixed Sample: Segmentation Example\n\n\n\nExample of a segmentation problem: input on the left and output on the right.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#overfit-1",
    "href": "slides/practical.html#overfit-1",
    "title": "Practical Considerations",
    "section": "3) Overfit",
    "text": "3) Overfit\nAt this point, you should have a good understanding of the dataset, high confidence in the evaluation pipeline, and initial baselines from simple models. Now, look for a model that performs well on the training set.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#overfit-2",
    "href": "slides/practical.html#overfit-2",
    "title": "Practical Considerations",
    "section": "3) Overfit",
    "text": "3) Overfit\nLook for a good model architecture. Follow the principle “Don’t be a hero”. Prefer already implemented/established architectures.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#regularization-1",
    "href": "slides/practical.html#regularization-1",
    "title": "Practical Considerations",
    "section": "4) Regularization",
    "text": "4) Regularization\nAt this point, you should have achieved good performance on the training set. Now, focus on the validation set.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#regularization-2",
    "href": "slides/practical.html#regularization-2",
    "title": "Practical Considerations",
    "section": "4) Regularization",
    "text": "4) Regularization\nThe simplest measure to achieve better performance (and also reduce overfitting) is to collect more training data. However, this is often expensive!",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#learning-curve",
    "href": "slides/practical.html#learning-curve",
    "title": "Practical Considerations",
    "section": "Learning Curve",
    "text": "Learning Curve\nIs it worth collecting more data?\n\nExample of a learning curve. X-axis: Performance, Y-axis: Number of training samples. Left panel with Gaussian Naive Bayes and right panel with Support Vector Classifier.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#regularization-3",
    "href": "slides/practical.html#regularization-3",
    "title": "Practical Considerations",
    "section": "4) Regularization",
    "text": "4) Regularization\nAnother possibility is data augmentation. New data points are generated from existing ones by making random changes to the data. Typically, data points are augmented on-the-fly.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#data-augmentation-augly",
    "href": "slides/practical.html#data-augmentation-augly",
    "title": "Practical Considerations",
    "section": "Data Augmentation: Augly",
    "text": "Data Augmentation: Augly\n\n\n\nAugLy",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#data-augmentation-albumentations",
    "href": "slides/practical.html#data-augmentation-albumentations",
    "title": "Practical Considerations",
    "section": "Data Augmentation: Albumentations",
    "text": "Data Augmentation: Albumentations\n\nAlbumentations",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#data-augmentation-kornia",
    "href": "slides/practical.html#data-augmentation-kornia",
    "title": "Practical Considerations",
    "section": "Data Augmentation: Kornia",
    "text": "Data Augmentation: Kornia\n\n\n\nKornia",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#data-augmentation-example",
    "href": "slides/practical.html#data-augmentation-example",
    "title": "Practical Considerations",
    "section": "Data Augmentation: Example",
    "text": "Data Augmentation: Example\n\n\n\nData augmentation example.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#data-augmentation-synthetic-data",
    "href": "slides/practical.html#data-augmentation-synthetic-data",
    "title": "Practical Considerations",
    "section": "Data Augmentation: Synthetic Data",
    "text": "Data Augmentation: Synthetic Data\n\n\n\nFrom Beery et al. (2020). Synthetic and semi-synthetic data.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#regularization-4",
    "href": "slides/practical.html#regularization-4",
    "title": "Practical Considerations",
    "section": "4) Regularization",
    "text": "4) Regularization\nWith early stopping, a model is trained and periodically evaluated on a validation set, e.g., after each epoch. Training is stopped if no significant improvement is achieved after x evaluation cycles.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#early-stopping",
    "href": "slides/practical.html#early-stopping",
    "title": "Practical Considerations",
    "section": "Early Stopping",
    "text": "Early Stopping\n\n\n\nSource: Link",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#regularization-5",
    "href": "slides/practical.html#regularization-5",
    "title": "Practical Considerations",
    "section": "4) Regularization",
    "text": "4) Regularization\nEarly stopping in PyTorch.\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\n\nclass LitModel(LightningModule):\n    def validation_step(self, batch, batch_idx):\n        loss = ...\n        self.log(\"val_loss\", loss)\n\nmodel = LitModel()\ntrainer = Trainer(callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\")])\ntrainer.fit(model)",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#regularization-6",
    "href": "slides/practical.html#regularization-6",
    "title": "Practical Considerations",
    "section": "4) Regularization",
    "text": "4) Regularization\nWith weight decay, a model can be regularized. The update step in gradient descent is modified.\n\\[\\begin{equation}\n\\theta_{t+1} = \\theta_t (1 - \\lambda) - \\eta \\nabla J(\\theta)\n\\end{equation}\\]\nWhere \\(t\\) is the iteration, \\(\\theta\\) the model parameters, \\(\\eta\\) the learning rate, and \\(\\lambda\\) the decay parameter.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#regularization-7",
    "href": "slides/practical.html#regularization-7",
    "title": "Practical Considerations",
    "section": "4) Regularization",
    "text": "4) Regularization\nTransfer learning involves adapting a pre-trained model on a large dataset (e.g., ImageNet) to a new task. The last layer is removed and replaced according to the new task. The network is then further trained. Layers can be frozen (weights not updated) or fine-tuned (weights further trained).",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#transfer-learning",
    "href": "slides/practical.html#transfer-learning",
    "title": "Practical Considerations",
    "section": "Transfer Learning",
    "text": "Transfer Learning\n\n\n\nSource: Johnson and Fouhey (2021)",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#regularization-8",
    "href": "slides/practical.html#regularization-8",
    "title": "Practical Considerations",
    "section": "4) Regularization",
    "text": "4) Regularization\nIn PyTorch, you can freeze the parameters:\ndef set_parameter_requires_grad(model, feature_extracting):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#hyper-parameter-tuning",
    "href": "slides/practical.html#hyper-parameter-tuning",
    "title": "Practical Considerations",
    "section": "5) Hyper-Parameter Tuning",
    "text": "5) Hyper-Parameter Tuning\nIn this step, different hyperparameters and architectures are systematically evaluated. Techniques such as grid search or random search can be used, with random search being preferred.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#hyper-parameter-tuning-1",
    "href": "slides/practical.html#hyper-parameter-tuning-1",
    "title": "Practical Considerations",
    "section": "5) Hyper-Parameter Tuning",
    "text": "5) Hyper-Parameter Tuning\nParameterized architecture:\nclass Net(nn.Module):\n    def __init__(self, l1=120, l2=84):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, l1)\n        self.fc2 = nn.Linear(l1, l2)\n        self.fc3 = nn.Linear(l2, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#squeeze-out-the-juice",
    "href": "slides/practical.html#squeeze-out-the-juice",
    "title": "Practical Considerations",
    "section": "6) Squeeze out the Juice",
    "text": "6) Squeeze out the Juice\nAfter finding the best architectures and hyperparameters, there are further ways to squeeze out more performance.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#squeeze-out-the-juice-1",
    "href": "slides/practical.html#squeeze-out-the-juice-1",
    "title": "Practical Considerations",
    "section": "6) Squeeze out the Juice",
    "text": "6) Squeeze out the Juice\nModel ensembling.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#squeeze-out-the-juice-2",
    "href": "slides/practical.html#squeeze-out-the-juice-2",
    "title": "Practical Considerations",
    "section": "6) Squeeze out the Juice",
    "text": "6) Squeeze out the Juice\nTrain longer.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#double-descent",
    "href": "slides/practical.html#double-descent",
    "title": "Practical Considerations",
    "section": "Double Descent",
    "text": "Double Descent\n\nSource: Nakkiran et al. (2019)",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#squeeze-out-the-juice-3",
    "href": "slides/practical.html#squeeze-out-the-juice-3",
    "title": "Practical Considerations",
    "section": "6) Squeeze out the Juice",
    "text": "6) Squeeze out the Juice\nOther training techniques:\n\nSpecial optimizer (AdamW)\nComplex data augmentation techniques (Mixup, Cutmix, RandAugment)\nRegularization techniques (Stochastic Depth)\nLabel smoothing",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#huggingface",
    "href": "slides/practical.html#huggingface",
    "title": "Practical Considerations",
    "section": "HuggingFace",
    "text": "HuggingFace\nHuggingFace",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#timm",
    "href": "slides/practical.html#timm",
    "title": "Practical Considerations",
    "section": "timm",
    "text": "timm\nPyTorch Image Models (timm)",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "slides/practical.html#links",
    "href": "slides/practical.html#links",
    "title": "Practical Considerations",
    "section": "Links",
    "text": "Links\n\nDS-cookie cutter\nPyTorch Lightning\nHydra\nWeights & Biases\nNeptune AI\nVersion Control Systems for ML Projects",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  }
]