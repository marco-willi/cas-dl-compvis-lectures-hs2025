[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Willkommen zum Modul Computer Vision mit Deep Learning",
    "section": "",
    "text": "Herzlich willkommen zum Modul Computer Vision mit Deep Learning (1. Teil)!\nHier finden Sie Unterlagen und aktuelle Informationen zum Modul.\nModul Computer Vision mit DL\nCAS Page",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#lernziele",
    "href": "index.html#lernziele",
    "title": "Willkommen zum Modul Computer Vision mit Deep Learning",
    "section": "Lernziele",
    "text": "Lernziele\n\nDie fundamentalen Herausforderungen beim Modellieren von Bilddaten verstehen\nConvolutional Neural Networks verstehen, implementieren und trainieren können\nFoundation-Models als Grundlage für viele Vision Tasks verstehen und einsetzten können\nBildklassifikation verstehen und systematisch umsetzen können\nMit Deep Learning Frameworks umgehen und Libraries verwenden können\nPraktische Anwendung umsetzen, vorstellen und erklären können",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#inhalte",
    "href": "index.html#inhalte",
    "title": "Willkommen zum Modul Computer Vision mit Deep Learning",
    "section": "Inhalte",
    "text": "Inhalte\n\nTag 1 - Grundlagen CNNs und Bildklassifikation\n\n\n\nZeit\nThema\n\n\n\n\n8:45 - 9:30\nEinführung Computer Vision mit Deep Learning\n\n\n9:30 - 10:30\nÜbung: Deep Learning mit PyTorch und Bildern\n\n\n10:30 - 10:45\nPause\n\n\n10:45 - 12:00\nConvolutional Neural Networks\n\n\n12:00 - 13:00\nMittagspause\n\n\n13:00 - 14:00\nÜbung: CNNs vs MLPs\n\n\n14:15 - 15:00\nTheorie: Bildklassifikation\n\n\n15:15 - 16:30\nÜbung: Bildklassifikation\n\n\n\n\n\nTag 2 - Foundation Models und Praktische Tipps\n\n\n\nZeit\nThema\n\n\n\n\n8:45 - 10:00\nTheorie: Foundation Models & Representation Learning\n\n\n10:15 - 11:15\nÜbung: Foundation Models\n\n\n11:30 - 12:00\nTheorie: Praktische Tipps\n\n\n12:00 - 13:00\nMittagspause\n\n\n13:00 - 15:30\nÜbung: End-To-End Beispiel\n\n\n15:30 - 15:45\nPause\n\n\n15:45 - 16:30\nQ&A / Buffer",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "pages/slides/recent_advances.html#what-are-they",
    "href": "pages/slides/recent_advances.html#what-are-they",
    "title": "Recent Advances",
    "section": "What are they?",
    "text": "What are they?\nFoundation models are large-scale machine learning models trained on vast amounts of data that can be fine-tuned for various downstream tasks. These models have demonstrated remarkable capabilities in natural language processing, computer vision, and other fields.",
    "crumbs": [
      "Slides",
      "Recent Advances"
    ]
  },
  {
    "objectID": "pages/slides/recent_advances.html#characteristics-of-foundation-models",
    "href": "pages/slides/recent_advances.html#characteristics-of-foundation-models",
    "title": "Recent Advances",
    "section": "Characteristics of Foundation Models",
    "text": "Characteristics of Foundation Models\n\nLarge-scale Pre-training: Foundation models are pre-trained on extensive datasets, enabling them to capture a wide range of knowledge.\nTransfer Learning: These models can be fine-tuned on specific tasks with relatively small datasets, making them versatile and efficient.\nMultimodal Capabilities: Some foundation models can process and integrate multiple types of data, such as text and images.\n\n\nExample: CLIP (Contrastive Language-Image Pre-training, Radford et al. (2021)) is a foundation model developed by OpenAI. It is designed to understand images and text jointly, making it capable of tasks like zero-shot image classification.",
    "crumbs": [
      "Slides",
      "Recent Advances"
    ]
  },
  {
    "objectID": "pages/slides/recent_advances.html#how-clip-works",
    "href": "pages/slides/recent_advances.html#how-clip-works",
    "title": "Recent Advances",
    "section": "How CLIP Works",
    "text": "How CLIP Works\n\n\nFigure 1: Illustration of CLIP. From Radford et al. (2021).\nCLIP is pre-trained on a diverse dataset of images and their corresponding textual descriptions. It learns to associate images with their textual descriptions using a contrastive learning approach, which maximizes the similarity between correct image-text pairs and minimizes the similarity between incorrect pairs.",
    "crumbs": [
      "Slides",
      "Recent Advances"
    ]
  },
  {
    "objectID": "pages/slides/recent_advances.html#how-clip-can-be-applied",
    "href": "pages/slides/recent_advances.html#how-clip-can-be-applied",
    "title": "Recent Advances",
    "section": "How CLIP can be applied",
    "text": "How CLIP can be applied\n\n\nFigure 2: Zero-shot learning with CLIP. From Radford et al. (2021).",
    "crumbs": [
      "Slides",
      "Recent Advances"
    ]
  },
  {
    "objectID": "pages/slides/recent_advances.html#applications-of-clip",
    "href": "pages/slides/recent_advances.html#applications-of-clip",
    "title": "Recent Advances",
    "section": "Applications of CLIP",
    "text": "Applications of CLIP\n\nZero-Shot Classification: CLIP can classify images into categories it has not explicitly been trained on by leveraging its understanding of language.\nImage Search: By inputting a textual description, CLIP can retrieve relevant images from a database.\n(near) Duplicate Detection: CLIP can assist in identifying near duplicate images.",
    "crumbs": [
      "Slides",
      "Recent Advances"
    ]
  },
  {
    "objectID": "pages/slides/recent_advances.html#example",
    "href": "pages/slides/recent_advances.html#example",
    "title": "Recent Advances",
    "section": "Example",
    "text": "Example\nHere’s a simple example of using CLIP for zero-shot image classification:\n\nimport requests\nfrom PIL import Image\nfrom transformers import CLIPModel, CLIPProcessor\n\n# 1) Get Models and Data Pre-Processors\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n# 2) Define Input Data: Image and Texts\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprompt_template = \"a photo of a {}\"\nclasses = [\"cat\", \"dog\", \"car\", \"tree\"]\nprompts = [prompt_template.format(class_name) for class_name in classes]\n\n# 3) Pre-Process Inputs\ninputs = processor(\n    text=prompts,\n    images=image,\n    return_tensors=\"pt\",\n    padding=True,\n)\n\n# 4) Forward-Pass\nwith torch.no_grad():\n    outputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n\n# 5) Find most likely label\nprobs = logits_per_image.softmax(dim=1)\nprint(\"Label:\", classes[probs.argmax().item()])",
    "crumbs": [
      "Slides",
      "Recent Advances"
    ]
  },
  {
    "objectID": "pages/slides/recent_advances.html#fine-tuning-clip",
    "href": "pages/slides/recent_advances.html#fine-tuning-clip",
    "title": "Recent Advances",
    "section": "Fine-Tuning CLIP",
    "text": "Fine-Tuning CLIP\nIf CLIP does not perform well with zero-shot learning, it can be fine-tuned with various techniques. This is a powerful mechanism because the features that CLIP has learned are very powerful and might be useful for your target task.",
    "crumbs": [
      "Slides",
      "Recent Advances"
    ]
  },
  {
    "objectID": "pages/slides/recent_advances.html#what-are-they-1",
    "href": "pages/slides/recent_advances.html#what-are-they-1",
    "title": "Recent Advances",
    "section": "What are they?",
    "text": "What are they?\nMulti-modal models are able to process multiple types of data (modes) simultaneously.",
    "crumbs": [
      "Slides",
      "Recent Advances"
    ]
  },
  {
    "objectID": "pages/slides/recent_advances.html#example-visual-question-answering",
    "href": "pages/slides/recent_advances.html#example-visual-question-answering",
    "title": "Recent Advances",
    "section": "Example: Visual Question Answering",
    "text": "Example: Visual Question Answering\nOne task where multi-modality is required, is Visual Question Answering (VQA). Only by jointly understanding the visual input and the question can such tasks be solved.\n\n\nFigure 3: BLIP2 examples from a chat system. Source: Li et al. (2023)",
    "crumbs": [
      "Slides",
      "Recent Advances"
    ]
  },
  {
    "objectID": "pages/slides/recent_advances.html#how-vqa-works",
    "href": "pages/slides/recent_advances.html#how-vqa-works",
    "title": "Recent Advances",
    "section": "How VQA Works",
    "text": "How VQA Works\nVQA models combine visual data (images) with textual data (questions) to generate accurate answers. These models are typically pre-trained on large datasets containing images, questions about those images, and the corresponding answers.\n\n\nFigure 4: Training data examples. Source: Alayrac et al. (2022)",
    "crumbs": [
      "Slides",
      "Recent Advances"
    ]
  },
  {
    "objectID": "pages/slides/recent_advances.html#applications-of-vqa",
    "href": "pages/slides/recent_advances.html#applications-of-vqa",
    "title": "Recent Advances",
    "section": "Applications of VQA",
    "text": "Applications of VQA\n\nAccessibility: VQA can help visually impaired users by answering questions about their surroundings based on images captured by a camera.\nChat Bots with Visual Understanding: VQA systems can be used in chat bots to incorporate visual inputs.\nCustomer Support: VQA can enhance customer support by allowing users to submit images and ask questions about products or services.",
    "crumbs": [
      "Slides",
      "Recent Advances"
    ]
  },
  {
    "objectID": "pages/slides/recent_advances.html#what-are-transformers",
    "href": "pages/slides/recent_advances.html#what-are-transformers",
    "title": "Recent Advances",
    "section": "What Are Transformers?",
    "text": "What Are Transformers?\nTransformers, introduced by Vaswani et al. (2017) use a self-attention mechanism that enables the model to consider relationships between different parts of the input data. This enables transformers to process data in parallel and capture long-range dependencies more effectively than previous architectures.\n\nKey concepts:\n\nSelf-Attention: Enables each input element to focus on all other elements, capturing complex dependencies within the data.\nPositional Encoding: Injects information about the order of the sequence, allowing the transformer to process sequential data despite its parallel structure.\nScalability: The architecture scales well, making it suitable for training on large datasets across various domains.",
    "crumbs": [
      "Slides",
      "Recent Advances"
    ]
  },
  {
    "objectID": "pages/slides/recent_advances.html#vision-transformers-vit",
    "href": "pages/slides/recent_advances.html#vision-transformers-vit",
    "title": "Recent Advances",
    "section": "Vision Transformers (ViT)",
    "text": "Vision Transformers (ViT)\nVision Transformers (ViTs), proposed by Dosovitskiy et al. (2020), apply the transformer architecture to image data by splitting images into fixed-size patches and treating each patch as a token in a sequence. This approach enables ViTs to capture spatial relationships in images without convolutional layers.\n\n\nFigure 5: Illustration of ViTs. Source: Dosovitskiy et al. (2020)",
    "crumbs": [
      "Slides",
      "Recent Advances"
    ]
  },
  {
    "objectID": "pages/slides/recent_advances.html#applications-of-vision-transformers",
    "href": "pages/slides/recent_advances.html#applications-of-vision-transformers",
    "title": "Recent Advances",
    "section": "Applications of Vision Transformers",
    "text": "Applications of Vision Transformers\n\nImage Classification: ViTs have achieved competitive results in image classification, sometimes outperforming CNNs.\nObject Detection and Segmentation: Transformers can be adapted for tasks like object detection by directly predicting bounding boxes.\nMultimodal Models: Transformers enable models to integrate data from different sources, such as text and images, for tasks like image captioning and visual question answering.",
    "crumbs": [
      "Slides",
      "Recent Advances"
    ]
  },
  {
    "objectID": "pages/slides/recent_advances.html#comparison-vision-transformers-and-convolutional-neural-networks",
    "href": "pages/slides/recent_advances.html#comparison-vision-transformers-and-convolutional-neural-networks",
    "title": "Recent Advances",
    "section": "Comparison: Vision Transformers and Convolutional Neural Networks",
    "text": "Comparison: Vision Transformers and Convolutional Neural Networks\n\nArchitectural differences\nScalability and flexibility\nrobustness and transferability",
    "crumbs": [
      "Slides",
      "Recent Advances"
    ]
  },
  {
    "objectID": "pages/slides/recent_advances.html#pros-and-cons-cnns",
    "href": "pages/slides/recent_advances.html#pros-and-cons-cnns",
    "title": "Recent Advances",
    "section": "Pros and Cons CNNS",
    "text": "Pros and Cons CNNS\n\nPros:\n\nEfficiency for Small Datasets: CNNs excel in small to moderate datasets.\nRobustness to Variations: Their design enables resilience to real-world applications.\n\nCons:\n\nLimited Long-Range Dependencies: Due to their focus on local receptive fields, CNNs may struggle with long-range dependencies.",
    "crumbs": [
      "Slides",
      "Recent Advances"
    ]
  },
  {
    "objectID": "pages/slides/recent_advances.html#pros-and-cons-vits",
    "href": "pages/slides/recent_advances.html#pros-and-cons-vits",
    "title": "Recent Advances",
    "section": "Pros and Cons ViTs",
    "text": "Pros and Cons ViTs\n\nPros:\n\nScalability and Flexibility: ViTs perform well with large datasets and benefit from self-supervised pre-training.\nLong-Range Dependency Capture: Self-attention mechanisms allow ViTs to capture long-range dependencies.\n\nCons:\n\nData Efficiency: ViTs often require more data and pre-training.\nComputational Intensity: Self-attention mechanisms can be computationally demanding.",
    "crumbs": [
      "Slides",
      "Recent Advances"
    ]
  },
  {
    "objectID": "pages/slides/recent_advances.html#conclusion",
    "href": "pages/slides/recent_advances.html#conclusion",
    "title": "Recent Advances",
    "section": "Conclusion",
    "text": "Conclusion\nCurrently, transformers and CNNs perform similarly when pre-trained on massive datasets. Hybrid architectures combining convolutional layers with transformer layers are also being explored. Further research will continue to determine which architecture excels in specific contexts.",
    "crumbs": [
      "Slides",
      "Recent Advances"
    ]
  },
  {
    "objectID": "pages/slides/intro.html#species-identification",
    "href": "pages/slides/intro.html#species-identification",
    "title": "Introduction",
    "section": "Species Identification",
    "text": "Species Identification\n\nSource: Breitenmoser-Würsten et al. (2024)",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "pages/slides/intro.html#synthetic-image-detection",
    "href": "pages/slides/intro.html#synthetic-image-detection",
    "title": "Introduction",
    "section": "Synthetic Image Detection",
    "text": "Synthetic Image Detection\n\n\n\n\n\n\nSource\n\n\n\n\n\n\n\nSource",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "pages/slides/intro.html#object-identification-and-translation",
    "href": "pages/slides/intro.html#object-identification-and-translation",
    "title": "Introduction",
    "section": "Object Identification and Translation",
    "text": "Object Identification and Translation\n\n\n\n\n\n\nIdentification & Search\n\n\n\n\n\n\n\nTranslation\n\n\n\n\n\nGoogle Lens",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "pages/slides/intro.html#self-driving",
    "href": "pages/slides/intro.html#self-driving",
    "title": "Introduction",
    "section": "Self-Driving",
    "text": "Self-Driving\n\nExample from Waymo.",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "pages/slides/intro.html#biometric-id",
    "href": "pages/slides/intro.html#biometric-id",
    "title": "Introduction",
    "section": "Biometric ID",
    "text": "Biometric ID\n\nExample from Apple Face ID",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "pages/slides/intro.html#precision-agriculture",
    "href": "pages/slides/intro.html#precision-agriculture",
    "title": "Introduction",
    "section": "Precision Agriculture",
    "text": "Precision Agriculture\n\n\nExample from Häni, Roy, and Isler (2020)",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "pages/slides/intro.html#medical-segmentation",
    "href": "pages/slides/intro.html#medical-segmentation",
    "title": "Introduction",
    "section": "Medical Segmentation",
    "text": "Medical Segmentation\n\nExample from Ma et al. (2024).",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "pages/slides/intro.html#photo-enhancement",
    "href": "pages/slides/intro.html#photo-enhancement",
    "title": "Introduction",
    "section": "Photo Enhancement",
    "text": "Photo Enhancement\n\nExample from Google Magic Editor",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "pages/slides/intro.html#ai-chips",
    "href": "pages/slides/intro.html#ai-chips",
    "title": "Introduction",
    "section": "AI Chips",
    "text": "AI Chips\n\nFrom Link.",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "pages/slides/intro.html#image-classification",
    "href": "pages/slides/intro.html#image-classification",
    "title": "Introduction",
    "section": "Image Classification",
    "text": "Image Classification\n\nMulti-Class Image Classification Beispiel (aus Krizhevsky, Sutskever, and Hinton (2012)).",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "pages/slides/intro.html#object-detection",
    "href": "pages/slides/intro.html#object-detection",
    "title": "Introduction",
    "section": "Object Detection",
    "text": "Object Detection\n\nObject Detection Beispiel (aus (redmon_you_2016?)). Bounding boxes lokalisieren die Objekte, wobei für jedes Objekt die wahrscheinlichste Klasse, sowie deren Konfidenz angegeben ist.",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "pages/slides/intro.html#segmentation",
    "href": "pages/slides/intro.html#segmentation",
    "title": "Introduction",
    "section": "Segmentation",
    "text": "Segmentation\n\n\n\n\n\nObject Segmentation Beispiel (aus He et al. (2018)).",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "pages/slides/intro.html#image-generation---manipulation",
    "href": "pages/slides/intro.html#image-generation---manipulation",
    "title": "Introduction",
    "section": "Image Generation - Manipulation",
    "text": "Image Generation - Manipulation\n\nSource: Link, DragGAN by Pan et al. (2023)",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "pages/slides/intro.html#image-generation---translation",
    "href": "pages/slides/intro.html#image-generation---translation",
    "title": "Introduction",
    "section": "Image Generation - Translation",
    "text": "Image Generation - Translation\n\nImage Generation Beispiel (aus Isola et al. (2018)).",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "pages/slides/intro.html#image-generation---super-resolution",
    "href": "pages/slides/intro.html#image-generation---super-resolution",
    "title": "Introduction",
    "section": "Image Generation - Super Resolution",
    "text": "Image Generation - Super Resolution\n\nNvidia dlss: Link",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "pages/slides/intro.html#image-generation---colorization",
    "href": "pages/slides/intro.html#image-generation---colorization",
    "title": "Introduction",
    "section": "Image Generation - Colorization",
    "text": "Image Generation - Colorization\n\nNorwegian Bride (est late 1890s) aus DeOldify: Link",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "pages/slides/intro.html#many-tasks",
    "href": "pages/slides/intro.html#many-tasks",
    "title": "Introduction",
    "section": "Many tasks",
    "text": "Many tasks\n\n\n\nImage Classification\nObject Detection (and Tracking)\nImage Segmentation\n\nSemantic Segmentation\nInstance Segmentation\n\nOptical Character Recognition (OCR)\nPose Estimation\nFacial Recognition\nAction Recognition\n\n\n\nImage Generation\n\nStyle Transfer\nImage Inpainting\nSuper-Resolution\nText-to-Image (and more)\n\nImage Captioning\n3D Reconstruction\nImage Retrieval\n\n\nList is not exhaustive!",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "pages/slides/intro.html#what-about-videos",
    "href": "pages/slides/intro.html#what-about-videos",
    "title": "Introduction",
    "section": "What About Videos?",
    "text": "What About Videos?\n\n\n\n\n\n\nFigure 4: OpenAI Sora 2 - Text-to-video generation",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "pages/slides/intro.html#what-about-videos-1",
    "href": "pages/slides/intro.html#what-about-videos-1",
    "title": "Introduction",
    "section": "What About Videos?",
    "text": "What About Videos?\n\n\n\n\n\n\nFigure 5: CoTracker - Tracking points across video frames (Karaev et al. (2024))",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "pages/slides/intro.html#semantic-gap",
    "href": "pages/slides/intro.html#semantic-gap",
    "title": "Introduction",
    "section": "Semantic Gap",
    "text": "Semantic Gap\n\n\nFigure 6: Illustration of the semantic gap: computers see numbers, humans see meaning.",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "pages/slides/intro.html#viewpoint",
    "href": "pages/slides/intro.html#viewpoint",
    "title": "Introduction",
    "section": "Viewpoint",
    "text": "Viewpoint\n\n\nFigure 7: The same cat from different viewpoints looks completely different at the pixel level.",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "pages/slides/intro.html#deformation",
    "href": "pages/slides/intro.html#deformation",
    "title": "Introduction",
    "section": "Deformation",
    "text": "Deformation\n\n\nFigure 8: Cats exhibit significant shape variation due to pose changes. Source",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "pages/slides/intro.html#illumination",
    "href": "pages/slides/intro.html#illumination",
    "title": "Introduction",
    "section": "Illumination",
    "text": "Illumination\n\n\nFigure 9: The same scene under different lighting conditions. Source",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "pages/slides/intro.html#background-clutter",
    "href": "pages/slides/intro.html#background-clutter",
    "title": "Introduction",
    "section": "Background Clutter",
    "text": "Background Clutter\n\n\nFigure 10: Cat camouflaged against a similarly-textured background. Source",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "pages/slides/intro.html#occlusion",
    "href": "pages/slides/intro.html#occlusion",
    "title": "Introduction",
    "section": "Occlusion",
    "text": "Occlusion\n\n\nFigure 11: Partially occluded cat - only part of the object is visible. Source",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "pages/slides/intro.html#intra-class-variation",
    "href": "pages/slides/intro.html#intra-class-variation",
    "title": "Introduction",
    "section": "Intra-Class Variation",
    "text": "Intra-Class Variation\n\n\n\nFigure 12: High intra-class variation: all are cats but with vastly different appearances. Source",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "pages/slides/intro.html#context-dependence---blurred-objects",
    "href": "pages/slides/intro.html#context-dependence---blurred-objects",
    "title": "Introduction",
    "section": "Context Dependence - Blurred Objects",
    "text": "Context Dependence - Blurred Objects\n\n\nFigure 13: Source Torralba, Murphy, and Freeman (2010)",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "pages/slides/intro.html#machine-learning-approach",
    "href": "pages/slides/intro.html#machine-learning-approach",
    "title": "Introduction",
    "section": "Machine Learning Approach",
    "text": "Machine Learning Approach\nWith Machine Learning, we follow a data-driven approach to solve various tasks:\n\nCollect a dataset of images and their labels.\nUse a machine learning algorithm to train a model (e.g., a classifier).\nEvaluate and apply the model to new data.\n\n\ndef train(images, labels):\n    \"\"\" Train a Model \"\"\"\n    # Fit Model here\n    return model\n\ndef predict(test_images, model):\n    \"\"\" Predict \"\"\"\n    predictions = model(test_images)\n    return predictions",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "pages/slides/intro.html#question",
    "href": "pages/slides/intro.html#question",
    "title": "Introduction",
    "section": "Question",
    "text": "Question\nImage Super Resolution\nHow would you train a model for image super resolution?\nThe task of the model would be to scale low-resolution images to high-resolution images with the best possible quality.",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "pages/slides/intro.html#machine-learning-pipeline",
    "href": "pages/slides/intro.html#machine-learning-pipeline",
    "title": "Introduction",
    "section": "Machine Learning Pipeline",
    "text": "Machine Learning Pipeline",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "pages/slides/intro.html#pytorch",
    "href": "pages/slides/intro.html#pytorch",
    "title": "Introduction",
    "section": "PyTorch",
    "text": "PyTorch\nIn this class, we use PyTorch. PyTorch has gained immense popularity in recent years, characterized by high flexibility, a clean API, and many open-source resources.\nFundamental Concepts:\n\nTensor: N-dimensional array, like numpy.array\nAutograd: Functionality to create computational graphs and compute gradients.\nModule: Class to define components of neural networks\n\nLet’s check it out! (on images)",
    "crumbs": [
      "Slides",
      "Introduction"
    ]
  },
  {
    "objectID": "pages/slides/foundation_models.html#what-are-they",
    "href": "pages/slides/foundation_models.html#what-are-they",
    "title": "Foundation Models",
    "section": "What are they?",
    "text": "What are they?\nFoundation models are large-scale machine learning models trained on vast amounts of data that can be fine-tuned for various downstream tasks. These models have demonstrated remarkable capabilities in natural language processing, computer vision, and other fields."
  },
  {
    "objectID": "pages/slides/foundation_models.html#characteristics-of-foundation-models",
    "href": "pages/slides/foundation_models.html#characteristics-of-foundation-models",
    "title": "Foundation Models",
    "section": "Characteristics of Foundation Models",
    "text": "Characteristics of Foundation Models\n\nLarge-scale Pre-training: Foundation models are pre-trained on extensive datasets, enabling them to capture a wide range of knowledge.\nTransfer Learning: These models can be fine-tuned on specific tasks with relatively small datasets, making them versatile and efficient.\nMultimodal Capabilities: Some foundation models can process and integrate multiple types of data, such as text and images."
  },
  {
    "objectID": "pages/slides/foundation_models.html#clip-a-foundation-model-example",
    "href": "pages/slides/foundation_models.html#clip-a-foundation-model-example",
    "title": "Foundation Models",
    "section": "CLIP: A Foundation Model Example",
    "text": "CLIP: A Foundation Model Example\nCLIP (Contrastive Language-Image Pre-training, Radford et al. (2021)) is a foundation model developed by OpenAI. It is designed to understand images and text jointly, making it capable of tasks like zero-shot image classification."
  },
  {
    "objectID": "pages/slides/foundation_models.html#how-clip-works",
    "href": "pages/slides/foundation_models.html#how-clip-works",
    "title": "Foundation Models",
    "section": "How CLIP Works",
    "text": "How CLIP Works\n\nSource: Radford et al. (2021)CLIP is pre-trained on a diverse dataset of images and their corresponding textual descriptions. It learns to associate images with their textual descriptions using a contrastive learning approach, which maximizes the similarity between correct image-text pairs and minimizes the similarity between incorrect pairs."
  },
  {
    "objectID": "pages/slides/foundation_models.html#how-clip-can-be-applied",
    "href": "pages/slides/foundation_models.html#how-clip-can-be-applied",
    "title": "Foundation Models",
    "section": "How CLIP can be applied",
    "text": "How CLIP can be applied\n\nSource: Radford et al. (2021)"
  },
  {
    "objectID": "pages/slides/foundation_models.html#applications-of-clip",
    "href": "pages/slides/foundation_models.html#applications-of-clip",
    "title": "Foundation Models",
    "section": "Applications of CLIP",
    "text": "Applications of CLIP\n\nZero-Shot Classification: CLIP can classify images into categories it has not explicitly been trained on by leveraging its understanding of language.\nImage Search: By inputting a textual description, CLIP can retrieve relevant images from a database.\n(near) Duplicate Detection: CLIP can assist in identifying near duplicate images."
  },
  {
    "objectID": "pages/slides/foundation_models.html#example",
    "href": "pages/slides/foundation_models.html#example",
    "title": "Foundation Models",
    "section": "Example",
    "text": "Example\nHere’s a simple example of using CLIP for zero-shot image classification:\n\nimport requests\nfrom PIL import Image\nfrom transformers import CLIPModel, CLIPProcessor\n\n# 1) Get Models and Data Pre-Processors\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n# 2) Define Input Data: Image and Texts\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprompt_template = \"a photo of a {}\"\nclasses = [\"cat\", \"dog\", \"car\", \"tree\"]\nprompts = [prompt_template.format(class_name) for class_name in classes]\n\n# 3) Pre-Process Inputs\ninputs = processor(\n    text=prompts,\n    images=image,\n    return_tensors=\"pt\",\n    padding=True,\n)\n\n# 4) Forward-Pass\nwith torch.no_grad():\n    outputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n\n# 5) Find most likely label\nprobs = logits_per_image.softmax(dim=1)\nprint(\"Label:\", classes[probs.argmax().item()])"
  },
  {
    "objectID": "pages/slides/foundation_models.html#fine-tuning-clip",
    "href": "pages/slides/foundation_models.html#fine-tuning-clip",
    "title": "Foundation Models",
    "section": "Fine-Tuning CLIP",
    "text": "Fine-Tuning CLIP\nIf CLIP does not perform well with zero-shot learning, it can be fine-tuned with various techniques. This is a powerful mechanism because the features that CLIP has learned are very powerful and might be useful for your target task."
  },
  {
    "objectID": "pages/slides/foundation_models.html#visual-question-anwering",
    "href": "pages/slides/foundation_models.html#visual-question-anwering",
    "title": "Foundation Models",
    "section": "Visual Question Anwering",
    "text": "Visual Question Anwering\nMulti-modal models extend the capabilities of foundation models by integrating and processing multiple types of data simultaneously. One notable example of a multi-modal model is a Visual Question Answering (VQA) system, which can understand and answer questions about images. Figure 1 shows an example.\n\nSource: Li et al. (2023)"
  },
  {
    "objectID": "pages/slides/foundation_models.html#how-vqa-works",
    "href": "pages/slides/foundation_models.html#how-vqa-works",
    "title": "Foundation Models",
    "section": "How VQA Works",
    "text": "How VQA Works\nVQA models combine visual data (images) with textual data (questions) to generate accurate answers. These models are typically pre-trained on large datasets containing images, questions about those images, and the corresponding answers. Figure 2 shows training data examples that were used in the Flamingo model Alayrac et al. (2022).\n\nSource: Alayrac et al. (2022)"
  },
  {
    "objectID": "pages/slides/foundation_models.html#applications-of-vqa",
    "href": "pages/slides/foundation_models.html#applications-of-vqa",
    "title": "Foundation Models",
    "section": "Applications of VQA",
    "text": "Applications of VQA\n\nAccessibility: VQA can help visually impaired users by answering questions about their surroundings based on images captured by a camera.\nChat Bots with Visual Understanding: VQA systems can be used in chat bots to incorporate visual inputs.\nCustomer Support: VQA can enhance customer support by allowing users to submit images and ask questions about products or services."
  },
  {
    "objectID": "pages/slides/foundation_models.html#conclusion",
    "href": "pages/slides/foundation_models.html#conclusion",
    "title": "Foundation Models",
    "section": "Conclusion",
    "text": "Conclusion\nFoundation models like CLIP and multi-modal models such as VQA represent significant advancements in machine learning, offering powerful capabilities across various tasks. Their ability to learn from large datasets and generalize to new tasks makes them valuable tools in the AI landscape."
  },
  {
    "objectID": "pages/misc/student_projects.html",
    "href": "pages/misc/student_projects.html",
    "title": "Project Ideas",
    "section": "",
    "text": "You can choose your own topic!\nAlternatively, feel free to pick one of the following topics.\n\n\nGoal: Develop a model to classify satellite images. You should consider RGB images and images with 13 spectral bands (see wiki) in modeling. There are 10 classes and 27,000 images.\nApproach: Investigate various model architectures and decide what works best. Compare pre-trained models with those you train from scratch. Use appropriate data augmentation techniques. Since the dataset is relatively small, you should be careful of overfitting and robustly compare different models. Use RGB-only models for simplicity.\nFocus: Compare different pre-trained models (foundation models) and a model trained from scratch.\nDataset: The two datasets can be found here: https://github.com/phelber/eurosat. There is a dataset in RGB format and a dataset with 13 spectral bands.\n\n\n\n\n\n\nFigure 1: Source: Link\n\n\n\nDifficulty/Effort: Small - Medium\n\n\n\nGoal: Develop a model to classify images of food. Pick 10 food classes (≈10k imgs)\nApproach: Use pre-trained (foundation) models.\nFocus: Compare different models, including CLIP zero-shot learning, and a DINO variant. Investigate how DINO improves with more labels.\nDataset: The dataset can be found here: https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/, see Bossard, Guillaumin, and Van Gool (2014). It is 5 GB.\n\n\n\n\n\n\nFigure 2: Source: Link\n\n\n\nDifficulty/Effort: Small - Medium\n\n\n\nGoal: Develop a model to classify images into either synthetic or not (binary classification). You have to use pre-trained CLIP (Radford et al. (2021)) models and fine-tune them for this task.\nApproach: Choose a CLIP model (for example from here: HuggingFace). Understand how it works, and how the outputs of this model are organized. Use the pre-trained image encoder as a feature extractor and learn a small model on top of these features. Carefully split the dataset to train and evaluate it (image id, datasets, generative models).\nFocus: Focus on making the model work (establish a working baseline). Train your model on a train-split of the Synthbuster/RAISE1-K dataset and evaluate on a validation-split. Test your model on SynthwildX. Report overall and model-specific performances.\nDataset: There are three ready-made datasets (Synthbuster, SynthWildX and RAISE-1K) in the dataset format. It is important to read the documentation to efficiently use this format. The datasets consist of real images (RAISE-1K) and synthetic images (SynthwildX and Synthbuster) from different generative models. The Synthbuster dataset was created to match the RAISE-1K dataset: each real image was used to create a corresponding synthetic image (see Figure 3). The SynthwildX dataset was scraped from social media and is thus a good independent test set.\n\nSynthwildX - 450 MB\nSynthbuster and RAISE-1K - 2.3 GB\n(original link) Synthbuster\n(original link) RAISE-1k\n\n\n\n\n\n\n\nFigure 3: Images from (bammey_synthbuster_2023?) and (dang-nguyen_raise_2015?). The first row are real photographs that represent templates to generate synthetic images from (see second row onward).\n\n\n\nDifficulty/Effort: HIGH!\n\n\n\nGoal: Develop a model to process high-resolution images taken from drones to count sea turtles. The results can be used to survey marine turtles. The dataset includes approximately 1’000 images of high resolution (4,048 × 3,048 pixels).\nApproach: Read the paper (gray_convolutional_2019?) as inspiration. It contains an easy way to break the problem down to an image classification task (see Figure 4). Implement your own CNN architecture and train a model. Use the train/validation splits as described in the paper. Compare different architectures and compare to the results reported in the paper. You can test different variations and techniques: CNN architectures, window-size, data augmentation and regularization.\nDataset: Available here: https://zenodo.org/records/5004596#.ZChnr3ZBxD8.\n\n\n\n\n\n\nFigure 4: Source (gray_convolutional_2019?)\n\n\n\nDifficulty/Effort: Medium - High\n\n\n\nGoal: Develop a segmentation model to segment underwater camera images. You should classify pixels into 8 classes on 1,500 images.\nApproach: Develop your architecture based on your intuition and knowledge from the course. Read the paper (islam_semantic_2020?) and implement one of the architectures presented (SUIM-Net RSB or SUIM-Net VGG) and compare with your architecture. Since the dataset is relatively small, you should be careful of overfitting and robustly compare different models.\nDataset: Available here: https://irvlab.cs.umn.edu/resources/suim-dataset.\n\n\n\n\n\n\nFigure 5: Source: Link\n\n\n\nDifficulty/Effort: Medium\n\n\n\nGoal: Segment images from street surfaces to detect regions with potholes and cracks. Classify each pixel into either normal, pothole or crack. The dataset consists of 4’340 images and masks.\nApproach. This dataset was used in a competition. Make sure to consult a description of the best methods as described in (thompson_shrec_2022?). Start with developing your architecture based on your intuition and knowledge from the course. One problem is the class imbalance, with many pixels being normal. Consider this when designing your loss function.\nDataset: Available here: https://data.mendeley.com/datasets/kfth5g2xk3/2. See also (andrea_ranieri_pothole_2022?). This datset is actually a collection of different datasets. See Figure 6 for examples.\nDifficulty/Effort: Medium\n\n\n\n\n\n\nFigure 6: Examples from different datasets. Red are cracks, blue potholes. From (thompson_shrec_2022?).\n\n\n\n\n\n\nGoal: Develop an object detection model to detect polyps in images from endoscopic procedures. The goal is to detect each polyp with a bounding box. There are 1’000 images for training. See left and mid image of Figure 7.\nApproach: Develop your own architecture based on the lecture notes. Follow the best practices and implement your ML pipeline carefully from the ground up by gradually adding complexity. Compare a few architectures and hyper-parameters.\nDataset: Available here: https://datasets.simula.no/kvasir-seg/.\n\n\n\n\n\n\nFigure 7: Input image (left), semantic segmentation (middle) and object detection (right). From (ro_kvasir-seg_2020?).\n\n\n\nDifficulty/Effort: High",
    "crumbs": [
      "Resources",
      "Project Ideas"
    ]
  },
  {
    "objectID": "pages/misc/student_projects.html#topic-list",
    "href": "pages/misc/student_projects.html#topic-list",
    "title": "Project Ideas",
    "section": "",
    "text": "You can choose your own topic!\nAlternatively, feel free to pick one of the following topics.\n\n\nGoal: Develop a model to classify satellite images. You should consider RGB images and images with 13 spectral bands (see wiki) in modeling. There are 10 classes and 27,000 images.\nApproach: Investigate various model architectures and decide what works best. Compare pre-trained models with those you train from scratch. Use appropriate data augmentation techniques. Since the dataset is relatively small, you should be careful of overfitting and robustly compare different models. Use RGB-only models for simplicity.\nFocus: Compare different pre-trained models (foundation models) and a model trained from scratch.\nDataset: The two datasets can be found here: https://github.com/phelber/eurosat. There is a dataset in RGB format and a dataset with 13 spectral bands.\n\n\n\n\n\n\nFigure 1: Source: Link\n\n\n\nDifficulty/Effort: Small - Medium\n\n\n\nGoal: Develop a model to classify images of food. Pick 10 food classes (≈10k imgs)\nApproach: Use pre-trained (foundation) models.\nFocus: Compare different models, including CLIP zero-shot learning, and a DINO variant. Investigate how DINO improves with more labels.\nDataset: The dataset can be found here: https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/, see Bossard, Guillaumin, and Van Gool (2014). It is 5 GB.\n\n\n\n\n\n\nFigure 2: Source: Link\n\n\n\nDifficulty/Effort: Small - Medium\n\n\n\nGoal: Develop a model to classify images into either synthetic or not (binary classification). You have to use pre-trained CLIP (Radford et al. (2021)) models and fine-tune them for this task.\nApproach: Choose a CLIP model (for example from here: HuggingFace). Understand how it works, and how the outputs of this model are organized. Use the pre-trained image encoder as a feature extractor and learn a small model on top of these features. Carefully split the dataset to train and evaluate it (image id, datasets, generative models).\nFocus: Focus on making the model work (establish a working baseline). Train your model on a train-split of the Synthbuster/RAISE1-K dataset and evaluate on a validation-split. Test your model on SynthwildX. Report overall and model-specific performances.\nDataset: There are three ready-made datasets (Synthbuster, SynthWildX and RAISE-1K) in the dataset format. It is important to read the documentation to efficiently use this format. The datasets consist of real images (RAISE-1K) and synthetic images (SynthwildX and Synthbuster) from different generative models. The Synthbuster dataset was created to match the RAISE-1K dataset: each real image was used to create a corresponding synthetic image (see Figure 3). The SynthwildX dataset was scraped from social media and is thus a good independent test set.\n\nSynthwildX - 450 MB\nSynthbuster and RAISE-1K - 2.3 GB\n(original link) Synthbuster\n(original link) RAISE-1k\n\n\n\n\n\n\n\nFigure 3: Images from (bammey_synthbuster_2023?) and (dang-nguyen_raise_2015?). The first row are real photographs that represent templates to generate synthetic images from (see second row onward).\n\n\n\nDifficulty/Effort: HIGH!\n\n\n\nGoal: Develop a model to process high-resolution images taken from drones to count sea turtles. The results can be used to survey marine turtles. The dataset includes approximately 1’000 images of high resolution (4,048 × 3,048 pixels).\nApproach: Read the paper (gray_convolutional_2019?) as inspiration. It contains an easy way to break the problem down to an image classification task (see Figure 4). Implement your own CNN architecture and train a model. Use the train/validation splits as described in the paper. Compare different architectures and compare to the results reported in the paper. You can test different variations and techniques: CNN architectures, window-size, data augmentation and regularization.\nDataset: Available here: https://zenodo.org/records/5004596#.ZChnr3ZBxD8.\n\n\n\n\n\n\nFigure 4: Source (gray_convolutional_2019?)\n\n\n\nDifficulty/Effort: Medium - High\n\n\n\nGoal: Develop a segmentation model to segment underwater camera images. You should classify pixels into 8 classes on 1,500 images.\nApproach: Develop your architecture based on your intuition and knowledge from the course. Read the paper (islam_semantic_2020?) and implement one of the architectures presented (SUIM-Net RSB or SUIM-Net VGG) and compare with your architecture. Since the dataset is relatively small, you should be careful of overfitting and robustly compare different models.\nDataset: Available here: https://irvlab.cs.umn.edu/resources/suim-dataset.\n\n\n\n\n\n\nFigure 5: Source: Link\n\n\n\nDifficulty/Effort: Medium\n\n\n\nGoal: Segment images from street surfaces to detect regions with potholes and cracks. Classify each pixel into either normal, pothole or crack. The dataset consists of 4’340 images and masks.\nApproach. This dataset was used in a competition. Make sure to consult a description of the best methods as described in (thompson_shrec_2022?). Start with developing your architecture based on your intuition and knowledge from the course. One problem is the class imbalance, with many pixels being normal. Consider this when designing your loss function.\nDataset: Available here: https://data.mendeley.com/datasets/kfth5g2xk3/2. See also (andrea_ranieri_pothole_2022?). This datset is actually a collection of different datasets. See Figure 6 for examples.\nDifficulty/Effort: Medium\n\n\n\n\n\n\nFigure 6: Examples from different datasets. Red are cracks, blue potholes. From (thompson_shrec_2022?).\n\n\n\n\n\n\nGoal: Develop an object detection model to detect polyps in images from endoscopic procedures. The goal is to detect each polyp with a bounding box. There are 1’000 images for training. See left and mid image of Figure 7.\nApproach: Develop your own architecture based on the lecture notes. Follow the best practices and implement your ML pipeline carefully from the ground up by gradually adding complexity. Compare a few architectures and hyper-parameters.\nDataset: Available here: https://datasets.simula.no/kvasir-seg/.\n\n\n\n\n\n\nFigure 7: Input image (left), semantic segmentation (middle) and object detection (right). From (ro_kvasir-seg_2020?).\n\n\n\nDifficulty/Effort: High",
    "crumbs": [
      "Resources",
      "Project Ideas"
    ]
  },
  {
    "objectID": "pages/misc/notation.html",
    "href": "pages/misc/notation.html",
    "title": "Mathematical Notation",
    "section": "",
    "text": "Syntax\nDescription\n\n\n\n\n\\(a\\)\nA scalar (integer or real)\n\n\n\\(\\mathbf{a}\\)\nA vector\n\n\n\\(\\mathbf{A}\\)\nA matrix\n\n\n\\(\\mathbf{\\mathsf{A}}\\)\nA tensor\n\n\n\\(\\mathbf{I}_n\\)\nIdentity matrix with \\(n\\) rows and \\(n\\) columns\n\n\n\\(\\mathbf{I}\\)\nIdentity matrix with dimensionality implied by context\n\n\n\\(\\mathbf{e}^{(i)}\\)\nStandard basis vector \\([0,\\dots,0,1,0,\\dots,0]\\) with a 1 at position \\(i\\)\n\n\n\\(\\text{diag}(\\mathbf{a})\\)\nA square, diagonal matrix with diagonal entries given by \\(\\mathbf{a}\\)\n\n\n\\(\\textnormal{a}\\)\nA scalar random variable\n\n\n\\(\\mathbf{a}\\)\nA vector-valued random variable\n\n\n\\(\\mathbf{A}\\)\nA matrix-valued random variable\n\n\n\\(\\theta\\)\nParameters of a model\n\n\n\\(f(\\theta, \\mathbf{x})\\)\nA function (model) with paramters \\(\\theta\\) and data \\(\\mathbf{x}\\)\n\n\n\\(\\mathbf{A} \\odot \\mathbf{B}\\)\nElement-wise (Hadamard) product of \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\)",
    "crumbs": [
      "Resources",
      "Mathematical Notation"
    ]
  },
  {
    "objectID": "pages/misc/notation.html#numbers-and-arrays",
    "href": "pages/misc/notation.html#numbers-and-arrays",
    "title": "Mathematical Notation",
    "section": "",
    "text": "Syntax\nDescription\n\n\n\n\n\\(a\\)\nA scalar (integer or real)\n\n\n\\(\\mathbf{a}\\)\nA vector\n\n\n\\(\\mathbf{A}\\)\nA matrix\n\n\n\\(\\mathbf{\\mathsf{A}}\\)\nA tensor\n\n\n\\(\\mathbf{I}_n\\)\nIdentity matrix with \\(n\\) rows and \\(n\\) columns\n\n\n\\(\\mathbf{I}\\)\nIdentity matrix with dimensionality implied by context\n\n\n\\(\\mathbf{e}^{(i)}\\)\nStandard basis vector \\([0,\\dots,0,1,0,\\dots,0]\\) with a 1 at position \\(i\\)\n\n\n\\(\\text{diag}(\\mathbf{a})\\)\nA square, diagonal matrix with diagonal entries given by \\(\\mathbf{a}\\)\n\n\n\\(\\textnormal{a}\\)\nA scalar random variable\n\n\n\\(\\mathbf{a}\\)\nA vector-valued random variable\n\n\n\\(\\mathbf{A}\\)\nA matrix-valued random variable\n\n\n\\(\\theta\\)\nParameters of a model\n\n\n\\(f(\\theta, \\mathbf{x})\\)\nA function (model) with paramters \\(\\theta\\) and data \\(\\mathbf{x}\\)\n\n\n\\(\\mathbf{A} \\odot \\mathbf{B}\\)\nElement-wise (Hadamard) product of \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\)",
    "crumbs": [
      "Resources",
      "Mathematical Notation"
    ]
  },
  {
    "objectID": "pages/misc/notation.html#indexing",
    "href": "pages/misc/notation.html#indexing",
    "title": "Mathematical Notation",
    "section": "2 Indexing",
    "text": "2 Indexing\n\n\n\n\n\n\n\nSyntax\nDescription\n\n\n\n\n\\(a_i\\)\nElement \\(i\\) of vector \\(\\mathbf{a}\\), with indexing starting at 1\n\n\n\\(A_{i,j}\\)\nElement \\(i, j\\) of matrix \\(\\mathbf{A}\\)",
    "crumbs": [
      "Resources",
      "Mathematical Notation"
    ]
  },
  {
    "objectID": "pages/misc/notation.html#datasets-and-distributions",
    "href": "pages/misc/notation.html#datasets-and-distributions",
    "title": "Mathematical Notation",
    "section": "3 Datasets and Distributions",
    "text": "3 Datasets and Distributions\n\n\n\n\n\n\n\nSyntax\nDescription\n\n\n\n\n\\(\\mathbf{X}\\)\nThe design matrix with dimensionality \\(nxp\\) with \\(n\\) samples with \\(p\\) features.\n\n\n\\(\\mathbf{x}^{(i)}\\)\nThe i-th training example.\n\n\n\\(\\mathbf{y}^{(i)}\\)\nThe label-vector for the i-th training example.\n\n\n\\(y^{(i)}\\)\nThe label for the i-th training example.",
    "crumbs": [
      "Resources",
      "Mathematical Notation"
    ]
  },
  {
    "objectID": "pages/misc/notation.html#probability-theory",
    "href": "pages/misc/notation.html#probability-theory",
    "title": "Mathematical Notation",
    "section": "4 Probability Theory",
    "text": "4 Probability Theory\n\n\n\n\n\n\n\nSyntax\nDescription\n\n\n\n\n\\(P(x)\\)\nA probability distribution over a discrete variable.\n\n\n\\(p(x)\\)\nA probability distribution over a contiuous variable or over a variable whose type has not been specified.\n\n\n\\(\\mathbb{E}_{x \\sim P} [ f(x) ]\\text{ or } \\mathbb{E} f(x)\\)\nExpectation of \\(f(x)\\) with respect to \\(P(x)\\)\n\n\n\\(\\mathcal{N} ( \\mathbf{x} ; \\mu , \\Sigma)\\)\nGaussian distribution over \\(\\mathbf{x}\\) with mean \\(\\mu\\) and covariance \\(\\Sigma\\)\n\n\n\\(x \\sim \\mathcal{N} (\\mu , \\sigma)\\)\nGaussian distribution over \\(x\\) with mean \\(\\mu\\) and variance \\(\\sigma\\)",
    "crumbs": [
      "Resources",
      "Mathematical Notation"
    ]
  },
  {
    "objectID": "pages/misc/notation.html#calculus",
    "href": "pages/misc/notation.html#calculus",
    "title": "Mathematical Notation",
    "section": "5 Calculus",
    "text": "5 Calculus\n\n\n\n\n\n\n\nSyntax\nDescription\n\n\n\n\n\\(\\nabla_{\\mathbf{w}} J\\)\nGradient of \\(J\\) with respect to \\(\\mathbf{w}\\)\n\n\n\\(\\frac{\\partial J}{\\partial w}\\)\nPartial derivative of \\(J\\) with respect to \\(w\\)",
    "crumbs": [
      "Resources",
      "Mathematical Notation"
    ]
  },
  {
    "objectID": "pages/misc/notation.html#functions",
    "href": "pages/misc/notation.html#functions",
    "title": "Mathematical Notation",
    "section": "6 Functions",
    "text": "6 Functions\n\n\n\nSyntax\nDescription\n\n\n\n\n\\(\\log x\\)\nThe natural logarithm of \\(x\\).\n\n\n\\(\\lVert \\mathbf{x} \\rVert_p\\)\n\\(L^p\\) norm of \\(\\mathbf{x}\\)\n\n\n\\(\\lVert \\mathbf{x} \\rVert\\)\n\\(L^2\\) norm of \\(\\mathbf{x}\\)",
    "crumbs": [
      "Resources",
      "Mathematical Notation"
    ]
  },
  {
    "objectID": "pages/misc/notation.html#deep-learning",
    "href": "pages/misc/notation.html#deep-learning",
    "title": "Mathematical Notation",
    "section": "7 Deep Learning",
    "text": "7 Deep Learning\n\n\n\n\n\n\n\nSyntax\nDescription\n\n\n\n\nNCHW\nThe input format of images and activations in PyTorch. N: number of images (batch size), C: number of channels, H: height, W: width",
    "crumbs": [
      "Resources",
      "Mathematical Notation"
    ]
  },
  {
    "objectID": "pages/misc/links.html",
    "href": "pages/misc/links.html",
    "title": "Helpful Links & Resources",
    "section": "",
    "text": "Links and ressources to different topics related to Machine Learning, Deep Learning, and Images.",
    "crumbs": [
      "Resources",
      "Helpful Links & Resources"
    ]
  },
  {
    "objectID": "pages/misc/links.html#theory",
    "href": "pages/misc/links.html#theory",
    "title": "Helpful Links & Resources",
    "section": "1 Theory",
    "text": "1 Theory\n\n1.1 PyTorch\nPyTorch internals - Blog Post\n\n\n1.2 Deep Learning and Computer Vision\nUniversity of Michigan - Deep Learning for Computer Vision\n\nSehr gute Vorlesung zum Thema\n\nUniversity of California, Berkeley - Modern Computer Vision and Deep Learning\n\nSehr gute Vorlesung zum Thema\n\n\n\n1.3 Neuronale Netzwerke - Basics\nPerceptron Learning Rule S. Raschka\nCS229 Stanford MLP Backpropagation\nNotes on Backpropagation\n3Blue1Brown Gradient Descent\n3Blue1Brown Backpropagation Calculus\nAndrew Ng Backprop\nAndrej Karpathy - Backpropagation from the ground up\n\n\n1.4 Model Selection\nPaper von S.Raschka: “Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning”",
    "crumbs": [
      "Resources",
      "Helpful Links & Resources"
    ]
  },
  {
    "objectID": "pages/misc/links.html#practical",
    "href": "pages/misc/links.html#practical",
    "title": "Helpful Links & Resources",
    "section": "2 Practical",
    "text": "2 Practical\nAndrej Karpathy - A Recipe for Training Neural Networks\n\n2.1 ML Best Practices Videos\nMartin Zinkevich - Best Practices for ML Engineering\nAndrew Ng - Advice For Applying Machine Learning | Deciding What To Try Next\nAndrew Ng - Advice For Applying Machine Learning | Learning Curves\nAndrew Ng - Advice For Applying Machine Learning | Deciding What To Do Next (Revisited)\nAndrew Ng - Machine Learning System Design | Prioritizing What To Work On\nAndrew Ng - Machine Learning System Design | Error Analysis\nAndrew Ng - Machine Learning System Design | Data For Machine Learning",
    "crumbs": [
      "Resources",
      "Helpful Links & Resources"
    ]
  },
  {
    "objectID": "pages/misc/links.html#tools",
    "href": "pages/misc/links.html#tools",
    "title": "Helpful Links & Resources",
    "section": "3 Tools",
    "text": "3 Tools\n\n3.1 Data Science Repository\nBuild a Reproducible and Maintainable Data Science Project\n\ngreat jupyter book to learen about how to structure a repository and more\n\nLightning-Hydra-Template\n\ntemplate to strcuture a repository based on experiment configuration with Hydra and Pytorch-Lightning\n\n\n\n3.2 Data Handling\ndatasets\n\nGreat package to create and manage (large) image datasets\n\nimg2dataset\n\nPackage to download large image datasets from urls\n\nDVC\n\nPackage for data version control\n\n\n\n3.3 PyTorch\nLightning\n\nboilerplate code to easily train models and use gpu, etc.",
    "crumbs": [
      "Resources",
      "Helpful Links & Resources"
    ]
  },
  {
    "objectID": "pages/misc/demos.html",
    "href": "pages/misc/demos.html",
    "title": "Demos",
    "section": "",
    "text": "1 CNN Filters Visualization\nVisualize filters and their effect of a pre-trained ResNet-18.\n\n  \n\n\n\n2 CLIP Demo\nThis model can calculate similarities between and among images and texts. It can be used for zero-shot (no labels) image classification.\n\n  \n\n\n\n3 Visual Question Answering\nTest a model that can answer questions given an image. The notebook contains a small model which can be run on cpu and a much larger model which ideally is run on GPU."
  },
  {
    "objectID": "pages/background/neural_networks.html",
    "href": "pages/background/neural_networks.html",
    "title": "2 - Neural Networks",
    "section": "",
    "text": "TipLearning Objectives\n\n\n\n\n\nAfter this lecture you should be able to:\n\nExplain the relationship between linear models and multilayer perceptrons.\nDescribe why activation functions enable non-linear function approximation.\nInterpret the role of depth vs width (efficiency of representation, hierarchical features).\nImplement and configure simple MLP architectures in PyTorch.",
    "crumbs": [
      "Background & Preparation",
      "2 - Neural Networks"
    ]
  },
  {
    "objectID": "pages/background/neural_networks.html#biological-neural-networks",
    "href": "pages/background/neural_networks.html#biological-neural-networks",
    "title": "2 - Neural Networks",
    "section": "1 Biological Neural Networks",
    "text": "1 Biological Neural Networks\nA biological neural network is a part of the nervous system and consists of interconnected neurons. A neuron is connected to other neurons via dendrites (these are “weighted” input signals) and via the axon (output signal) (see Figure 1). If the input signals exceed a certain threshold, the neuron “fires” and sends a signal through the axon, which then serves as an input signal for other neurons. Humans have about 86 billion neurons, each connected to about 1000 others on average (See Wikipedia).\n\n\n\n\n\n\nFigure 1: Schematic representation of connected neurons. Phillips (2015)\n\n\n\nOptical signals are processed, among other things, in the visual cortex (Figure 2). Signals are processed hierarchically, with the first layers recognizing simple patterns and later layers recognizing shapes and objects. See also the work of Hubel and Wiesel Hubel and Wiesel (1959).\n\n\n\n\n\n\nFigure 2: Representation of transformations in the visual cortex. Kubilius (2017)\n\n\n\nThe question that arises is:\nCan we create artificial neural networks and reproduce the performance of biological neural networks?",
    "crumbs": [
      "Background & Preparation",
      "2 - Neural Networks"
    ]
  },
  {
    "objectID": "pages/background/neural_networks.html#artificial-neural-networks",
    "href": "pages/background/neural_networks.html#artificial-neural-networks",
    "title": "2 - Neural Networks",
    "section": "2 Artificial Neural Networks",
    "text": "2 Artificial Neural Networks\nArtificial neural networks are models of biological networks. Such models were created and implemented technically as early as the 1940s.\nNeural networks are often represented with a graph. The nodes are individual neurons in the network, and the edges are connections between the neurons (see Figure 3). The neurons are arranged in layers, with each neuron in a layer connected to every neuron in adjacent layers. The input layer represents the data, the output layer the (observable) outputs, and the hidden layers are within the network. The connections between the neurons are weighted.\n\n\n\n\n\n\nFigure 3: A neural network with two hidden layers. The lines show connections between neurons. Source: Li (2022).",
    "crumbs": [
      "Background & Preparation",
      "2 - Neural Networks"
    ]
  },
  {
    "objectID": "pages/background/neural_networks.html#sec-nn-mlps",
    "href": "pages/background/neural_networks.html#sec-nn-mlps",
    "title": "2 - Neural Networks",
    "section": "3 From Linear Models to Neural Networks",
    "text": "3 From Linear Models to Neural Networks\nIn the following, we will describe neural networks mathematically. We start with a linear model. A linear model has the following form (see Equation 1):\n\\[\n   f(\\mathbf{x}^{(i)}) = \\mathbf{W} \\mathbf{x}^{(i)}  +  \\mathbf{b}\n\\tag{1}\\]\nWith the following elements:\n\nThe \\(i\\)-th data point \\(\\mathbf{x}^{(i)}\\) is a \\(p\\)-dimensional vector: \\(\\in \\mathbb{R}^{p \\times 1}\\)\nModel weights \\(\\mathbf{W} \\in \\mathbb{R}^{k \\times p}\\) linearily project the data point to \\(k\\) dimensions.\nA bias term \\(\\mathbf{b} \\in \\mathbb{R}^{k \\times 1}\\) is added to the result.\n\n\\(f(\\mathbf{x}^{(i)})\\) is thus \\(\\in  \\mathbb{R}^{k \\times 1}\\).\nThe following parameters must be learned from training data using an optimization method: \\(\\mathbf{W}\\) and \\(\\mathbf{b}\\).\nIn neural networks, the linear model is extended with additional layers. Equation 2 defines a neural network with two layers (one hidden layer \\(\\mathbf{W}^{(1)}, \\mathbf{b}^{(1)}\\) and one output layer \\(\\mathbf{W}^{(2)}, \\mathbf{b}^{(2)}\\)).\n\\[\n   f(\\mathbf{x}^{(i)}) = \\mathbf{W}^{(2)} g\\big(\\mathbf{W}^{(1)} \\mathbf{x}^{(i)}  +  \\mathbf{b}^{(1)} \\big)  +  \\mathbf{b}^{(2)}\n\\tag{2}\\]\nWhere \\(g()\\) is an activation function, such as the ReLU function shown in Equation 3. Only due to activation functions non-linear relationships can be modelled. Without activation functions, the model collapses to a simple linear model with one layer.\n\\[\n\\text{ReLU}(x) = \\begin{cases}\nx, & \\text{if } x \\geq 0 \\\\\n0, & \\text{if } x &lt; 0\n\\end{cases}\n\\tag{3}\\]\nEvery layer between the input and output layer is called a hidden layer.\nTypically, not just one data point is processed but a mini-batch of data, or even the entire dataset. The data points are arranged in a data matrix \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times p}\\). The linear model from Equation 1 can be rewritten in matrix notation as shown in Equation 4.\n\\[\n   f( \\mathbf{X}) = \\mathbf{X} \\mathbf{W}^T  +  \\mathbf{b}^T\n\\tag{4}\\]\nThe bias term \\(\\mathbf{b}\\) is broadcast (details here), so it can be added to each projected input.\n\n3.1 Activation Functions\nThe following code shows how activation functions are important for modeling non-linear relationships. The model has a hidden layer with several neurons but (left) no activation function and (right) with ReLU activation.\n\n\n\n\n\n\n\n\nFigure 4: Linear (left) vs non-linear (right) activation function.\n\n\n\n\n\nFigure 5 now shows a neural network including activation functions (ReLU). Sometimes the biases are also shown as nodes that feed into the next layer.\n\n\n\n\n\n\nFigure 5: A neural network with a hidden layer. The lines show connections between neurons and their weights \\(w_{i,j}\\).\n\n\n\n\n\n\n\n\n\nNoteActivation Function Cheat Sheet\n\n\n\nCommon choices:\n\nReLU: \\(\\max(0, x)\\) (sparse activations, mitigates vanishing gradients).\nLeakyReLU / GELU: Smoother or with negative slope; GELU often used in transformers.\nSigmoid: Saturates; mainly for probabilities/logits output.\nTanh: Zero-centered but still saturates; rarely preferred over ReLU-family now.\nSoftplus / Mish / Swish: Smooth alternatives; sometimes modest gains, higher compute.\n\nGuideline: Start with ReLU (or GELU for transformer-like blocks); change only if you have empirical evidence.\n\n\n\n\n3.2 Universal Approximation Theorem\nWith a shallow neural network, any continuous function can be modeled with arbitrary accuracy (Universal Approximation Theorem). The following graphic illustrates that as the number of linear functions (and thus piecewise linear regions) increases, the approximation of the underlying function becomes more accurate.\n\n\n\n\n\n\nFigure 6: Approximation of a 1-D function with piecewise linear regions. The more regions, the more accurate the approximation. Source: Prince (2023)\n\n\n\nNeural networks are therefore a particularly powerful class of models!\n\n\n\n\n\n\nTipRefresh your neural network know-how!",
    "crumbs": [
      "Background & Preparation",
      "2 - Neural Networks"
    ]
  },
  {
    "objectID": "pages/background/neural_networks.html#deep-learning",
    "href": "pages/background/neural_networks.html#deep-learning",
    "title": "2 - Neural Networks",
    "section": "4 Deep Learning",
    "text": "4 Deep Learning\nUsing neural networks with multiple hidden layers is refered to as deep learning. Figure 7 illustrates such a model with 5 hidden layers and ReLU activation functions. Such a neural network is also called a multilayer perceptron (MLP) or a fully connected neural network or a network with dense layers. It consists of linear layers and activation functions.\n\n\n\n\n\n\nFigure 7: Illustration of a deep learning model with 5 hidden layers, from Johnson (2022)\n\n\n\nAlthough shallow neural networks can theoretically model arbitrarily complex functions, the number of neurons required is often impractical. It can be shown that adding hidden layers (increasing the depth of a network) is much more efficient (requires fewer neurons) for modeling complex functions.\nIn practice, it has been shown that the first layers in a network learn simple features. These are combined in deeper layers to learn and detect increasingly abstract concepts. This is well illustrated with images. When a neural network is applied to images, the first layers learn to detect simple features, such as edges and colors. Further layers then detect shapes and objects, leading to specific items or people (see Figure 8).\n\n\n\n\n\n\nFigure 8: Hierarchical features, from Lee et al. (2011)\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nTrain Neural Networks in your Browser!\nTo directly observe the functioning of a neural network and try out the influence of various configurations, you can train networks directly in the browser using the following link: TensorFlow Playground.",
    "crumbs": [
      "Background & Preparation",
      "2 - Neural Networks"
    ]
  },
  {
    "objectID": "pages/background/neural_networks.html#implementation-in-pytorch",
    "href": "pages/background/neural_networks.html#implementation-in-pytorch",
    "title": "2 - Neural Networks",
    "section": "5 Implementation in PyTorch",
    "text": "5 Implementation in PyTorch\nThe following code shows how we can implement a configurable neural network.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MLP(nn.Module):\n    def __init__(self, input_size, hidden_layer_sizes, num_outputs):\n        super().__init__()\n\n        # Initialize submodules of your module - typically layers\n        # that your module needs - these can then be used in the\n        # forward pass\n\n        self.flatten = nn.Flatten()\n\n        # a ModuleList allows you to flexibly chain submodules\n        # in a list - depending e.g. on configuration parameters\n\n        self.hidden_layers = nn.ModuleList()\n        last_size = input_size\n        for size in hidden_layer_sizes:\n            self.hidden_layers.append(nn.Linear(last_size, size))\n            last_size = size\n\n        self.output_layer = nn.Linear(last_size, num_outputs)\n\n    def forward(self, x):\n        \"\"\"The forward pass of your module.\"\"\"\n        x = self.flatten(x)\n        for layer in self.hidden_layers:\n            x = F.relu(layer(x))\n        x = self.output_layer(x)\n        return x\n\n# Example usage:\n# Initialize the model with input size 784, one hidden layer of size 128, and 10 output units.\nmodel = MLP(784, [128], 10)\n\n# Example input vector (batch size of 32, input size of 784)\nx = torch.randn(32, 784)\n\n# Forward pass\noutput = model(x)\n\nWe can easily display the architecture details with torchinfo\n\n# Display the model architecture\nfrom torchinfo import summary\nsummary(model, input_size=(32, 784))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nMLP                                      [32, 10]                  --\n├─Flatten: 1-1                           [32, 784]                 --\n├─ModuleList: 1-2                        --                        --\n│    └─Linear: 2-1                       [32, 128]                 100,480\n├─Linear: 1-3                            [32, 10]                  1,290\n==========================================================================================\nTotal params: 101,770\nTrainable params: 101,770\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 3.26\n==========================================================================================\nInput size (MB): 0.10\nForward/backward pass size (MB): 0.04\nParams size (MB): 0.41\nEstimated Total Size (MB): 0.54\n==========================================================================================\n\n\n\n\n\n\n\n\nQuestion\nHow many parameters does this neural network have, and why?\n\n\nAnswer\n\nBased on the torchinfo summary output above, the network has:\nLayer 1: 784 inputs × 128 hidden units + 128 biases = 100,480 parameters\nLayer 2: 128 inputs × 10 outputs + 10 biases = 1,290 parameters\nTotal: 101,770 parameters\nEach fully connected layer has: - Weight matrix of size (input_dim × output_dim) - Bias vector of size (output_dim)\nThis is why the number of parameters grows quickly with network size!",
    "crumbs": [
      "Background & Preparation",
      "2 - Neural Networks"
    ]
  },
  {
    "objectID": "pages/background/neural_networks.html#references",
    "href": "pages/background/neural_networks.html#references",
    "title": "2 - Neural Networks",
    "section": "6 References",
    "text": "6 References\n\n\nHubel, D. H., and T. N. Wiesel. 1959. “Receptive Fields of Single Neurones in the Cat’s Striate Cortex.” The Journal of Physiology 148 (3): 574–91. https://doi.org/10.1113/jphysiol.1959.sp006308.\n\n\nJohnson, Justin. 2022. “EECS 498.008 / 598.008 Deep Learning for Computer Vision.” Lecture {Notes} / {Slides}. https://web.eecs.umich.edu/~justincj/teaching/eecs498/WI2022/.\n\n\nKubilius, Jonas. 2017. “Ventral Visual Stream.” https://figshare.com/articles/figure/Ventral_visual_stream/106794.\n\n\nLee, Honglak, Roger Grosse, Rajesh Ranganath, and Andrew Y. Ng. 2011. “Unsupervised Learning of Hierarchical Representations with Convolutional Deep Belief Networks.” Communications of the ACM 54 (10): 95–103. https://doi.org/10.1145/2001269.2001295.\n\n\nLi, Fei-Fei. 2022. “CS231n Convolutional Neural Networks for Visual Recognition.” Lecture {Notes}. https://cs231n.github.io.\n\n\nPhillips, Devin K. 2015. “Speed of the Human Brain.” Ask A Biologist, May. https://askabiologist.asu.edu/plosable/speed-human-brain.\n\n\nPrince, Simon J. D. 2023. Understanding Deep Learning. MIT Press. https://udlbook.github.io/udlbook/.",
    "crumbs": [
      "Background & Preparation",
      "2 - Neural Networks"
    ]
  },
  {
    "objectID": "pages/background/frameworks.html",
    "href": "pages/background/frameworks.html",
    "title": "3 - Software & Hardware for Deep Learning",
    "section": "",
    "text": "TipLearning Objectives\n\n\n\n\n\nAfter this lecture you should be able to:\n\nExplain the role of computational graphs and automatic differentiation in deep learning frameworks.\nConstruct and inspect tensors, modules, and optimization loops in PyTorch.\nDistinguish autograd vs manual gradient computation and identify common pitfalls (forgetting zero_grad, device mismatches).\nDescribe core hardware considerations (GPU parallelism, data loading bottlenecks, CUDA/cuDNN impact).\nLoad and adapt pre-trained models responsibly for downstream tasks.",
    "crumbs": [
      "Background & Preparation",
      "3 - Software & Hardware for Deep Learning"
    ]
  },
  {
    "objectID": "pages/background/frameworks.html#computational-graph-autograd",
    "href": "pages/background/frameworks.html#computational-graph-autograd",
    "title": "3 - Software & Hardware for Deep Learning",
    "section": "1.1 Computational Graph & Autograd",
    "text": "1.1 Computational Graph & Autograd\nAt the core of neural networks is the Computational Graph. It automatically embeds dependent operations in a directed acyclic graph (DAG). Gradients are tracked as needed, allowing variables to be efficiently updated/trained.\nFigure 2 displays a graph, representing Equation 1. An example with numpy is shown in Listing 1, using random data to evaluate the function including the derivatives.\n\\[\n    f(\\mathbf{A}, \\mathbf{B}, \\mathbf{C}) =  \\sum_{ij} \\big((\\mathbf{A} \\odot \\mathbf{B}) + \\mathbf{C}\\big)_{ij}\n\\tag{1}\\]\n\n\n\n\n\n\nFigure 2: Computational Graph.\n\n\n\n\n\n\n\nListing 1: Manual differentiation with numpy\n\n\nimport numpy as np\n\nnp.random.seed(123)\n\nH, W = 2, 3\n\na = np.random.random(size=(H, W))\nb = np.random.random(size=(H, W))\nc = np.random.random(size=(H, W))\n\nd = a * b\ne = d + c\nf = e.sum()\n\ndf_de = 1.0               # d f / d e\nde_dd = 1.0               # d e / d d   (since e = d + c)\nde_dc = np.ones_like(c)   # d e / d c   (derivative of addition w.r.t. c)\ndd_da = b                 # d (a*b) / d a\ndd_db = a                 # d (a*b) / d b\n\ndf_da = df_de * de_dd * dd_da          # chain rule\ndf_db = df_de * de_dd * dd_db\ndf_dc = df_de * de_dc                  # equals ones\n\nprint(\"df/da=\\n\", df_da)\nprint(\"df/db=\\n\", df_db)\nprint(\"df/dc=\\n\", df_dc)\n\n\n\n\ndf/da=\n [[0.9807642  0.68482974 0.4809319 ]\n [0.39211752 0.34317802 0.72904971]]\ndf/db=\n [[0.69646919 0.28613933 0.22685145]\n [0.55131477 0.71946897 0.42310646]]\ndf/dc=\n [[1. 1. 1.]\n [1. 1. 1.]]\n\n\nListing 2 shows the same example in PyTorch. Using x.backward(), gradients with respect to x are computed for variables connected to x.\n\n\n\n\nListing 2: Automatic differentiation with pytorch\n\n\nimport torch\n\nnp.random.seed(123)\n\nH, W = 2, 3\n\na = torch.tensor(a, requires_grad=True)\nb = torch.tensor(b, requires_grad=True)\nc = torch.tensor(c, requires_grad=True)\n\nd = a * b\ne = d + c\nf = e.sum()\n\nf.backward()\nprint(a.grad)\n\n\n\n\ntensor([[0.9808, 0.6848, 0.4809],\n        [0.3921, 0.3432, 0.7290]], dtype=torch.float64)\n\n\nFigure 3 shows the nodes of the computational graph.\n\nfrom torchviz import make_dot\nmake_dot(f, params={'a': a, 'b': b, 'c': c, 'f':f , 'd': d, 'e':e })\n\n\n\n\n\n\n\nFigure 3: computational graph\n\n\n\n\n\nTo perform the computation on a GPU, a simple instruction (see Listing 3).\n\n\n\n\nListing 3: Move computations to GPU\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device} device\")\n\na = a.to(device=device)\nb = b.to(device=device)\nc = c.to(device=device)\n\n\n\n\nUsing cpu device",
    "crumbs": [
      "Background & Preparation",
      "3 - Software & Hardware for Deep Learning"
    ]
  },
  {
    "objectID": "pages/background/frameworks.html#fundamental-concepts",
    "href": "pages/background/frameworks.html#fundamental-concepts",
    "title": "3 - Software & Hardware for Deep Learning",
    "section": "2.1 Fundamental Concepts",
    "text": "2.1 Fundamental Concepts\nPyTorch is built around three core concepts:\n\nTensor: N-dimensional array, similar to numpy.array but with GPU acceleration\nAutograd: Automatic differentiation to create computational graphs and compute gradients\nModule: Base class (nn.Module) to define components of neural networks with learnable parameters",
    "crumbs": [
      "Background & Preparation",
      "3 - Software & Hardware for Deep Learning"
    ]
  },
  {
    "objectID": "pages/background/frameworks.html#tensors",
    "href": "pages/background/frameworks.html#tensors",
    "title": "3 - Software & Hardware for Deep Learning",
    "section": "2.2 Tensors",
    "text": "2.2 Tensors\ntorch.Tensor is the central data structure in PyTorch. Essentially very similar to numpy.array, it can be easily loaded onto GPUs.\nTensors can be created in various ways. For example, from lists:\n\ndata = [[1, 2],[3, 4]]\nx_data = torch.tensor(data)\nprint(x_data)\n\ntensor([[1, 2],\n        [3, 4]])\n\n\nOr from numpy.ndarray:\n\nnp_array = np.array(data)\nx_np = torch.from_numpy(np_array)\nprint(x_np)\n\ntensor([[1, 2],\n        [3, 4]])\n\n\nOr from other tensors:\n\nx_ones = torch.ones_like(x_data) # retains the properties of x_data\nprint(f\"Ones Tensor: \\n {x_ones} \\n\")\n\nx_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\nprint(f\"Random Tensor: \\n {x_rand} \\n\")\n\nOnes Tensor: \n tensor([[1, 1],\n        [1, 1]]) \n\nRandom Tensor: \n tensor([[0.5693, 0.1311],\n        [0.5135, 0.8947]]) \n\n\n\nOr with randomly generated numbers or constants:\n\nshape = (2,3,)\nrand_tensor = torch.rand(shape)\nones_tensor = torch.ones(shape)\nzeros_tensor = torch.zeros(shape)\n\nprint(f\"Random Tensor: \\n {rand_tensor} \\n\")\nprint(f\"Ones Tensor: \\n {ones_tensor} \\n\")\nprint(f\"Zeros Tensor: \\n {zeros_tensor}\")\n\nRandom Tensor: \n tensor([[0.8125, 0.7230, 0.3602],\n        [0.9850, 0.7342, 0.5373]]) \n\nOnes Tensor: \n tensor([[1., 1., 1.],\n        [1., 1., 1.]]) \n\nZeros Tensor: \n tensor([[0., 0., 0.],\n        [0., 0., 0.]])\n\n\nTensor attributes:\n\ntensor = torch.rand(3,4)\n\nprint(f\"Shape of tensor: {tensor.shape}\")\nprint(f\"Datatype of tensor: {tensor.dtype}\")\nprint(f\"Device tensor is stored on: {tensor.device}\")\n\nShape of tensor: torch.Size([3, 4])\nDatatype of tensor: torch.float32\nDevice tensor is stored on: cpu\n\n\nThere are over 100 operations that can be performed on a tensor. The full list is available here.\nIndexing and Slicing:\n\ntensor = torch.ones(4, 4)\nprint(f\"First row: {tensor[0]}\")\nprint(f\"First column: {tensor[:, 0]}\")\nprint(f\"Last column: {tensor[:, -1]}\")\ntensor[:,1] = 0\nprint(tensor)\n\nFirst row: tensor([1., 1., 1., 1.])\nFirst column: tensor([1., 1., 1., 1.])\nLast column: tensor([1., 1., 1., 1.])\ntensor([[1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.]])\n\n\nJoining tensors:\n\nt1 = torch.cat([tensor, tensor, tensor], dim=1)\nprint(t1)\n\ntensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n\n\nArithmetic operations:\n\n# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\ny1 = tensor @ tensor.T\ny2 = tensor.matmul(tensor.T)\n\ny3 = torch.rand_like(y1)\ntorch.matmul(tensor, tensor.T, out=y3)\n\n\n# This computes the element-wise product. z1, z2, z3 will have the same value\nz1 = tensor * tensor\nz2 = tensor.mul(tensor)\n\nz3 = torch.rand_like(tensor)\ntorch.mul(tensor, tensor, out=z3)\n\ntensor([[1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.]])",
    "crumbs": [
      "Background & Preparation",
      "3 - Software & Hardware for Deep Learning"
    ]
  },
  {
    "objectID": "pages/background/frameworks.html#autograd",
    "href": "pages/background/frameworks.html#autograd",
    "title": "3 - Software & Hardware for Deep Learning",
    "section": "2.3 Autograd",
    "text": "2.3 Autograd\nTo train neural networks, backpropagation is typically used. This calculates the gradient of the loss function with respect to the model parameters. To compute these gradients, PyTorch provides an auto-diff functionality: torch.autograd. This can automatically compute gradients for a computational graph.\n\n\n\n\n\n\nTipAutograd Key Points\n\n\n\n\nSet requires_grad=True on tensors you want to track for gradient computation\nCall .backward() on a scalar loss to compute all gradients\nAccess gradients via .grad attribute of tensors\nUse torch.no_grad() context for inference to save memory and speed up computation\n\n\n\nThe following is an example using a 1-layer neural network (see Figure 4 ):\n\n\n\n\n\n\nFigure 4: Source: PyTorch\n\n\n\nThe network from Figure 4 is defined in PyTorch code in Listing 4.\n\n\n\n\nListing 4: Example computational graph.\n\n\nimport torch\n\nx = torch.ones(5)  # input tensor\ny = torch.zeros(3)  # expected output\nw = torch.randn(5, 3, requires_grad=True)\nb = torch.randn(3, requires_grad=True)\nz = torch.matmul(x, w)+b\nloss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n\n\n\n\nWe can now use Autograd to compute the gradient:\n\nloss.backward()\nprint(w.grad)\nprint(b.grad)\n\ntensor([[0.1172, 0.3256, 0.3068],\n        [0.1172, 0.3256, 0.3068],\n        [0.1172, 0.3256, 0.3068],\n        [0.1172, 0.3256, 0.3068],\n        [0.1172, 0.3256, 0.3068]])\ntensor([0.1172, 0.3256, 0.3068])",
    "crumbs": [
      "Background & Preparation",
      "3 - Software & Hardware for Deep Learning"
    ]
  },
  {
    "objectID": "pages/background/frameworks.html#torch.nn",
    "href": "pages/background/frameworks.html#torch.nn",
    "title": "3 - Software & Hardware for Deep Learning",
    "section": "2.4 torch.nn",
    "text": "2.4 torch.nn\nPyTorch provides various building blocks for creating neural networks. These are available in torch.nn. Additionally, you can define any compositions of such building blocks that inherit from torch.nn.Module. A neural network is typically a torch.nn.Module. Each module implements the forward() method to define how data is processed.\n\n\n\n\n\n\nTipBuilding Blocks in torch.nn\n\n\n\nCommon layers:\n\nnn.Linear: Fully connected layer\nnn.Conv2d: 2D convolutional layer\nnn.MaxPool2d: Max pooling layer\nnn.Dropout: Regularization via random dropout\nnn.BatchNorm2d: Batch normalization\n\nCommon activations:\n\nnn.ReLU(), nn.LeakyReLU(), nn.GELU()\nnn.Sigmoid(), nn.Softmax()\n\nLoss functions:\n\nnn.CrossEntropyLoss(): Classification\nnn.BCEWithLogitsLoss(): Binary classification\nnn.MSELoss(): Regression\n\n\n\nListing 5 depicts an example of a class that inherits from torch.nn.Module.\n\n\n\n\nListing 5: Example nn.Module.\n\n\nfrom torch import nn\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\n\n\n\nYou can also visualize the model:\n\nmodel = NeuralNetwork()\nprint(model)\n\nNeuralNetwork(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear_relu_stack): Sequential(\n    (0): Linear(in_features=784, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=512, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=512, out_features=10, bias=True)\n  )\n)\n\n\nTo use the model, you can pass input data. This will execute the forward() method, along with background operations.\n\nX = torch.rand(1, 28, 28)\nlogits = model(X)\npred_probab = nn.Softmax(dim=1)(logits)\ny_pred = pred_probab.argmax(1)\nprint(f\"Predicted class: {y_pred}\")\n\nPredicted class: tensor([1])\n\n\nThe executed operations will look like this:\n\nfrom torchviz import make_dot\nmake_dot(logits)\n\n\n\n\n\n\n\n\nFor more in-depth information about the model you can use the external library torchinfo as seen in Listing 6.\n\n\n\n\nListing 6: Example output of torchinfo.\n\n\nfrom torchinfo import summary\nmodel = NeuralNetwork()\nsummary(model, input_size=(128, 1, 28, 28))\n\n\n\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nNeuralNetwork                            [128, 10]                 --\n├─Flatten: 1-1                           [128, 784]                --\n├─Sequential: 1-2                        [128, 10]                 --\n│    └─Linear: 2-1                       [128, 512]                401,920\n│    └─ReLU: 2-2                         [128, 512]                --\n│    └─Linear: 2-3                       [128, 512]                262,656\n│    └─ReLU: 2-4                         [128, 512]                --\n│    └─Linear: 2-5                       [128, 10]                 5,130\n==========================================================================================\nTotal params: 669,706\nTrainable params: 669,706\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 85.72\n==========================================================================================\nInput size (MB): 0.40\nForward/backward pass size (MB): 1.06\nParams size (MB): 2.68\nEstimated Total Size (MB): 4.14\n==========================================================================================",
    "crumbs": [
      "Background & Preparation",
      "3 - Software & Hardware for Deep Learning"
    ]
  },
  {
    "objectID": "pages/background/frameworks.html#torch.optim",
    "href": "pages/background/frameworks.html#torch.optim",
    "title": "3 - Software & Hardware for Deep Learning",
    "section": "2.5 torch.optim",
    "text": "2.5 torch.optim\nTo optimize the parameters of a model, you need an optimization algorithm. torch.optim implements various algorithms, such as Stochastic Gradient Descent or the often used Adam Optimizer.\n\n\n\n\n\n\n\nTipCommon Optimizers\n\n\n\n\nSGD: Basic stochastic gradient descent (with optional momentum)\nAdam: Adaptive learning rates, good default choice\nAdamW: Adam with weight decay, often better for transformers (see Loshchilov and Hutter (2019))\nRMSprop: Adaptive learning rates, good for RNNs\nLearning rate schedulers: Adjust learning rate during training (e.g., torch.optim.lr_scheduler)\n\nRule of thumb: Start with Adam, then try SGD with momentum if you need better generalization.\n\n\n\nfrom torch import optim\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n\nYou can then use the optimizer to adjust the parameters by minimizing a loss function. See Listing 7 for an example.\n\n\n\n\nListing 7: Dummy iterations to minimize a loss function using an optimizer.\n\n\nloss_fn = torch.nn.CrossEntropyLoss()\nfor i in range(0, 3):\n    input, target = torch.rand(1, 28, 28), torch.randint(low=0, high=10, size=(1, ))\n    optimizer.zero_grad()\n    output = model(input)\n    loss = loss_fn(output, target)\n    loss.backward()\n    optimizer.step()\n\n\n\n\n\n\n\n\n\n\nTipCommon Pitfalls & Solutions\n\n\n\nPitfall 1: Forgetting zero_grad()\n# ❌ Wrong - gradients accumulate\nfor batch in dataloader:\n    loss = compute_loss(batch)\n    loss.backward()\n    optimizer.step()\n\n# ✅ Correct\nfor batch in dataloader:\n    optimizer.zero_grad()  # Clear previous gradients\n    loss = compute_loss(batch)\n    loss.backward()\n    optimizer.step()\nPitfall 2: Device mismatch\n# ❌ Wrong - model on GPU, data on CPU\nmodel = model.cuda()\nfor batch_x, batch_y in dataloader:\n    pred = model(batch_x)  # Error!\n\n# ✅ Correct\nmodel = model.cuda()\nfor batch_x, batch_y in dataloader:\n    batch_x = batch_x.cuda()\n    batch_y = batch_y.cuda()\n    pred = model(batch_x)\nPitfall 3: Not using eval() mode for inference\n# ❌ Wrong - dropout and batchnorm active during inference\npredictions = model(test_data)\n\n# ✅ Correct\nmodel.eval()\nwith torch.no_grad():\n    predictions = model(test_data)\nmodel.train()  # Switch back to training mode",
    "crumbs": [
      "Background & Preparation",
      "3 - Software & Hardware for Deep Learning"
    ]
  },
  {
    "objectID": "pages/background/frameworks.html#training-loops",
    "href": "pages/background/frameworks.html#training-loops",
    "title": "3 - Software & Hardware for Deep Learning",
    "section": "2.6 Training Loops",
    "text": "2.6 Training Loops\nTypically, you put together a training loop to train a model. A training loop iterates over batches of data and optimizes the model parameters with each iteration.\n\n\n\n\nListing 8: Training and test loop.\n\n\ndef train_loop(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    model.train()  # &lt;-- important!\n    for batch, (X, y) in enumerate(dataloader):\n        # Compute prediction and loss\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:&gt;7f}  [{current:&gt;5d}/{size:&gt;5d}]\")\n\n\ndef test_loop(dataloader, model, loss_fn):\n    model.eval()  # &lt;-- important!\n\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    test_loss, correct = 0, 0\n\n    with torch.no_grad():  # disables gradient tracking for efficiency\n        for X, y in dataloader:\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n\n    test_loss /= num_batches\n    correct /= size\n    print(\n        f\"Test Error:\\n Accuracy: {(100 * correct):&gt;0.1f}%, Avg loss: {test_loss:&gt;8f}\\n\"\n    )\n\n\n\n\n\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\nepochs = 10\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train_loop(train_dataloader, model, loss_fn, optimizer)\n    test_loop(test_dataloader, model, loss_fn)\nprint(\"Done!\")\n\n\n\n\n\n\n\nNote\n\n\n\nHigh-level APIs such as Lightning and Keras provide many functionalities to simplify managing training loops. It is highly recommended to use such libraries to reduce boiler-plate code. However, it depends on the individual complexity of a project to what degree such libraries are useful.",
    "crumbs": [
      "Background & Preparation",
      "3 - Software & Hardware for Deep Learning"
    ]
  },
  {
    "objectID": "pages/background/frameworks.html#pre-trained-models",
    "href": "pages/background/frameworks.html#pre-trained-models",
    "title": "3 - Software & Hardware for Deep Learning",
    "section": "2.7 Pre-trained models",
    "text": "2.7 Pre-trained models\nSince training models can be time-consuming and expensive, pre-trained models are often used. They allow models to be adapted to a specific task more quickly and cost-effectively. In many areas, particularly NLP and computer vision, using pre-trained models is standard. PyTorch provides torchvision for computer vision applications. torchvision provides functionalities useful for modeling image data. Pre-trained models can also be easily integrated, as shown in Listing 9.\n\n\n\n\nListing 9: Example of importing ResNet-50 model with pre-trained weights from ImageNet.\n\n\nfrom torchvision.models import resnet50, ResNet50_Weights\n\nweights = ResNet50_Weights.IMAGENET1K_V2\nmodel = resnet50(weights=weights)",
    "crumbs": [
      "Background & Preparation",
      "3 - Software & Hardware for Deep Learning"
    ]
  },
  {
    "objectID": "pages/background/frameworks.html#tensorflow",
    "href": "pages/background/frameworks.html#tensorflow",
    "title": "3 - Software & Hardware for Deep Learning",
    "section": "3.1 TensorFlow",
    "text": "3.1 TensorFlow\nFor a long time, PyTorch and TensorFlow have been the two dominant deep learning frameworks. TensorFlow, developed by Google, is known for its production-readiness, ecosystem integration (e.g., TensorFlow Extended (TFX) for MLOps pipelines, LiteRT for mobile, and TensorFlow.js for web), and its scalability across distributed hardware.\nIn recent years, however, TensorFlow’s low-level API has become less popular in research compared to PyTorch, which offers more flexibility and Pythonic design. Today, most TensorFlow users rely almost exclusively on Keras, its high-level API, to define and train models, while the TensorFlow backend provides performance, device management, and deployment capabilities.",
    "crumbs": [
      "Background & Preparation",
      "3 - Software & Hardware for Deep Learning"
    ]
  },
  {
    "objectID": "pages/background/frameworks.html#keras",
    "href": "pages/background/frameworks.html#keras",
    "title": "3 - Software & Hardware for Deep Learning",
    "section": "3.2 Keras",
    "text": "3.2 Keras\nKeras started as an independent high-level deep learning API designed to simplify model creation with a clean and intuitive syntax. Since 2017, it has been tightly integrated into TensorFlow as its official front-end (tf.keras), and in 2023, Keras Core was introduced, a framework-agnostic version that can run on multiple backends such as TensorFlow, JAX, and PyTorch.\nKeras focuses on ease of use, modularity, and rapid prototyping, making it an excellent choice for teaching, applied machine learning, and fast experimentation, while still being production-ready through TensorFlow’s ecosystem. See Listing 10 for example code using Keras.\n\n\n\n\nListing 10: Example with TensorFlow.\n\n\nimport tensorflow as tf\n\nclass NeuralNetwork(tf.keras.Model):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.flatten = tf.keras.layers.Flatten()\n        self.linear_relu_stack = tf.keras.Sequential([\n            tf.keras.layers.Dense(512, activation='relu'),\n            tf.keras.layers.Dense(512, activation='relu'),\n            tf.keras.layers.Dense(10)  # No activation for logits\n        ])\n\n    def call(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits",
    "crumbs": [
      "Background & Preparation",
      "3 - Software & Hardware for Deep Learning"
    ]
  },
  {
    "objectID": "pages/background/frameworks.html#jax",
    "href": "pages/background/frameworks.html#jax",
    "title": "3 - Software & Hardware for Deep Learning",
    "section": "3.3 Jax",
    "text": "3.3 Jax\nJax has gained significant popularity in recent years. Developed by researchers at Google, it is primarily used in research and foundation-model development. Jax provides a NumPy-like API with automatic differentiation (autodiff) and function transformations such as jit (for compilation) and vmap (for vectorization). It enables high-performance, composable numerical computing, and serves as the foundation for frameworks such as Flax (Neural Networks) and Haiku (ML Research). See Listing 11 for an example using Flax.\n\n\n\n\nListing 11: Example with JAX and flax.\n\n\nfrom flax import linen as nn\nimport jax.numpy as jnp\n\nclass NeuralNetwork(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        x = x.reshape((x.shape[0], -1))  # Flatten the input\n        x = nn.Dense(512)(x)\n        x = nn.relu(x)\n        x = nn.Dense(512)(x)\n        x = nn.relu(x)\n        logits = nn.Dense(10)(x)\n        return logits",
    "crumbs": [
      "Background & Preparation",
      "3 - Software & Hardware for Deep Learning"
    ]
  },
  {
    "objectID": "pages/background/frameworks.html#scikit-learn",
    "href": "pages/background/frameworks.html#scikit-learn",
    "title": "3 - Software & Hardware for Deep Learning",
    "section": "3.4 Scikit-Learn",
    "text": "3.4 Scikit-Learn\nScikit-Learn is THE machine learning framework in Python. However, Scikit-Learn never covered the area of neural networks and lacks auto-diff functionality. Therefore, Scikit-Learn is irrelevant when training neural networks. However, Scikit-Learn functionalities are often used to carry out the machine learning process, such as splitting datasets into train, validation, and test sets. Also, visualizations, such as the confusion matrix or calculating metrics, can be done via Scikit-Learn.",
    "crumbs": [
      "Background & Preparation",
      "3 - Software & Hardware for Deep Learning"
    ]
  },
  {
    "objectID": "pages/background/frameworks.html#onnx",
    "href": "pages/background/frameworks.html#onnx",
    "title": "3 - Software & Hardware for Deep Learning",
    "section": "3.5 ONNX",
    "text": "3.5 ONNX\ny§ ONNX (Open Neural Network Exchange) is an open format to represent machine learning models. It allows models trained in one framework to be transferred to another. Trained models can also be deployed on various platforms.",
    "crumbs": [
      "Background & Preparation",
      "3 - Software & Hardware for Deep Learning"
    ]
  },
  {
    "objectID": "pages/background/frameworks.html#monitoring",
    "href": "pages/background/frameworks.html#monitoring",
    "title": "3 - Software & Hardware for Deep Learning",
    "section": "3.6 Monitoring",
    "text": "3.6 Monitoring\nWhen training models, monitoring the training process, debugging, and logging hyperparameters, metrics, etc., is very important. Various tools enable these functionalities. Well-known examples are TensorBoard and Weights & Biases.",
    "crumbs": [
      "Background & Preparation",
      "3 - Software & Hardware for Deep Learning"
    ]
  },
  {
    "objectID": "pages/background/frameworks.html#tensor-operations",
    "href": "pages/background/frameworks.html#tensor-operations",
    "title": "3 - Software & Hardware for Deep Learning",
    "section": "4.1 Tensor Operations",
    "text": "4.1 Tensor Operations\nIn neural networks, there are many tensor operations. Tensors are essentially multi-dimensional arrays, such as a scalar \\(x\\), a vector \\(\\mathbf{x}\\), or a matrix \\(\\mathbf{X}\\).\nFigure 5 illustrates a matrix multiplication, a typical representative of a tensor operation. As you can see, the calculations (entries of the matrix \\(\\mathbf{A}\\mathbf{C}\\)) are independent of each other and can be fully parallelized.\n\n\n\n\n\n\nFigure 5: Matrix Multiplication (from Li (2022)).",
    "crumbs": [
      "Background & Preparation",
      "3 - Software & Hardware for Deep Learning"
    ]
  },
  {
    "objectID": "pages/background/frameworks.html#graphics-processing-units-gpus",
    "href": "pages/background/frameworks.html#graphics-processing-units-gpus",
    "title": "3 - Software & Hardware for Deep Learning",
    "section": "4.2 Graphics Processing Units (GPUs)",
    "text": "4.2 Graphics Processing Units (GPUs)\nGPUs have made deep learning possible in the first place. With their parallel structure, they can efficiently compute parallelizable tasks such as tensor operations.\nCPUs have far fewer cores than GPUs, but they are faster and can handle more complex tasks. CPUs are therefore ideal for sequential tasks. GPUs have many more cores, which are less complex and slower. Therefore, GPUs are excellent for parallel tasks. Figure 6 illustrates the differences.\n\n\n\n\n\n\nFigure 6: CPU vs GPU example (from Li (2022)).",
    "crumbs": [
      "Background & Preparation",
      "3 - Software & Hardware for Deep Learning"
    ]
  },
  {
    "objectID": "pages/background/frameworks.html#cuda-cudnn",
    "href": "pages/background/frameworks.html#cuda-cudnn",
    "title": "3 - Software & Hardware for Deep Learning",
    "section": "4.3 CUDA & cuDNN",
    "text": "4.3 CUDA & cuDNN\nCUDA is an API by Nvidia to perform computations on the GPU. It allows parallelizable tasks to be implemented efficiently. cuDNN is a library that efficiently executes certain operations, such as convolutions, in neural networks on the GPU. cuDNN is based on CUDA and significantly accelerates the training of neural networks. Figure 7 illustrates speed differences when training various neural networks with CPU, GPU, and optimized cuDNN.\n\n\n\n\n\n\nFigure 7: Speed comparison (from Li (2022), data from Link)",
    "crumbs": [
      "Background & Preparation",
      "3 - Software & Hardware for Deep Learning"
    ]
  },
  {
    "objectID": "pages/background/frameworks.html#data-loading",
    "href": "pages/background/frameworks.html#data-loading",
    "title": "3 - Software & Hardware for Deep Learning",
    "section": "4.4 Data Loading",
    "text": "4.4 Data Loading\nA crucial bottleneck in practice is the transfer of data (such as images) from the disk to the GPU. If this transfer is not fast enough, it is referred to as GPU starvation. There are several approaches to solve this problem:\n\nRead the data into RAM (not feasible for larger datasets)\nUse fast disks, such as SSDs\nUtilize multiple CPU threads to read data in parallel and keep it in RAM (pre-fetching)\n\nFigure 8 shows the various components.\n\n\n\n\n\n\nFigure 8: Source: Li (2022)\n\n\n\nDeep learning frameworks like PyTorch implement special classes that allow data to be prepared in multiple threads. Sometimes a certain number of CPU cores is needed to supply a GPU with enough data. Figure 9 shows a starved GPU: You can clearly see that the utilization repeatedly drops to 0 because the GPU has to wait for data.\n\n\n\n\n\n\nFigure 9: The Y-axis shows the GPU utilization in percentage, while the X-axis represents time. Source",
    "crumbs": [
      "Background & Preparation",
      "3 - Software & Hardware for Deep Learning"
    ]
  },
  {
    "objectID": "pages/background/frameworks.html#gpu-parallelism",
    "href": "pages/background/frameworks.html#gpu-parallelism",
    "title": "3 - Software & Hardware for Deep Learning",
    "section": "4.5 GPU Parallelism",
    "text": "4.5 GPU Parallelism\nModels can also be trained on multiple GPUs. There are two main paradigms: data parallelism and model parallelism (see Figure 10 ). With data parallelism, each GPU has a copy of the model, and each GPU is trained on different data batches. With model parallelism, the model is split across multiple GPUs. Models can be trained on a server with multiple GPUs or even over the network (distributed). ML frameworks provide functionalities to handle these.\n\n\n\n\n\n\nFigure 10: Data and Model Parallelism (from Li (2022)).",
    "crumbs": [
      "Background & Preparation",
      "3 - Software & Hardware for Deep Learning"
    ]
  },
  {
    "objectID": "pages/lectures/recent_advances.html",
    "href": "pages/lectures/recent_advances.html",
    "title": "6 - Recent Advances",
    "section": "",
    "text": "TipLearning Objectives\n\n\n\n\n\nBy the end of this lecture you should be able to:\n\nDefine foundation models and explain how large-scale pre-training enables transfer learning across diverse downstream tasks with minimal task-specific data.\nDescribe CLIP’s contrastive learning approach for joint vision-language understanding and apply it to zero-shot image classification and retrieval tasks.\nUnderstand multi-modal models (VQA, BLIP2, Flamingo) that process visual and textual data simultaneously for tasks like visual question answering and accessibility applications.\nExplain Vision Transformer (ViT) architecture, including how images are split into patches, linearly embedded, and processed with self-attention mechanisms.\nCompare architectural differences between CNNs and Vision Transformers regarding inductive biases, receptive fields, and weight sharing patterns.\nEvaluate trade-offs between CNNs and ViTs in terms of data efficiency, scalability, robustness to corruptions, and computational requirements.\nApply appropriate architecture selection strategies based on dataset size, computational resources, and task requirements (local vs global context).\nRecognize that hybrid architectures combining convolutional and transformer layers leverage benefits of both inductive biases and global attention mechanisms.",
    "crumbs": [
      "Lectures",
      "6 - Recent Advances"
    ]
  },
  {
    "objectID": "pages/lectures/recent_advances.html#foundation-models",
    "href": "pages/lectures/recent_advances.html#foundation-models",
    "title": "6 - Recent Advances",
    "section": "1 Foundation Models",
    "text": "1 Foundation Models\nFoundation models are large-scale machine learning models trained on vast amounts of data that can be fine-tuned for various downstream tasks. These models have demonstrated remarkable capabilities in natural language processing, computer vision, and other fields.\n\n1.1 Characteristics of Foundation Models\n\nLarge-scale Pre-training: Foundation models are pre-trained on extensive datasets, enabling them to capture a wide range of knowledge.\nTransfer Learning: These models can be fine-tuned on specific tasks with relatively small datasets, making them versatile and efficient.\nMultimodal Capabilities: Some foundation models can process and integrate multiple types of data, such as text and images.\n\n\n\n1.2 CLIP: A Foundation Model Example\nCLIP (Contrastive Language-Image Pre-training, Radford et al. (2021)) is a foundation model developed by OpenAI. It is designed to understand images and text jointly, making it capable of tasks like zero-shot image classification.\n\n1.2.1 How CLIP Works\n\n\n\n\n\n\nFigure 1: Illustration of CLIP. From Radford et al. (2021).\n\n\n\nCLIP is pre-trained on a diverse dataset of images and their corresponding textual descriptions. It learns to associate images with their textual descriptions using a contrastive learning approach, which maximizes the similarity between correct image-text pairs and minimizes the similarity between incorrect pairs.\n\n\n1.2.2 How CLIP can be applied\n\n\n\n\n\n\nFigure 2: Zero-shot learning with CLIP. From Radford et al. (2021).\n\n\n\n\n\n1.2.3 Applications of CLIP\n\nZero-Shot Classification: CLIP can classify images into categories it has not explicitly been trained on by leveraging its understanding of language.\nImage Search: By inputting a textual description, CLIP can retrieve relevant images from a database.\n(near) Duplicate Detection: CLIP can assist in identifying near duplicate images.\n\n\n\n\n1.3 Example\nHere’s a simple example of using CLIP for zero-shot image classification:\n\nimport requests\nfrom PIL import Image\nfrom transformers import CLIPModel, CLIPProcessor\n\n# 1) Get Models and Data Pre-Processors\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n# 2) Define Input Data: Image and Texts\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprompt_template = \"a photo of a {}\"\nclasses = [\"cat\", \"dog\", \"car\", \"tree\"]\nprompts = [prompt_template.format(class_name) for class_name in classes]\n\n# 3) Pre-Process Inputs\ninputs = processor(\n    text=prompts,\n    images=image,\n    return_tensors=\"pt\",\n    padding=True,\n)\n\n# 4) Forward-Pass\nwith torch.no_grad():\n    outputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n\n# 5) Find most likely label\nprobs = logits_per_image.softmax(dim=1)\nprint(\"Label:\", classes[probs.argmax().item()])\n\n\n1.3.1 Fine-Tuning CLIP\nIf CLIP does not perform well with zero-shot learning, it can be fine-tuned with various techniques. This is a powerful mechanism because the features that CLIP has learned are very powerful and might be useful for your target task.",
    "crumbs": [
      "Lectures",
      "6 - Recent Advances"
    ]
  },
  {
    "objectID": "pages/lectures/recent_advances.html#multi-modal-models",
    "href": "pages/lectures/recent_advances.html#multi-modal-models",
    "title": "6 - Recent Advances",
    "section": "2 Multi-Modal Models",
    "text": "2 Multi-Modal Models\nMulti-modal models are able to process mutliple types of data (modes) simultaneously.\n\n2.1 Visual Question Anwering\nOne task where multi-modality is required, is Visual Question Answering (VQA). Only by jointly understanding the visual input and the question such tasks can be solved. See Figure 3 for an example from a chat system.\n\n\n\n\n\n\nFigure 3: BLIP2 examples from a chat system. Source: Li et al. (2023)\n\n\n\n\n\n2.2 How VQA Works\nVQA models combine visual data (images) with textual data (questions) to generate accurate answers. These models are typically pre-trained on large datasets containing images, questions about those images, and the corresponding answers. Figure 4 shows training data examples that were used in the Flamingo model Alayrac et al. (2022). That particular model, and many other multi-modal models, is based on pre-trained foundation language and visual models and subsequently fine-tuned to learn how to combine their features for multi-modal tasks.\n\n\n\n\n\n\nFigure 4: Training data examples. Source: Alayrac et al. (2022)\n\n\n\n\n\n2.3 Applications of VQA\n\nAccessibility: VQA can help visually impaired users by answering questions about their surroundings based on images captured by a camera.\nChat Bots with Visual Understanding: VQA systems can be used in chat bots to incorporate visual inputs.\nCustomer Support: VQA can enhance customer support by allowing users to submit images and ask questions about products or services.",
    "crumbs": [
      "Lectures",
      "6 - Recent Advances"
    ]
  },
  {
    "objectID": "pages/lectures/recent_advances.html#vision-transfomers",
    "href": "pages/lectures/recent_advances.html#vision-transfomers",
    "title": "6 - Recent Advances",
    "section": "3 Vision Transfomers",
    "text": "3 Vision Transfomers\nTransformers have emerged as foundational architectures in modern machine learning, significantly impacting NLP, computer vision, and multimodal tasks.\n\n3.1 What Are Transformers?\nTransformers, introduced by Vaswani et al. (2017), use a self-attention mechanism that enables the model to consider relationships between different parts of the input data. This enables transformers to process data in parallel and capture long-range dependencies more effectively than previous architectures (such as recurrent neural networks).\nKey Concepts in Transformers:\n\nSelf-Attention: Enables each input element to focus on all other elements, capturing complex dependencies within the data.\nPositional Encoding: Injects information about the order of the sequence, allowing the transformer to process sequential data despite its parallel structure.\nScalability: The architecture scales well, making it suitable for training on large datasets across various domains.\n\n\n\n3.2 Vision Transformers (ViT)\nVision Transformers (ViTs), proposed by Dosovitskiy et al. (2020), apply the transformer architecture to image data by splitting images into fixed-size patches and treating each patch as a token in a sequence. This approach enables ViTs to capture spatial relationships in images without convolutional layers.\n\n\n\n\n\n\nFigure 5: An image is split into fixed size patches. Each patch is linearly transformed (patch embedding). According to the patch position in the image, a specific position embedding is added. The sequence of embeddings is then processed with a standard transfomer. Source: Dosovitskiy et al. (2020)\n\n\n\nApplications of Vision Transformers\n\nImage Classification: ViTs have achieved competitive results in image classification, sometimes outperforming CNNs.\nObject Detection and Segmentation: Transformers can be adapted for tasks like object detection by directly predicting bounding boxes, as shown in models like DETR (Carion et al. (2020)).\nMultimodal Models: Transformers enable models to integrate data from different sources, such as text and images, for tasks like image captioning and visual question answering.\n\n\n\n3.3 Comparison: Vision Transformers and Convolutional Neural Networks\nViTs and CNNs are both prominent architectures in computer vision, each with unique strengths and weaknesses.\n\n3.3.1 Architectural Differences\nCNNs are designed with built-in spatial inductive biases. Using convolutions assumes that neighbouring pixels are related and that local patterns are relevant (restricted receptive field) and should be detected everywhere in the input (weight sharing). This makes CNNs efficient with limited data and effective at capturing local image features. Local features can be aggregatred hierarchically to capture global patterns.\nViTs process images by dividing them into fixed-sized patches, treating each patch as a “token” akin to words in natural language processing. They then use self-attention mechanisms to capture relationships between all patches simultaneously, allowing the model to learn global dependencies without focusing exclusively on local information already in the first layer (no restrictions in the receptive field).\n\n\n3.3.2 Scalability and Flexibility\nGenerally: inductive biases are beneficial because they do not need to be learned from data. However, inductive biases also restrict a models flexibility since such biases might not always be perfectly aligned with the true data distribution. While CNNs excel in low-data regimes and with resource-limited applications, ViTs are scalable and perform well with self-supervised learning on large datasets. ViTs have shown excellent performance on massive datasets (10M+ images), even surpassing CNNs when pretrained with sufficient data and augmented with techniques like self-supervised learning.\n\n\n3.3.3 Robustness and Transferability\nRobustness to noise and corruptions (see Figure 6), as well as the ability to transfer learned features to downstream tasks is highly relevant in practical settings. Some studies indicate that ViTs can handle image corruptions and data shifts more robustly than CNN (Naseer et al. (2021)), however, is contradicted by newer studies (Pinto, Torr, and Dokania (2022)).\n\n\n\n\n\n\nFigure 6: An image with different distortions to evaluate the robustness of methods. Source: Naseer et al. (2021).\n\n\n\n\n\n3.3.4 Pros and Cons\nCNNs:\n\nPros:\n\nEfficiency for Small Datasets: CNNs excel in small to moderate datasets, as they are highly effective in learning spatial features with fewer parameters.\nRobustness to Variations: Their design enables in- and equivariance to small geometrical transformations, making them resilient in many real-world applications.\n\nCons:\n\nLimited Long-Range Dependencies: Due to their focus on local receptive fields, CNNs may struggle with capturing long-range dependencies unless deeper architectures are used.\n\n\nVision Transformers (ViTs):\n\nPros:\n\nScalability and Flexibility: ViTs perform well with large datasets and can be scaled efficiently by increasing model size, often benefiting from self-supervised pre-training on massive image corpora.\nLong-Range Dependency Capture: Self-attention mechanisms allow ViTs to capture long-range dependencies between patches, which can be beneficial in tasks requiring global context.\n\nCons:\n\nData Efficiency: ViTs often require significantly more data and pre-training to achieve competitive performance, as they lack the inherent spatial inductive bias of CNNs.\nComputational Intensity: Self-attention mechanisms, especially for high-resolution images, can be computationally demanding and require substantial memory, limiting their efficiency.\n\n\nIn summary, while CNNs remain highly effective for many computer vision tasks, especially when data or compute resources are limited, ViTs provide a scalable alternative that excels with larger datasets and tasks requiring a global view of image data.\n\n\n\n3.4 Conclusion\nCurrently, transformers and CNNs perform similarily when pre-trained on massive datasets. There has been various researach comparing both architectures, none seems to be able to conclusively judge which architecture is superior (e.g. Smith et al. (2023)). The inductive biases of CNNs are often valuable, as is the ability of transformers to consider global relations early. This has led to the development of hybrid architectures that combine convolutional layers with transformer layers. It remains to be seen where this research leads.\n\n\n\n\n\n\nHere is an interesting blog post on the topic of which architecture to choose: Werff (2024).",
    "crumbs": [
      "Lectures",
      "6 - Recent Advances"
    ]
  },
  {
    "objectID": "pages/lectures/recent_advances.html#references",
    "href": "pages/lectures/recent_advances.html#references",
    "title": "6 - Recent Advances",
    "section": "4 References",
    "text": "4 References\n\n\nAlayrac, Jean-Baptiste, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, et al. 2022. “Flamingo: A Visual Language Model for Few-Shot Learning.” arXiv. http://arxiv.org/abs/2204.14198.\n\n\nCarion, Nicolas, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. 2020. “End-to-End Object Detection with Transformers.” arXiv. http://arxiv.org/abs/2005.12872.\n\n\nDosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2020. “An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.” arXiv:2010.11929 [Cs], October. http://arxiv.org/abs/2010.11929.\n\n\nLi, Junnan, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. “BLIP-2: Bootstrapping Language-Image Pre-Training with Frozen Image Encoders and Large Language Models.” arXiv. http://arxiv.org/abs/2301.12597.\n\n\nNaseer, Muzammal, Kanchana Ranasinghe, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. 2021. “Intriguing Properties of Vision Transformers.” arXiv. http://arxiv.org/abs/2105.10497.\n\n\nPinto, Francesco, Philip H. S. Torr, and Puneet K. Dokania. 2022. “An Impartial Take to the CNN Vs Transformer Robustness Contest.” arXiv. http://arxiv.org/abs/2207.11347.\n\n\nRadford, Alec, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, et al. 2021. “Learning Transferable Visual Models From Natural Language Supervision.” arXiv:2103.00020 [Cs], February. http://arxiv.org/abs/2103.00020.\n\n\nSmith, Samuel L., Andrew Brock, Leonard Berrada, and Soham De. 2023. “ConvNets Match Vision Transformers at Scale.” arXiv. http://arxiv.org/abs/2310.16764.\n\n\nVaswani, Ashish, Google Brain, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” Advances in Neural Information Processing Systems, no. Nips: 5998–6008. https://arxiv.org/abs/1706.03762.\n\n\nWerff, Tobias van der. 2024. “CNN Vs. Vision Transformer: A Practitioner’s Guide to Selecting the Right Model.” https://tobiasvanderwerff.github.io/2024/05/15/cnn-vs-vit.html.",
    "crumbs": [
      "Lectures",
      "6 - Recent Advances"
    ]
  },
  {
    "objectID": "pages/lectures/intro.html",
    "href": "pages/lectures/intro.html",
    "title": "1 - Introduction",
    "section": "",
    "text": "TipLearning Objectives\n\n\n\n\n\nAfter this lecture you should be able to:\n\nDistinguish core computer vision tasks.\nDescribe key real-world application domains and practical challenges.\nExplain why deep learning reshaped computer vision.",
    "crumbs": [
      "Lectures",
      "1 - Introduction"
    ]
  },
  {
    "objectID": "pages/lectures/intro.html#motivation",
    "href": "pages/lectures/intro.html#motivation",
    "title": "1 - Introduction",
    "section": "1 Motivation",
    "text": "1 Motivation\nDeep Learning models have played a transformative role in Computer Vision over the last decade, significantly enhancing and expanding the capabilities to process visual data.\nThe integration of deep learning in computer vision has massively improved the accuracy and efficiency of visual recognition, classification, and analysis. This has opened up new possibilities in applications such as automated driving, facial recognition, and medical image analysis. These models are now embedded in a wide variety of products and services.\nLet’s take a look at several applications of deep learning assisted computer vision applications.\n\n1.1 Applications\nTo monitor wildlife populations biologists might deploy camera traps to capture images from passing animals (see Figure 1). These images need to be analysed, for example, biologists need to know which species can be seen in each image. Deep learning can speed up that process significantly (Willi et al. (2019)).\n\n\n\n\n\n\nSource: Breitenmoser-Würsten et al. (2024)\n\n\n\n\nFigure 1: Image from a camera trap.\n\n\n\nRecent advances in generative deep learning enable anyone to create photorealistic synthetic images (see Figure 2 for examples). While these technologies have many positive applications (art, design, prototyping), they also raise concerns about misinformation and deepfakes, motivating research into synthetic media detection (Mathys, Willi, and Meier (2024)).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Examples of synthetic images: left from Mathys et al. (2024), right from Source.\n\n\n\n\nApple’s photo library uses deep learning to automatically classify and search through your photos. The system can recognize scenes (beach, mountain), objects (dog, car), and even activities (birthday, wedding). Additionally, sophisticated safety systems scan for problematic content while preserving privacy through on-device processing (Figure 3).\n\n\n\n\n\n\nFigure 3: Apple’s on-device safety features detect sensitive content while maintaining user privacy. Source\n\n\n\nDeep learning can be used to identify objects, to find similar images (image search / retrieval), ro to find (near-)duplicates (perceptual hashing), see Figure 4 (left). Increasingly, models can cross modalities and, for example, process visual data and produce textual outputs. An interesting example is image captioning or the detection and translation of text in images (optical character recognition - OCR), see Figure 4 (right).\n\n\n\n\n\n\nIdentification & Search\n\n\n\n\n\n\n\nTranslation\n\n\n\nFigure 4: Google Lens\n\nOne area with high commercial interest and competition is the development of self-driving capabilities. Figure 5 shows a demonstration of the perception system required for autonomous driving, including detecting vehicles, pedestrians, cyclists, and understanding lane markings and traffic signs in real-time.\n\n\n\n\n\n\nFigure 5: Example from Waymo. Observe how the system simultaneously tracks multiple objects.\n\n\n\nFace recognition technology, such as Apple’s Face ID (Figure 6), has become ubiquitous in consumer devices. The system uses infrared depth sensing to create a 3D map of facial features, making it more secure than 2D image-based recognition and robust to different lighting conditions.\n\n\n\n\n\n\nFigure 6: Example from Apple Face ID. Watch how the infrared dot projector creates a depth map of the face.\n\n\n\nComputer vision has also been very useful in agricultur. Figure 7 shows an application where apples can be counted from image data. Furthermore, drones might be used to estimate crop yield, type and health.\n\n\n\n\n\n\nFigure 7: Example from Häni, Roy, and Isler (2020)\n\n\n\nDeep learning has also been applied extensively in the medical area. In particular, many imaging technologies, such as MRI or CT, can be analysed with computer vision techniques. Figure 8 illustrates a generic segmentation model which can be used to identify organs and other anatomical structures on pixel-level.\n\n\n\n\n\n\nFigure 8: Example from Ma et al. (2024).\n\n\n\nPhoto-editing has been brought to new levels of sophistication with AI-based capabilities. Figure 9 shows an impressive example from the Google Magic Editor. Maybe even a bit too good?\n\n\n\n\n\n\nFigure 9: Example from Google Magic Editor\n\n\n\n\n\n\n\n\n\n🤔 Think About It\nWhat computer vision tasks do you think the model in Figure 9 must perform to enable this editing capability?\n\n\nPossible tasks involved\n\n\nObject detection and segmentation (identifying what to remove/move)\nInpainting (filling in removed areas)\nStyle transfer (matching lighting/texture)\nScene understanding (maintaining realistic perspective)\n\n\n\n\n\nThe increasingly better models and the ability to run them quickly and resource-efficiently on mobile devices have enabled such applications. Figure 10 shows that special chips have been developed to process images with Deep Learning models quickly.\n\n\n\n\n\n\nFigure 10: From Link.",
    "crumbs": [
      "Lectures",
      "1 - Introduction"
    ]
  },
  {
    "objectID": "pages/lectures/intro.html#computer-vision-tasks",
    "href": "pages/lectures/intro.html#computer-vision-tasks",
    "title": "1 - Introduction",
    "section": "2 Computer Vision Tasks",
    "text": "2 Computer Vision Tasks\n\n2.1 Image Classification\nIn image classification, an image is assigned to a predefined set of classes. In multi-class classification, there are \\(\\gt 2\\) classes; in binary classification, there are exactly 2 classes; and in multi-label classification, there are multiple sets of classes. Figure 11 shows an example of multi-class classification.\n\n\n\n\n\n\nFigure 11: Multi-Class Image Classification example: a model predicts the most likely class for an image along with confidence scores (Krizhevsky, Sutskever, and Hinton (2012)).\n\n\n\n\n\n2.2 Object Detection\nObject detection involves locating and recognizing multiple objects in an image. Figure 12 shows an example from the paper by Redmon et al. (2016). Each object is localized with a bounding box and assigned an object class with a confidence score.\n\n\n\n\n\n\nFigure 12: Object Detection example (from Redmon et al. (2016)). Bounding boxes localize the objects, showing the predicted class and confidence for each detection.\n\n\n\n\n\n2.3 Segmentation\nIn segmentation, individual pixels are assigned to specific objects (instance segmentation) or classes (semantic segmentation). Figure 13 shows an example of instance segmentation from the paper by He et al. (2018), where individual objects are detected and precisely localized at the pixel level.\n\n\n\n\n\n\nFigure 13: Instance Segmentation example (from He et al. (2018)). Each person is individually segmented with pixel-level masks.\n\n\n\nThe following video shows an example of semantic segmentation:\n\n\n\n\n\n\nFigure 14: Semantic segmentation applied to aerial imagery of agricultural fields. Each pixel is classified into crop types, field boundaries, and other land categories.\n\n\n\n\n\n2.4 Keypoint Detection\nIn keypoint detection, specific points (keypoints) on objects—typically human body joints—are localized. Figure 15 shows an example of keypoint detection from the paper by He et al. (2018), where individual people are detected and their joints localized. This can be used to transfer movements from a person to an avatar or to recognize human activities (action recognition).\n\n\n\n\n\n\nFigure 15: Keypoint Detection example (from He et al. (2018)).\n\n\n\n\n\n2.5 Image Generation\nThere are various applications where models transform input images into specific output images (image-to-image translation) or generate completely new images from scratch (unconditional generation). Below are example applications.\n\n2.5.1 Image Manipulation\nInteractive manipulation allows users to modify images by dragging points to reshape objects while maintaining realistic appearance.\n\n\n\n\n\n\nFigure 16: DragGAN by Pan et al. (2023)\n\n\n\n\n\n2.5.2 Image Translation\nFigure 17 shows an example of image generation from the paper by Isola et al. (2018), where images are generated conditioned on image inputs (translated). This can be used, for example, to convert a sketch of an object into a photorealistic depiction of it.\n\n\n\n\n\n\nFigure 17: Image Generation example (from Isola et al. (2018)).\n\n\n\n\n\n2.5.3 Image Super Resolution\nIn the gaming industry, Deep Learning is used to upscale low-resolution images to high-resolution in real-time (image super resolution), as shown in Figure 18. This enables higher frame rates while maintaining visual quality.\n\n\n\n\n\n\nFigure 18: Nvidia DLSS (Deep Learning Super Sampling) improves gaming performance. Source\n\n\n\n\n\n2.5.4 Image Colorization\nImage colorization can be learned with Deep Learning. This involves transforming a black-and-white image into an RGB image. Figure 19 shows an example.\n\n\n\n\n\n\nFigure 19: Norwegian Bride (est late 1890s) from DeOldify: Link\n\n\n\n\n\n2.5.5 View Synthesis\nIn view synthesis, new camera viewpoints of a scene are generated from a model. Neural Radiance Fields (NeRFs) are models that can generate novel views from a set of input images taken from known viewpoints. Figure 20 shows training data (input images from different angles) and the synthesized novel views the model can generate.\n\n\n\n\n\n\nFigure 20: Neural Radiance Fields - example (from Mildenhall et al. (2020)).\n\n\n\nModern variants can produce impressive quality, as shown in Figure 21.\n\n\n\n\n\n\nFigure 21: 3D Gaussian Splatting for real-time novel view synthesis Kerbl et al. (2023).\n\n\n\n\n\n2.5.6 Unconditional Generation\nIn unconditional image generation, images are generated that resemble the training data distribution without specific input guidance. While you have no direct control over what specific image is generated, you can often manipulate latent variables to modify generated images or interpolate between samples. Figure 22 shows photorealistic portraits generated by a model trained on human faces.\n\n\n\n\n\n\nFigure 22: StyleGan3 (from Karras et al. (2021)).\n\n\n\n\n\n2.5.7 Text to Image\nFigure 23 shows an example of image generation from the paper by Rombach et al. (2022), where images are generated conditioned on text inputs.\n\n\n\n\n\n\nFigure 23: Image Generation example (from Rombach et al. (2022).)\n\n\n\nOn civitai, there are numerous examples and models to admire or download.\n\n\n\n2.6 Further tasks\nThere are many more computer vision tasks. The following list is not exhaustive:\n\n\n\nImage Classification\nObject Detection (and Tracking)\nImage Segmentation\n\nSemantic Segmentation\nInstance Segmentation\n\nOptical Character Recognition (OCR)\nPose Estimation\nFacial Recognition\nAction Recognition\n\n\n\nImage Generation\n\nStyle Transfer\nImage Inpainting\nSuper-Resolution\nText-to-Image (and more)\n\nImage Captioning\n3D Reconstruction\nImage Retrieval\n\n\n\n\n\n2.7 What about videos?\nMany of the image-level tasks can be extended to videos. After all, videos are nothing but a sequence of images.\nThe most recent impressive examples are from video generative models:\n\n\n\n\n\n\nFigure 24: OpenAI Sora 2 - Prompt: a man rides a horse which is on another horse\n\n\n\nBut also object / keypoint tracking:\n\n\n\n\n\n\nFigure 25: CoTracker - Tracking points across video frames for motion analysis (Karaev et al. (2024))\n\n\n\n\n\n\n\n\n\nNoteQuiz: Which Application belongs to which task?\n\n\n\n\n\nMatch each application to the most appropriate computer vision task:\nApplications:\n\nAnalyzing camera trap images - Identifying which animals appear in wildlife monitoring photos\nApple’s on-device image categorization and safety filters - Classifying photos into categories or detecting inappropriate content\nImage search - Finding visually similar images in a large database\nFinding tumors in medical images - Precisely localizing and delineating abnormal tissue in CT or MRI scans\nCounting apples in an orchard - Detecting and tallying individual fruits on trees\n\nAvailable Tasks:\n\nImage Classification\nObject Detection\nImage Segmentation\nImage Retrieval/Embedding\n\n\n\nClick for result\n\n\nAnalyzing camera trap images → Image Classification or Object Detection\n\nImage Classification if you only need to know which species are present (one or more labels per image)\nObject Detection if you need to count individuals or localize where each animal appears in the frame\nThe choice depends on the specific analysis requirements\n\nApple’s on-device image categorization and safety filters → Image Classification\n\nThis involves assigning images to predefined categories (e.g., “food”, “people”, “landscape”) or detecting inappropriate content\nMulti-label classification is used when an image can belong to multiple categories\n\nImage search → Tricky - Not directly a standard task\n\nThis doesn’t fit neatly into classification, detection, or segmentation\nClosest to Image Classification conceptually, as it relies on learned image representations/embeddings\nIn practice, uses feature extraction (embedding) to compute similarity between images\nOften leverages classification networks trained on large datasets, but applied for retrieval rather than classification\n\nFinding tumors in medical images → Image Segmentation\n\nRequires pixel-level precision to delineate tumor boundaries\nSemantic Segmentation if identifying tumor vs. non-tumor tissue\nInstance Segmentation if distinguishing between multiple separate tumors\n\nCounting apples in an orchard → Object Detection\n\nNeed to localize and count individual apples\nEach apple gets a bounding box, and counting is straightforward from detection results\nCould also use instance segmentation for more precise boundaries, but detection is typically sufficient",
    "crumbs": [
      "Lectures",
      "1 - Introduction"
    ]
  },
  {
    "objectID": "pages/lectures/intro.html#challenges",
    "href": "pages/lectures/intro.html#challenges",
    "title": "1 - Introduction",
    "section": "3 Challenges",
    "text": "3 Challenges\nWe will now explore some challenges that must be overcome when analyzing images with machine learning models.\n\n3.1 Semantic Gap\nThe semantic gap refers to the discrepancy between the low-level pixel information that can be directly extracted from an image and the high-level semantic interpretation understood by humans. In simpler terms: an image consists of millions of pixel values, but deriving semantically meaningful concepts like “cat” or “sitting” from these raw numbers is extremely challenging.\n\n\n\n\n\n\nFigure 26: Illustration of the semantic gap: computers see numbers, humans see meaning.\n\n\n\n\n\n3.2 Viewpoint\nThe semantic meaning of an image remains constant regardless of camera viewpoint, but the pixel values change dramatically. A model must learn to recognize the same object from different angles.\n\n\n\n\n\n\nFigure 27: The same cat from different viewpoints looks completely different at the pixel level.\n\n\n\n\n\n3.3 Deformation\nObjects, especially living beings, are often flexible and can appear in vastly different shapes and poses. Models must recognize a cat whether it’s curled up sleeping or stretched out running.\n\n\n\n\n\n\nFigure 28: Cats exhibit significant shape variation due to pose changes. Source\n\n\n\n\n\n3.4 Illumination\nChanges in lighting conditions dramatically affect pixel values and can obscure object features. The same object under different lighting can produce vastly different images.\n\n\n\n\n\n\nFigure 29: The same scene under different lighting conditions. Source\n\n\n\n\n\n3.5 Background Clutter\nBackground pixels can visually resemble the object of interest, making precise object localization and segmentation difficult. The model must learn to distinguish foreground from background.\n\n\n\n\n\n\nFigure 30: Cat camouflaged against a similarly-textured background. Source\n\n\n\n\n\n3.6 Occlusion\nObjects are often partially hidden by other objects in the scene. Models must learn to recognize objects even when only part of them is visible.\n\n\n\n\n\n\nFigure 31: Partially occluded cat - only part of the object is visible. Source\n\n\n\n\n\n3.7 Intra-Class Variation\nObjects belonging to the same class can exhibit enormous variability in appearance, color, size, and texture. All cats are cats, but they can look very different from each other.\n\n\n\n\n\n\nFigure 32: High intra-class variation: all are cats but with vastly different appearances. Source\n\n\n\n\n\n3.8 Context Dependence\nFigure 33 shows that contextual information can be crucial for correct object classification. The surrounding scene provides important cues about what objects are likely to be present.\n\n\n\n\n\n\nFigure 33: Is this a tiger? Context clues help determine the answer. Source\n\n\n\nFigure 34 represents an interesting example where context is important: Shown are four images with severe quality degradation (blur) and oval objects indicated with dashed black lines. Notably, each marked object is exactly identical on pixel level (apart from a rotation) - yet we can form clear hypotheses what each of the object represents.\n\n\n\n\n\n\nFigure 34: In the presence of image degradation a visual system (e.g. humans) heavily use context information. Even though the circled blops are identical, we interpret them as either: plate, bottle, cell phone, car, pedestrian or shoe. Source Torralba, Murphy, and Freeman (2010)",
    "crumbs": [
      "Lectures",
      "1 - Introduction"
    ]
  },
  {
    "objectID": "pages/lectures/intro.html#machine-learning",
    "href": "pages/lectures/intro.html#machine-learning",
    "title": "1 - Introduction",
    "section": "4 Machine Learning",
    "text": "4 Machine Learning\n\n4.1 Machine Learning Basics\n\n\n\n\n\n\nNote📚 Quick ML Refresher\n\n\n\nThis course uses machine learning concepts throughout. If you need a detailed refresher on ML fundamentals, see our Machine Learning Basics background page.\nQuick summary: We follow a data-driven approach:\n\nCollect a dataset of images and labels\nTrain a model that learns to map images to labels\nEvaluate the model on new data\n\n\ndef train(images, labels):\n \"\"\" Train a Model \"\"\"\n # Fit Model here\n return model\n\ndef predict(test_images, model):\n \"\"\" Predict \"\"\"\n predictions = model(test_images)\n return predictions\n\nKey concepts:\n\nModel: Function \\(f_{\\theta}(\\mathbf{x}) = \\hat{y}\\) with learnable parameters \\(\\theta\\)\nOptimization: Find \\(\\theta\\) that minimizes loss \\(J(f_{\\theta}(\\mathbf{X}), \\mathbf{y})\\)\nModel Selection: Use train/validation/test splits to avoid overfitting\n\n\n\n\n\n4.2 ML with Classical Computer Vision\nImages typically have very high dimensionality. For example, an RGB image with a resolution of \\(800 \\times 600\\) has a dimensionality of \\(800 \\times 600 \\times 3 = 1,440,000\\). Classical machine learning algorithms often struggle with such high dimensionalities:\n\nThey are very slow or require a lot of memory.\nThey cannot exploit the 2-D structure of images.\nThey are very sensitive to slight changes in images (e.g., rotations).\nThey can easily overfit, as the number of features is close to the number of observations (training set).\n\nWhen modeling images with (classical) machine learning algorithms, methods from (classical) computer vision are often used. With the help of such methods, features can be extracted from images, and the algorithms can learn on these features. This avoids modeling high-dimensional raw data. Figure 35, Figure 36, and Figure 37 show various feature extraction methods.\nFigure 35 shows that, for example, the distribution over the color spectrum can be extracted from an image. This could be an important feature.\n\n\n\n\n\n\nFigure 35: Color Histograms as Features (Source: Johnson (2022))\n\n\n\nFigure 36 shows that techniques like Histogram of Oriented Gradients (HOG) Dalal and Triggs (2005) can be used to extract structures from images. Such features were successfully used for pedestrian detection Dalal and Triggs (2005).\n\n\n\n\n\n\nFigure 36: HOG as Features (Source: Johnson (2022))\n\n\n\nFigure 37 shows another feature variant. Visual patches can be extracted from a dataset, clustered, and then used as descriptors.\n\n\n\n\n\n\nFigure 37: Bag of (visual) words Features (Source: Johnson (2022))\n\n\n\nFinally, all features can be combined, often more is better, as shown in Figure 38.\n\n\n\n\n\n\nFigure 38: Image Features (Source: Johnson (2022))\n\n\n\nDepending on the parameterization, HOG descriptors, for example, can be very large for images. The resulting feature vector from Figure 38 can still be reduced in dimensionality, e.g., with Principal Component Analysis.\nFigure 39 shows the CIFAR10 dataset. A well-known dataset for testing models, consisting of 10 classes with 6,000 images each.\n\n\n\n\n\n\nFigure 39: CIFAR10 Dataset Source",
    "crumbs": [
      "Lectures",
      "1 - Introduction"
    ]
  },
  {
    "objectID": "pages/lectures/intro.html#deep-learning",
    "href": "pages/lectures/intro.html#deep-learning",
    "title": "1 - Introduction",
    "section": "5 Deep Learning",
    "text": "5 Deep Learning\nTo accelerate research in computer vision and make progress more measurable, the PASCAL VOC Challenges were introduced Everingham et al. (2007). These involved various tasks, such as detecting objects in photographs (Figure 40).\n\n\n\n\n\n\nFigure 40: Images/illustrations from Link and Johnson (2022). On the left, you see object annotations in images, and on the right, you see the development of Mean Average Precision over the years.\n\n\n\nThe easy availability of images on the internet has made it possible to collect increasingly larger datasets. ImageNet is such a very large, hierarchically annotated image dataset Deng et al. (2009) with over 1.4 million images, categorized into 1,000 object classes. Figure 41 illustrates the dataset.\n\n\n\n\n\n\nFigure 41: ImageNet, Image Source, details in Deng et al. (2009)\n\n\n\nSince 2010, challenges have been regularly conducted on the ImageNet dataset Russakovsky et al. (2015), such as image classification and object detection. Figure 42 shows the development of the error rate over time.\n\n\n\n\n\n\nFigure 42: Source: Johnson (2022)\n\n\n\n\n5.0.1 The ImageNet Breakthrough (2012)\nThe year 2012 marked a watershed moment in computer vision. Until then, the state-of-the-art in the ImageNet competition relied on classical approaches. In 2011, the winning team Perronnin et al. (2010) combined various hand-crafted feature extraction methods (SIFT features) with Support Vector Machines (SVMs), achieving a top-5 error rate of 25.8%.\nThen came the breakthrough: Krizhevsky et al. Krizhevsky, Sutskever, and Hinton (2012) implemented AlexNet, a convolutional neural network (CNN) with multiple layers, and achieved a top-5 error rate of just 16.4%—a dramatic 9.4 percentage point improvement. This wasn’t just an incremental gain; it fundamentally demonstrated that deep learning could outperform decades of hand-crafted computer vision methods.\n\n\n\n\n\n\nFigure 43: AlexNet architecture Krizhevsky, Sutskever, and Hinton (2012): 5 convolutional layers followed by 3 fully connected layers, trained on dual GPUs.\n\n\n\nWhat made AlexNet work?\n\nDeep architecture: 8 layers (5 convolutional + 3 fully connected)\nGPU training: Used dual GPUs to handle 60 million parameters\nReLU activations: Faster training than traditional sigmoid/tanh\nDropout regularization: Reduced overfitting\n\nThis success marked the end of classical computer vision methods in many areas and triggered an explosion of deep learning research.\nWhile classical computer vision trains a machine learning model on features extracted with hand-crafted algorithms Figure 44, the development is increasingly moving towards end-to-end learning. In this approach, one avoids as much as possible static/hand-designed components and learns everything, including feature extraction, with machine learning Figure 45.\n\n\n\n\n\n\nFigure 44: Illustration from Johnson (2022)\n\n\n\n\n\n\n\n\n\nFigure 45: Illustration from Johnson (2022)\n\n\n\nDeep learning-based approaches have several key advantages over classical machine learning methods:\n\nAutomatic feature extraction: No need for hand-crafted features like SIFT or HOG.\nHierarchical representations: Learn features at multiple levels of abstraction (edges → textures → parts → objects).\nBetter scaling: Performance improves with more training data and compute.\nEnd-to-end learning: Unified optimization from raw inputs to task outputs.\nBuilt-in invariances: Architectures like CNNs are naturally invariant to certain transformations (e.g., translation).\nTransfer learning: Pre-trained models can be adapted to new tasks with limited data.\n\n\n\n5.1 Deep Learning History\nWe will now explore the most important milestones in deep learning for image analysis.\n\n5.1.1 Hubel and Wiesel\nHubel and Wiesel (1959) showed in experiments on cats that there are complex cells/neurons in the visual cortex that only respond to certain patterns. In addition to these complex cells, there are also simple cells that only respond to lines at a certain angle.\n\n\n\n\n\n\nFigure 46: Illustration Source\n\n\n\n\n\n5.1.2 Neocognitron\nFukushima (1980) defined a model of a neural network that can recognize visual patterns. It already has the hierarchical structure of a modern convolutional neural network and is inspired by biological neural networks, particularly from the insights of Hubel and Wiesel (1959).\n\n\n\n\n\n\nFigure 47: The Neocognitron Fukushima (1980).\n\n\n\n\n\n5.1.3 Backpropagation\nRumelhart, Hinton, and Williams (1986) introduced the backpropagation algorithm in the context of neural networks. This algorithm is used to train modern neural networks: it finds the parameters of an artificial neural network (ANN) to solve a specific task. Backpropagation is based on the chain rule from calculus and is also important for other machine learning models.\n\n\n\n\n\n\nFigure 48: Backpropagation in neural networks Rumelhart, Hinton, and Williams (1986).\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nA good video on backpropagation: 3Blue1Brown Backpropagation Calculus\n\n\n\n\n5.1.4 Convolutional Neural Networks (CNNs)\nLecun et al. (1998) implemented convolutional neural networks (CNNs) to recognize handwritten digits. The architecture is specialized for the 2-D structure of image data and trained a model very similar to modern CNNs, as shown in Figure 49.\n\n\n\n\n\n\nFigure 49: LeNet-5 architecture Lecun et al. (1998): the first successful CNN for digit recognition.\n\n\n\nAfter AlexNet’s breakthrough in 2012 (see previous section), CNNs rapidly evolved and were applied to increasingly complex tasks. Notable developments include:\n\nVGGNet (2014): Showed that depth matters—16-19 layers with small 3×3 filters\nResNet (2015): Introduced skip connections, enabling networks with 50-152 layers\nCOCO Challenges: Expanded beyond ImageNet to multi-task benchmarks (detection, segmentation, keypoints)\nMobile architectures: Efficient CNNs for deployment on phones and embedded devices\n\n\n\n5.1.5 Other Architectures in Computer Vision\nCNNs are still in use today (2025). Meanwhile, there are alternative architectures, such as transformer-based models (Dosovitskiy et al. (2020)), which are extremely successful in language modeling, or multilayer perceptron-based architectures (Liu et al. (2021)).\nThe question of which architecture type will prevail is open. Currently, it seems that both CNNs and transformer-based models have their place, and often perform similarly (Smith et al. (2023)). Furthermore, hybrid-architectures, combining the strengths of CNNs and transformers have been proposed Vasu et al. (2023). Some works use both architecture types, depending on whether parameter efficiency (CNNs) or raw performance (transformers) are prioritized Siméoni et al. (2025).",
    "crumbs": [
      "Lectures",
      "1 - Introduction"
    ]
  },
  {
    "objectID": "pages/lectures/intro.html#references",
    "href": "pages/lectures/intro.html#references",
    "title": "1 - Introduction",
    "section": "6 References",
    "text": "6 References\n\n\nBreitenmoser-Würsten, Christine, Fridolin Zimmermann, Kristina Vogt, Manuela von Arx, Sven Signe, and Christian Stauffer. 2024. “Das Projekt LUNO - Abschlussbericht.” 121. Stiftung KORA. https://portal.issn.org/#.\n\n\nDalal, N., and B. Triggs. 2005. “Histograms of Oriented Gradients for Human Detection.” In 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05), 1:886–93. San Diego, CA, USA: IEEE. https://doi.org/10.1109/CVPR.2005.177.\n\n\nDeng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. “ImageNet: A Large-Scale Hierarchical Image Database.” In 2009 IEEE Conference on Computer Vision and Pattern Recognition, 248–55. Miami, FL: IEEE. https://doi.org/10.1109/CVPR.2009.5206848.\n\n\nDosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2020. “An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.” arXiv:2010.11929 [Cs], October. http://arxiv.org/abs/2010.11929.\n\n\nEveringham, M., L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. 2007. “The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results.” http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html.\n\n\nFukushima, Kunihiko. 1980. “Neocognitron: A Self-Organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position.” Biological Cybernetics 36 (4): 193–202. https://doi.org/10.1007/BF00344251.\n\n\nHäni, Nicolai, Pravakar Roy, and Volkan Isler. 2020. “MinneApple: A Benchmark Dataset for Apple Detection and Segmentation.” IEEE Robotics and Automation Letters 5 (2): 852–58. https://doi.org/10.1109/LRA.2020.2965061.\n\n\nHe, Kaiming, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. 2018. “Mask R-CNN.” arXiv. http://arxiv.org/abs/1703.06870.\n\n\nHubel, D. H., and T. N. Wiesel. 1959. “Receptive Fields of Single Neurones in the Cat’s Striate Cortex.” The Journal of Physiology 148 (3): 574–91. https://doi.org/10.1113/jphysiol.1959.sp006308.\n\n\nIsola, Phillip, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. 2018. “Image-to-Image Translation with Conditional Adversarial Networks.” arXiv. http://arxiv.org/abs/1611.07004.\n\n\nJohnson, Justin. 2022. “EECS 498.008 / 598.008 Deep Learning for Computer Vision.” Lecture {Notes} / {Slides}. https://web.eecs.umich.edu/~justincj/teaching/eecs498/WI2022/.\n\n\nKaraev, Nikita, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. 2024. “CoTracker: It Is Better to Track Together.” arXiv. https://doi.org/10.48550/arXiv.2307.07635.\n\n\nKarras, Tero, Miika Aittala, Samuli Laine, Erik Härkönen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. 2021. “Alias-Free Generative Adversarial Networks.” arXiv. http://arxiv.org/abs/2106.12423.\n\n\nKerbl, Bernhard, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 2023. “3D Gaussian Splatting for Real-Time Radiance Field Rendering.” arXiv. https://doi.org/10.48550/arXiv.2308.04079.\n\n\nKrizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. “ImageNet Classification with Deep Convolutional Neural Networks.” In Advances in Neural Information Processing Systems, edited by F. Pereira, C. J. Burges, L. Bottou, and K. Q. Weinberger. Vol. 25. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf.\n\n\nLecun, Y., L. Bottou, Y. Bengio, and P. Haffner. 1998. “Gradient-Based Learning Applied to Document Recognition.” Proceedings of the IEEE 86 (11): 2278–2324. https://doi.org/10.1109/5.726791.\n\n\nLiu, Hanxiao, Zihang Dai, David R. So, and Quoc V. Le. 2021. “Pay Attention to MLPs.” arXiv:2105.08050 [Cs], June. http://arxiv.org/abs/2105.08050.\n\n\nMa, Jun, Yuting He, Feifei Li, Lin Han, Chenyu You, and Bo Wang. 2024. “Segment Anything in Medical Images.” Nature Communications 15 (1): 654. https://doi.org/10.1038/s41467-024-44824-z.\n\n\nMathys, Melanie, Marco Willi, Michael Graber, and Raphael Meier. 2024. “Synthetic Image Generation in Cyber Influence Operations: An Emergent Threat?” arXiv. https://doi.org/10.48550/arXiv.2403.12207.\n\n\nMathys, Melanie, Marco Willi, and Raphael Meier. 2024. “Synthetic Photography Detection: A Visual Guidance for Identifying Synthetic Images Created by AI.” arXiv. https://doi.org/10.48550/arXiv.2408.06398.\n\n\nMildenhall, Ben, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. 2020. “NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis.” arXiv. http://arxiv.org/abs/2003.08934.\n\n\nPan, Xingang, Ayush Tewari, Thomas Leimkühler, Lingjie Liu, Abhimitra Meka, and Christian Theobalt. 2023. “Drag Your GAN: Interactive Point-Based Manipulation on the Generative Image Manifold.” arXiv. http://arxiv.org/abs/2305.10973.\n\n\nPerronnin, Florent, Yan Liu, Jorge Sanchez, and Herve Poirier. 2010. “Large-Scale Image Retrieval with Compressed Fisher Vectors.” In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 3384–91. San Francisco, CA, USA: IEEE. https://doi.org/10.1109/CVPR.2010.5540009.\n\n\nRedmon, Joseph, Santosh Divvala, Ross Girshick, and Ali Farhadi. 2016. “You Only Look Once: Unified, Real-Time Object Detection.” Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition 2016-Decem: 779–88. https://doi.org/10.1109/CVPR.2016.91.\n\n\nRombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. “High-Resolution Image Synthesis with Latent Diffusion Models.” arXiv. http://arxiv.org/abs/2112.10752.\n\n\nRumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. 1986. “Learning Representations by Back-Propagating Errors.” Nature 323 (6088): 533–36. https://doi.org/10.1038/323533a0.\n\n\nRussakovsky, Olga, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, et al. 2015. “ImageNet Large Scale Visual Recognition Challenge.” arXiv. http://arxiv.org/abs/1409.0575.\n\n\nSiméoni, Oriane, Huy V. Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, et al. 2025. “DINOv3.” arXiv. https://doi.org/10.48550/arXiv.2508.10104.\n\n\nSmith, Samuel L., Andrew Brock, Leonard Berrada, and Soham De. 2023. “ConvNets Match Vision Transformers at Scale.” arXiv. http://arxiv.org/abs/2310.16764.\n\n\nTorralba, Antonio, Kevin P Murphy, and William T Freeman. 2010. “Using the Forest to See the Trees: Exploiting Context for Visual Object Detection and Localization.” Communications of the ACM 53 (3): 107–14.\n\n\nVasu, Pavan Kumar Anasosalu, James Gabriel, Jeff Zhu, Oncel Tuzel, and Anurag Ranjan. 2023. “FastViT: A Fast Hybrid Vision Transformer Using Structural Reparameterization.” arXiv. https://doi.org/10.48550/arXiv.2303.14189.\n\n\nWilli, Marco, Ross T. Pitman, Anabelle W. Cardoso, Christina Locke, Alexandra Swanson, Amy Boyer, Marten Veldthuis, and Lucy Fortson. 2019. “Identifying Animal Species in Camera Trap Images Using Deep Learning and Citizen Science.” Edited by Oscar Gaggiotti. Methods in Ecology and Evolution 10 (1): 80–91. https://doi.org/10.1111/2041-210X.13099.",
    "crumbs": [
      "Lectures",
      "1 - Introduction"
    ]
  },
  {
    "objectID": "pages/lectures/classification.html",
    "href": "pages/lectures/classification.html",
    "title": "3 - Image Classification",
    "section": "",
    "text": "TipLearning Objectives\n\n\n\n\n\nAfter this lecture you should be able to:\n\nDefine multi-class vs multi-label classification and their output encodings.\nDerive the softmax and cross-entropy from a maximum likelihood perspective.\nInterpret logits, probabilities, and calibration (temperature, label smoothing high-level).\nCompare classic CNN architectures (AlexNet→VGG→ResNet→ConvNeXt) and choose a baseline.\nApply transfer learning and explain when to freeze vs fine-tune layers.",
    "crumbs": [
      "Lectures",
      "3 - Image Classification"
    ]
  },
  {
    "objectID": "pages/lectures/classification.html#introduction",
    "href": "pages/lectures/classification.html#introduction",
    "title": "3 - Image Classification",
    "section": "1 Introduction",
    "text": "1 Introduction\nImage classification is a core task of computer vision. In image classification, an image is assigned to a predefined set of classes. There are different variants of image classification:\n\nin multi-class classification there are \\(&gt; 2\\) classes\nin binary classification there are 2 classes\nin multi-label classification there are several sets of classes.\n\nFigure 1 illustrates a multi-class classification task.\n\n\n\n\n\n\nFigure 1: Image Classification example.\n\n\n\nFigure 2 shows an example from the paper by Krizhevsky, Sutskever, and Hinton (2012), which achieved the best results in the 2012 ImageNet competition, demonstrating how well CNNs work. Note that each image had to be assigned to one of 1’000 classes.\n\n\n\n\n\n\nFigure 2: Image Classification example (from Krizhevsky, Sutskever, and Hinton (2012)).\n\n\n\nFigure 3 illustrates the challenge with images taken by camera traps, which need to be classified along animal species.\n\n\n\n\n\n\nFigure 3: Example images from camera traps.",
    "crumbs": [
      "Lectures",
      "3 - Image Classification"
    ]
  },
  {
    "objectID": "pages/lectures/classification.html#parametric-approach",
    "href": "pages/lectures/classification.html#parametric-approach",
    "title": "3 - Image Classification",
    "section": "2 Parametric Approach",
    "text": "2 Parametric Approach\nIn a parametric approach, we seek a model of the following form:\n\\[\n\\hat{y}^{(i)} = f(\\theta, \\mathbf{x}^{(i)})\n\\]\nWe want to find model parameters \\(\\theta\\) that output a score/prediction \\(\\hat{y}^{(i)}\\) for any data points \\(\\mathbf{x}^{(i)}\\) for each class \\(k \\in K\\). We then want to assess how good this score is with the help of a loss function.\nNote: The model parameters \\(\\theta\\) are all the learnable parameters of a model, e.g. the filters of the convolutional layers.\n\n2.1 Softmax Classifier\nWith a softmax classifier, we interpret model predictions/scores as probabilities of class memberships: \\(P(Y=\\mathbf{y}^{(i)}| X = \\mathbf{x}^{(i)})\\). We interpret the output of a model as the parameters of a Categorical Distribution over all possible classes.\nTo obtain a valid probability distribution, the untransformed outputs \\(\\mathbf{z}\\), also called logits, of a model are transformed with the softmax function \\(\\sigma(\\mathbf{z})\\):\n\\[\nP(Y = k| X = \\mathbf{x}^{(i)}) = \\sigma(\\mathbf{z})_k = \\frac{e^{z_k}}{\\sum_i^K e^{z_i}}\n\\]\nFigure 4 shows an example of the effect of the softmax transformation.\n\n\n\n\n\n\n\n\nFigure 4: Logits (left) to probabilities with the Softmax function (right).\n\n\n\n\n\n\n\n\n\n\n\n\nTip🎮 Interactive Exploration\n\n\n\n\n\nTry different logit distributions and see how softmax transforms them into probabilities!\n\n\n                            \n                                            \n\n\nKey observations:\n\nLogits can be any real number (negative, zero, or positive)\nProbabilities always sum to exactly 1.0 (see green box)\nEntropy measures uncertainty: lower = more confident, higher = more uncertain\nTry “Uniform Logits” vs “Very Confident” to see the extremes!",
    "crumbs": [
      "Lectures",
      "3 - Image Classification"
    ]
  },
  {
    "objectID": "pages/lectures/classification.html#loss-function",
    "href": "pages/lectures/classification.html#loss-function",
    "title": "3 - Image Classification",
    "section": "3 Loss Function",
    "text": "3 Loss Function\nNow we look at how to define the loss function \\(L(\\mathbf{X}, \\mathbf{y}, \\theta)\\) for a multi-class classification problem.\n\n\n\n\n\n\nNote\n\n\n\nThe loss function for a binary classification problem is identical. If we have a multi-label classification problem we have multiple outputs and thus multiple softmax functions. The principle is identical.\n\n\n\n3.1 Likelihood\nThe likelihood of a data point \\((\\mathbf{x}^{(i)}, y^{(i)})\\) is the probability of observing a data point, given a specific model:\n\\[\nP(Y=y^{(i)}| X = \\mathbf{x}^{(i)}) = f(\\theta, \\mathbf{x}^{(i)})\n\\]\nThis means we formulate a model with a probabilistic interpretation of predictions: \\(f(\\theta, \\mathbf{x}^{(i)}): \\mathbb{R}^{n} \\mapsto [0, 1]\\)\nFor a multi-class classification, the label vector is one-hot encoded \\(\\mathbf{y}^{(i)} \\in \\{0, 1\\}^K\\), where the true class is coded with 1 and the rest with 0. The likelihood of a data point is defined as:\n\\[\n\\prod_{j=1}^K  P(Y = j| X = \\mathbf{x}^{(i)})^{y^{(i)}_j}\n\\]\nSince only one entry in \\(\\mathbf{y}^{(i)}\\) is 1, the likelihood is simply the prediction for the true class \\(P(Y = y^{(i)}| X = \\mathbf{x}^{(i)})\\).\nMore Info\n\n\n\n\n\n\nNote📝 Quiz: Cat vs Dog Likelihood\n\n\n\n\n\nScenario: Your CNN model classifies 5 images from our dataset. Let’s explore how different prediction confidences affect likelihood!\nThe Images:     \nTrue Labels: \\(\\mathbf{y} = [\\text{Cat}, \\text{Dog}, \\text{Cat}, \\text{Dog}, \\text{Dog}]\\) or \\([0, 1, 0, 1, 1]\\) in binary encoding.\nQuestion 1: Model A predicts: \\(P(\\text{Cat}) = [0.9, 0.2, 0.8, 0.3, 0.1]\\). What’s the likelihood?\nThink first: For each image, what’s the probability of the correct class?\n\n\nClick for result\n\n\nimport numpy as np\n\n# True labels: [Cat=0, Dog=1, Cat=0, Dog=1, Dog=1]\ny_true = np.array([0, 1, 0, 1, 1])\n\n# Model A predictions: P(Cat) for each image\np_cat = np.array([0.9, 0.2, 0.8, 0.3, 0.1])\np_dog = 1 - p_cat  # P(Dog) = 1 - P(Cat)\n\n# Extract probability of correct class for each image\np_correct = np.where(y_true == 0, p_cat, p_dog)\nprint(f\"P(correct class): {p_correct}\")\n\n# Likelihood = product of correct probabilities\nlikelihood = np.prod(p_correct)\nprint(f\"Likelihood = {likelihood:.6f}\")\n\n# Individual contributions\nfor i, (true_label, prob) in enumerate(zip(y_true, p_correct)):\n    label_name = \"Cat\" if true_label == 0 else \"Dog\"\n    print(f\"Image {i+1} ({label_name}): P = {prob:.1f}\")\n\nP(correct class): [0.9 0.8 0.8 0.7 0.9]\nLikelihood = 0.362880\nImage 1 (Cat): P = 0.9\nImage 2 (Dog): P = 0.8\nImage 3 (Cat): P = 0.8\nImage 4 (Dog): P = 0.7\nImage 5 (Dog): P = 0.9\n\n\nInterpretation: Model A is confident about the first cat (0.9) and the last dog (0.9), but uncertain about the middle images.\n\nQuestion 2: Model B predicts: \\(P(\\text{Cat}) = [0.8, 0.1, 0.9, 0.2, 0.1]\\). Which model is better?\nHint: Compare the likelihoods. Higher likelihood = better model!\n\n\nClick for result\n\n\n# Model B predictions\np_cat_B = np.array([0.8, 0.1, 0.9, 0.2, 0.1])\np_dog_B = 1 - p_cat_B\n\n# Extract probability of correct class\np_correct_B = np.where(y_true == 0, p_cat_B, p_dog_B)\nprint(f\"Model B - P(correct class): {p_correct_B}\")\n\nlikelihood_B = np.prod(p_correct_B)\nprint(f\"Model B - Likelihood = {likelihood_B:.6f}\")\n\nprint(f\"\\nComparison:\")\nprint(f\"Model A: {likelihood:.6f}\")\nprint(f\"Model B: {likelihood_B:.6f}\")\nprint(f\"Model {'B' if likelihood_B &gt; likelihood else 'A'} is better!\")\n\nModel B - P(correct class): [0.8 0.9 0.9 0.8 0.9]\nModel B - Likelihood = 0.466560\n\nComparison:\nModel A: 0.362880\nModel B: 0.466560\nModel B is better!\n\n\nKey Insight: Model B is more confident on the second cat (0.9 vs 0.8), leading to higher overall likelihood.\n\nQuestion 3: What if we have a poorly calibrated model? Model C: \\(P(\\text{Cat}) = [0.6, 0.4, 0.6, 0.4, 0.4]\\)\nConsider: This model is not very confident about anything. How does this affect likelihood?\n\n\nClick for result\n\n\n# Model C: low confidence predictions\np_cat_C = np.array([0.6, 0.4, 0.6, 0.4, 0.4])\np_dog_C = 1 - p_cat_C\n\np_correct_C = np.where(y_true == 0, p_cat_C, p_dog_C)\nprint(f\"Model C - P(correct class): {p_correct_C}\")\n\nlikelihood_C = np.prod(p_correct_C)\nprint(f\"Model C - Likelihood = {likelihood_C:.6f}\")\n\nprint(f\"\\nAll Models Comparison:\")\nprint(f\"Model A (mixed confidence): {likelihood:.6f}\")\nprint(f\"Model B (high confidence):  {likelihood_B:.6f}\")\nprint(f\"Model C (low confidence):   {likelihood_C:.6f}\")\n\n# Log-likelihoods (what we actually optimize)\nprint(f\"\\nNegative Log-Likelihoods (Cross-Entropy Loss):\")\nprint(f\"Model A: {-np.log(likelihood):.3f}\")\nprint(f\"Model B: {-np.log(likelihood_B):.3f}\")\nprint(f\"Model C: {-np.log(likelihood_C):.3f}\")\nprint(f\"Lower is better for loss!\")\n\nModel C - P(correct class): [0.6 0.6 0.6 0.6 0.6]\nModel C - Likelihood = 0.077760\n\nAll Models Comparison:\nModel A (mixed confidence): 0.362880\nModel B (high confidence):  0.466560\nModel C (low confidence):   0.077760\n\nNegative Log-Likelihoods (Cross-Entropy Loss):\nModel A: 1.014\nModel B: 0.762\nModel C: 2.554\nLower is better for loss!\n\n\nCritical Observation: - Confident correct predictions (Model B) → High likelihood → Low loss ✅ - Low confidence (Model C) → Low likelihood → High loss ❌ - This is why we train models to be confident when correct!\n\nQuestion 4: What happens if we increase the dataset by ten fold? For example what happens with the likelihood of Model A if \\(P(\\text{Cat}) = [0.9, 0.2, 0.8, 0.3, 0.1]\\) is repeated 10 times?\n\n\nClick for result\n\n\n# Repeat the dataset 10 times\np_cat_repeated = np.tile(p_cat, 10)\ny_true_repeated = np.tile(y_true, 10)\np_dog_repeated = 1 - p_cat_repeated\n\np_correct_repeated = np.where(y_true_repeated == 0, p_cat_repeated, p_dog_repeated)\nprint(f\"Dataset repeated 10x (50 samples total)\")\nprint(f\"P(correct class) for first 5: {p_correct_repeated[:5]}\")\n\nlikelihood_10x = np.prod(p_correct_repeated)\nprint(f\"\\nLikelihood (10x dataset): {likelihood_10x:.2e}\")\nprint(f\"Original likelihood (1x):  {likelihood:.6f}\")\nprint(f\"\\nRatio: {likelihood_10x / likelihood:.2e}\")\n\n# The key insight: likelihood shrinks exponentially!\nprint(f\"\\nNumerical underflow risk!\")\nprint(f\"This is why we use LOG-likelihood instead:\")\nprint(f\"Log-likelihood (10x): {np.sum(np.log(p_correct_repeated)):.3f}\")\nprint(f\"Log-likelihood (1x):  {np.sum(np.log(p_correct)):.3f}\")\nprint(f\"The log version just scales linearly (10x) without numerical issues!\")\nprint(f\"Lower is better for loss!\")\n\nDataset repeated 10x (50 samples total)\nP(correct class) for first 5: [0.9 0.8 0.8 0.7 0.9]\n\nLikelihood (10x dataset): 3.96e-05\nOriginal likelihood (1x):  0.362880\n\nRatio: 1.09e-04\n\nNumerical underflow risk!\nThis is why we use LOG-likelihood instead:\nLog-likelihood (10x): -10.137\nLog-likelihood (1x):  -1.014\nThe log version just scales linearly (10x) without numerical issues!\nLower is better for loss!\n\n\nCritical Observation:\n\nWith 10x data, likelihood becomes TINY (underflow risk)\nBut log-likelihood scales linearly → numerically stable!\n\n\nBonus Question: What’s the perfect model’s likelihood?\n\n\nPerfect Model Analysis\n\n\n# Perfect model: 100% confident and always correct\np_cat_perfect = np.array([1.0, 0.0, 1.0, 0.0, 0.0])  # Certain about each class\np_dog_perfect = 1 - p_cat_perfect\n\np_correct_perfect = np.where(y_true == 0, p_cat_perfect, p_dog_perfect)\nprint(f\"Perfect Model - P(correct class): {p_correct_perfect}\")\n\nlikelihood_perfect = np.prod(p_correct_perfect)\nprint(f\"Perfect Model - Likelihood = {likelihood_perfect}\")\nprint(f\"Perfect Model - Negative Log-Likelihood = {-np.log(likelihood_perfect)}\")\n\nprint(f\"\\n🎯 Maximum possible likelihood = 1.0\")\nprint(f\"🎯 Minimum possible loss = 0.0\")\n\nPerfect Model - P(correct class): [1. 1. 1. 1. 1.]\nPerfect Model - Likelihood = 1.0\nPerfect Model - Negative Log-Likelihood = -0.0\n\n🎯 Maximum possible likelihood = 1.0\n🎯 Minimum possible loss = 0.0\n\n\nThe Goal: Train your CNN to approach this perfect confidence on the training set (while generalizing to new data)!\n\n\n\n\n\n\n3.2 Maximum Likelihood\nThe likelihood \\(P(\\mathbf{y} | \\theta, \\mathbf{X})\\) of observing our entire dataset \\((\\mathbf{X}, \\mathbf{y})\\), given the parameters \\(\\theta\\) and assuming that the data points \\((\\mathbf{x}^{(i)}, y^{(i)})\\) are independent and identically distributed, can be calculated as:\n\\[\\begin{equation}\n\\prod_{i=1}^N \\prod_{j=1}^K  P(Y = j| X = \\mathbf{x}^{(i)})^{y^{(i)}_j}\n\\end{equation}\\]\nUnder the Maximum Likelihood approach, we seek the parameters \\(\\theta\\) that maximize the likelihood of observing the dataset.\n\n\n3.3 Negative Log-Likelihood\nWith Maximum Likelihood, we aim to choose the parameters \\(\\theta\\) such that \\(p(\\mathbf{y} | \\theta, \\mathbf{X})\\) is maximized. However, this function can be complex to handle, and we can use some mathematical tricks. We take the logarithm of the likelihood so that the product of probabilities becomes a sum. Since the logarithm is a monotonically increasing function, we can maximize its logarithm instead of the function \\(p(\\mathbf{y} | \\theta, \\mathbf{X})\\). Finally, we take the negative of the function, allowing us to minimize it.\n\\[\\begin{align}\nL(\\mathbf{X}, \\mathbf{y}, \\theta) &= - \\log \\prod_{i=1}^N \\prod_{j=1}^K P(Y = j| X = \\mathbf{x}^{(i)})^{y^{(i)}_j} \\\\\n&= -\\sum_{i=1}^N \\sum_{j=1}^K y^{(i)}_j \\log  P(Y = j| X = \\mathbf{x}^{(i)})\n\\end{align}\\]\n\n\n\n\n\n\nTip💡 Maximum Likelihood Intuition\n\n\n\n\n\nVisualize the connection: Likelihood → Log-Likelihood → Cross-Entropy Loss:\n\n\n                            \n                                            \n\n\nCritical Insights:\n\nX-axis: Model calibration = probability assigned to the correct class (ranges from 0.5=random guessing to 0.98=well-calibrated)\nTop panel: Likelihood shrinks exponentially with dataset size → numerical underflow risk!\nMiddle panel: Log-likelihood converts products to sums → numerically stable, same maximum\nBottom panel: Negative log-likelihood (NLL) = Cross-Entropy Loss used in training\nKey Connection: max L(θ) ⟺ max log L(θ) ⟺ min -log L(θ) ⟺ min Cross-Entropy\n\nThe visualization shows: as model calibration improves (x increases), likelihood increases, log-likelihood increases, and loss (NLL) decreases.\n\n\n\n\n\n3.4 Cross-Entropy\nThe loss function, derived with Maximum Likelihood, can also be viewed through the lens of cross-entropy between two discrete probability functions. Specifically, we can calculate and minimize the cross-entropy between the true distribution \\(\\mathbf{y}^{(i)}\\) and the predicted \\(\\mathbf{\\hat{y}}^{(i)}\\). Cross-entropy comes from information theory and measures how many bits/nats on average are needed to describe an event of a probability distribution \\(p(x)\\) when using the approximation \\(q(x)\\).\n\\[\\begin{equation}\nCE = - \\sum_{x \\in X} p(x) \\log q(x) \\\\\nCE = - \\sum_{i=1}^N \\sum_{j=1}^K y_j^{(i)} \\log \\hat{y}_j^{(i)}\n\\end{equation}\\]\nIt is evident that cross-entropy is identical to the negative log-likelihood.\n\n\n\n\n\n\nTip📊 Cross-Entropy Loss Landscape\n\n\n\n\n\nVisualize how cross-entropy penalizes predictions:\n\n\n                            \n                                            \n\n\nKey Insights:\n\nLoss approaches ∞ when model is confidently wrong (p→0 for true class)\nLoss approaches 0 when model is confidently correct (p→1 for true class)\nAsymmetric penalty: Being wrong and confident is much worse than being uncertain\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: True Distribution (left) and Predicted Distribution (right).\n\n\n\n\n\nFigure 5 shows an example with a cross-entropy value of: 0.266.\n\n\n\n\n\n\nTip🎯 Interactive Multi-Class Cross-Entropy\n\n\n\n\n\nSee how different predictions map to loss values on the curve:\n\n\n                            \n                                            \n\n\nKey Takeaways:\n\nGreen bar = True class; Red dot = Current position on loss curve\nWatch how the red dot moves along the curve as predictions change\nPerfect prediction (p=1.0) → Dot at left, Loss ≈ 0\nConfident wrong (p=0.05) → Dot at right, Loss ≈ 3.0\n\n\n\n\n\n\n3.5 Try it out yourself!\n\n\n\n\n\n\nTipTrain a CNN for image classification in your browser!\n\n\n\nCNN Explainer",
    "crumbs": [
      "Lectures",
      "3 - Image Classification"
    ]
  },
  {
    "objectID": "pages/lectures/classification.html#architectures",
    "href": "pages/lectures/classification.html#architectures",
    "title": "3 - Image Classification",
    "section": "4 Architectures",
    "text": "4 Architectures\nArchitecture refers to the complete definition of an untrained (deep-learning) model, i.e., the sequence and configuration of layers. The following are some important architectures that have been successfully used for image classification.\n\n\n4.1 AlexNet\nCNNs became extremely popular after winning the ImageNet Competition. Krizhevsky, Sutskever, and Hinton (2012) implemented a CNN with multiple layers, known as the AlexNet architecture, as shown in Figure 6. ImageNet is a large, hierarchical image dataset Deng et al. (2009), which enabled efficient training of CNNs for the first time.\nAlexNet consists of 5 convolutional layers and 3 fully-connected layers. The last layer is a 1000-way softmax output to model the classes in ImageNet.\nThe model was trained with two GPUs (GTX 580) with 3GB memory each. Since 3GB was insufficient to train the model, the architecture was split across the GPUs. Some layers were split between the GPUs, allowing a larger network to be trained.\nFigure 6 shows the detailed architecture, including kernel sizes, the number of filters per layer, activation map dimensions, and pooling layers. ReLU was used as the activation function. This representation is from the original paper and shows how the model was split across two GPUs.\n\n\n\n\n\n\nFigure 6: AlexNet Krizhevsky, Sutskever, and Hinton (2012).\n\n\n\nSince the split across two GPUs is no longer necessary, the architecture is somewhat simplified. Modern implementations are shown in Figure 7.\n\n\n\n\n\n\nFigure 7: AlexNet Prince (2023)\n\n\n\nFigure 8 presents the operations in AlexNet in tabular form.\n\n\n\n\n\n\nFigure 8: Source: Johnson (2019).\n\n\n\nWe can also easily load AlexNet via torchvision.\n\nimport torch\nimport torchvision.models as models\nimport torchinfo\n\nalexnet = models.alexnet()\nx = torch.zeros(1, 3, 224, 224, dtype=torch.float, requires_grad=False)\nyhat = alexnet(x)\n\nprint(torchinfo.summary(alexnet, input_size=(1, 3, 224, 224)))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nAlexNet                                  [1, 1000]                 --\n├─Sequential: 1-1                        [1, 256, 6, 6]            --\n│    └─Conv2d: 2-1                       [1, 64, 55, 55]           23,296\n│    └─ReLU: 2-2                         [1, 64, 55, 55]           --\n│    └─MaxPool2d: 2-3                    [1, 64, 27, 27]           --\n│    └─Conv2d: 2-4                       [1, 192, 27, 27]          307,392\n│    └─ReLU: 2-5                         [1, 192, 27, 27]          --\n│    └─MaxPool2d: 2-6                    [1, 192, 13, 13]          --\n│    └─Conv2d: 2-7                       [1, 384, 13, 13]          663,936\n│    └─ReLU: 2-8                         [1, 384, 13, 13]          --\n│    └─Conv2d: 2-9                       [1, 256, 13, 13]          884,992\n│    └─ReLU: 2-10                        [1, 256, 13, 13]          --\n│    └─Conv2d: 2-11                      [1, 256, 13, 13]          590,080\n│    └─ReLU: 2-12                        [1, 256, 13, 13]          --\n│    └─MaxPool2d: 2-13                   [1, 256, 6, 6]            --\n├─AdaptiveAvgPool2d: 1-2                 [1, 256, 6, 6]            --\n├─Sequential: 1-3                        [1, 1000]                 --\n│    └─Dropout: 2-14                     [1, 9216]                 --\n│    └─Linear: 2-15                      [1, 4096]                 37,752,832\n│    └─ReLU: 2-16                        [1, 4096]                 --\n│    └─Dropout: 2-17                     [1, 4096]                 --\n│    └─Linear: 2-18                      [1, 4096]                 16,781,312\n│    └─ReLU: 2-19                        [1, 4096]                 --\n│    └─Linear: 2-20                      [1, 1000]                 4,097,000\n==========================================================================================\nTotal params: 61,100,840\nTrainable params: 61,100,840\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 714.68\n==========================================================================================\nInput size (MB): 0.60\nForward/backward pass size (MB): 3.95\nParams size (MB): 244.40\nEstimated Total Size (MB): 248.96\n==========================================================================================\n\n\n\n\n4.2 VGG\nSimonyan and Zisserman (2015) won the ImageNet Challenge in 2014 with their VGG architecture. They showed that smaller 3x3 kernels work significantly better and that deeper networks with 16-19 layers can be trained. Figure 9 shows the architecture as presented in the original paper. Figure 10 visualizes the architecture.\n\n\n\n\n\n\nFigure 9: VGG Simonyan and Zisserman (2015).\n\n\n\n\n\n\n\n\n\nFigure 10: VGG Prince (2023)\n\n\n\nVGG introduced a popular design element: A layer has the same number of filters as the previous layer unless the activation map dimensions are halved, in which case the number of filters is doubled (see also Figure 10). This was done to maintain the time complexity of the layers. VGG does not use normalization layers.\nFigure 11 compares VGG with AlexNet.\n\n\n\n\n\n\nFigure 11: Source: Johnson (2019).\n\n\n\n\n\n4.3 ResNet\nHe et al. (2016) wondered whether CNNs could be improved simply by making them deeper, i.e., adding more layers. Their experiments showed that adding layers eventually stops being beneficial, and performance saturates and then rapidly degrades (see Figure 12). They noticed that the performance did not degrade due to overfitting (see Figure 13, which shows that training error is also poor), but because back-propagation becomes less effective and the weights do not optimize well. He et al. (2016) hypothesized that deeper networks could not perform worse than shallower ones because the additional layers could simply pass on activations unchanged.\n\n\n\n\n\n\nFigure 12: Source: He et al. (2016)\n\n\n\n\n\n\n\n\n\nFigure 13: Source: He et al. (2016)\n\n\n\nThey then hypothesized that passing on activations unchanged as a default behavior could avoid this problem. They introduced identity mappings. Figure 14 shows such a connection. These connections are also called residual connections because the network only needs to learn the change in activations from layer \\(i\\) to layer \\(i+1\\), the residue.\n\n\n\n\n\n\nFigure 14: ResNet He et al. (2016) (Graphic from Johnson (2019).)\n\n\n\nResNet was otherwise inspired by VGG16. The convolutional layers consist of 3x3 kernels (except the first one), and the number of filters is doubled when the activation map dimensions are halved. ResNet uses convolutions with stride 2 for down-sampling and no max pooling. At the end, ResNet uses a global average pooling layer followed by a fully-connected layer with the number of classes. Variants of ResNet have 18, 34, 50, and over 100 layers. ResNet also uses batch normalization. Figure 15 shows the architecture.\n\n\n\n\n\n\nFigure 15: Source: He et al. (2016).\n\n\n\nResNet is very popular and is still widely used today (there are now more modern variants). Recent studies of identity mappings (or more generally skip connections) have shown that the gradient surface of the loss function becomes smoother, allowing the network to be optimized better. This is impressively illustrated in Figure 16.\n\n\n\n\n\n\nFigure 16: Source: Li et al. (2018).\n\n\n\n\n\n4.4 ConvNext\nOne of the most modern CNN architectures was described in Liu et al. (2022). This architecture uses tricks and implementation ideas accumulated over decades from various architectures. Figure 17 shows, starting from a modern version of ResNet, what has been adjusted to define this state-of-the-art architecture. Examples include: larger kernels, different activation functions, layer normalization instead of batch normalization, and depthwise separable convolutions.\n\n\n\n\n\n\nFigure 17: Convnext Liu et al. (2022).\n\n\n\nThere is already a new version of this architecture Woo et al. (2023).\n\n\n4.5 ImageNet Performance\nFigure 18 shows the development of ImageNet performance. As of 2023, we are much closer to 100%, see Link.\n\n\n\n\n\n\nFigure 18: Graphic from Johnson (2019)\n\n\n\n\n\n4.6 Which Architecture?\nWhich architecture should be chosen for a specific problem? A common tip is: Don’t be a hero.\nOne should rely on off-the-shelf architectures and not implement their own without a good reason.\nTypically, ResNet-50 or ResNet-101 are good choices. However, there are also models that require significantly fewer parameters, such as Efficient Nets.\nImportant considerations are also the requirements regarding accuracy, performance (FLOPs), and model size (memory).",
    "crumbs": [
      "Lectures",
      "3 - Image Classification"
    ]
  },
  {
    "objectID": "pages/lectures/classification.html#further-variations",
    "href": "pages/lectures/classification.html#further-variations",
    "title": "3 - Image Classification",
    "section": "5 Further Variations",
    "text": "5 Further Variations\nThere are countless other architectures and especially specific layers or layer blocks that can be used in various architectures. The following are Squeeze/Excite Networks and the important class of normalization layers, which are often used and can significantly improve models or accelerate the optimization process.\n\n5.1 Squeeze/Excite Networks\nSqueeze-and-Excite Networks (SE-Networks) were introduced in 2019 Hu et al. (2019). These include so-called Squeeze and Excite blocks (SE blocks), which allow the scaling of activation maps of a layer. This scaling is learnable through a few additional parameters. In practice, significant performance gains have been observed. Figure 19 shows an illustration.\n\n\n\n\n\n\nFigure 19: Source: Hu et al. (2019).\n\n\n\nThese SE blocks can be easily applied to arbitrary activation maps. Figure 19 shows an input feature map \\(\\mathbf{\\mathsf{X}} \\in \\mathbb{R}^{H' \\times W' \\times C'}\\) that is transformed with \\(F_{tr}\\) (e.g., with a convolutional layer). This results in the activation maps \\(\\mathbf{\\mathsf{U}} \\in \\mathbb{R}^{H \\times W \\times C}\\).\nThe transformation \\(F_{sq}(\\cdot)\\) applies the squeeze operation, a global average pooling, to generate a description of each channel by aggregating spatial information of the activation map \\(\\mathbf{\\mathsf{U}}\\). This results in a vector \\(z \\in \\mathbb{R}^{1 \\times 1 \\times C}\\).\nThe excitation operation \\(F_{ex}(\\cdot, W)\\) uses a gating mechanism with parameters \\(W\\), implemented with two fully-connected layers and activation functions. The result is \\(s \\in \\mathbb{R}^{1 \\times 1 \\times C}\\), the channel weights. It is called gating because the weights range from \\([0, 1]\\) and thus control how much information of a channel flows through (gatekeeping).\nFinally, \\(F_{scale}(\\cdot)\\) scales the activation maps \\(\\mathbf{\\mathsf{U}}\\) with the channel weights.\nThe operation can be described as follows:\n\\[\\begin{align}\nU &= F_{tr}(X) \\\\\nz &= F_{sq}(U) = \\text{GlobalAvgPool}(U) \\\\\ns &= F_{ex}(z, W) = \\sigma(\\mathbf{W}_2 g(\\mathbf{W}_1 z)) \\\\\n\\hat{X} &= F_{scale}(U, s) = U \\odot s\n\\end{align}\\]\nwhere \\(g()\\) represents the ReLU function, and \\(\\sigma\\) represents the sigmoid function.\n\n\n5.2 Normalization Layers\nNormalization layers normalize activation maps to improve parameter learning. There are many variations, as shown in Figure 20. In modern architectures, normalization layers are typically used. Popular is, for example, layer normalization (see Ba, Kiros, and Hinton (2016)). The general form of normalization is given in equation Equation 1. The parameters \\(\\gamma\\) and \\(\\beta\\) are learned, while the means \\(E[x]\\) and variances \\(\\sigma^2[x]\\) are estimated from the activations. Why normalization layers work and which ones to prefer are still subjects of research and are often empirically tested in practice (considered as hyper-parameters). It is believed that the cost function becomes smoother overall, allowing the network to train faster and better Santurkar et al. (2019).\n\\[\ny = \\frac{x - E[x]}{\\sqrt{\\sigma^2[x] + \\epsilon}} * \\gamma + \\beta\n\\tag{1}\\]\n\n\n\n\n\n\nFigure 20: Source: Qiao et al. (2020).\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNormalization layers are very common and used in almost any architecture. While batch normalization has traditionally been used in CNNs, newer models rely on layer normalization.",
    "crumbs": [
      "Lectures",
      "3 - Image Classification"
    ]
  },
  {
    "objectID": "pages/lectures/classification.html#pre-processing",
    "href": "pages/lectures/classification.html#pre-processing",
    "title": "3 - Image Classification",
    "section": "6 Pre-Processing",
    "text": "6 Pre-Processing\nTypically, images are pre-processed before being processed by the CNN. This is done to make the optimization process converge faster. Common steps include:\n\nResizing/cropping to a fixed size, e.g., 224x224x3. This is necessary for the images to be processed in a batch (they must have the same dimensions).\nScaling: Images are scaled from the interval [0, 255] to the interval [0, 1].\nNormalization: Normalization is often done along the color channels so that the mean value of the pixel values is 0.\n\nIn torchvision, the following pre-processing steps are used: Link",
    "crumbs": [
      "Lectures",
      "3 - Image Classification"
    ]
  },
  {
    "objectID": "pages/lectures/classification.html#transfer-learning",
    "href": "pages/lectures/classification.html#transfer-learning",
    "title": "3 - Image Classification",
    "section": "7 Transfer Learning",
    "text": "7 Transfer Learning\nTransfer Learning refers to the process of adapting a trained model for Task A to Task B. Adapting pre-trained models often leads to better results and also reduces the number of training iterations. Deep learning frameworks often provide easy access to such models (also known as Model Zoo). For example, from PyTorch: Link.\n\nfrom torchvision.models import resnet50, ResNet50_Weights\nresnet50(weights=ResNet50_Weights.IMAGENET1K_V2)",
    "crumbs": [
      "Lectures",
      "3 - Image Classification"
    ]
  },
  {
    "objectID": "pages/lectures/classification.html#pytorch-example",
    "href": "pages/lectures/classification.html#pytorch-example",
    "title": "3 - Image Classification",
    "section": "8 PyTorch Example",
    "text": "8 PyTorch Example\n\n8.1 Architecture\nExample of a CNN architecture.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nnet = Net()\n\n\n\n8.2 Loss Function\n\nimport torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)",
    "crumbs": [
      "Lectures",
      "3 - Image Classification"
    ]
  },
  {
    "objectID": "pages/lectures/classification.html#references",
    "href": "pages/lectures/classification.html#references",
    "title": "3 - Image Classification",
    "section": "9 References",
    "text": "9 References\n\n\nBa, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. “Layer Normalization.” arXiv. http://arxiv.org/abs/1607.06450.\n\n\nDeng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. “ImageNet: A Large-Scale Hierarchical Image Database.” In 2009 IEEE Conference on Computer Vision and Pattern Recognition, 248–55. Miami, FL: IEEE. https://doi.org/10.1109/CVPR.2009.5206848.\n\n\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. “Deep Residual Learning for Image Recognition.” In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 770–78. IEEE. https://doi.org/10.1109/CVPR.2016.90.\n\n\nHu, Jie, Li Shen, Samuel Albanie, Gang Sun, and Enhua Wu. 2019. “Squeeze-and-Excitation Networks.” arXiv. http://arxiv.org/abs/1709.01507.\n\n\nJohnson, Justin. 2019. “EECS 498-007 / 598-005: Deep Learning for Computer Vision.” Lecture {Notes} / {Slides}. https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2019/.\n\n\nKrizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. “ImageNet Classification with Deep Convolutional Neural Networks.” In Advances in Neural Information Processing Systems, edited by F. Pereira, C. J. Burges, L. Bottou, and K. Q. Weinberger. Vol. 25. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf.\n\n\nLi, Hao, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. 2018. “Visualizing the Loss Landscape of Neural Nets.” arXiv. http://arxiv.org/abs/1712.09913.\n\n\nLiu, Zhuang, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. 2022. “A ConvNet for the 2020s.” arXiv. http://arxiv.org/abs/2201.03545.\n\n\nPrince, Simon J. D. 2023. Understanding Deep Learning. MIT Press. https://udlbook.github.io/udlbook/.\n\n\nQiao, Siyuan, Huiyu Wang, Chenxi Liu, Wei Shen, and Alan Yuille. 2020. “Micro-Batch Training with Batch-Channel Normalization and Weight Standardization.” arXiv. http://arxiv.org/abs/1903.10520.\n\n\nSanturkar, Shibani, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. 2019. “How Does Batch Normalization Help Optimization?” arXiv. http://arxiv.org/abs/1805.11604.\n\n\nSimonyan, Karen, and Andrew Zisserman. 2015. “Very Deep Convolutional Networks for Large-Scale Image Recognition.” arXiv. http://arxiv.org/abs/1409.1556.\n\n\nWoo, Sanghyun, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, and Saining Xie. 2023. “ConvNeXt V2: Co-Designing and Scaling ConvNets with Masked Autoencoders.” arXiv. http://arxiv.org/abs/2301.00808.",
    "crumbs": [
      "Lectures",
      "3 - Image Classification"
    ]
  },
  {
    "objectID": "pages/lectures/cnns.html",
    "href": "pages/lectures/cnns.html",
    "title": "2 - Convolutional Neural Networks",
    "section": "",
    "text": "TipLearning Objectives\n\n\n\n\n\nAfter this lecture you should be able to:\n\nExplain why CNN inductive biases improve efficiency on images over MLPs.\nCompute output shapes and parameter counts for convolutional layers.\nDistinguish invariance vs equivariance and relate them to layer design.\nDescribe receptive field growth through stacking and dilation.\nCompare alternative convolution variants (depthwise separable, 1x1, transposed) and their use cases.",
    "crumbs": [
      "Lectures",
      "2 - Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/lectures/cnns.html#introduction-motivation",
    "href": "pages/lectures/cnns.html#introduction-motivation",
    "title": "2 - Convolutional Neural Networks",
    "section": "1 Introduction & Motivation",
    "text": "1 Introduction & Motivation\nImages have unique properties that make them challenging for standard neural networks:\n\nHigh-Dimensional: An RGB image of size \\(224 \\times 224\\) has \\(224 \\times 224 \\times 3 = 150{,}528\\) values\nLocality: Nearby pixels are statistically related (edges, textures, objects)\nStability under transformations: Interpretation of an image does not change under many geometric transformations. A cat should be recognized regardless of its position in the image.\nHierarchical Structure: Images contain features at multiple scales (pixels → edges → textures → objects)\n\n\n\n\n\n\n\nFigure 1: A high-resolution photograph. Image Source\n\n\n\n\n\n\n\n\n\nNoteQuestion: How are nearby pixels related?\n\n\n\nLooking at Figure 1, can you explain how nearby pixels are related?\nThink about it: What visual patterns or structures do you notice?\n\n\nClick for answer\n\nNearby pixels in images are strongly correlated due to:\n\nObject Coherence: Objects occupy contiguous regions—pixels within an object (like a cow’s body) share similar colors and textures\nSmooth Transitions: Natural images have gradual changes in color and intensity, not abrupt random variations\nTexture Patterns: Repeating structures (like grass blades or fur) create local statistical patterns\nEdges and Boundaries: Sharp transitions occur at object boundaries, but even edges span multiple neighboring pixels\n\n\n\n\nEven though we will soon see that Convolutional Neural Networks (CNNs) are much better suited to model images as compared to Multi-Layer Pereptrons (MLPs), we start by investigating the properties and difficulties to model images with MLPs. This will help understand how properties embedded in the architecture of CNNs (inductive biases) make them much more suitable to model computer vision tasks.\n\n1.1 MLPs and Images\nThe input to an MLP is a vector \\(\\mathbf{x}^{(i)}\\), which is transformed through multiple hidden layers to the output layer. Each hidden layer has a certain number of neurons, each connected to all neurons in the previous layer (fully-connected layers). The fully connected layers can only process 1-D vectors. Therefore, images \\(\\in \\mathbb{R}^{H \\times W \\times C}\\) must be flattened into 1-D vectors \\(\\in \\mathbb{R}^p\\). Here, \\(p= H \\times W \\times C\\). This causes MLPs to become very large (having many learnable parameters) when applied to high-dimensional inputs such as images. See Figure 2 for an illustration.\n\n\n\n\n\n\nFigure 2: Illustration of an MLP applied to an image.\n\n\n\nIn the CIFAR-10 dataset, which consists of very small images of \\(\\in \\mathbb{R}^{3 \\times 32 \\times 32}\\) (colors, height, width), a single neuron in the first hidden layer has 3 * 32 * 32 = 3,072 weights to learn (see Figure 3).\n\n\n\n\n\n\nFigure 3: Source: Johnson (2019)\n\n\n\n\nFor larger images, which are more common in practice, the number of weights is correspondingly much larger. Furthermore, to identify all significant patterns many neurons are required, further increasing the number of parameters. This may lead to overfitting and difficulty in learning the weights.\nThe dimensionality of weight matrix \\(\\mathbf{W}\\) scales linearly with input size: \\(\\mathbf{W} \\in \\mathbb{R}^{d \\times k}\\) (\\(d\\) is input dimensionality, \\(k\\) the number of neurons), or quadratically with the side length of an image (assuming a square image).\nSince each neuron in the first hidden layer is connected to each input value (single color channel of a pixel) and each such connection has a specific weight (matrix \\(\\mathbf{W}\\) ) the number of weights for each neuron is identical to the number of input values. Therefore, the weights of individual neurons (columns of \\(\\mathbf{W}\\) ) can be reshaped to the image dimensionality and thus be visualized. This reveals the patterns a specific neuron is reacting to. See Figure 4 for an illustration.\n\n\n\n\n\n\nFigure 4: Illustration of weight matrix \\(\\mathbf{W}\\) and how the weights of a single neuron can be reshaped into the input image dimensionality.\n\n\n\nOften the patterns we want to learn are not stationary. Ideally we want to recognize them under translations of the inputs, e.g. \\(g(\\mathbf{x})\\) where \\(g()\\) is a spatial translation. Figure 6 illustrates the problem, where the same pattern appears at different spatial positions.\n\n\n\n\n\n\nFigure 5: Illustration of different inputs with the same pattern but shifted spatially. Illustrated are the weights of the first neuron. Green cells represent high weight values and yellow cells represent low values.\n\n\n\n\n\n\n\n\n\nNoteQuestion: Patterns at different locations\n\n\n\nCan an MLP learn to detect the same pattern at different spatial locations? If yes, how? See Figure 6.\nThink about it: If an MLP learns to detect a cat in the center of an image, what happens when the same cat appears in the corner?\n\n\nClick for answer\n\nMLPs must learn separate detectors for each possible position! This is extremely inefficient because:\n\nParameter explosion: Need different weights for each spatial location\nData requirements: Must see target pattern in every possible position during training\nPoor generalization: Fails on positions not seen during training\n\nThis is why MLPs with the same number of parameters perform much worse on image tasks compared to CNNs.\n\n\n\n\n\n\n\n\n\n\nNoteQuestion: MLPs and permutations.\n\n\n\nCan an MLP successfully learn patterns in images \\(\\mathbf{x}\\) that are permuted with a permutation matrix \\(\\mathbf{P}\\), i.e., \\(f(P(\\mathbf{x}))\\)? See Figure 6.\nHint: Think carefully - what if EVERY image is permuted with the SAME permutation matrix?\n\n\nClick for the surprising answer\n\nYes, MLPs CAN learn on consistently permuted images! This is a crucial insight:\nWhy MLPs can handle consistent permutation:\n\nConsistent mapping: If every image uses the same permutation \\(P\\), pixel position relationships are preserved\nLearning is still possible: The MLP can learn that “pixel at position 247 (originally position 12) connects to pixel at position 891 (originally position 13)”\nNo information is lost: Permutation is just a reordering, not destruction of information\n\nThe real insight:\n\n✅ Same permutation for all images: MLP can learn the task\n❌ Different permutations per image: MLP cannot learn effectively\n❌ No permutation but objects move around: MLP struggles with translation\n\nWhy this matters: This thought experiment reveals that MLPs’ problem with images isn’t about permutation per se, but about spatial relationships and translation invariance:\n\nNatural images: Objects appear at different positions → MLP must learn separate detectors for each position\nConsistently permuted images: Spatial relationships are preserved (just reordered) → MLP can learn\nCNNs: Built-in translation equivariance makes them efficient for natural images regardless of object position\n\nKey takeaway: The issue isn’t that spatial structure is “destroyed” by permutation, but that CNNs are specifically designed to handle the translation variability in natural images efficiently.\n\n\n\n\n\n\n\n\n\nFigure 6: Inputs transformed with a permuation matrix.\n\n\n\nTo summarize MLPs on images:\n\nHigh-Dimensional: The size of an MLP scales with the input dimensionality which might blow beyond memory and compute budgets.\nLocality: MLPs have no notion of locality and thus can’t exploit this inherent bias in natural images.\nStability under transformations: MLPs need to learn position-dependent patterns which is very inefficient.\n\n\n\n\n\n\n\nTipTry MLPs on images in your browser.\n\n\n\nTry to draw the same digit at different positions and observe what happens!\n\nMLP Demo\n\n\n\n\n\n1.2 Invariance and Equivariance\nFor many tasks involving image data, small variations in the input should either not change the model output (invariance) or should change the output in tandem with the input changes (equivariance).\nA function \\(f(\\mathbf{x})\\) is invariant to transformation \\(g(\\mathbf{x})\\) if: \\[f(g(\\mathbf{x})) = f(\\mathbf{x})\\]\nA function \\(f(\\mathbf{x})\\) is equivariant to transformation \\(g(\\mathbf{x})\\) if: \\[f(g(\\mathbf{x})) = g(f(\\mathbf{x}))\\]\n\n\n\n\n\n\nImportantIntuition\n\n\n\n\nInvariant = “I don’t care about the transformation”\nEquivariant = “I transform my output the same way the input was transformed”\n\n\n\nFigure 7 shows an example of image classification. In this case, a model needs to be invariant to spatial translations of objects (if these do not change the image’s class).\n\n\n\n\n\n\nNoteQuestion: Inivariance for Image Classification\n\n\n\nWhat type of transformations \\(g()\\) does an image classification model as in Figure 7 need to be invariant to?\nThink about it: Consider what changes when the cows move to different positions in the image.\n\n\nClick for answer\n\nTranslation invariance is crucial for image classification. The model should output the same class label (e.g., “cow”) regardless of where the object appears in the image.\nKey transformations for classification:\n\nTranslation: Object position in the frame\nScale: Object size (small vs large)\nRotation: Object orientation (within reasonable limits)\nLighting: Brightness, contrast variations\n\nFor classification tasks specifically, we need invariance because: - The class label doesn’t depend on spatial location - A cow is a cow whether it’s in the center or corner of the image - The model should “pool” spatial information to make position-independent predictions\nThis is typically achieved by combining CNNs’ translation equivariance with operations like: - Pooling layers (max-pooling, average-pooling) - Global average pooling at the end of the network - These convert equivariant features into invariant predictions\n\n\n\n\n\n\n\n\n\nFigure 7: When objects in the input translate spatially, the output (in this case the classification of the images) does not change. The model \\(f(\\mathbf{x})\\) is thus invariant to spatial translations.\n\n\n\nFigure 8 shows an application of object detection. In this case, the model needs to be equivariant with respect to spatial translations (i.e., the bounding boxes that indicate an object’s location need to translate with the input). The model should remain invariant with respect to small changes in color, such as brightness adjustments.\n\n\n\n\n\n\nFigure 8: When objects in the input (first row) translate spatially, the detections (bounding boxes) change accordingly (bottom row). The model \\(f(\\mathbf{x})\\) that produces the bounding boxes is thus equivariant with respect to spatial translations.\n\n\n\nWe can summarise the required properties for different tasks:\n\n\n\n\n\n\nNoteWhen do we want each?\n\n\n\n\n\n\n\n\n\n\n\nTask Type\nProperty Needed\nWhy?\n\n\n\n\nClassification\nTranslation Invariance\nCat is still a cat regardless of position\n\n\nObject Detection\nTranslation Equivariance\nIf object moves right, bounding box moves right\n\n\nSemantic Segmentation\nTranslation Equivariance\nIf input shifts, pixel-wise labels shift\n\n\n\nUnderstanding this distinction is crucial for choosing the right architecture!",
    "crumbs": [
      "Lectures",
      "2 - Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/lectures/cnns.html#convolutional-neural-networks",
    "href": "pages/lectures/cnns.html#convolutional-neural-networks",
    "title": "2 - Convolutional Neural Networks",
    "section": "2 Convolutional Neural Networks",
    "text": "2 Convolutional Neural Networks\nNow that we’ve seen the limitations of MLPs for images, let’s explore how Convolutional Neural Networks (CNNs) address these challenges.\n\n\n\n\n\n\nTipThe Big Idea\n\n\n\nInstead of connecting every neuron to every input, CNNs use three key principles:\n\nLocal Connectivity: Each neuron only looks at a small spatial region\nParameter Sharing: The same filter/detector is used across all spatial locations\nTranslation Equivariance: If the input shifts, the output shifts correspondingly\n\nThese aren’t just technical tricks—they encode our prior knowledge about how images work!\n\n\n\n2.1 The CNN Advantage\nA single neuron in a CNN is only connected to a small portion (local connectivity) of the image (see Figure 9). As a result:\n\n✅ Fewer Parameters: Neurons have far fewer connections than in MLPs\n✅ Spatial Structure Preserved: No need to flatten 2D images into 1D vectors\n✅ Translation Equivariance: Same detector works everywhere in the image\n✅ Hierarchical Feature Learning: Stack layers to learn increasingly complex patterns\n\n\n\n\n\n\n\nFigure 9: Source: Johnson (2019)\n\n\n\n\n\n\n\n2.2 Convolutional Neural Networks\nCNNs consist of a sequence of different layers. Each layer transforms activations from the previous layer into new activations through a differentiable operation. Below we look at the main layer types: convolutional layers, pooling layers, activation layers, and fully connected layers. Arranged in a specific sequence, this is referred to as the architecture of the model.\nFigure 10 shows an example architecture. The activation maps of the various layers are shown, representing the corresponding outputs of the layers.\n\n\n\n\n\n\nFigure 10: The activations of a ConvNet architecture are shown. The input image is on the left and the predictions on the right. Source: Li (2022).\n\n\n\nSometimes different layers are combined and referred to as a block. For example, the combination of a convolutional layer followed by an activation layer and a pooling layer is often used. This would be a CONV-ACT-POOL block. –&gt;\n\n\n2.3 Convolution Operation\nConvolutional layers are the primary layers in CNNs, responsible for extracting visual features. The weights of a convolutional layer consist of a set of learnable filters. Each filter is typically small along the spatial dimensions (height, width) relative to the input but extends across the entire input depth. For example, a typical filter in the first layer might have dimensions \\(3 \\times 7 \\times 7\\) (3 pixels for the three color channels, 7 pixels along height/width). During the forward pass, the filters are convolved along the height and width over the input. At each position, the dot product between the filter and input is calculated (viewing both as 1-D vectors), producing a 2-D activation map that represents the filter’s response at each position in the input. Intuitively, the CNN learns filters corresponding to typical visual patterns, such as edges and colors, in the first layer. A set of \\(C_{out}\\) filters produces activation maps with a depth of \\(C_{out}\\).\n\n\n\n\n\n\nImportantFilters vs Kernels\n\n\n\nFilter and kernel are sometimes used synonymously. Here, we differentiate by considering a filter as 3-dimensional (CxHxW) and a kernel as 2-dimensional (HxW). A filter consists of C kernels.\n\n\n\\[\nS(i, j) = (K * I)(i, j) =\\sum_m\\sum_n I(i + m, j + n)K(m, n)\n\\tag{1}\\]\nWith the following components:\n\n\\(I\\) is the input (for example, an image),\n\\(K\\) is the kernel (typically smaller than \\(I\\)),\n\\(b\\) is a bias term added to the weighted sum,\n\\(i\\) and \\(j\\) index the spatial positions.\n\nIf \\(I\\) is an RGB image (e.g., in the first layer of a CNN), the filter \\(K\\) would have dimensions \\(3 \\times k \\times k\\) (assuming a square kernel). More generally, we learn filters with dimensions \\(C_{in} \\times k \\times k\\).\nMultiple filters, say \\(C_{out}\\) filters, can be grouped together: \\(C_{out} \\times C_{in} \\times k \\times k\\).\nWe often refer to such tensors as filters or filter banks.\n\n\n\n\n\n\nNote\n\n\n\nConvolution in deep learning is typically implemented as cross-correlation. Equation 1 actually defines cross-correlation.\n\n\nConvolution on RGB images is nicely illusrated in Figure 11. We can see that the filter is multiplied element-wise with the corresponding values in the image, the result is summed top produce one single value in the output.\n\n\n\n\n\n\nSource: Prince (2023)\n\n\n\n\nFigure 11: An RGB image is represented as a grid of pixels, each consisting of three color values \\(C_{in}=3\\). Illusrated is how a filter with dimensionality \\(3 \\times 3 \\times 3 (C \\times H \\times W)\\) is first multiplied elementwise with a specific input volume \\(3 \\times 3 \\times 3 (C \\times H \\times W)\\) and then summed up to produce one single value in the output activation map (bright gray). Source: Prince (2023)\n\n\n\nThe following figures show further step-by-step illustration of applying convolutions on input activations that produce multiple output activation maps.\nFigure 12 illustrates how a filter produces one value per position by taking the dot product at a specific location (and adding the bias term).\n\n\n\n\n\n\nFigure 12: Source: Johnson (2019)\n\n\n\nFigure 13 illustrates how a filter produces one activation map, which is an output with one channel and (almost) the same spatial dimensionality as the input activations.\n\n\n\n\n\n\nFigure 13: Source: Johnson (2019)\n\n\n\nFigure 14 illustrates how multiple filters produce multiple outputs. After all, we often need to extract / detect different features, each represented by a specific filter.\n\n\n\n\n\n\nFigure 14: Source: Johnson (2019)\n\n\n\nFinally, Figure 15 illustrates how a convolutional layer is defined: it consists of a set of filters and their corresponding biases. Note that the output is essentially a \\(C_{out}\\)-dimensional image.\n\n\n\n\n\n\nFigure 15: Source: Johnson (2019)\n\n\n\nThe data is processed in mini-batches, i.e., multiple images at once, as shown in Figure 16.\n\n\n\n\n\n\nFigure 16: Source: Johnson (2019)\n\n\n\n\n\n2.4 Padding, Stride, Kernel Size, Dilation\nTo define a convolution, various hyperparameters need to be set. Some of the most important ones are:\n\nPadding: How much padding is added to the input?\nStride: What is the step size (stride) of the kernel?\nKernel-Size: What is the kernel size?\nDilation: What is the dilation rate?\n\nStride determines how the filters are convolved over the input activations, essentially the step size. If the stride is 1, the filter moves one pixel at a time to compute the next activation (see Figure 17). If the stride is greater, e.g., 2, it moves two pixels at a time, making the activation maps smaller in width and height (see Figure 18).\n\n\n\n\n\n\nFigure 17: Convolution with stride (1, 1). Left the input activations with (red) the individual positions of the center pixel of the kernel / filter.\n\n\n\n\n\n\n\n\n\nFigure 18: Convolution with stride (2, 2). Left the input activations with (red) the individual positions of the center pixel of the kernel / filter.\n\n\n\n\n\n\n\n\n\nTipStride &gt; 1 often used!\n\n\n\nConvolutions with stride provide a way to reduce the spatial dimensionality within a CNN and are commonly used. We will see later that other operations, such as pooling layers, also reduce spatial dimensionality. Using convolutions, however, is sometimes advantageous, since the downsampling is learned.\n\n\nPadding refers to adding (typically) zeros to the border of the input activations before performing the convolution. This can be useful to ensure, for example, that the spatial dimensions of the activation maps are identical to those of the input activations. This is essential for segmentation tasks. Figure 19 illustrates how padding helps maintain the spatial resolution when padding the input activations.\n\n\n\n\n\n\nFigure 19: Left: Input (Yellow) with Zero-Padding (white border), Middle: Filter, Right: Output.\n\n\n\nKernel-size determines the spatial extend from which a value in the output activation map migh aggregate values. It is the receptive field of an output value with respect to the input values. Figure 20 and Figure 21 show convolutions with different kernel sizes. Note that the amount of padding increases with kernel-size. Kernels usually have an odd number of pixels in height and width to make sure they have a center pixel. Also, the number of parameters increases with kernel-size \\((C_{in} \\times k \\times k) + C_{out}\\), which is often an important consideration.\n\n\n\n\n\n\nFigure 20: Convolution with zero-padding and \\(3 \\times 3\\) kernel that produces activations maps with unchanged spatial resolution.\n\n\n\n\n\n\n\n\n\nFigure 21: Convolution with zero-padding and \\(5 \\times 5\\) kernel that produces activations maps with unchanged spatial resolution.\n\n\n\n\n\n\n\n\n\nQuestion\nWhat is the difference between using 3×3 vs 5×5 kernels? What is the same?\nThink first: Consider computational cost, receptive field, and parameter count.\n\n\nClick for detailed comparison\n\n\n\n\n\n\n\n\n\nAspect\n3×3 Kernel\n5×5 Kernel\n\n\n\n\nParameters\n9 weights (plus bias) per input channel\n25 weights (plus bias) per input channel\n\n\nComputation\nLess (fewer multiplications)\nMore (more multiplications)\n\n\nReceptive Field\nSmaller (grows slowly)\nLarger (grows faster)\n\n\nDetail Capture\nFine-grained features\nBroader spatial patterns\n\n\n\nWhat’s the same: - Both preserve translation equivariance - Both use parameter sharing - Both can learn spatial features\n\n\n\n\n\n\n\n\n\n\nTipBest Practice\n\n\n\nModern CNN Design: Use multiple 3×3 convolutions instead of larger kernels to get:\n\n✅ Fewer parameters\n✅ More non-linearity (more activation functions)\n✅ Better gradient flow\n✅ Similar receptive field coverage\n\n\n\nDilation specifies how much the kernel is spread out and thus directly influences the receptive field. Figure 22 shows how it works.\n\n\n\n\n\n\nFigure 22: Illustration of how a kernel with dilation=2 (middle) is applied to the input activations (left) to produce activations (right). Note the reduced spatial dimensionality of the output activations.\n\n\n\n\n\n\n\n\n\nNoteQuestoin: Dilation?\n\n\n\nWhy would we use dilation \\(d &gt; 1\\)?\nConsider: What happens to the receptive field? What about computational cost?\n\n\nClick for comprehensive answer\n\nDilation increases receptive field without adding parameters or computation!\nBenefits of dilation:\n\nLarger receptive field: See more context without more layers\nSame parameter count: Filter size stays the same\nMulti-scale features: Different dilation rates capture different scales\n\nUse cases:\n\nSemantic segmentation: Need large receptive fields while preserving spatial resolution\nDense prediction tasks: Want to see global context for local predictions\nEfficient architectures: Alternative to deeper networks\n\nTrade-offs:\n\n✅ Larger receptive field with same parameters\n❌ May miss fine-grained spatial relationships\n❌ Can create “grid artifacts” if not used carefully\n\nExample: A 3×3 kernel with dilation=2 has the receptive field of a 5×5 kernel but only 9 parameters instead of 25!\nAtrous/Dilated Convolutions are especially popular in:\n\nDeepLab (semantic segmentation)\nWaveNet (audio generation)\nDilated ResNets (dense prediction)\n\nThe key insight: effectively expand the receptive field!\n\n\n\n\n\n\n\n\n\nDumoulin and Visin (2016) has created some animations for better understanding of convolutions and published them here: https://github.com/vdumoulin/conv_arithmetic.\n\n\n\nConvolving a 3x3 kernel over a 5x5 input without padding and with stride 1x1. Source Dumoulin and Visin (2016)\n\n\n\n\n\n\n\n2.5 Calculations: Spatial Dimensionality of Activation Maps\nYou can calculate the dimensionality of the activation maps with the following formulas:\n\n\\(i\\): Side length of the input activations (assumption: square inputs)\n\\(k\\): Kernel size (assumption: square kernel)\n\\(o\\): Side length of the output activation maps\n\\(s\\): Stride (assumption: same stride along spatial dimensions)\n\\(p\\): Number of paddings on each side (assumption: same number of paddings along spatial dimensions)\n\\(d\\): Dilation rate.\n\n\\[\\begin{equation}\no = \\left\\lfloor \\frac{i + 2p - d\\cdot(k-1) - 1}{s} + 1 \\right\\rfloor\n\\end{equation}\\]\n\n\n\n\n\n\n\nNote🧮 Quiz: Output Shape Calculation\n\n\n\n\n\nProblem: You’re designing a CNN layer with the following parameters:\n\nInput size: \\(i = 32\\) (height and width)\nKernel size: \\(k = 3\\)\nPadding: \\(p = 1\\)\nStride: \\(s = 2\\)\nDilation: \\(d = 1\\)\n\nQuestion: What is the output size \\(o\\)?\n\n\nClick for step-by-step solution\n\nStep-by-step solution:\n\nApply the formula: \\(o = \\lfloor \\frac{i + 2p - d(k-1) - 1}{s} + 1 \\rfloor\\)\nSubstitute: \\(o = \\lfloor \\frac{32 + 2(1) - 1(3-1) - 1}{2} + 1 \\rfloor\\)\nSimplify: \\(o = \\lfloor \\frac{32 + 2 - 2 - 1}{2} + 1 \\rfloor\\)\nCalculate: \\(o = \\lfloor \\frac{31}{2} + 1 \\rfloor = \\lfloor 15.5 + 1 \\rfloor = 16\\)\n\nAnswer: \\(o = 16\\)\nIntuition: With stride 2, we’re roughly halving the spatial dimensions, but padding helps maintain some size.\n\n\n\n\n\n\n2.6 Calculations: Number of Weights in a Convolutional Layer\nYou can calculate the total number of weights in a convolutional layer with the following formula:\n\n\\(k\\): Kernel size (assumption: square kernel, so kernel has dimensions \\(k \\times k\\))\n\\(C_{in}\\): Number of input channels\n\\(C_{out}\\): Number of output channels\n\n\\[\\begin{equation}\n\\text{Total Weights} = (k \\times k) \\times C_{in} \\times C_{out}\n\\end{equation}\\]\nEach filter has a size of \\(k \\times k \\times C_{in}\\), and there are \\(C_{out}\\) filters in total, resulting in \\((k \\times k \\times C_{in}) \\times C_{out}\\) weights.\nBiases: If each output channel has a bias term, add an additional \\(C_{out}\\) weights for the biases.\n\\[\\begin{equation}\n\\text{Total Parameters} = (k \\times k \\times C_{in} \\times C_{out}) + C_{out}\n\\end{equation}\\]\n\n\n\n\n\n\nNote🧮 Quiz: Parameter Counting\n\n\n\n\n\nScenario: Design a convolutional layer with:\n\nInput: \\(3 \\times 32 \\times 32\\) (RGB image)\n64 output channels\nKernel size: \\(5 \\times 5\\)\nInclude bias terms\n\nQuestions:\n\nHow many weights in the filters?\nHow many bias parameters?\nTotal parameters?\nIf we used a fully connected layer instead, how many parameters would that require?\n\n\n\nClick for solutions\n\nSolutions:\n\nFilter weights: \\((5 \\times 5) \\times 3 \\times 64 = 4{,}800\\)\nBias parameters: \\(64\\) (one per output channel)\nTotal parameters: \\(4{,}800 + 64 = 4{,}864\\)\nFully connected alternative: \\((32 \\times 32 \\times 3) \\times 64 = 196{,}608\\)\n\nInsight: CNNs use 40× fewer parameters while being more effective for images!",
    "crumbs": [
      "Lectures",
      "2 - Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/lectures/cnns.html#properties",
    "href": "pages/lectures/cnns.html#properties",
    "title": "2 - Convolutional Neural Networks",
    "section": "3 Properties",
    "text": "3 Properties\n\n3.1 Local (Sparse) Connectivity & Parameter Sharing\nFully connected layers are impractical when working with high-dimensional inputs like images. If all neurons in a layer were connected to all previous neurons, the number of parameters to be estimated would increase massively, which is inefficient and leads to overfitting. Each neuron in a CNN is therefore only connected to a local region of the input volume (see Figure 12). The spatial extent of this region is a hyperparameter and is called the receptive field of a neuron (kernel size) on the input volume. The connections along the depth (C) extend over the entire depth of the input volume \\(C_{in}\\). The connections are therefore local along the spatial dimensions (width and height) but complete along the depth.\nParameter sharing in convolutional layers is used to reduce the number of parameters. Since the filters are convolved over the inputs, the individual weights of the filters are identical over the spatial extent of the input volume. One of the main assumptions behind CNNs is the following: If it is useful to learn a specific (visual) feature at a certain position, then it is probably useful at other positions as well. In other words: If I learn filters that detect edges, corners, or cats, then it is a reasonable assumption that I want to do this throughout the image.\n\n\n\n\n\n\nNote\n\n\n\nSometimes parameter sharing does not make sense. This can be the case, for example, if we have centered structures in the images. Then you might want to learn position-dependent features. An example is images of faces that have been centered, where you might want to learn filters that detect the mouth only in the lower middle area (locally connected layers).\n\n\nListing 1 shows the number of parameters of an MLP, while Listing 2 shows the number of parameters of a CNN (each with two hidden layers) on the CIFAR10 dataset.\n\n\n\n\nListing 1: PyTorch example of an MLP.\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchinfo\n\nclass MLP(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.hidden_layer1 = nn.Linear(3 * 32 * 32, 64)\n        self.hidden_layer2 = nn.Linear(64, 32)\n        self.output_layer = nn.Linear(32, 10)\n\n    def forward(self, x):\n        x = self.flatten(x)\n        x = torch.relu(self.hidden_layer1(x))\n        x = torch.relu(self.hidden_layer2(x))\n        x = self.output_layer(x)\n        return x\n\nnet = MLP()\nprint(torchinfo.summary(net, input_size=(1, 3, 32, 32)))\n\n\n\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nMLP                                      [1, 10]                   --\n├─Flatten: 1-1                           [1, 3072]                 --\n├─Linear: 1-2                            [1, 64]                   196,672\n├─Linear: 1-3                            [1, 32]                   2,080\n├─Linear: 1-4                            [1, 10]                   330\n==========================================================================================\nTotal params: 199,082\nTrainable params: 199,082\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 0.20\n==========================================================================================\nInput size (MB): 0.01\nForward/backward pass size (MB): 0.00\nParams size (MB): 0.80\nEstimated Total Size (MB): 0.81\n==========================================================================================\n\n\n\n\n\n\nListing 2: PyTorch example of a CNN.\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchinfo\n\nclass CNN(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 16, 7, stride=2, padding=3)\n        self.conv2 = nn.Conv2d(16, 16, 3, stride=2, padding=1)\n        self.flatten = nn.Flatten()\n        self.output_layer = nn.Linear(16 * 8 * 8 , 10)\n\n    def forward(self, x):\n        x = torch.relu(self.conv1(x))\n        x = torch.relu(self.conv2(x))\n        x = self.flatten(x)\n        x = self.output_layer(x)\n        return x\n\ncnn = CNN()\nprint(torchinfo.summary(cnn, input_size=(1, 3, 32, 32)))\n\n\n\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nCNN                                      [1, 10]                   --\n├─Conv2d: 1-1                            [1, 16, 16, 16]           2,368\n├─Conv2d: 1-2                            [1, 16, 8, 8]             2,320\n├─Flatten: 1-3                           [1, 1024]                 --\n├─Linear: 1-4                            [1, 10]                   10,250\n==========================================================================================\nTotal params: 14,938\nTrainable params: 14,938\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 0.76\n==========================================================================================\nInput size (MB): 0.01\nForward/backward pass size (MB): 0.04\nParams size (MB): 0.06\nEstimated Total Size (MB): 0.11\n==========================================================================================\n\n\nWe can see that the number of parameters of the CNN is much smaller. Feel free to compare performances.\n\n\n\n\n\n\nQuestion\nWhy in Listing 2 is pading=3 in self.conv1 = nn.Conv2d(3, 16, 7, stride=2, padding=3)? Would padding=0also be possible?\n\n\nClick for answer\n\nAnswer:\nThe padding=3 is chosen to maintain the spatial dimensions when using stride=2.\nUsing the output size formula: \\[o = \\left\\lfloor \\frac{i + 2p - d\\cdot(k-1) - 1}{s} + 1 \\right\\rfloor\\]\nWith the given parameters: - Input: \\(i = 32\\) - Kernel: \\(k = 7\\) - Stride: \\(s = 2\\) - Dilation: \\(d = 1\\) - Padding: \\(p = 3\\)\n\\[o = \\left\\lfloor \\frac{32 + 2(3) - 1(7-1) - 1}{2} + 1 \\right\\rfloor = \\left\\lfloor \\frac{32 + 6 - 6 - 1}{2} + 1 \\right\\rfloor = \\left\\lfloor \\frac{31}{2} + 1 \\right\\rfloor = 16\\]\nWould padding=0 work?\nYes, but it would produce a different output size: \\[o = \\left\\lfloor \\frac{32 + 0 - 6 - 1}{2} + 1 \\right\\rfloor = \\left\\lfloor \\frac{25}{2} + 1 \\right\\rfloor = 13\\]\nWhy padding=3 is preferred here:\n\nMaintains a “nice” output dimension (\\(16 \\times 16\\)) that’s easy to work with in subsequent layers\nPreserves more spatial information from the edges of the input\nCommon practice: use padding to keep output size as \\(\\lfloor \\frac{i}{s} \\rfloor\\) when possible\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nA convolution can also be expressed as a single matrix multiplication and is thus a special case of a linear transformation (such as in an MLP). Figure 23 shows how a linear layer in an MLP (top left) has different connections between each input \\(x_1, \\cdots, x_6\\) and neurons in a hidden layer \\(h_1, \\cdots, h_6\\) and the corresponding weight matrix (bottom left). On the right, the same is shown for a 1-D convolution with kernel size 3, however, note that the weight matrix is sparse (white values) and that the weights are shared (repeated patterns across the diagonal).\n\n\n\n\n\n\nFigure 23: Illustration of linear layer in an MLP (left column), including graph (top) and weight matrix (bottom). Convolutional layer (right) represented as a linear transformation. Source Prince (2023).\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion: Linear Transformation\n\n\n\n\n\nHow should the linear transformation be defined to obtain the desired result in Figure 24? How many parameters are needed? How could this be done with a convolution?\n\n\n\n\n\n\nFigure 24: Input in 2-D (top left), the flattened version of it (bottom left), expected output (right), and unknown transformation (middle).\n\n\n\n\n\n\n\n\n3.2 Translation Equivariance\nConvolutions are translation equivariant, as illustrated well in the following example:\n\n\n\n\n\n\nFigure 25: Illustration of translation equivariance in convolutional layers.\n\n\n\nSee Section 1.2 for more information.\n\n\n3.3 Stacking Convolutions\nMultiple convolutions can be executed in sequence (stacking). Each convolution is performed on the activation maps of another, preceeding convolution. Figure 26 illustrates the process.\n\n\n\n\n\n\nFigure 26: Source: Johnson (2019)\n\n\n\nA convolution is therefore not only performed directly on the input (e.g., images) but is generally defined on inputs of dimensionality \\(C \\times H \\times W\\).\nThe principle of stacking convolutional layers and therefore convolutions are not just applied on raw pixel values from an image is important to understand! While the filters in the first convolutional layer, which directly operate on the input image, typically learn basic features such as lines in different orientations or colors, filters in subsequent layers learn more complex features by combining the features from preceeding layers. This is similar to our visual cortex. Zeiler and Fergus (2014) used a special technique to visualize patterns that filters in deeper layers are reacting to (see Figure 27).\nHowever, non-linear activation functions must be used between the convolutions. Otherwise, the stacked convolution can be expressed with a simple convolution (similar to an MLP, which can be expressed with a linear transformation without activation functions).\n\n\n\n\n\n\nFigure 27: Shown are patterns the learned flters are reacting to for several layers (in gray) and the corresponding image patches that achieve maximum activation values for these patterns. Source: Zeiler and Fergus (2014)\n\n\n\n\n\n3.4 Receptive Field\nThe receptive field defines which inputs influence the activations of a neuron. The receptive field of a neuron with respect to the input activations (the image pixels) is quite important. If the receptive field is smaller than the objects of interest, the network is unable to “see” the whole object. This is particularly problematic for object detection. See Figure 28 and Figure 31 to illustrate the concept.\n\n\n\n\n\n\nFigure 28: Receptive field with a 5x5 kernel.\n\n\n\n\n\n\n\n\n\nFigure 29\n\n\n\n\n\n\n\n\n\nFigure 30: Receptive field with a 5x5 kernel and two layers.\n\n\n\n\n\n\n\n\n\nFigure 31: Receptive field with a 5x5 kernel and two layers. The output activation at the center (right) aggregates values over the union of all blue boxes in the left figure and has thus a receptive field of \\(9 \\times 9\\) with respect to the input activations.\n\n\n\nThe receptive field \\(r_0\\) of a convolutional neural network can be calculated as follows:\n\nDefine the recurrence relation:\n\n\\[\\begin{equation}\nr_{l-1} = s_l \\cdot r_l + (k_l - s_l)\n\\end{equation}\\]\n\n\\(r_l\\): Receptive field at layer \\(l\\).\n\\(s_l\\): Stride of layer \\(l\\).\n\\(k_l\\): Kernel size of layer \\(l\\).\n\n\nSolving this recurrence relation, we get the closed-form solution for the receptive field at the input layer \\(r_0\\):\n\n\\[\\begin{equation}\nr_0 = \\sum_{l=1}^L \\left( (k_l - 1) \\prod_{i=1}^{l-1} s_i \\right) + 1\n\\end{equation}\\]\n\n\\(L\\): Total number of layers.\n\\(k_l\\): Kernel size at layer \\(l\\).\n\\(s_i\\): Stride at layer \\(i\\), from layer 1 up to \\(l-1\\).\n\nAraujo, Norris, and Sim (2019) have written a nice article about receptive fields and how to calculate them (also refer to this article for more details on the notation and justification).\n\n\n\n\n\n\nNote🎯 Challenge Quiz: Receptive Field\n\n\n\n\n\nNetwork Architecture:\n\nLayer 1: Conv2d(kernel=3, stride=1)\nLayer 2: Conv2d(kernel=3, stride=2)\nLayer 3: Conv2d(kernel=5, stride=1)\n\nQuestion: What is the receptive field of the final layer with respect to the input?\n\n\nClick for solution\n\nSolution using the formula: \\[r_0 = \\sum_{l=1}^3 \\left( (k_l - 1) \\prod_{i=1}^{l-1} s_i \\right) + 1\\]\n\n\\(r_0 = (3-1) \\cdot 1 + (3-1) \\cdot 1 + (5-1) \\cdot (1 \\cdot 2) + 1\\)\n\\(r_0 = 2 + 2 + 8 + 1 = 13\\)\n\nAnswer: \\(13 \\times 13\\) receptive field\nPractical insight: Each output pixel “sees” a \\(13 \\times 13\\) region of the input image.\n\n\n\n\n\n\n3.5 Comparing MLPs and CNNs\nNow that we understand how CNNs work, let’s compare them systematically to MLPs:\n\n\n\n\n\n\nNoteQuestion: MLP vs CNNs\n\n\n\nHow do MLPs and CNNs differ in their approach to processing images?\nThink first: Consider parameter count, assumptions about data, and what each architecture “knows” about images.\n\n\nClick for detailed comparison\n\n\n\n\n\n\n\n\n\nAspect\nMLP\nCNN\n\n\n\n\nInput\nFlattened vector\nPreserve spatial structure (H×W×C)\n\n\nConnections\nFully connected\nLocally connected\n\n\nParameter Sharing\nNone (each weight unique)\nShared across spatial locations\n\n\nTranslation Equivariance\n❌ No\n✅ Yes (by design)\n\n\nAssumptions\nNone about spatial structure\nLocality + stationarity of statistics\n\n\nParameter Count\nVery high\nMuch lower\n\n\nLearns Position\nMust learn same feature at each position\nLearns once, applies everywhere\n\n\nBest For\nTabular data, general patterns\nImages, spatial data\n\n\n\nKey Takeaway: MLPs can theoretically learn to process images (universal approximation theorem), but CNNs bake in the right inductive biases, making learning vastly more efficient.\n\n\n\n\n\n\n\n\n\nTipDeep Learning Insight: Inductive Bias\n\n\n\nInductive bias refers to the assumptions a learning algorithm makes to generalize from training data to unseen data.\nMLPs: Minimal inductive bias (very general, but requires lots of data) CNNs: Strong inductive bias (assumes locality, translation equivariance → more efficient learning)\nThe right inductive bias for your data is key to success! This is why:\n\nCNNs excel at images (spatial locality matters)\nTransformers excel at sequences (long-range dependencies matter)\nGraph Neural Networks excel at networks (graph structure matters)\n\nChoosing architecture = choosing which assumptions to bake in!",
    "crumbs": [
      "Lectures",
      "2 - Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/lectures/cnns.html#more-layers-operations",
    "href": "pages/lectures/cnns.html#more-layers-operations",
    "title": "2 - Convolutional Neural Networks",
    "section": "4 More Layers & Operations",
    "text": "4 More Layers & Operations\nCNNs often do not consist solely of convolutional layers but have other layers with specific purposes.\n\n4.1 Downsampling\nSpatial downsampling is often a desired property of convolutional layers. We can achieve that by choosing a stride \\(\\gt 1\\). However, there are other options, such as pooling layers. Figure 32 illustrates some pooling variants.\n\n\n\n\n\n\nFigure 32: Shown are a) Sub-Sampling, b) Max Pooling, c) Average Pooling. Source: Prince (2023)\n\n\n\n\n\n4.2 Upsampling\nSometimes we need to increase the spatial dimensionality of our activation maps.\n\n\n\n\n\n\nFigure 33: Shown are a) Duplicate, b) Max-Unpooling, c) Bilinear Interpolation . Source: Prince (2023)\n\n\n\nUpsampling can also be learned with Transposed Convolutions. More on that topic in Image Segmentation.\n\n\n\n\n4.3 Changing the number of channels\n\\(1 \\times 1\\) convolutions have a kernel size of 1x1 and thus no spatial extent. These layers are often used in CNNs to change the number (\\(C\\)) of activation maps with few parameters. For example, activation maps of dimensionality (\\(C \\times H \\times W\\)) can be changed to a volume of (\\(C2 \\times H \\times W\\)) using only \\(C2 * (C + 1)\\) parameters. This can be useful, for example, to save parameters before more complex layers or at the end of the CNN to adjust the size of the activation maps to the number of classes to be modeled (for classification problems) or to reduce to 3 color channels (\\(C2=3\\)) for image generation models. Figure 34 shows an example that uses only 6 weights (excluding bias) to reduce the number of channels to one.\n\n\n\n\n\n\nFigure 34: \\(1 \\times 1\\) convolution. Source: Prince (2023)\n\n\n\n\n\n4.4 Global Average Pooling\nGlobal Average Pooling is often an important component. It computes the average of the activations along the depth dimension, reducing activation maps from (C x H x W) to (C x 1 x 1). This is useful for directly modeling logits in a classification problem with C classes, enabling architectures that completely eliminate fully-connected layers.\n\n\n\n\n\n\nFigure 35: Global Average pooling, input (left) and output (right).\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis can be achieved using PyTorchs https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html\n\n\n\n\n4.5 Other Layers\nThere are many other types of layers, also there are more flavours of convolutional layers.\nOne layer often used to reduce the number of parameters are depthwise separable convolutions. Instead of extending filters over the entire depth of the input activations, a separate filter (kernel) is used for each input channel, with the dimensionality (\\(1 \\times k \\times k\\)). Figure 36 shows an example. Subsequently, 1x1 convolutions are used to combine information across the input channels. See Figure 37 for a comparison of ‘normal’ convolutions and depthwise separable convolutions. Since 1x1 convolutions require fewer parameters, activation maps can be generated with fewer parameters.\n\n\n\n\n\n\nFigure 36: Source: https://paperswithcode.com/method/depthwise-convolution\n\n\n\n\n\n\n\n\n\nFigure 37: Source: Yu and Koltun (2016)\n\n\n\nAdditionally, important layers include the diverse familiy of normalization layers which can improve convergence of the optimization procedure dramatically.",
    "crumbs": [
      "Lectures",
      "2 - Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/lectures/cnns.html#pytorch-examples",
    "href": "pages/lectures/cnns.html#pytorch-examples",
    "title": "2 - Convolutional Neural Networks",
    "section": "5 PyTorch Examples",
    "text": "5 PyTorch Examples\n\nimport numpy as np\nimport torch\nfrom torch.nn import functional as F\nimport torchshow as ts\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\n\n\n#img = Image.open({{&lt; meta params.images_path &gt;}}'cat.jpg')\nimage_path = \"../../assets/images/cnns/cat.jpg\"\nimg = Image.open(image_path)\nimg\n\n\n\n\n\n\n\n\n\nfilter_ = torch.tensor(\n    [   [[1, 0, -1], [1, 0, -1], [1, 0, -1]], # R\n        [[1, 0, -1], [1, 0, -1], [1, 0, -1]], # G\n        [[1, 0, -1], [1, 0, -1], [1, 0, -1]], # B\n    ]).unsqueeze(0).float()\nts.show(filter_, show_axis=False)\n\n\n\n\n\n\n\n\n\ninput = torch.tensor(np.array(img)).unsqueeze(0).permute(0, 3, 1, 2).float() # (N, C, H, W)\ninput /= 255.0\ninput -= 1.0\nresult = F.conv2d(input, filter_, stride=1, padding=0, dilation=1, groups=1)\n\n\nts.show(result)\n\n/opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages/torchshow/visualization.py:385: UserWarning:\n\nOriginal input range is not 0-1 when using grayscale mode. Auto-rescaling it to 0-1 by default.\n\n\n\n\n\n\n\n\n\n\n2D-Convolution:\n\nresult = F.conv2d(input, filter_, stride=6, padding=0, dilation=1, groups=1)\nts.show(result)\n\n\n\n\n\n\n\n\nTransposed convolution:\n\nresult = F.conv2d(input, filter_, stride=6, padding=0, dilation=1, groups=1)\nresult = F.conv_transpose2d(result, weight=torch.ones_like(filter_))\nts.show(result)\n\n\n\n\n\n\n\n\nMax-Pooling:\n\nresult = F.max_pool2d(input, kernel_size=8, stride=8)\nts.show(result)",
    "crumbs": [
      "Lectures",
      "2 - Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/lectures/cnns.html#references",
    "href": "pages/lectures/cnns.html#references",
    "title": "2 - Convolutional Neural Networks",
    "section": "6 References",
    "text": "6 References\n\n\nAraujo, Andr&eacute;, Wade Norris, and Jack Sim. 2019. “Computing Receptive Fields of Convolutional Neural Networks.” Distill 4 (11): 10.23915/distill.00021. https://doi.org/10.23915/distill.00021.\n\n\nDumoulin, Vincent, and Francesco Visin. 2016. “A Guide to Convolution Arithmetic for Deep Learning.” ArXiv e-Prints, March.\n\n\nJohnson, Justin. 2019. “EECS 498-007 / 598-005: Deep Learning for Computer Vision.” Lecture {Notes} / {Slides}. https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2019/.\n\n\nLi, Fei-Fei. 2022. “CS231n Convolutional Neural Networks for Visual Recognition.” Lecture {Notes}. https://cs231n.github.io.\n\n\nPrince, Simon J. D. 2023. Understanding Deep Learning. MIT Press. https://udlbook.github.io/udlbook/.\n\n\nYu, Fisher, and Vladlen Koltun. 2016. “Multi-Scale Context Aggregation by Dilated Convolutions.” arXiv. http://arxiv.org/abs/1511.07122.\n\n\nZeiler, Matthew D., and Rob Fergus. 2014. “Visualizing and Understanding Convolutional Networks.” Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 8689 LNCS (PART 1): 818–33. https://doi.org/10.1007/978-3-319-10590-1_53.",
    "crumbs": [
      "Lectures",
      "2 - Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/lectures/practical.html",
    "href": "pages/lectures/practical.html",
    "title": "5 - Practical",
    "section": "",
    "text": "TipLearning Objectives\n\n\n\n\n\nBy the end of this lecture you should be able to:\n\nApply a systematic step-by-step recipe for training neural networks, from data exploration through hyperparameter tuning to final optimization.\nDiagnose common training failures by understanding leaky abstractions and silent failure modes in deep learning frameworks.\nEstablish robust evaluation pipelines with appropriate metrics, train/validation/test splits, and baseline models before scaling to complex architectures.\nImplement regularization strategies including data augmentation, early stopping, weight decay, and transfer learning to improve generalization performance.\nSelect appropriate data augmentation techniques (AugLy, Albumentations, Kornia) and understand when synthetic data can enhance training.\nRecognize dataset characteristics (class imbalance, label quality, image resolution requirements) that impact modeling decisions and expected performance.\nConfigure reproducible experiments with proper random seeding, experiment tracking (Weights & Biases, Neptune), and structured project organization.\nOptimize model performance through systematic hyperparameter tuning (grid search, random search, Ray Tune) and understand advanced phenomena like double descent and grokking.",
    "crumbs": [
      "Lectures",
      "5 - Practical"
    ]
  },
  {
    "objectID": "pages/lectures/practical.html#a-recipe",
    "href": "pages/lectures/practical.html#a-recipe",
    "title": "5 - Practical",
    "section": "1 A Recipe",
    "text": "1 A Recipe\nHow to successfully train Deep Learning models? Below is a step-by-step recipe. This recipe is based in part on Andrej Karpathy’s blog post A Recipe for Training Neural Networks.\nKarpathy observed that training neural networks with modern ML frameworks is highly abstracted, but these abstractions are “leaky”, meaning that without a deep understanding of internal mechanisms such as back-propagation and gradient descent, mistakes can occur. Therefore, one is not completely protected from understanding what happens within the abstraction.\n\nyour_data = # plug your awesome dataset here\nmodel = SuperCrossValidator(SuperDuper.fit, your_data, ResNet50, SGDOptimizer)\n\nMoreover, faulty implementations or sub-optimal model training do not lead to an error message. This means training neural networks fails silently. For example, the learning rate might be too high, causing the model to converge too early and achieve sub-optimal performance. Such issues would not be flagged with an error message but would require hyper-parameter tuning or other diagnostic plots.\nFor these reasons, Karpathy developed a recipe to avoid the most significant pitfalls.\n\n1.1 1) Become Familiar with the Data\nIn the first step, the data to be modeled should be thoroughly inspected. Particularly for images, one can examine sample data points, their distribution, patterns, and anomalies. Corrupted images, skewed class distributions, duplicates, etc., may be found. Often, just by looking, one can gauge if modeling will work or be challenging, assess label quality, or determine how much downscaling images can tolerate.\n\n\n\n\n\n\nFigure 1: Examples of images from camera traps. They may still be usable despite being distorted by heat.\n\n\n\nFigure 2 shows typical challenging cases in modeling images from camera traps. Small ROIs suggest that the image resolution should not be excessively reduced. Typical resolutions like 224x224 might be insufficient for certain species.\n\n\n\n\n\n\nFigure 2: Examples of images from camera traps. Source: Beery, Van Horn, and Perona (2018).\n\n\n\nFigure 3 displays the distribution of a typical camera trap dataset. While some species are frequent, others are rare, making modeling difficult and requiring special techniques.\n\n\n\n\n\n\nFigure 3: Species distribution in a typical camera trap dataset. “Deer” comprises about 65% of cases.\n\n\n\nLater in the modeling process, examining misclassified samples is crucial to understand where more data labeling is necessary. Figure 4 shows an example of a rare species that the model misclassified.\n\n\n\n\n\n\nFigure 4: An image of a serval. Model confidences are displayed below.\n\n\n\nSometimes, one may also encounter certain limitations, such as images that are ambiguous or belong to multiple classes. Figure 5 shows an example with a herd of animals containing multiple species (Thomson’s and Grant’s Gazelle). Here, consideration of the modeling approach’s suitability is necessary.\n\n\n\n\n\n\nFigure 5: Example of an image from a camera trap with various species.\n\n\n\n\n\n1.2 2) Baselines\nAfter becoming familiar with the data, set up an evaluation pipeline. Define a metric to measure model performance and establish train/validation/test splits or cross-validation procedures. Also, implement or use tools for monitoring and logging training.\n\n\n\n\n\n\nFigure 6: The components of a typical machine learning process. Source: Raschka and Mirjalili (2020).\n\n\n\nNext, use a simple model, which is unlikely to be incorrectly implemented, to test the pipeline. Visualize metrics, losses, and predictions, and test different hypotheses to verify pipeline accuracy.\n\n\n\n\n\n\nFigure 7: Weights and Biases experiment tracking.\n\n\n\nSome Tips for This Step:\nEnsure reproducibility by fixing the random seed so that results remain consistent. See PyTorch Reproducibility.\n\nimport torch\ntorch.manual_seed(0)\n\nAvoid unnecessary techniques and complexities, such as data augmentation, which may introduce bugs that are hard to detect.\nUse a human baseline if possible. For certain tasks, labels may be subjective, with annotators potentially disagreeing on how to label a data point. This implies an upper bound on model performance, which can be estimated by having some data points annotated multiple times. Examples of such images are shown in Figure 8 and Figure 9.\n\n\n\n\n\n\nFigure 8: Example of a challenging species to classify (for amateurs). It could be either Hartebeest or Topi.\n\n\n\n\n\n\n\n\n\nFigure 9: A challenging image to classify from a camera trap dataset. In the Snapshot Serengeti dataset, amateur annotators achieved a 96.6% agreement rate with experts.\n\n\n\nTrain an input-independent baseline, setting all pixel values to the same value, such as zero. This helps determine if a simple model is learning anything at all.\n\n\n\n\n\n\nNoteQuiz: What would you expect?\n\n\n\n\n\nWhat kind of output would you expect for an input-independent baseline?\n\n\nClick for result\n\nSince the model receives no information from the input (all pixels are the same), it cannot distinguish between individual samples. The optimal strategy is to minimize the average loss across all training samples.\nFor classification with cross-entropy loss, this means the model will learn to predict the class distribution of the training set. For example, if 30% of training images are cats and 70% are dogs, the model will output approximately P(cat) ≈ 0.3 and P(dog) ≈ 0.7 for every input, regardless of what it actually contains.\nThis serves as a sanity check: if your model performs worse than this baseline, something is fundamentally wrong with your training pipeline.\n\n\n\n\nAnother option is to overfit the model on a batch of data. If everything is functioning correctly, one should achieve perfect train metrics/losses.\nIt is also helpful to visualize what goes into the model. This can help detect errors in data augmentation or data preprocessing. This should be done just before executing this line of code:\n\ny_hat = model(x)\n\nObserving predictions from a fixed sample throughout training can provide insight into the dynamics of the process. In object detection, one might display bounding boxes and classification scores. Figure 10 shows an example of a segmentation problem: the input image is on the left, and the model’s segmentation is on the right.\n\n\n\n\n\n\nFigure 10: Example of a segmentation problem: left is the input, right is the output.\n\n\n\n\n\n1.3 3) Overfit\nAt this point, you should have a good understanding of the dataset, confidence in the evaluation pipeline, and initial baselines for simple models. Additionally, you should have a sense of how well a human could perform on this task (if relevant). In this step, focus on achieving good performance on the training set, accepting overfitting temporarily. Next, focus on the validation set.\nFor model architecture, follow the principle “Don’t be a hero.” Use existing, well-established architectures, such as ResNet-50 for image classification.\nThe choice of optimizer should also be conservative. The adam optimizer is often effective and less sensitive to hyper-parameters like the learning rate.\n\n\n1.4 4) Regularization\nAt this point, you should have achieved good performance on the training set. Now, shift focus to the validation set, as it is critical for estimating expected performance on new data. This often involves a reduction in training performance due to regularization.\nMore Training Data\nThe simplest way to improve performance (and reduce overfitting) is to collect more training data. If possible, this approach is more effective than other methods and helps a model generalize better. The benefit of additional data can be estimated by training several models on different training set sizes and observing performance changes (learning curve).\n\n\n\n\n\n\nFigure 11: Example of a learning curve. X-axis is performance, Y-axis is the number of training samples. Left panel: Gaussian Naive Bayes; right panel: Support Vector Classifier.\n\n\n\nData Augmentation\nAnother approach is data augmentation, generating new data points from existing ones by making random alterations. Data points are typically augmented on-the-fly rather than in advance. ML frameworks like PyTorch provide built-in transformations, and additional packages offer more complex augmentations.\nFor example, AugLy, see Figure 12.\n\n\n\n\n\n\nFigure 12: AugLy\n\n\n\nAnother option is Albumentations, see Figure 13.\n\n\n\n\n\n\nFigure 13: Albumentations\n\n\n\nKornia provides even more complex augmentations, see Figure 14.\n\n\n\n\n\n\nFigure 14: Kornia\n\n\n\nFigure 15 shows an example of data augmentation with color changes, cropping, and rotations.\n\n\n\n\n\n\nFigure 15: Data augmentation example.\n\n\n\nModern Augmentation Strategies:\n\nFor fine-tuning foundation models: Use minimal augmentation (crops, flips only)\nFor training from scratch: Heavy augmentation (RandAugment, MixUp, CutMix)\nSelf-supervised models (DINO, MAE) are already robust to augmentations\n\nSynthetic data can also be added to the training process, which can be successful. Figure 16 shows an example where 3D models of animals were created using a game engine and placed either in synthetic landscapes or spliced into real camera trap images.\n\n\n\n\n\n\nFigure 16: From Beery et al. (2020). Synthetic and semi-synthetic data.\n\n\n\nEarly Stopping\nEarly stopping involves training a model and evaluating it on a validation set periodically (e.g., after each epoch). Training is halted if no significant improvement is observed after a set number of cycles, preventing overfitting.\n\n\n\n\n\n\nFigure 17: Source: Link\n\n\n\nFrameworks like pytorch-lightning provide Callbacks to implement early stopping.\n\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\n\nclass LitModel(LightningModule):\n    def validation_step(self, batch, batch_idx):\n        loss = ...\n        self.log(\"val_loss\", loss)\n\nmodel = LitModel()\ntrainer = Trainer(callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\")])\ntrainer.fit(model)\n\nWeight Decay\nWeight decay is another common regularization technique. In gradient descent, weights are reduced by a factor \\(\\lambda\\) during each update. The modified update step is:\n[ _{t+1} = _t (1 - ) - J() ]\nwhere \\(t\\) is the iteration, \\(\\theta\\) are the model parameters, and \\(\\eta\\) is the learning rate. This can be applied directly in PyTorch torch.optim.SGD.\nTransfer Learning\nTransfer learning also has a regularizing effect and can reduce model training time. This approach typically involves using a pre-trained model on a large dataset (e.g., ImageNet), removing the last layer, and replacing it with a new one suited to the target task. Various layers can be frozen or fine-tuned.\n\n\n\n\n\n\nFigure 18: Source: Johnson and Fouhey (2021)\n\n\n\nModern Pre-trained Model Options:\nFor general-purpose vision features, prefer:\n\nDINOv3 (self-supervised): Best for fine-grained tasks, robust to distribution shifts\nCLIP (vision-language): Excellent for semantic tasks, zero-shot capabilities\nResNet-50/EfficientNet (supervised): Good baseline, efficient\n\nFor domain-specific tasks:\n\nStart with foundation models, then fine-tune\nUse timm library: timm.create_model('vit_base_patch14_dinov2.lvd142m', pretrained=True)\nUse Hugging Face: transformers.CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n\nThe following example code shows how to freeze the parameters of a pre-trained model by setting requires_grad = False.\n\ndef set_parameter_requires_grad(model, feature_extracting):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False\n\nAdaptation Strategy Hierarchy:\nWhen fine-tuning pre-trained models, follow this progression:\n\nk-NN baseline: Freeze backbone, classify with nearest neighbors (fastest validation)\nLinear probe: Freeze backbone, train only classification head\nPartial fine-tune: Unfreeze last few layers + head\nFull fine-tune: Unfreeze all layers (only if data &gt;&gt; 10k samples)\n\n\n\n\n\n\n\nTip\n\n\n\nRule of thumb: Start with linear probing. Only move to full fine-tuning if you have &gt;10k labeled samples per class and computational budget allows. k-NN is even simpler, however, might be slow for inference if the training dataset is large.\n\n\n\n\n1.5 5) Hyper-Parameter Tuning\nThis step involves systematically evaluating various hyper-parameters and architectures. Techniques include grid-search or random-search, with random-search generally preferred.\nComplete packages are available for this, such as Ray Tune.\nHyper-parameter tuning requires configurable architecture implementations. Below is an example of configurable fully connected layers.\n\nclass Net(nn.Module):\n    def __init__(self, l1=120, l2=84):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, l1)\n        self.fc2 = nn.Linear(l1, l2)\n        self.fc3 = nn.Linear(l2, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nFor domain-specific problems, consult relevant literature for ideas on possible hyper-parameter variations and architectures (e.g., medical imaging or satellite images).\n\n\n1.6 6) Squeeze out the Juice\nOnce the best architectures and hyper-parameters are identified, there are additional ways to maximize performance.\nOne is ensembling: training multiple models and combining their predictions often yields slightly better performance than a single model. However, this increases training and inference costs.\nSurprising effects sometimes occur in deep learning, like double descent (Nakkiran et al. (2019)) or grokking (Power et al. (2022)). For instance, using highly complex models (an overfitting risk) can work well, and after a plateau, validation loss might improve if training continues. So it may be worthwhile to train a model longer (without early stopping).\n\n\n\n\n\n\nFigure 19: Source: Nakkiran et al. (2019). The X-axis represents model complexity, and the Y-axis shows test and train error. Test error decreases, increases (overfitting), and then decreases again (double descent).\n\n\n\nAdditional techniques often yield small improvements. For example, Liu et al. (2022) reports that the following techniques improved accuracy by about 2.7%:\n\nLonger training\nSpecialized optimizer (AdamW)\nComplex data augmentation techniques (Mixup, Cutmix, RandAugment)\nRegularization techniques (Stochastic Depth)\nLabel smoothing",
    "crumbs": [
      "Lectures",
      "5 - Practical"
    ]
  },
  {
    "objectID": "pages/lectures/practical.html#model-zoos-and-frameworks",
    "href": "pages/lectures/practical.html#model-zoos-and-frameworks",
    "title": "5 - Practical",
    "section": "2 Model Zoos and Frameworks",
    "text": "2 Model Zoos and Frameworks\n\n2.1 Hugging-Face\nHuggingFace provides a wide range of models, datasets, and APIs. It offers easy-to-use Python APIs to integrate, use, and adapt models for PyTorch.\n\n\n2.2 timm\nPyTorch Image Models (timm) is a library with SOTA models, layers, and functions for using, reproducing, and adapting modern image models.",
    "crumbs": [
      "Lectures",
      "5 - Practical"
    ]
  },
  {
    "objectID": "pages/lectures/practical.html#code-setup-configuration",
    "href": "pages/lectures/practical.html#code-setup-configuration",
    "title": "5 - Practical",
    "section": "3 Code Setup / Configuration",
    "text": "3 Code Setup / Configuration\nSetting up a well-organized repository structure can significantly aid in model development. Key aspects include reproducibility, configurability, tracking, and logging experiments.\nSuggested repository structures:\n\nDS-cookie cutter: DrivenData\nPyTorch-Lightning example: Lightning Template\nConfigurations: Hydra\nExperiment tracking: Weights & Biases, Neptune AI\nVersion Control for ML projects: DVC",
    "crumbs": [
      "Lectures",
      "5 - Practical"
    ]
  },
  {
    "objectID": "pages/lectures/practical.html#references",
    "href": "pages/lectures/practical.html#references",
    "title": "5 - Practical",
    "section": "4 References",
    "text": "4 References\n\n\nBeery, Sara, Yang Liu, Dan Morris, Jim Piavis, Ashish Kapoor, Markus Meister, Neel Joshi, and Pietro Perona. 2020. “Synthetic Examples Improve Generalization for Rare Classes.” Proceedings - 2020 IEEE Winter Conference on Applications of Computer Vision, WACV 2020, 852–62. https://doi.org/10.1109/WACV45572.2020.9093570.\n\n\nBeery, Sara, Grant Van Horn, and Pietro Perona. 2018. “Recognition in Terra Incognita.” In Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 11220 LNCS:472–89. https://doi.org/10.1007/978-3-030-01270-0_28.\n\n\nJohnson, Justin, and David Fouhey. 2021. “EECS 442: Computer Vision.” Lecture {Notes} / {Slides}. https://web.eecs.umich.edu/~justincj/teaching/eecs442/WI2021/.\n\n\nLiu, Zhuang, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. 2022. “A ConvNet for the 2020s.” arXiv. http://arxiv.org/abs/2201.03545.\n\n\nNakkiran, Preetum, Boaz Barak, Gal Kaplun, Ilya Sutskever, Yamini Bansal, and Tristan Yang. 2019. “Deep Double Descent: Where Bigger Models and More Data Hurt.” arXiv, 1–24.\n\n\nPower, Alethea, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. 2022. “Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets.” arXiv. http://arxiv.org/abs/2201.02177.\n\n\nRaschka, Sebastian, and Vahid Mirjalili. 2020. Python Machine Learning: Machine Learning and Deep Learning with Python, Scikit-Learn, and TensorFlow. Second edition, fourth release,[fully revised and updated]. Expert Insight. Birmingham Mumbai: Packt Publishing.",
    "crumbs": [
      "Lectures",
      "5 - Practical"
    ]
  },
  {
    "objectID": "pages/lectures/representation_learning.html",
    "href": "pages/lectures/representation_learning.html",
    "title": "4 - Representation Learning",
    "section": "",
    "text": "TipLearning Objectives\n\n\n\n\n\nBy the end of this lecture you should be able to:\n\nDefine representation learning and explain why transforming raw pixels into semantic embeddings enables efficient downstream task performance.\nUnderstand the balance between invariance (robustness to nuisance factors) and selectivity (discriminating semantically distinct inputs) in learned representations.\nKnow supervised, self-supervised (contrastive, masked autoencoding, self-distillation), and weakly supervised (vision-language) learning paradigms.\nDistinguish between global representations (single embedding per image) and local representations (spatially-resolved patch tokens) and identify appropriate use cases for each.\nSelect appropriate adaptation strategies (k-NN → linear probe → adapters/LoRA → partial fine-tuning → full fine-tuning) based on dataset size, computational resources, and task requirements.\nJustify model selection between DINOv3, CLIP, MAE, and supervised models for specific downstream tasks considering training objectives, data modalities, and few-shot vs zero-shot requirements.\nApply practical transfer learning workflows using Hugging Face Transformers and timm libraries for real-world computer vision tasks.",
    "crumbs": [
      "Lectures",
      "4 - Representation Learning"
    ]
  },
  {
    "objectID": "pages/lectures/representation_learning.html#motivation",
    "href": "pages/lectures/representation_learning.html#motivation",
    "title": "4 - Representation Learning",
    "section": "1 Motivation",
    "text": "1 Motivation\nDifferent properties and challenges, such as the semantic gap, are reasons modelling images is difficult. In particular extracting generic, semantically meaningful information is hard. Humans, however, excel at this task and can easily and comprehensively describe images with relatively few bits of information. A good example is Figure 1:\n\n\n\n\n\n\n\n\n\n\nFigure 1: Example Image Source.\n\n\n\nIt is easy for humans to describe the image. For example:\n\n\"\"\"\nA photograph of an elephant facing the camera.\nThe elephant has large tusks and ears and is standing in a flat savannah.\nIs is bright day light with a cloudy sky.\n\"\"\"\n\nOf course: a lot of information is lost, however, practically relevant information is preserved!\n\n\n\n\n\n\nNoteInformation Content: Raw vs Natural Language\n\n\n\n\n\nLet’s quantify the dramatic compression achieved by semantic description:\nRaw Image (Pixel Representation):\n\nResolution: 640 × 360 pixels\nChannels: 3 (RGB)\nBits per pixel: 8 bits per channel\n\n\n\n\n\n\nRepresentation\nSize (bits)\nSize (KB)\nCompression Factor\n\n\n\n\nRaw RGB Image\n5,529,600\n675.0\n1×\n\n\nNatural Language (UTF-8)\n1,312\n0.16\n4215×\n\n\nNatural Language (ASCII)\n1,148\n0.14\n4817×\n\n\n\n\n\n\n\n\nWe can also say: By describing the image we represent the image with natural language, instead of the raw pixels.\nThis is exactly what Representation Learning is interested in: How can we find compact representations wich are significantly smaller than the raw, original representations while retaining relevant information?\nThe idea is that such compact representations are well suited for down-stream tasks, such as image classification.\nOne motivation to represent images in a different space than with raw pixels (data space) is the enormous difference between the set of all possible images and the set of natural (real-world) images. This indicates that representing images in pixel space is not efficient. As illustrated in Figure 2, the subset of natural images occupies only a tiny fraction of the entire image space. For instance, for RGB images of size \\(256 \\times 256\\) pixels with 255 possible intensity values per channel, the total number of possible images is \\(255^{256 \\times 256 \\times 3}\\), an astronomically large number, effectively infinite from a practical standpoint. This enables strong compression if we can find a more suitable space or coordinate system to represent images.\n\n\n\n\n\n\nFigure 2: Illustrated are the set of all possible images and the subset of natural images.",
    "crumbs": [
      "Lectures",
      "4 - Representation Learning"
    ]
  },
  {
    "objectID": "pages/lectures/representation_learning.html#what-is-representation-learning",
    "href": "pages/lectures/representation_learning.html#what-is-representation-learning",
    "title": "4 - Representation Learning",
    "section": "2 What Is Representation Learning?",
    "text": "2 What Is Representation Learning?\nA representation is the output \\(\\mathbf{z} = f(\\mathbf{x})\\) produced by an encoder \\(f\\) applied to input \\(\\mathbf{x}\\); \\(\\mathbf{z}\\) retains task‑relevant structure while discarding nuisance variation. The encoder \\(f\\) is the mapping; the representation \\(\\mathbf{z}\\) is its result.\nTherefore, when we refer to representations we typically mean vector-valued (distributed) representations and not, for example, natural language descriptions.\n\n2.1 Intuition\nLet’s take a look at the (synthetic) images in Figure 3:\n\n\n\n\n\n\n\n\nElephant - Day 1\n\n\n\n\n\n\n\nElephant - Day 1\n\n\n\n\n\n\n\nElephant - Day 1\n\n\n\n\n\n\n\nElephant - Day 2\n\n\n\n\n\n\n\n\n\nGiraffe - Day\n\n\n\n\n\n\n\nRhino - Night\n\n\n\n\n\n\n\nElephant - Night\n\n\n\n\n\n\n\nCar - Day\n\n\n\n\n\n\nFigure 3: Synthetic images illustrating representation learning challenges.\n\n\n\nImagine you have to represent those in 2-D: \\(\\mathbf{z} \\in \\mathbb{R}^2\\). How would you proceed from a human point of view?\n\n\n\n\n\n\nFigure 4: Empty 2-dimensional representation space for mapping image similarity.\n\n\n\n\n\n\n\n\n\nNoteQuestion: Representing Images\n\n\n\n\n\nHow would you represent the images of Figure 3 in 2 dimensions? Why?\nHint: Similar images might should be close to each other. Exact positions do not matter, only distances.\n\n\nClick for result\n\nKey Considerations:\nThe images demonstrate different types of similarity challenges in representation learning:\n\nSame class, same conditions (Elephant Day 1 & Elephant Day 2): These are most similar - both elephants photographed in daylight. They exhibit intra-class variation while maintaining the same semantic category.\nSame class, different conditions (Elephant Day vs. Elephant Night): Moderate similarity - same animal class but dramatically different lighting conditions. This tests illumination invariance.\nDifferent classes, same conditions (Elephants, Giraffe, Rhino by day): Animals share similar contexts (daytime, natural habitat) but different semantics, requiring inter-class discrimination.\nDifferent domain (Animals vs. Car): Least similar - represents a complete domain shift from wildlife to manufactured objects.\n\nIdeal representation learning: A good model should group the two daytime elephants closely, maintain reasonable similarity between day/night elephants, distinguish between different animal classes, and clearly separate animals from vehicles.\n\n\n\n\n\n\n2.2 What are good representations?\nGood representations transform raw data into a format that makes relevant patterns explicit and accessible. They represent a semantic coordinate systems where distances \\(\\|\\mathbf{z}_i - \\mathbf{z}_j\\|_2\\) reflect meaningful relationships.\n\n\n\n\n\n\nFigure 5: Data Space vs Representation Space: The encoder \\(f\\) transforms high-dimensional input samples \\(\\mathbf{x}_i\\) from data space into compact representations \\(\\mathbf{z}_i\\) in representation space, where semantic relationships become explicit through distances and clustering.\n\n\n\nMathematical Framework:\nGiven input \\(\\mathbf{x} \\in \\mathcal{X}\\) and encoder \\(f: \\mathcal{X} \\to \\mathcal{Z}\\), we want representation \\(\\mathbf{z} = f(\\mathbf{x})\\) such that:\n\n2.2.1 Invariance & Selectivity Balance\n\nInvariance: \\(\\|f(\\mathbf{x}) - f(g(\\mathbf{x}))\\|_2 &lt; \\epsilon\\) for nuisance transform \\(g\\) (lighting, pose)\nSelectivity: \\(\\|f(\\mathbf{x}_i) - f(\\mathbf{x}_j)\\|_2 &gt; \\delta\\) for semantically different \\(\\mathbf{x}_i, \\mathbf{x}_j\\)\nTrade-off: Too much invariance → loss of discriminative details\nSweet spot: Preserve task-relevant variations, discard irrelevant ones\n\n\n\n2.2.2 Geometric Structure Preservation\n\nSmooth manifold: Similar semantic concepts cluster in representation space\nComposability: \\(f(\\mathbf{x}_1) \\oplus f(\\mathbf{x}_2) \\approx f(\\text{combine}(\\mathbf{x}_1, \\mathbf{x}_2))\\) for vector operations\nInterpolability: Linear interpolation \\(\\alpha f(\\mathbf{x}_1) + (1-\\alpha) f(\\mathbf{x}_2)\\) yields meaningful intermediate concepts\n\n\n\n2.2.3 Transfer Efficiency\n\nLow sample complexity: Few examples needed for downstream adaptation\nBroad applicability: Same \\(f(\\cdot)\\) works across multiple tasks/domains\nGraceful degradation: Performance degrades slowly with domain shift\n\n\n\n2.2.4 Computational Practicality\n\nCompact dimensionality: \\(\\text{dim}(\\mathbf{z}) \\ll \\text{dim}(\\mathbf{x})\\) while preserving information\nFast computation: Forward pass \\(f(\\mathbf{x})\\) efficient for real-time applications\nStable training: Representation learning converges reliably",
    "crumbs": [
      "Lectures",
      "4 - Representation Learning"
    ]
  },
  {
    "objectID": "pages/lectures/representation_learning.html#deep-image-representations",
    "href": "pages/lectures/representation_learning.html#deep-image-representations",
    "title": "4 - Representation Learning",
    "section": "3 Deep Image Representations",
    "text": "3 Deep Image Representations\nThere are multiple ways to learn image representations with deep learning, using different learning paradigms and objectives.\n\n3.1 Supervised Learning\nSupervised pretraining learns \\(f(\\cdot)\\) by predicting human labels (e.g., ImageNet classes) with a loss like cross-entropy. Deep networks progressively transform pixels into higher-level semantics, and linear probes on the penultimate layer often separate classes.\n\n\n\n\n\n\nFigure 6: Supervised Pre-training Architecture: Input images are processed by an encoder (CNN or ViT) that extracts features, followed by a classification head trained with cross-entropy loss on human-provided labels. The penultimate layer embeddings serve as transferable representations for downstream tasks.\n\n\n\n\n\n3.2 Weakly Supervised Learning\nWeak supervision uses noisy/partial labels at scale—hashtags, alt-text, or image–text pairs. A landmark example is CLIP (see Radford et al. (2021)), trained to align images and their captions via contrastive learning. It enables zero-shot recognition by turning text prompts into classifiers (e.g., “a photo of a cat”). This reduces annotation cost and yields broadly transferable features. See Figure 7 for an illustration.\n\n\n\n\n\n\nNoteCLIP Loss\n\n\n\n\n\nComposite objective. CLIP jointly trains an image encoder \\(f_\\theta\\) and a text encoder \\(g_\\phi\\) to align matched image–text pairs using a symmetric contrastive (InfoNCE) loss with a learned temperature (logit scale).\nSetup. For a batch of \\(N\\) paired samples \\(\\{(x_i, y_i)\\}_{i=1}^N\\):\n\nImage features: \\(\\tilde{\\mathbf{v}}_i = \\frac{f_\\theta(x_i)}{\\|f_\\theta(x_i)\\|}\\)\nText features: \\(\\tilde{\\mathbf{t}}_i = \\frac{g_\\phi(y_i)}{\\|g_\\phi(y_i)\\|}\\)\nLearned logit scale: \\(\\alpha = \\exp(s)\\) with parameter \\(s\\) (equivalently temperature \\(\\tau = 1/\\alpha\\))\nSimilarities (cosine scaled): \\(s_{ij} = \\alpha \\, \\tilde{\\mathbf{v}}_i^\\top \\tilde{\\mathbf{t}}_j\\)\n\nImage-to-text loss (with in-batch negatives). \\[\\begin{equation}\n\\mathcal{L}_{i \\to t}\n= \\frac{1}{N} \\sum_{i=1}^{N}\n-\\log \\frac{\\exp\\!\\big(s_{ii}\\big)}\n{\\sum_{j=1}^{N} \\exp\\!\\big(s_{ij}\\big)} \\, .\n\\end{equation}\\]\nText-to-image loss (symmetric direction). \\[\\begin{equation}\n\\mathcal{L}_{t \\to i}\n= \\frac{1}{N} \\sum_{j=1}^{N}\n-\\log \\frac{\\exp\\!\\big(s_{jj}\\big)}\n{\\sum_{i=1}^{N} \\exp\\!\\big(s_{ij}\\big)} \\, .\n\\end{equation}\\]\nTotal objective. \\[\\begin{equation}\n\\mathcal{L}_{\\text{CLIP}}\n= \\tfrac{1}{2} \\left( \\mathcal{L}_{i \\to t} + \\mathcal{L}_{t \\to i} \\right) \\, .\n\\end{equation}\\]\nComponents (at a glance).\n\n\\(f_\\theta, g_\\phi\\): image/text encoders (e.g., ViT/CNN and Transformer).\n\\(\\tilde{\\mathbf{v}}_i, \\tilde{\\mathbf{t}}_i\\): \\(\\ell_2\\)-normalized embeddings (cosine similarity).\n\\(\\alpha\\) (or \\(1/\\tau\\)): learned scaling controlling softness of the softmax.\nIn-batch negatives: all non-matching pairs in the current batch act as negatives.\nOptimization: cross-entropy over the similarity matrix in both directions.\n\nInference note: zero-shot classification uses text prompts \\(y_c =\\) “a photo of a \\(\\{class\\}\\)” to form class prototypes \\(\\tilde{\\mathbf{t}}_c\\) and picks \\(\\arg\\max_c \\tilde{\\mathbf{v}}^\\top \\tilde{\\mathbf{t}}_c\\).\n\n\n\n\n\n\n\n\n\nFigure 7: CLIP Contrastive Pre-training: Shown are the image and text encoders processing a batch of paired data. The resulting embeddings are compared via a similarity matrix where matching pairs (diagonal) are maximized and non-matching pairs (off-diagonal) are minimized through contrastive learning.\n\n\n\n\n\n3.3 Self-Supervised Learning\nSelf-supervised learning (SSL) creates pretext tasks from raw data (no human labels) to shape \\(f(\\cdot)\\). Two dominant families are imputation/masked modeling and contrastive/self-distillation.\n\n3.3.1 Imputation\nMasked image modeling hides random patches and trains the model to reconstruct them. Masked Autoencoding (MAE) (see He et al. (2021)) shows that an asymmetric encoder–decoder and high masking ratio (≈75%) scale well and produce strong features for transfer. Intuition: to fill in what’s missing, the model must internalize global structure and semantics.\n\n\n\n\n\n\nNoteMasked Autoencoding Loss\n\n\n\n\n\nSetup. Given an image \\(x\\), MAE patchifies it and masks a large subset of patches \\(\\mathcal{M}\\) (typically \\(\\sim 75\\%\\)), keeping visible patches \\(\\mathcal{V}\\).\n\nPatchify: \\(X = \\mathrm{Patchify}(x) \\in \\mathbb{R}^{P \\times d}\\) with \\(P\\) patches and patch-dimension \\(d\\).\nSplit: \\(X = \\{X_{\\mathcal{V}}, X_{\\mathcal{M}}\\}\\).\nEncoder \\(E_\\theta\\) processes only visible tokens \\(X_{\\mathcal{V}}\\) to produce latents \\(H = E_\\theta(X_{\\mathcal{V}})\\).\nDecoder \\(D_\\phi\\) receives \\(H\\) plus learned mask tokens at masked positions and predicts reconstructions \\(\\hat{X}_{\\mathcal{M}} = D_\\phi(H, \\text{mask tokens})\\).\n\nReconstruction loss (masked patches only). \\[\\begin{equation}\n\\mathcal{L}_{\\text{MAE}}\n=\n\\frac{1}{|\\mathcal{M}|} \\sum_{p \\in \\mathcal{M}}\n\\left\\| \\hat{X}_{p} - \\tilde{X}_{p} \\right\\|_2^2 ,\n\\end{equation}\\] where \\(\\tilde{X}\\) are normalized pixel targets (e.g., per-channel mean/variance normalization) in patch space.\nComponents (at a glance).\n\n\\(X\\): patchified image tokens; \\(\\mathcal{V}/\\mathcal{M}\\): visible/masked index sets.\n\\(E_\\theta\\) (ViT encoder): operates on visible tokens only (asymmetric design).\n\\(D_\\phi\\) (lightweight decoder): reconstructs masked tokens from encoder latents \\(+\\) mask tokens.\n\\(\\hat{X}_{\\mathcal{M}}\\): decoder predictions at masked positions.\n\\(\\tilde{X}_{\\mathcal{M}}\\): normalized pixel targets at masked positions.\nLoss averages only over masked patches, encouraging learning of global structure.\n\nNotes.\n\nHigh mask ratio \\(+\\) asymmetric encoder/decoder yields efficient pretraining and strong transfer.\nSome variants replace pixel MSE with feature/perceptual losses, but vanilla MAE uses MSE on normalized pixels.\n\n\n\n\n\n\n\n\n\n\nFigure 8: MAE Pre-training Pipeline: A large fraction of image patches is randomly masked. The encoder processes only visible patches for efficiency. After encoding, lightweight mask tokens are inserted, and a small decoder reconstructs the original image pixels from the full token set. Post-training, the decoder is discarded; the encoder alone extracts features from complete (unmasked) images for downstream tasks.\n\n\n\n\n\n3.3.2 Contrastive Learning\nContrastive learning is a learning paradigm whereby a model learns to represent data points such that similar points are nearby and disimilar points are far apart. The SimCLR (Chen et al. (2020)) paper demonstrated how such a model can be learned and achieve high-accuracy on down-stream tasks. Figure 9 illustrateds the concepts: From a given image create two views (augmented versions) and contrast them with different images in a large batch of images (InfoNCE loss).\n\n\n\n\n\n\nNoteInfoNCE Loss\n\n\n\n\n\nCore Idea: InfoNCE maximizes mutual information between positive pairs while contrasting against negative samples.\nMathematical Form: \\[\n\\mathcal{L}_{\\text{InfoNCE}} = -\\mathbb{E}\\left[\\log \\frac{\\exp(\\text{sim}(z_i, z_i^+)/\\tau)}{\\exp(\\text{sim}(z_i, z_i^+)/\\tau) + \\sum_{j=1}^{N-1} \\exp(\\text{sim}(z_i, z_j^-)/\\tau)}\\right]\n\\]\nComponents:\n\n\\(z_i\\): Anchor embedding (encoded from image \\(x_i\\))\n\\(z_i^+\\): Positive embedding (different augmentation of same image)\n\\(z_j^-\\): Negative embeddings (from different images in batch)\n\\(\\text{sim}(\\cdot, \\cdot)\\): Similarity function (typically cosine similarity)\n\\(\\tau\\): Temperature parameter controlling concentration\n\nTemperature Effect:\n\nSmall \\(\\tau\\) (\\(\\to 0\\)): Focuses on hardest negatives, sharper distributions\nLarge \\(\\tau\\) (\\(\\to \\infty\\)): Uniform weighting, softer distributions\nTypical values: \\(\\tau \\in [0.07, 0.5]\\)\n\nIntuition: The loss encourages the model to:\n\nPull positive pairs close together in embedding space\nPush negative pairs apart\nForm tight clusters of semantically similar samples\n\nPractical Considerations:\n\nRequires large batch sizes (e.g., 256-4096) for sufficient negatives\nAlternative: Memory banks (MoCo) or queue-based negatives\nHard negative mining can improve learning efficiency\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: Left: Learning representations by contrasting similar inputs (different views of same sample) against other inputs. Right: Architectural view showing the encoder and projection head pipeline.\n\n\n\n\n\n3.3.3 Self-Distillation\nSelf-distillation is an interesting concept: A student model tries to match the outputs of a teacher model’s distribution over a given image. This principle was introduced in the BYOL paper (see Grill et al. (2020)) and led to the DINO-series of models: DINO (Caron et al. (2021)), DINOv2 (Oquab et al. (2024)), and DINOv3 (Siméoni et al. (2025)) are non-contrastive: a student matches the teacher’s distribution over crops (multi-crop), using centering and sharpening. This yields strong global descriptors and high-quality local (patch) tokens.\n\n\n\n\n\n\nNoteDINOv3 Loss\n\n\n\n\n\nComposite objective. DINOv3 trains a student ViT against an EMA teacher with multi-crop views, combining a global DINO loss, a local iBOT loss, a KoLeo uniformity regularizer, and (in a refinement stage) a Gram anchoring loss on patch similarities:\n\\[\\begin{align}\n\\mathcal{L}\n&= \\underbrace{\\mathcal{L}_{\\text{DINO}}}_{\\text{global crops}}\n+ \\lambda_{\\text{iBOT}} \\underbrace{\\mathcal{L}_{\\text{iBOT}}}_{\\text{masked local tokens}}\n+ \\lambda_{\\text{KoLeo}} \\underbrace{\\mathcal{L}_{\\text{KoLeo}}}_{\\text{uniformity on }[\\text{CLS}]}\n+ \\lambda_{\\text{Gram}} \\underbrace{\\mathcal{L}_{\\text{Gram}}}_{\\text{refinement}}.\n\\end{align}\\]\nGlobal DINO (distribution matching, no negatives). Student probabilities \\(p_s=\\mathrm{softmax}(g_s(z_s)/\\tau_s)\\); teacher targets \\(q_t=\\mathrm{stopgrad}\\!\\left[\\mathrm{softmax}\\!\\left((g_t(z_t)-c)/\\tau_t\\right)\\right]\\) with centering \\(c\\) and temperatures \\(\\tau_s,\\tau_t\\). \\[\\begin{equation}\n\\mathcal{L}_{\\text{DINO}} \\;=\\; - \\sum_{k} q_t^{(k)} \\log p_s^{(k)}\n\\quad \\text{(summed over pairs of student/teacher crops).}\n\\end{equation}\\]\nLocal iBOT (masked token prediction). For masked student patches \\(m \\in \\mathcal{M}\\), match teacher patch-level distributions: \\[\\begin{equation}\n\\mathcal{L}_{\\text{iBOT}} \\;=\\; - \\frac{1}{|\\mathcal{M}|}\\sum_{m \\in \\mathcal{M}}\\sum_{k} q_{t,m}^{(k)} \\log p_{s,m}^{(k)} .\n\\end{equation}\\]\nKoLeo (feature uniformity). Encourages spread (near-uniform) class-token embeddings; implemented as the KoLeo entropy estimator on mini-batches (schematic form): \\[\\begin{equation}\n\\mathcal{L}_{\\text{KoLeo}} \\;\\approx\\; -\\frac{1}{N}\\sum_{i=1}^{N} \\log d_i\n\\quad \\text{with } d_i \\text{ the NN distance of } z^{[\\text{CLS}]}_i .\n\\end{equation}\\]\nGram anchoring (refinement step). Align student patch-similarity structure to a “Gram teacher.” Let \\(\\tilde{Z}_s,\\tilde{Z}_g \\in \\mathbb{R}^{P \\times D}\\) be \\(\\ell_2\\)-normalized patch features (rows) for student and Gram-teacher on a global crop. With Gram matrices \\(G(\\tilde{Z})=\\tilde{Z}\\tilde{Z}^\\top\\): \\[\\begin{equation}\n\\mathcal{L}_{\\text{Gram}} \\;=\\; \\big\\| G(\\tilde{Z}_s) - G(\\tilde{Z}_g) \\big\\|_F^2 .\n\\end{equation}\\]\nNotes.\n\nTeacher is an EMA of the student; teachers view only global crops, students see global + local crops; iBOT masks a subset of student patches.\nGram anchoring is applied in a late refinement phase to restore patch-level consistency without hurting global performance.\n\n\n\n\n\n\n\n\n\n\nFigure 10: The student network processes various crops (global and local views) of the same image, attempting to match the output distribution of the momentum-updated teacher network. The teacher processes only global views and uses centering and sharpening to avoid collapse. No explicit negatives are required; the model learns by aligning predictions across different augmented views.\n\n\n\n\n\n\n\n\n\nNoteDINOv3 Overview Video\n\n\n\n\n\nWatch Meta’s introduction to DINOv3, explaining the key improvements over DINOv2 and the self-distillation approach:\n\n\n\n\n\n\n\n3.4 Global vs Local Representations\nGlobal representations summarize an image into a single embedding (e.g., a [CLS] token or pooled feature) that captures overall semantics.\nCLIP is a prototypical global model: it aligns an image-wide vector with a text embedding, which is ideal for zero-shot classification, retrieval, and broad semantic similarity at scale.\nIn contrast, the DINO family (incl. DINO/DINOv2/DINOv3) produces both strong global descriptors and rich local tokens via self-distillation and multi-crop training. These local features carry spatially resolved cues useful for tasks such as segmentation or obejct detection. The global token remains competitive for classification and retrieval.\n\n\n\n\n\n\nFigure 11: Shown are the cosine similarities of feature embeddings at different locations across the input image (middle) for different query points (shown as red crosses). The visualization highlights semantic consistency across the image.",
    "crumbs": [
      "Lectures",
      "4 - Representation Learning"
    ]
  },
  {
    "objectID": "pages/lectures/representation_learning.html#comparing-representations",
    "href": "pages/lectures/representation_learning.html#comparing-representations",
    "title": "4 - Representation Learning",
    "section": "4 Comparing Representations",
    "text": "4 Comparing Representations\nThe following illustrates different representations for the images in Figure 12.\n\n\n\n\n\n\nFigure 12: Example images to compare different representations.\n\n\n\nIn Figure 14, Figure 15, and Figure 16 we show cosine similarities between normalized representations and compare to pixel-space similarity (Figure 13).\nSimilarity Metrics:\nFor images represented as vectors \\(\\mathbf{z}_i, \\mathbf{z}_j \\in \\mathbb{R}^d\\):\n\nEuclidean Distance (Pixel Space): \\[d_{\\text{L2}}(\\mathbf{z}_i, \\mathbf{z}_j) = \\|\\mathbf{z}_i - \\mathbf{z}_j\\|_2 = \\sqrt{\\sum_{k=1}^d (z_i^{(k)} - z_j^{(k)})^2}\\]\nCosine Similarity: \\[\\text{sim}_{\\cos}(\\mathbf{z}_i, \\mathbf{z}_j) = \\frac{\\mathbf{z}_i^\\top \\mathbf{z}_j}{\\|\\mathbf{z}_i\\|_2 \\|\\mathbf{z}_j\\|_2} = \\frac{\\sum_{k=1}^d z_i^{(k)} z_j^{(k)}}{\\sqrt{\\sum_{k=1}^d (z_i^{(k)})^2} \\sqrt{\\sum_{k=1}^d (z_j^{(k)})^2}}\\]\n\nwhere \\(\\text{sim}_{\\cos} \\in [-1, 1]\\) with 1 indicating perfect alignment, 0 indicating orthogonality, and -1 indicating opposite directions.\n\nPixel SpaceResNet-50CLIPDINOv2\n\n\n\n\n\n\n\n\nFigure 13: Raw pixel-level similarity fails to capture semantic relationships.\n\n\n\n\n\n\n\n\n\n\n\nFigure 14: Supervised CNN representations group semantically similar images.\n\n\n\n\n\n\n\n\n\n\n\nFigure 15: Language-aligned embeddings from CLIP show text-driven semantic structure.\n\n\n\n\n\n\n\n\n\n\n\nFigure 16: Self-distilled DINOv2 features reveal fine-grained semantic clusters.\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion: Which Representations Do You Favour? And Why?\n\n\n\n\n\nLooking at the similarity matrices, which representation do you prefer and why?\n\n\nClick for Discussion\n\nPixel Space (Figure 13):\n\nNot suitable for any semantic task\nCaptures low-level appearance variations (lighting, pose, exact pixel alignment)\nCannot distinguish semantic categories from superficial similarities\n\nResNet-50 Supervised (Figure 14):\n\nMuch better than pixel space but still limited\nShows semantic grouping (elephants cluster)\nTrained on ImageNet classes, may not transfer optimally to fine-grained tasks\nSome sensitivity to lightning (elephant night vs day)\n\nCLIP Multimodal (Figure 15):\n\nGood semantic structure aligned with natural language concepts\nClear separation between major categories (animals vs. vehicles)\nLimitation: Lower differentiation within categories\nBest for: Broad semantic retrieval and zero-shot open-vocabulary tasks\nTrade-off: Language alignment may sacrifice fine-grained visual distinctions\n\nDINOv2 Self-Supervised (Figure 16):\n\nSharp discrimination between semantic classes (Elephant vs Giraffe)\nStrong within-category structure (elephant day variants)\nClear night/day distinctions while preserving identity\nBest for: Few-shot learning, fine-grained classification, dense prediction tasks\nPure vision objective without language bias preserves visual details\n\nRecommendation hierarchy:\n\nDINOv2/v3: Default choice for vision-centric tasks requiring discriminative power\nCLIP: When zero-shot capabilities or text-image alignment is critical\nSupervised CNN: When computation is limited and broad categories suffice\nPixel space: Never for semantic tasks (only for exact duplicate detection)\n\n\n\n\n\n\n\n\n\n\n\nWarningTask-Relevant Invariance\n\n\n\nCritical consideration: Invariance to task-relevant attributes can render representations useless.\nExamples of harmful invariance:\n\nColor-invariant features for flower classification: If representations discard color information, distinguishing between red and yellow tulips becomes impossible.\nRotation-invariant features for orientation tasks: Medical imaging often requires preserving anatomical orientation; rotation invariance destroys critical diagnostic information.\nScale-invariant features for size estimation: Object measurement tasks require preserving absolute or relative size cues.\nTexture-invariant features for material classification: Distinguishing wood from metal requires texture sensitivity.\n\nDesign principle: Match your representation’s invariances to your task requirements. When in doubt, preserve information and let the downstream model learn to ignore irrelevant variations.",
    "crumbs": [
      "Lectures",
      "4 - Representation Learning"
    ]
  },
  {
    "objectID": "pages/lectures/representation_learning.html#using-representations-a-practical-guide",
    "href": "pages/lectures/representation_learning.html#using-representations-a-practical-guide",
    "title": "4 - Representation Learning",
    "section": "5 Using Representations: A Practical Guide",
    "text": "5 Using Representations: A Practical Guide\nAlways begin with the simplest approach and only add complexity when performance gaps justify it. For classification/retrieval, global embeddings (e.g., CLIP) enable zero-/few-shot learning. For dense tasks (segmentation, object detection), local (patch) tokens from ViTs (e.g., DINOv3) feed pixel-aligned heads.\nTypical progression:\n\nk-NN baseline (no training) Think of this as “find the most similar examples.” We store the embeddings of labeled images and, for a new image, look up the k most similar ones; the majority label wins. It’s fast to set up, needs no training, and gives a quick reality check on representation quality. See Figure 17 for illustrative code.\nLinear probe (freeze encoder, train head) Keep the powerful pretrained encoder fixed and train a tiny linear layer on top. If this small layer reaches good accuracy, it means the embeddings already separate classes well. It’s cheap, stable, and a great first serious baseline. See Figure 18 for illustrative code.\nLoRA/Adapters (efficient parameter tuning) Instead of updating all model weights, add small “adapter” modules (or low-rank updates) and train only those. You keep most of the original knowledge while adapting to your task with far fewer trainable parameters—faster, lighter, and less prone to overfitting than full fine-tuning. See Figure 19 for illustrative code.\nPartial fine-tune (unfreeze top layers) Unfreeze just the last few layers of the encoder and train them (plus the head). This lets the model adjust higher-level features to your data while keeping the earlier, general features intact. It’s a good middle ground between speed and task fit. See Figure 20 for illustrative code.\nFull fine-tune (last resort) Train all the model’s weights on your dataset. This can yield the best alignment to your task but is computationally expensive and risks overfitting—especially with limited data. Use it when you have enough data/compute and simpler options plateau. See Figure 21 for illustrative code.\n\n\n\n\nimport torch, sklearn.neighbors as sk\nfrom transformers import AutoModel, AutoImageProcessor\n\nmodel_id = \"google/vit-base-patch16-224\"\nenc = AutoModel.from_pretrained(model_id).eval()\nproc = AutoImageProcessor.from_pretrained(model_id)\n\ndef feats(imgs):  # imgs: list of PIL images\n    px = proc(imgs, return_tensors=\"pt\")[\"pixel_values\"]\n    with torch.no_grad():\n        z = enc(pixel_values=px).last_hidden_state[:,0]  # [CLS]\n    return z.cpu().numpy()\n\nXtr = feats(train_images); Ytr = train_labels\nXte = feats(test_images)\nknn = sk.KNeighborsClassifier(n_neighbors=5).fit(Xtr, Ytr)\npred = knn.predict(Xte)\n\n\nFigure 17\n\n\n\n\n\n\nimport torch\nfrom torch import nn, optim\nimport timm\n\nbackbone = timm.create_model(\"vit_base_patch16_224\", pretrained=True)\nfor p in backbone.parameters():\n    p.requires_grad = False\nembed_dim = backbone.num_features\nhead = nn.Linear(embed_dim, num_classes)\n\ndef forward(images):\n    with torch.no_grad():\n        z = backbone.forward_features(images)[:,0]  # [CLS]\n    return head(z)\n\nopt = optim.AdamW(head.parameters(), lr=5e-4)\n# training loop: logits = forward(images); loss = CE(logits, y); loss.backward(); opt.step()\n\n\nFigure 18\n\n\n\n\n\n\nimport torch\nfrom transformers import AutoModel\nfrom peft import LoraConfig, get_peft_model\n\nmodel_id = \"google/vit-base-patch16-224\"\nmodel = AutoModel.from_pretrained(model_id)\n\nconfig = LoraConfig(\n    r=16, lora_alpha=32, lora_dropout=0.05,\n    target_modules=[\"query\",\"key\",\"value\",\"fc\"],  # adjust to module names\n    bias=\"none\", task_type=\"FEATURE_EXTRACTION\"\n)\nmodel = get_peft_model(model, config)\n\nhead = torch.nn.Linear(model.config.hidden_size, num_classes)\n\ndef forward(pixel_values):\n    z = model(pixel_values=pixel_values).last_hidden_state[:,0]\n    return head(z)\n# Optimizer over LoRA params + head params only\n\n\nFigure 19\n\n\n\n\n\n\nimport timm, torch\nfrom torch import nn\n\nmodel = timm.create_model(\"vit_base_patch16_224\", pretrained=True)\n# Freeze everything\nfor p in model.parameters():\n    p.requires_grad = False\n# Unfreeze top transformer block(s)\nfor p in model.blocks[-2:].parameters():\n    p.requires_grad = True  # last 2 blocks\n# Replace / add classification head\nmodel.head = nn.Linear(model.num_features, num_classes)\n# Optimizer over unfrozen blocks + head\ntrainable = [p for p in model.parameters() if p.requires_grad]\nopt = torch.optim.AdamW(trainable, lr=1e-4)\n\n\nFigure 20\n\n\n\n\n\n\nimport torch\nfrom transformers import AutoModelForImageClassification\n\nmodel_id = \"google/vit-base-patch16-224\"\nmodel = AutoModelForImageClassification.from_pretrained(\n    model_id, num_labels=num_classes, ignore_mismatched_sizes=True\n)\nfor p in model.parameters():\n    p.requires_grad = True  # unfreeze all\nopt = torch.optim.AdamW(model.parameters(), lr=5e-5)\n# Standard supervised loop with processor(...)-&gt; pixel_values -&gt; model(pixel_values, labels=y)\n\n\nFigure 21",
    "crumbs": [
      "Lectures",
      "4 - Representation Learning"
    ]
  },
  {
    "objectID": "pages/lectures/representation_learning.html#quiz",
    "href": "pages/lectures/representation_learning.html#quiz",
    "title": "4 - Representation Learning",
    "section": "6 Quiz",
    "text": "6 Quiz\n\n\n\n\n\n\nNoteQuestion 1: Understanding Representation Learning\n\n\n\nQuestion: Why do representations learned from raw pixels (pixel-space similarity) fail to capture semantic relationships between images?\n\n\nClick for Answer\n\nAnswer: Pixel-space representations measure low-level appearance differences (exact color values, lighting conditions, spatial alignment) rather than high-level semantic content. Two images of the same object under different lighting or poses will have large pixel-level distances despite being semantically identical. For example, in Figure 13, pixel distances cannot distinguish between “different elephants in similar lighting” vs “same elephant in different lighting”: both create similar pixel-level differences. Good representations compress the enormous space of possible images (\\(255^{H \\times W \\times C}\\) combinations) into a semantic coordinate system where distances reflect meaning.\n\n\n\n\n\n\n\n\n\nNoteQuestion 2: Invariance and Selectivity Trade-off\n\n\n\nQuestion: Explain the invariance-selectivity trade-off in representation learning. Provide one concrete example where too much invariance would harm performance on a specific task.\n\n\nClick for Answer\n\nAnswer:\nThe Trade-off: Good representations must balance two competing goals:\n\nInvariance: \\(\\|f(\\mathbf{x}) - f(g(\\mathbf{x}))\\|_2 &lt; \\epsilon\\) for nuisance transforms \\(g\\) (lighting, pose changes)\nSelectivity: \\(\\|f(\\mathbf{x}_i) - f(\\mathbf{x}_j)\\|_2 &gt; \\delta\\) for semantically different inputs\n\nConcrete Example - Flower Classification: If representations are completely color-invariant (treating all colors as identical), distinguishing between red roses and yellow roses becomes impossible, even though color is the primary discriminative feature. The model would collapse all flower images to similar embeddings regardless of color, destroying task-critical information.\nOther Examples:\n\nMedical imaging: Rotation invariance destroys anatomical orientation information needed for diagnosis\nObject size estimation: Scale invariance removes absolute/relative size cues\nMaterial classification: Texture invariance prevents distinguishing wood from metal\n\nDesign Principle: Match invariances to task requirements — preserve information the downstream task needs, discard only true nuisance factors.\n\n\n\n\n\n\n\n\n\nNoteQuestion 3: Comparing SSL Objectives\n\n\n\nQuestion: What is the key advantage of self-distillation methods (DINO/DINOv2/DINOv3) over contrastive learning methods (SimCLR)?\n\n\nClick for Answer\n\nAnswer:\nKey Advantage: Self-distillation does not require explicit negative samples or large batch sizes.\nContrastive Learning (SimCLR):\n\nRequires large batches (256-4096) to provide sufficient negative pairs\nInfoNCE loss: \\(\\mathcal{L} = -\\log \\frac{\\exp(\\text{sim}(z_i, z_i^+)/\\tau)}{\\exp(\\text{sim}(z_i, z_i^+)/\\tau) + \\sum_{j=1}^{N-1} \\exp(\\text{sim}(z_i, z_j^-)/\\tau)}\\)\nComputational cost and memory scale with batch size\nNeeds careful negative mining strategies\n\nSelf-Distillation (DINO):\n\nStudent network matches teacher’s probability distribution over crops\nTeacher is EMA of student weights (no separate training)\nEmergent clustering arises from teacher-student dynamics and centering/sharpening\nWorks with smaller batches, more memory-efficient\nBonus: Produces both strong global descriptors AND high-quality local (patch) tokens\n\nResult: DINO-family models achieve state-of-the-art transfer performance with simpler training dynamics and lower computational requirements.\n\n\n\n\n\n\n\n\n\nNoteQuestion 4: Global vs Local Representations\n\n\n\nQuestion: When should you use global representations versus local representations? Give one specific task example for each.\n\n\nClick for Answer\n\nAnswer:\nGlobal Representations (single embedding per image, e.g., [CLS] token):\n\nUse when: Task requires image-level understanding without spatial localization\nExamples:\n\nImage classification: “Is this a cat or dog?” — only need overall category\nImage retrieval: “Find similar vacation photos” — semantic similarity at image level\nZero-shot classification (CLIP): “Does this image contain a beach?” — image-text alignment\n\n\nLocal Representations (spatially-resolved patch tokens, e.g., DINOv3 patch features):\n\nUse when: Task requires pixel-level or region-level understanding\nExamples:\n\nSemantic segmentation: “Label every pixel as road/car/pedestrian” — need spatial structure\nObject detection: “Find all bounding boxes containing cars” — need localization\nDense prediction: “Generate depth map for each pixel” — requires preserved spatial information\n\n\nKey Difference: Global embeddings through pooling (e.g., Global Average Pooling: \\(\\text{GAP}(X) = \\frac{1}{H \\times W}\\sum_{i,j} X_{i,j}\\)) discard spatial structure for efficiency, while local features maintain the spatial grid of patch embeddings.\nPractical Tip: Many modern architectures (DINOv3, ViTs) provide both — use the [CLS] token for global tasks and patch tokens for dense tasks.\n\n\n\n\n\n\n\n\n\nNoteQuestion 5: Adaptation Strategy Selection\n\n\n\nQuestion: You have a dataset of 1,000 labeled medical images for a specialized classification task. Describe the recommended progression of adaptation strategies and explain why you should start with the simplest approach.\n\n\nClick for Answer\n\nAnswer:\nRecommended Progression (from simplest to most complex):\n\nk-NN Baseline (0 training):\n\nExtract features with frozen pretrained model\nClassify by finding k nearest neighbors in feature space\nWhy first: Instant results, no hyperparameters, reveals representation quality\nCode: Just sklearn.KNeighborsClassifier(n_neighbors=5).fit(features, labels)\n\nLinear Probe (~1-10K parameters):\n\nFreeze encoder, train only linear classification head\nWhy next: If this works well, embeddings already separate classes\nBenefit: Fast, stable, minimal overfitting risk with 1K samples\n\nLoRA/Adapters (~100K-1M parameters):\n\nAdd small adapter modules, train only those\nWhy: Allows task-specific adaptation without full fine-tuning\nTrade-off: Better fit than linear probe, much cheaper than full fine-tuning\n\nPartial Fine-tune (last 2-3 layers):\n\nUnfreeze top transformer blocks + head\nWhy: Adjust high-level features while keeping general low-level features\nRisk: Need careful learning rate tuning to avoid catastrophic forgetting\n\nFull Fine-tune (all parameters) — Last Resort:\n\nOnly if: Previous methods plateau AND you have sufficient data\nRisk: With only 1K samples, high overfitting risk\nMitigation: Strong regularization, small learning rates (1e-5), early stopping\n\n\nWhy Start Simple:\n\nDiagnostic value: If k-NN works, you know the representation is good\nResource efficiency: Avoid wasting compute on complex methods when simple ones suffice\nOverfitting prevention: With limited data (1K samples), simpler models generalize better\nBaseline establishment: Each step provides a performance ceiling to beat\n\nMedical Imaging Specific: Consider domain shift — medical images may differ significantly from natural images (ImageNet). If k-NN/linear probe fail, the pretrained features may not transfer well, suggesting need for more adaptation or domain-specific pretraining.\n\n\n\n\n\nCaron, Mathilde, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. 2021. “Emerging Properties in Self-Supervised Vision Transformers.” https://doi.org/10.48550/arXiv.2104.14294.\n\n\nChen, Ting, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. “A Simple Framework for Contrastive Learning of Visual Representations.” arXiv. https://doi.org/10.48550/arXiv.2002.05709.\n\n\nGrill, Jean-Bastien, Florian Strub, Florent Altché, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, et al. 2020. “Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning.” arXiv. http://arxiv.org/abs/2006.07733.\n\n\nHe, Kaiming, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. 2021. “Masked Autoencoders Are Scalable Vision Learners.” arXiv. https://doi.org/10.48550/arXiv.2111.06377.\n\n\nOquab, Maxime, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, et al. 2024. “DINOv2: Learning Robust Visual Features Without Supervision.” arXiv. https://doi.org/10.48550/arXiv.2304.07193.\n\n\nRadford, Alec, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, et al. 2021. “Learning Transferable Visual Models From Natural Language Supervision.” arXiv:2103.00020 [Cs], February. http://arxiv.org/abs/2103.00020.\n\n\nSiméoni, Oriane, Huy V. Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, et al. 2025. “DINOv3.” arXiv. https://doi.org/10.48550/arXiv.2508.10104.",
    "crumbs": [
      "Lectures",
      "4 - Representation Learning"
    ]
  },
  {
    "objectID": "pages/background/machine_learning.html",
    "href": "pages/background/machine_learning.html",
    "title": "1 - Machine Learning Basics",
    "section": "",
    "text": "TipLearning Objectives\n\n\n\n\n\nAfter reviewing this material you should be able to:\n\nDescribe the data-driven approach to machine learning and its core workflow (collect, train, evaluate).\nDefine what a model is mathematically and distinguish between parameters and hyperparameters.\nExplain the role of optimization in fitting models and identify common cost/loss functions.\nJustify why train/validation/test splits are necessary for unbiased model evaluation.\nInterpret the machine learning pipeline from data acquisition through deployment.\nApply model selection principles to choose between competing models.",
    "crumbs": [
      "Background & Preparation",
      "1 - Machine Learning Basics"
    ]
  },
  {
    "objectID": "pages/background/machine_learning.html#the-data-driven-approach",
    "href": "pages/background/machine_learning.html#the-data-driven-approach",
    "title": "1 - Machine Learning Basics",
    "section": "1 The Data-Driven Approach",
    "text": "1 The Data-Driven Approach\nWe follow a data-driven approach in machine learning to solve various tasks. Typically, the process involves:\n\nCollecting a dataset of observations (e.g. images) and their labels.\nUsing a machine learning algorithm to train a model that learns to associate observations with labels.\nEvaluating/applying the model on new data.\n\n\ndef train(observations, labels):\n    \"\"\"Train a Model\"\"\"\n    # Fit Model here\n    return model\n\n\ndef predict(test_observations, model):\n    \"\"\"Predict\"\"\"\n    predictions = model(test_observations)\n    return predictions\n\nNow let’s see this data-driven approach applied to a concrete computer vision task: image super-resolution. This example demonstrates how the same train() and predict() framework works for complex image processing problems.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Image super-resolution demonstration: Converting a low-resolution (100×100 pixels) image to high-resolution (400×400 pixels). The middle panel shows why naive upscaling fails—it remains pixelated. Machine learning models learn to add realistic high-frequency details.\n\n\n\n🤔 Think About It\nHow would you train a model for image super-resolution? The task is to upscale low-resolution images to high-resolution with the best possible quality.\n\n\nKey considerations\n\n\nTraining data: Pairs of low-res and high-res images (can be created by downsampling high-res images)\nLoss function: Measure difference between predicted high-res and actual high-res\nArchitecture: CNN that learns to add high-frequency details\nEvaluation: Visual quality metrics (PSNR, SSIM) and perceptual similarity",
    "crumbs": [
      "Background & Preparation",
      "1 - Machine Learning Basics"
    ]
  },
  {
    "objectID": "pages/background/machine_learning.html#machine-learning-process",
    "href": "pages/background/machine_learning.html#machine-learning-process",
    "title": "1 - Machine Learning Basics",
    "section": "2 Machine Learning Process",
    "text": "2 Machine Learning Process\nWhen modeling data, one often follows certain process steps: acquiring data, preparing it, training multiple models, selecting the most suitable model, estimating its future performance, and finally deploying it in production. Figure 2 illustrates this process graphically.\n\n\n\n\n\n\nFigure 2: Machine Learning Pipeline (Source: Raschka and Mirjalili (2020))\n\n\n\nAt the core of a machine learning application is typically a mathematical model, which is fitted to a dataset so that it can then be used for prediction (in supervised learning). We often refer to ‘models’, meaning the mathematical description of the dataset.",
    "crumbs": [
      "Background & Preparation",
      "1 - Machine Learning Basics"
    ]
  },
  {
    "objectID": "pages/background/machine_learning.html#models",
    "href": "pages/background/machine_learning.html#models",
    "title": "1 - Machine Learning Basics",
    "section": "3 Models",
    "text": "3 Models\nA model is typically described as a function of a data point, generating an output \\(\\hat{y}\\):\n\\[\\begin{align*}\nf(\\mathbf{x}^{(i)}) = \\hat{y}^{(i)}\n\\end{align*}\\]\nMost models have parameters or coefficients that describe the model. The entirety of all parameters is denoted by \\(\\theta\\).\n\\[\\begin{align*}\nf_{\\theta}(\\mathbf{x}^{(i)}) \\text{ or } f(\\theta, \\mathbf{x}^{(i)})\n\\end{align*}\\]\nFor simplicity, we often omit \\(\\theta\\): \\(f(\\mathbf{x}^{(i)})\\)\n\n\n\n\n\n\nNoteLinear Regression Example\n\n\n\n\n\nTo make this concrete, consider linear regression, one of the simplest models. It learns a straight line through data:\n\\[f_\\theta(x) = \\theta_0 + \\theta_1 x\\]\nWhere the parameters are:\n\n\\(\\theta_0\\) (intercept): Where the line crosses the y-axis\n\\(\\theta_1\\) (slope): How steep the line is\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom typing import Callable\n\n\ndef create_linear_model_string(lr, feature_names=None):\n    \"\"\"Create string representation of linear model\"\"\"\n    intercept = lr.intercept_.flatten()\n    coefficients = lr.coef_.flatten()\n    params = np.concatenate([intercept, coefficients])\n\n    if not feature_names:\n        feature_names = [f\"x_{i + 1}^{{(i)}}\" for i in range(0, len(coefficients))]\n    param_strings = [\n        f\"{value:.4f} * {name}\"\n        for name, value in zip([\"1\"] + list(feature_names), params)\n    ]\n    formula = \" + \".join(param_strings)\n    return f\"$y^{{(i)}} = {formula} + \\\\epsilon^{{(i)}}$\"\n\n\ndef calculate_grid_1d(X: np.ndarray, model, step_size: float = 0.02):\n    \"\"\"Calculates Model Predictions on a Grid over the Space of X\"\"\"\n    assert X.ndim == 1\n    x_grid = np.arange(X.min(), X.max(), step_size)\n    p = model.predict(x_grid.reshape(-1, 1))\n    return x_grid, p.ravel()\n\n\ndef add_noise(x: np.ndarray, scale: float = 0.5, seed: int = 123):\n    \"\"\"Add random noise to inputs\"\"\"\n    rng = np.random.default_rng(seed)\n    return x + rng.normal(loc=0.0, scale=scale, size=x.shape)\n\n\ndef create_dataset(\n    fun: Callable = lambda x: x,\n    n_samples: int = 20,\n    x_min: int = 0,\n    x_max: int = 10,\n    noise_scale: float = 0.3,\n    seed: int = 0,\n):\n    \"\"\"Sample Data from True Function.\"\"\"\n    x = np.linspace(0, x_max, n_samples).reshape(-1, 1)\n    y = fun(x)\n    # prevent negative values\n    y_noise = np.clip(add_noise(y, scale=noise_scale, seed=seed), 0, np.inf)\n    return x, y, y_noise\n\n\n# Create synthetic dataset (from lecture 2)\nX, y, y_noise = create_dataset(n_samples=40, noise_scale=1.0)\n\n# Fit linear regression model\nlr = LinearRegression()\nlr = lr.fit(X, y_noise)\ny_pred = lr.predict(X)\nx_grid, y_grid = calculate_grid_1d(X.ravel(), lr)\n\n# Create visualization\nfig, ax = plt.subplots(figsize=(8, 5))\nform = create_linear_model_string(lr)\ntitle = \"Linear Model 1-D\\n\" + form\n_ = sns.scatterplot(x=X.ravel(), y=y_noise.ravel(), ax=ax).set(\n    title=title, xlabel=\"x\", ylabel=\"y\"\n)\n_ = sns.lineplot(x=x_grid, y=y_grid, color=\"red\")\n_ = ax.vlines(\n    X.ravel(), y_pred, y_noise, color=\"black\", linestyles=\"dashed\", linewidths=0.5\n)\nplt.show()\n\nprint(f\"θ₀ (intercept): {lr.intercept_[0]:.3f}\")\nprint(f\"θ₁ (slope): {lr.coef_[0][0]:.3f}\")\n\n\n\n\n\nLinear regression fitted to synthetic 1D dataset. The red line shows the learned model, and dashed lines show residuals (errors) for each data point.\n\n\n\n\nθ₀ (intercept): -0.215\nθ₁ (slope): 1.031\n\n\nKey insight: The model \\(f_\\theta(x)\\) transforms any input \\(x\\) into a prediction \\(\\hat{y}\\) using the learned parameters \\(\\theta_0, \\theta_1\\).",
    "crumbs": [
      "Background & Preparation",
      "1 - Machine Learning Basics"
    ]
  },
  {
    "objectID": "pages/background/machine_learning.html#optimization",
    "href": "pages/background/machine_learning.html#optimization",
    "title": "1 - Machine Learning Basics",
    "section": "4 Optimization",
    "text": "4 Optimization\nThe coefficients are fitted to a training dataset through an optimization procedure.\nThe optimization procedure can often be influenced by additional factors, called hyperparameters (\\(\\alpha, \\lambda, \\dots\\)). These cannot be directly optimized.\nThe function/quantity to be optimized is usually called the cost function, i.e., cost function (other terms include objective function, loss function, etc.). We use \\(J(\\cdot)\\) to denote the cost function. Often, the cost function is also referred to as the loss function \\(L(\\cdot)\\). We use \\(l(\\cdot)\\) for the per-sample loss, i.e., the computation of the cost function on a single sample.\nOur goal is to find a model (and its parameters) that minimizes the cost function:\n\\[\\begin{equation*}\n\\mathsf{argmin}_{\\theta, \\lambda} J\\Big(f_{\\theta, \\lambda}(\\mathbf{X}), \\mathbf{y}\\Big)\n\\end{equation*}\\]\n\n\n\n\n\n\nNoteLinear Regression Loss Function\n\n\n\n\n\nFor linear regression, we measure prediction errors using Mean Squared Error (MSE) or the Least Squares approach:\n\\[J(\\theta) = \\frac{1}{n}\\sum_{i=1}^n (y^{(i)} - f_\\theta(x^{(i)}))^2\\]\nOften a factor \\(\\frac{1}{2n}\\) is used to make the derivative computation more elegant:\n\\[J(\\theta) = \\frac{1}{2n}\\sum_{i=1}^n (y^{(i)} - f_\\theta(x^{(i)}))^2\\]\nIntuition:\n\n\\((y^{(i)} - f_\\theta(x^{(i)}))\\) is the residual (error) for data point \\(i\\)\nSquaring penalizes large errors more than small ones\nWe average over all \\(n\\) training points\n\nOptimization goal: Find parameters \\(\\theta^* = [\\theta_0^*, \\theta_1^*]\\) that minimize MSE:\n\\[\\theta^* = \\mathsf{argmin}_{\\theta} J(\\theta)\\]\n\n\nCode\nfrom scipy.stats import norm\n\n\ndef calculate_x_y_coordinates_of_normal(x, y, std):\n    \"\"\"Calculate the x and y coordinates of a vertically drawn normal distr\n    at location x and y\n    \"\"\"\n    width_of_distr = 2.0\n    rv_norm = norm(loc=0, scale=std)\n    y_values_norm = np.arange(0 + 2 * std, 0 - 2 * std, -0.1)\n    x_values_norm = rv_norm.pdf(y_values_norm)\n    x_values_norm = (\n        np.abs(x_values_norm * width_of_distr)\n        - np.min(np.abs(x_values_norm) * width_of_distr)\n        + x\n    )\n    y_values_norm += y\n    return x_values_norm, y_values_norm\n\n\n# Use the same dataset from Models section\nstd = np.std(y_noise - y_pred)\n\nfig, ax = plt.subplots(figsize=(8, 5))\nform = \"$p(y|x) = \\\\mathcal{N}(wx, \\\\sigma^2) = \\\\mathcal{N}(\\\\hat{y}, \\\\sigma^2)$\"\ntitle = \"Lineare Regression 1-D\\n\" + form\n_ = sns.scatterplot(x=X.ravel(), y=y_noise.ravel(), ax=ax).set(\n    title=title, xlabel=\"x\", ylabel=\"y\"\n)\n_ = sns.lineplot(x=x_grid, y=y_grid, color=\"red\")\n\nfor x_loc in np.arange(1, 9, 2.0):\n    y_loc = lr.predict(np.array([x_loc]).reshape(-1, 1))\n    x_values_norm, y_values_norm = calculate_x_y_coordinates_of_normal(\n        x=x_loc, y=y_loc.ravel(), std=std\n    )\n    _ = ax.plot(x_values_norm, y_values_norm, color=\"green\", linewidth=1.5)\n    _ = ax.vlines(\n        x=x_loc,\n        ymin=np.min(y_values_norm),\n        ymax=np.max(y_values_norm),\n        colors=\"green\",\n        ls=\"--\",\n        lw=1.5,\n    )\nplt.show()\n\n# Compute and display the MSE\nfrom sklearn.metrics import mean_squared_error\n\nmse = mean_squared_error(y_noise, y_pred)\nprint(f\"Training MSE: {mse:.3f}\")\nprint(f\"Standard deviation of residuals: {std:.3f}\")\nprint(f\"The optimization found: θ₀*={lr.intercept_[0]:.3f}, θ₁*={lr.coef_[0][0]:.3f}\")\n\n\n\n\n\nProbabilistic view of linear regression. The red line shows the expected value E[y|x], and green curves show the modeled normal distributions p(y|x) at different x locations.\n\n\n\n\nTraining MSE: 0.611\nStandard deviation of residuals: 0.782\nThe optimization found: θ₀*=-0.215, θ₁*=1.031\n\n\nGeometric interpretation: Among all possible lines, we found the one with the smallest average squared distance to the data points.\n\n\n\nUsually, preprocessing of variables precedes the learning of the coefficients. Forms of preprocessing include standardizing, normalizing, feature encoding, dimensionality reduction, and more. This preprocessing also affects the optimization procedure and can be considered hyperparameters.",
    "crumbs": [
      "Background & Preparation",
      "1 - Machine Learning Basics"
    ]
  },
  {
    "objectID": "pages/background/machine_learning.html#model-selection",
    "href": "pages/background/machine_learning.html#model-selection",
    "title": "1 - Machine Learning Basics",
    "section": "5 Model Selection",
    "text": "5 Model Selection\nModel selection is one of the most important and complex components of the machine learning process. This step involves comparing multiple models and selecting the “best” model for the task to be modeled. Which model is the “best” must be defined based on a metric that measures the model’s performance.\nEvaluation Metrics are crucial for quantifying model performance and vary depending on the problem type. For regression tasks, common metrics include:\n\nMean Squared Error (MSE): \\(\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\), which penalizes large errors heavily\nMean Absolute Error (MAE): \\(\\text{MAE} = \\frac{1}{n}\\sum_{i=1}^n |y_i - \\hat{y}_i|\\), which treats all errors equally\n\nFor classification tasks, key metrics include:\n\nAccuracy: How often is the model correct (fraction of correct predictions).\nPrecision: If it predicts a specific class, how often is it correct (true positives / predicted positives).\nRecall: How many of the target class does the model identify correctly (true positives / actual positives).\nF1-Score: Precision can be traded against Recall, to balance both we can report F1 (harmonic mean of precision and recall).\n\nThe choice of metric depends on the specific application—for example, in medical diagnosis, recall might be more important than precision to avoid missing positive cases. Different ML-frameworks will implement these and many other evaluation metrics.\nIf we calculate the value of the metric on the training dataset, our model is usually too optimistic about its general performance. This is because the data points in the training dataset were directly used to optimize the cost function, and the model coefficients are thus optimally adjusted to them. New data points, for which predictions are to be made, could not have been used for optimization. Therefore, a dataset is usually divided into a training set and a test set. The model is trained with the training set and its performance is measured on the test set. When comparing many models, it is advisable to compare them on a separate validation set (see Figure 3) and evaluate only the best model on the test set. This makes the estimate on the test set more accurate.\n\n\n\n\n\n\nFigure 3: Train-Test Split to select and evaluate models.",
    "crumbs": [
      "Background & Preparation",
      "1 - Machine Learning Basics"
    ]
  },
  {
    "objectID": "pages/background/machine_learning.html#references",
    "href": "pages/background/machine_learning.html#references",
    "title": "1 - Machine Learning Basics",
    "section": "6 References",
    "text": "6 References\n\n\nRaschka, Sebastian, and Vahid Mirjalili. 2020. Python Machine Learning: Machine Learning and Deep Learning with Python, Scikit-Learn, and TensorFlow. Second edition, fourth release,[fully revised and updated]. Expert Insight. Birmingham Mumbai: Packt Publishing.",
    "crumbs": [
      "Background & Preparation",
      "1 - Machine Learning Basics"
    ]
  },
  {
    "objectID": "pages/misc/about.html",
    "href": "pages/misc/about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "pages/misc/exercises.html",
    "href": "pages/misc/exercises.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\n1 Exercises\nExercises can be found here: Link",
    "crumbs": [
      "Exercises"
    ]
  },
  {
    "objectID": "pages/misc/literature.html",
    "href": "pages/misc/literature.html",
    "title": "Books",
    "section": "",
    "text": "Torralba, A. and Isola, P. and Freeman, W.T., Foundations of Computer Vision, Torralba, Isola, and Freeman (2024)\n\nExzellentes Buch zu Computer Vision mit Deep Learning\nAusgezeichnete Illustrationen\nOnline verfügbar: Link",
    "crumbs": [
      "Resources",
      "Books"
    ]
  },
  {
    "objectID": "pages/misc/literature.html#computer-vision-mit-deep-learning",
    "href": "pages/misc/literature.html#computer-vision-mit-deep-learning",
    "title": "Books",
    "section": "",
    "text": "Torralba, A. and Isola, P. and Freeman, W.T., Foundations of Computer Vision, Torralba, Isola, and Freeman (2024)\n\nExzellentes Buch zu Computer Vision mit Deep Learning\nAusgezeichnete Illustrationen\nOnline verfügbar: Link",
    "crumbs": [
      "Resources",
      "Books"
    ]
  },
  {
    "objectID": "pages/misc/literature.html#deep-learning",
    "href": "pages/misc/literature.html#deep-learning",
    "title": "Books",
    "section": "2 Deep Learning",
    "text": "2 Deep Learning\nSimon J.D. Prince, Understanding Deep Learning, MIT Press, Prince (2023)\n\nUmfassende Einführung in Deep Learning mit sehr guten Illustrationen\nOnline verfügbar: Link\n\nChollet, François, Deep Learning with Python, Second Edition, Manning Publications, Chollet (2021)\n\nEin Klassiker für eine Einführung in Deep Learning (und Keras)",
    "crumbs": [
      "Resources",
      "Books"
    ]
  },
  {
    "objectID": "pages/misc/literature.html#machine-learning",
    "href": "pages/misc/literature.html#machine-learning",
    "title": "Books",
    "section": "3 Machine Learning",
    "text": "3 Machine Learning\nGéron A, Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition, O’Reilly 2019\n\nJupyter Notebooks sind öffentlich verfügbar: Link\nEinsteigerfreundliche Einführung in Machine Learning mit Scikit-Learn und TensorFlow\n\nRaschka S, Python Machine Learning, 3rd Edition, PACKT 2019\n\nEinsteigerfreundliche Einführung in Machine Learning mit Scikit-Learn und TensorFlow\n\nKevin P. Murphy, Probabilistic Machine Learning: An Introduction, MIT Press 2022\n\nVorabversion gratis verfügbar: Link\nUmfassende Einführung in Machine Learning mit ausführlichen theoretischen Hintergründen",
    "crumbs": [
      "Resources",
      "Books"
    ]
  },
  {
    "objectID": "pages/misc/literature.html#pytorch",
    "href": "pages/misc/literature.html#pytorch",
    "title": "Books",
    "section": "4 PyTorch",
    "text": "4 PyTorch\nStevens, Eli and Antiga, Luca and Viehmann, Thomas, Deep learning with PyTorch, Manning Publications Co, Stevens, Antiga, and Viehmann (2020)\n\nKann als PDF gratis heruntergeladen werden\nEinführung in PyTorch von Grund auf mit Anwendungsbeispielen",
    "crumbs": [
      "Resources",
      "Books"
    ]
  },
  {
    "objectID": "pages/misc/literature.html#honorable-mentions---nicht-mehr-aktuell",
    "href": "pages/misc/literature.html#honorable-mentions---nicht-mehr-aktuell",
    "title": "Books",
    "section": "5 Honorable Mentions - Nicht mehr aktuell",
    "text": "5 Honorable Mentions - Nicht mehr aktuell\nStevens et al, Deep Learning with PyTorch, Manning Publications, Stevens, Antiga, and Viehmann (2020)\nGoodfellow, Ian and Bengio, Yoshua and Courville, Aaron, Deep Learning, MIT Press, Goodfellow, Bengio, and Courville (2016)\n\nSehr gute und umfassende Einführung in Deep Learning\nEtwas älter aber immer noch in weiten Teilen aktuell\nOnline verfügbar: Link\n\nHastie T et al., Elements of Statistical Learning, Springer 2009.\n\nKann als pdf gratis runtergeladen werden: Link\nEnthält Machine Learning Grundlagen und viele Methoden (wenig über Neuronale Netzwerke)\n\nVanderPlas J, Python Data Science Handbook, O’Reilly 2017.\n\nWurde mit Jupyter Notebooks geschrieben.\nDer gesamte Inhalt finden sie auf einer website: Link\nDas Repository kann von github runtergelanden werden: Link",
    "crumbs": [
      "Resources",
      "Books"
    ]
  },
  {
    "objectID": "pages/misc/quiz.html",
    "href": "pages/misc/quiz.html",
    "title": "Quiz",
    "section": "",
    "text": "The following questions could be exam questions."
  },
  {
    "objectID": "pages/misc/quiz.html#convolutions",
    "href": "pages/misc/quiz.html#convolutions",
    "title": "Quiz",
    "section": "1 Convolutions",
    "text": "1 Convolutions\n\nA convolutional layer has 64 input activations (\\(C_{in} = 64\\) and \\(H=16\\), \\(W=16\\)). You want to reduce its spatial dimensionality by half, while doubling the number of channels. How do you parameterize your convolutional layer? Provide an example.\nIn the example above: How many weights do you need to learn?\nYou have very large images (\\(8000 \\times 8000\\) pixels). Your model always crashes with out-of-memory-errors. What options do you have when parameterizing your convolutions?"
  },
  {
    "objectID": "pages/misc/quiz.html#cnns",
    "href": "pages/misc/quiz.html#cnns",
    "title": "Quiz",
    "section": "2 CNNs",
    "text": "2 CNNs\n\nCan CNNs be used to count objects? Take a look at the following figure. You want to count in how many quadrants an objects occurs. Justify your answer.\n\n\n\n\nCan CNNs count objects\n\n\n\nCan CNNs be used to model inputs, e.g. satellite data that are not RGB images, i.e. have more than 3 input channels? Justify your answer.\nYou want to model images which are not square. They have a spatial resolution of 800x400. What is different within a CNN as opposed to if they were square?"
  },
  {
    "objectID": "pages/misc/quiz.html#image-classification",
    "href": "pages/misc/quiz.html#image-classification",
    "title": "Quiz",
    "section": "3 Image Classification",
    "text": "3 Image Classification\n\nYou trained a model to identify synthetic (fake) images. The model is quite good but not perfect. When deploying the model you have the option to send some images for manual verificaiton by an expert. Which do you choose? Justify your answer."
  },
  {
    "objectID": "pages/misc/quiz.html#foundation-models",
    "href": "pages/misc/quiz.html#foundation-models",
    "title": "Quiz",
    "section": "4 Foundation Models",
    "text": "4 Foundation Models\n\nYou applied CLIP on a dataset to identify synthetic / fake images and used the following prompts:\n\n\n“A synthetic image”.\n“A real image”.\n\nWhen you compare to the ground truth labels that you have collected you see that the model does not perform well. What options do you have to improve the model?"
  },
  {
    "objectID": "pages/slides/cnns.html#overview",
    "href": "pages/slides/cnns.html#overview",
    "title": "Convolutional Neural Networks",
    "section": "Overview",
    "text": "Overview\n\nIntroduction & Motivation\nConvolutional Layers\nProperties\nVariants and Layers\nVisualizations and Architectures",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#properties-of-image-data",
    "href": "pages/slides/cnns.html#properties-of-image-data",
    "title": "Convolutional Neural Networks",
    "section": "Properties of Image Data",
    "text": "Properties of Image Data\n\nImage Source\nHigh-Dimensional: An RGB image of size \\(224 \\times 224\\) has \\(224 \\times 224 \\times 3 = 150{,}528\\) values\nLocality: Nearby pixels are statistically related (edges, textures, objects)\nStability under transformations: A cat should be recognized regardless of position\nHierarchical Structure: Features at multiple scales (pixels → edges → textures → objects)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#multilayer-perceptron-and-images",
    "href": "pages/slides/cnns.html#multilayer-perceptron-and-images",
    "title": "Convolutional Neural Networks",
    "section": "Multilayer-Perceptron and Images",
    "text": "Multilayer-Perceptron and Images\n\n\nMLPs deal with flat inputs: The spatial structure gets lost.",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#mlps-and-high-dimensional-inputs",
    "href": "pages/slides/cnns.html#mlps-and-high-dimensional-inputs",
    "title": "Convolutional Neural Networks",
    "section": "MLPs and High-Dimensional Inputs",
    "text": "MLPs and High-Dimensional Inputs\n\n\nWeight matrix \\(\\mathbf{W} \\in \\mathbb{R}^{d \\times k}\\) scales with input size (\\(d\\) = input dimensionality, \\(k\\) = number of neurons)\n\n\nColumns of \\(\\mathbf{W}\\) can be visualized as images to show what patterns each neuron learns",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#mlps-and-translations",
    "href": "pages/slides/cnns.html#mlps-and-translations",
    "title": "Convolutional Neural Networks",
    "section": "MLPs and Translations",
    "text": "MLPs and Translations\n\nMLP and pattern shifts\nOften patterns appear at different positions (not stationary)\n\n\nIdeally we want to recognize them under translations: \\(g(\\mathbf{x})\\) where \\(g()\\) is a spatial translation\n\n\n\nHow do MLPs deal with this?",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#multilayer-perceptrons-and-pattern-learning",
    "href": "pages/slides/cnns.html#multilayer-perceptrons-and-pattern-learning",
    "title": "Convolutional Neural Networks",
    "section": "Multilayer-Perceptrons and Pattern Learning",
    "text": "Multilayer-Perceptrons and Pattern Learning\n\nMLP on example image.\nCan an MLP successfully learn patterns in images \\(\\mathbf{x}\\) that are permuted with a permutation matrix \\(\\mathbf{P}\\), i.e. \\(f(P(\\mathbf{x}))\\)?",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#problems-mlps-and-images",
    "href": "pages/slides/cnns.html#problems-mlps-and-images",
    "title": "Convolutional Neural Networks",
    "section": "Problems: MLPs and Images",
    "text": "Problems: MLPs and Images\n\nHigh-Dimensional: MLP size scales with input dimensionality → memory/compute issues\nNo Locality: MLPs can’t exploit spatial relationships in images\nInefficient Translation Handling: Must learn position-dependent patterns separately\n\n\nInteractive Demo: https://adamharley.com/nn_vis/mlp/2d.html\n\n\nSolution? → Need architecture with inductive biases for images!",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#invariance-and-equivariance",
    "href": "pages/slides/cnns.html#invariance-and-equivariance",
    "title": "Convolutional Neural Networks",
    "section": "Invariance and Equivariance",
    "text": "Invariance and Equivariance\nFor many tasks small variations in the input should either not change the model output (invariance) or should change the output in tandem with the input changes (equivariance).\n\nA function \\(f(\\mathbf{x})\\) is invariant to a transformation \\(g(\\mathbf{x})\\) if:\n\\[\\begin{align}\nf(g(\\mathbf{x})) = f(\\mathbf{x})\n\\end{align}\\]\n\n\nA function \\(f(\\mathbf{x})\\) (such as a layer in a neural network) of an image \\(\\mathbf{x}\\) is equivariant with respect to a transformation \\(g(\\mathbf{\\mathbf{x}})\\) if:\n\\[\\begin{align}\nf(g(\\mathbf{x})) = g(f(\\mathbf{x}))\n\\end{align}\\]",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#example-invariance",
    "href": "pages/slides/cnns.html#example-invariance",
    "title": "Convolutional Neural Networks",
    "section": "Example Invariance",
    "text": "Example Invariance\nExample where invariance is required:\n\nWhen objects in the input translate spatially, the output (in this case the classification of the images) does not change. The model \\(f(\\mathbf{x})\\) is thus invariant to spatial translations.",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#example-equivariance",
    "href": "pages/slides/cnns.html#example-equivariance",
    "title": "Convolutional Neural Networks",
    "section": "Example Equivariance",
    "text": "Example Equivariance\nExample where equivariance is required:\n\nWhen objects in the input (first row) translate spatially, the detections (bounding boxes) change accordingly (bottom row). The model \\(f(\\mathbf{x})\\) that produces the bounding boxes is thus equivariant with respect to spatial translations.",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#history-experiments-on-cats",
    "href": "pages/slides/cnns.html#history-experiments-on-cats",
    "title": "Convolutional Neural Networks",
    "section": "History: Experiments on Cats",
    "text": "History: Experiments on Cats\n\nIllustration Source",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#visual-cortex",
    "href": "pages/slides/cnns.html#visual-cortex",
    "title": "Convolutional Neural Networks",
    "section": "Visual Cortex",
    "text": "Visual Cortex\n\nRepresentation of transformations in the visual cortex. Source: Kubilius (2017)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#convolutional-neural-networks",
    "href": "pages/slides/cnns.html#convolutional-neural-networks",
    "title": "Convolutional Neural Networks",
    "section": "Convolutional Neural Networks",
    "text": "Convolutional Neural Networks\n\nThe activations of a ConvNet architecture. The input image is on the left, and the predictions are on the right. Source: Li (2022)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#convolution-on-rgb-images",
    "href": "pages/slides/cnns.html#convolution-on-rgb-images",
    "title": "Convolutional Neural Networks",
    "section": "Convolution on RGB Images",
    "text": "Convolution on RGB Images\n\nSource: Prince (2023)\n\\[\\begin{equation}\nS(i, j) = (K * I)(i, j) = \\sum_m \\sum_n I(i + m, j + n) K(m, n)\n\\end{equation}\\]\n\n\\(I\\): Input (e.g., image)\n\\(K\\): Kernel (typically smaller than \\(I\\))\nBias term \\(b\\) added to weighted sum",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#convolutional-layers-1",
    "href": "pages/slides/cnns.html#convolutional-layers-1",
    "title": "Convolutional Neural Networks",
    "section": "Convolutional Layers",
    "text": "Convolutional Layers\n\nSource: Johnson (2019)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#convolutional-layers-2",
    "href": "pages/slides/cnns.html#convolutional-layers-2",
    "title": "Convolutional Neural Networks",
    "section": "Convolutional Layers",
    "text": "Convolutional Layers\n\nSource: Johnson (2019)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#convolutional-layers-3",
    "href": "pages/slides/cnns.html#convolutional-layers-3",
    "title": "Convolutional Neural Networks",
    "section": "Convolutional Layers",
    "text": "Convolutional Layers\n\nSource: Johnson (2019)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#convolutional-layers-4",
    "href": "pages/slides/cnns.html#convolutional-layers-4",
    "title": "Convolutional Neural Networks",
    "section": "Convolutional Layers",
    "text": "Convolutional Layers\n\nSource: Johnson (2019)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#convolutional-layers-5",
    "href": "pages/slides/cnns.html#convolutional-layers-5",
    "title": "Convolutional Neural Networks",
    "section": "Convolutional Layers",
    "text": "Convolutional Layers\n\n\nSource: Johnson (2019)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#convolution-hyperparameters",
    "href": "pages/slides/cnns.html#convolution-hyperparameters",
    "title": "Convolutional Neural Networks",
    "section": "Convolution Hyperparameters",
    "text": "Convolution Hyperparameters\nKey parameters to configure a convolutional layer:\n\nPadding (\\(p\\)): Zero-padding added to input borders\nStride (\\(s\\)): Step size when moving the kernel\nKernel Size (\\(k\\)): Spatial extent (typically \\(3\\times3\\) or \\(5\\times5\\))\nDilation (\\(d\\)): Spacing between kernel elements",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#padding",
    "href": "pages/slides/cnns.html#padding",
    "title": "Convolutional Neural Networks",
    "section": "Padding",
    "text": "Padding\n\nQuestion: Output size when convolving with \\(3\\times3\\) kernel?\n\n\n\n\nWithout padding:\n\n\n\n\nWith zero-padding:",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#padding-effect",
    "href": "pages/slides/cnns.html#padding-effect",
    "title": "Convolutional Neural Networks",
    "section": "Padding Effect",
    "text": "Padding Effect\n\nLeft: Input (Yellow) with Zero-Padding (white border), Middle: Filter, Right: Output.\nKey Use: Preserve spatial dimensions → essential for segmentation!",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#stride-1",
    "href": "pages/slides/cnns.html#stride-1",
    "title": "Convolutional Neural Networks",
    "section": "Stride = 1",
    "text": "Stride = 1\n\nConvolution with stride \\((1, 1)\\)\nKernel moves 1 pixel at a time → preserves spatial resolution",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#stride-2",
    "href": "pages/slides/cnns.html#stride-2",
    "title": "Convolutional Neural Networks",
    "section": "Stride = 2",
    "text": "Stride = 2\n\nConvolution with stride \\((2, 2)\\)\nKernel moves 2 pixels at a time → downsamples by factor of 2\n\n\nAdvantage: Learned downsampling (vs. fixed pooling)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#kernel-size",
    "href": "pages/slides/cnns.html#kernel-size",
    "title": "Convolutional Neural Networks",
    "section": "Kernel-Size",
    "text": "Kernel-Size",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#kernel-size-1",
    "href": "pages/slides/cnns.html#kernel-size-1",
    "title": "Convolutional Neural Networks",
    "section": "Kernel-Size",
    "text": "Kernel-Size\n\n\nWhat is the difference between using 3x3 vs 5x5 kernels? What is the same?",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#dilation",
    "href": "pages/slides/cnns.html#dilation",
    "title": "Convolutional Neural Networks",
    "section": "Dilation",
    "text": "Dilation\n\n\nWhy would we use dilation \\(d \\gt 1\\)?",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#calculations-spatial-dimensionality-of-activation-maps",
    "href": "pages/slides/cnns.html#calculations-spatial-dimensionality-of-activation-maps",
    "title": "Convolutional Neural Networks",
    "section": "Calculations: Spatial Dimensionality of Activation Maps",
    "text": "Calculations: Spatial Dimensionality of Activation Maps\nYou can calculate the dimensionality of the activation maps with the following formulas:\n\n\\(i\\): Side length of the input activations (assumption: square inputs)\n\\(k\\): Kernel size (assumption: square kernel)\n\\(o\\): Side length of the output activation maps\n\\(s\\): Stride (assumption: same stride along spatial dimensions)\n\\(p\\): Number of paddings on each side (assumption: same number of paddings along spatial dimensions)\n\\(d\\): Dilation rate.\n\n\n\\[\\begin{equation}\no = \\left\\lfloor \\frac{i + 2p - d\\cdot(k-1) - 1}{s} + 1 \\right\\rfloor\n\\end{equation}\\]",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#calculations-number-of-weights-in-a-convolutional-layer",
    "href": "pages/slides/cnns.html#calculations-number-of-weights-in-a-convolutional-layer",
    "title": "Convolutional Neural Networks",
    "section": "Calculations: Number of Weights in a Convolutional Layer",
    "text": "Calculations: Number of Weights in a Convolutional Layer\nYou can calculate the total number of weights in a convolutional layer with the following formula:\n\n\\(k\\): Kernel size (assumption: square kernel, so kernel has dimensions \\(k \\times k\\))\n\\(C_{in}\\): Number of input channels\n\\(C_{out}\\): Number of output channels\n\n\n\\[\\begin{equation}\n\\text{Total Weights} = (k \\times k) \\times C_{in} \\times C_{out}\n\\end{equation}\\]\n\n\nEach filter has a size of \\(k \\times k \\times C_{in}\\), and there are \\(C_{out}\\) filters in total, resulting in \\((k \\times k \\times C_{in}) \\times C_{out}\\) weights.\n\n\nBiases: If each output channel has a bias term, add an additional \\(C_{out}\\) weights for the biases.\n\\[\\begin{equation}\n\\text{Total Parameters} = (k \\times k \\times C_{in} \\times C_{out}) + C_{out}\n\\end{equation}\\]",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#quiz-output-shape-parameters",
    "href": "pages/slides/cnns.html#quiz-output-shape-parameters",
    "title": "Convolutional Neural Networks",
    "section": "Quiz: Output Shape & Parameters",
    "text": "Quiz: Output Shape & Parameters\nScenario:\n\nInput: \\(3 \\times 32 \\times 32\\) (C, H, W)\nConvolution: 10 filters, kernel \\(5 \\times 5\\), stride=1, pad=2\n\n\nQuestions:\n\nWhat is the size of the output activation map?\nHow many parameters does this operation have?",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#quiz-output-shape-parameters-1",
    "href": "pages/slides/cnns.html#quiz-output-shape-parameters-1",
    "title": "Convolutional Neural Networks",
    "section": "Quiz: Output Shape & Parameters",
    "text": "Quiz: Output Shape & Parameters\nFormulas:\n\n\\[\\begin{equation}\no = \\left\\lfloor \\frac{i + 2p - d\\cdot(k-1) - 1}{s} + 1 \\right\\rfloor\n\\end{equation}\\]\n\n\n\\[\\begin{equation}\n\\text{Total Parameters} = (k \\times k \\times C_{in} \\times C_{out}) + C_{out}\n\\end{equation}\\]",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#quiz-solutions",
    "href": "pages/slides/cnns.html#quiz-solutions",
    "title": "Convolutional Neural Networks",
    "section": "Quiz: Solutions",
    "text": "Quiz: Solutions\n\n1. Output size:\n\\(o = \\lfloor \\frac{32 + 2(2) - 1(5-1) - 1}{1} + 1 \\rfloor = \\lfloor \\frac{32 + 4 - 4 - 1}{1} + 1 \\rfloor = 32\\)\n\n\nOutput shape: \\(10 \\times 32 \\times 32\\)\n\n\n2. Parameters:\n\\((5 \\times 5 \\times 3 \\times 10) + 10 = 750 + 10 = 760\\) parameters\n\n\nKey Insight: Padding=2 preserves spatial dimensions with kernel=5!",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#cnn-key-properties",
    "href": "pages/slides/cnns.html#cnn-key-properties",
    "title": "Convolutional Neural Networks",
    "section": "CNN Key Properties",
    "text": "CNN Key Properties\n\n1. Local (Sparse) Connectivity\nNeurons only connect to small spatial regions\n\n\n2. Parameter Sharing\nSame weights applied across entire input (translation equivariance!)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#think-about-it",
    "href": "pages/slides/cnns.html#think-about-it",
    "title": "Convolutional Neural Networks",
    "section": "Think About It",
    "text": "Think About It\n\nCan we recognize cats with a one-layer CNN?\nCan we recognize cats anywhere in an image with a CNN?\nIs parameter sharing always useful?\nWhat about images with position-dependent features?",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#translation-invariance-equivariance",
    "href": "pages/slides/cnns.html#translation-invariance-equivariance",
    "title": "Convolutional Neural Networks",
    "section": "Translation Invariance / Equivariance",
    "text": "Translation Invariance / Equivariance\nGiven a translation \\(g()\\), which spatially shifts inputs:\n\nTranslation invariance: \\(f(g(x))=f(x)\\)\nTranslation equivariance: \\(f(g(x))=g(f(x))\\)\n\n\nAre convolutions translation invariant, equivariant, or both?",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#stacking-convolutions",
    "href": "pages/slides/cnns.html#stacking-convolutions",
    "title": "Convolutional Neural Networks",
    "section": "Stacking Convolutions",
    "text": "Stacking Convolutions\n\n\n\nSource: Johnson (2019)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#receptive-field",
    "href": "pages/slides/cnns.html#receptive-field",
    "title": "Convolutional Neural Networks",
    "section": "Receptive Field",
    "text": "Receptive Field\n\nUsing a 5x5 kernel",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#receptive-field-1",
    "href": "pages/slides/cnns.html#receptive-field-1",
    "title": "Convolutional Neural Networks",
    "section": "Receptive Field",
    "text": "Receptive Field\n\nUsing a 5x5 kernel and two layers",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#receptive-field-2",
    "href": "pages/slides/cnns.html#receptive-field-2",
    "title": "Convolutional Neural Networks",
    "section": "Receptive Field",
    "text": "Receptive Field",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#receptive-field-3",
    "href": "pages/slides/cnns.html#receptive-field-3",
    "title": "Convolutional Neural Networks",
    "section": "Receptive Field",
    "text": "Receptive Field",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#receptive-field-formula",
    "href": "pages/slides/cnns.html#receptive-field-formula",
    "title": "Convolutional Neural Networks",
    "section": "Receptive Field: Formula",
    "text": "Receptive Field: Formula\nThe receptive field of a CNN \\(r_0\\):\n\n\\[\\begin{equation}\nr_0 = \\sum_{l=1}^L \\left( (k_l - 1) \\prod_{i=1}^{l-1} s_i \\right) + 1\n\\end{equation}\\]\n\n\n\n\\(L\\): Total number of layers\n\\(k_l\\): Kernel size at layer \\(l\\)\n\\(s_i\\): Stride at layer \\(i\\), from layer 1 up to \\(l-1\\)\n\n\n\nSpecial case: If all strides \\(s_i=1\\), receptive field = \\(\\sum (k_l - 1) + 1\\)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#quiz-receptive-field",
    "href": "pages/slides/cnns.html#quiz-receptive-field",
    "title": "Convolutional Neural Networks",
    "section": "Quiz: Receptive Field",
    "text": "Quiz: Receptive Field\nNetwork Architecture:\n\nLayer 1: \\(k_1 = 3\\), \\(s_1 = 1\\)\nLayer 2: \\(k_2 = 3\\), \\(s_2 = 1\\)\nLayer 3: \\(k_3 = 3\\), \\(s_3 = 1\\)\n\n\nQuestion: What is the receptive field \\(r_0\\)?",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#quiz-receptive-field-solution",
    "href": "pages/slides/cnns.html#quiz-receptive-field-solution",
    "title": "Convolutional Neural Networks",
    "section": "Quiz: Receptive Field Solution",
    "text": "Quiz: Receptive Field Solution\n\nUsing formula: \\(r_0 = \\sum_{l=1}^L \\left( (k_l - 1) \\prod_{i=1}^{l-1} s_i \\right) + 1\\)\n\n\nSince all \\(s_i = 1\\): \\[r_0 = (3-1) + (3-1) + (3-1) + 1 = 2 + 2 + 2 + 1 = 7\\]\n\n\nKey Insight: Stacking three \\(3\\times3\\) convs gives \\(7\\times7\\) receptive field!\n\n\nEfficiency: Three \\(3\\times3\\) = \\(3 \\times 9 = 27\\) params vs one \\(7\\times7\\) = 49 params",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#architecture",
    "href": "pages/slides/cnns.html#architecture",
    "title": "Convolutional Neural Networks",
    "section": "Architecture",
    "text": "Architecture\nSequence of layers and their hyper-parameters defines an architecture.\n\n\n\n\nVGG - Source: Prince (2023)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#learned-filters",
    "href": "pages/slides/cnns.html#learned-filters",
    "title": "Convolutional Neural Networks",
    "section": "Learned Filters",
    "text": "Learned Filters\n\nSource: Krizhevsky, Sutskever, and Hinton (2012)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#lets-test-it",
    "href": "pages/slides/cnns.html#lets-test-it",
    "title": "Convolutional Neural Networks",
    "section": "Let’s test it!",
    "text": "Let’s test it!\nLet’s train a CNN.\nHere is an interesting demo: https://poloclub.github.io/cnn-explainer/",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#downsampling",
    "href": "pages/slides/cnns.html#downsampling",
    "title": "Convolutional Neural Networks",
    "section": "Downsampling",
    "text": "Downsampling\n\nSpatial downsampling reduces resolution (H×W → H’×W’)\n\n\nOptions:\n\nStride \\(&gt; 1\\) (learned downsampling)\nPooling layers (fixed operation)\n\n\n\n\n\n\na) Sub-Sampling, b) Max Pooling, c) Average Pooling. Source: Prince (2023)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#upsampling",
    "href": "pages/slides/cnns.html#upsampling",
    "title": "Convolutional Neural Networks",
    "section": "Upsampling",
    "text": "Upsampling\n\nNeed to increase spatial dimensions (H×W → H’×W’)\n\n\nExample use case?\n\n\nImage Segmentation - need pixel-level predictions!",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#upsampling-methods",
    "href": "pages/slides/cnns.html#upsampling-methods",
    "title": "Convolutional Neural Networks",
    "section": "Upsampling Methods",
    "text": "Upsampling Methods\n\na) Duplicate, b) Max-Unpooling, c) Bilinear Interpolation. Source: Prince (2023)\nAlso: Transposed Convolutions (learnable upsampling)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#upsampling-with-transposed-convolutions",
    "href": "pages/slides/cnns.html#upsampling-with-transposed-convolutions",
    "title": "Convolutional Neural Networks",
    "section": "Upsampling with Transposed Convolutions",
    "text": "Upsampling with Transposed Convolutions\n\nMore details in image segmentation!",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#convolutions",
    "href": "pages/slides/cnns.html#convolutions",
    "title": "Convolutional Neural Networks",
    "section": "1×1 Convolutions",
    "text": "1×1 Convolutions\n\nPurpose: Change number of channels without spatial mixing\n\n\n\\((C_{in} \\times H \\times W) \\rightarrow (C_{out} \\times H \\times W)\\)\n\n\nUse cases:\n\nDimensionality reduction (bottleneck layers)\nIncrease model depth without parameters explosion\nCross-channel information mixing\n\n\n\n\nSource: Prince (2023)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#global-average-pooling",
    "href": "pages/slides/cnns.html#global-average-pooling",
    "title": "Convolutional Neural Networks",
    "section": "Global Average Pooling",
    "text": "Global Average Pooling\nGlobal Average Pooling is often an important component. It computes the average of the activations along the depth dimension, reducing activation maps from (C x H x W) to (C x 1 x 1). This is useful for directly modeling logits in a classification problem with C classes, enabling architectures that completely eliminate fully-connected layers.\n\nGlobal Average pooling, input (left) and output (right).",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#other-layers",
    "href": "pages/slides/cnns.html#other-layers",
    "title": "Convolutional Neural Networks",
    "section": "Other Layers",
    "text": "Other Layers\n\nMany other layer types exist:\n\n\nDepthwise separable convolutions (efficiency)\nDilated convolutions (large receptive fields)\nGrouped convolutions (parameter reduction)\nBatch normalization (training stability)\nDropout (regularization)",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#quiz-mlp-vs-cnn-efficiency",
    "href": "pages/slides/cnns.html#quiz-mlp-vs-cnn-efficiency",
    "title": "Convolutional Neural Networks",
    "section": "Quiz: MLP vs CNN Efficiency",
    "text": "Quiz: MLP vs CNN Efficiency\nCIFAR-10 Images: \\(32 \\times 32 \\times 3\\)\n\nMLP: 2 hidden layers (256, 128 neurons)\nHow many parameters in first layer?\n\n\nCNN: 2 conv layers (16 filters, \\(3\\times3\\), then 32 filters, \\(3\\times3\\))\nHow many parameters in first layer?",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/cnns.html#quiz-solutions-1",
    "href": "pages/slides/cnns.html#quiz-solutions-1",
    "title": "Convolutional Neural Networks",
    "section": "Quiz: Solutions",
    "text": "Quiz: Solutions\n\nMLP first layer:\n\\((32 \\times 32 \\times 3) \\times 256 = 3{,}072 \\times 256 = 786{,}432\\) parameters\n\n\nCNN first layer:\n\\((3 \\times 3 \\times 3 \\times 16) + 16 = 432 + 16 = 448\\) parameters\n\n\nCNN uses 1,755× fewer parameters! 🚀\n\n\nWhy? Local connectivity + parameter sharing = massive efficiency!",
    "crumbs": [
      "Slides",
      "Convolutional Neural Networks"
    ]
  },
  {
    "objectID": "pages/slides/image_classification.html#overview",
    "href": "pages/slides/image_classification.html#overview",
    "title": "Image Classification",
    "section": "Overview",
    "text": "Overview\n\nIntroduction\nModeling\nLoss Function\nArchitectures",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "pages/slides/image_classification.html#adversarial-panda",
    "href": "pages/slides/image_classification.html#adversarial-panda",
    "title": "Image Classification",
    "section": "Adversarial Panda",
    "text": "Adversarial Panda\n\nSource: Goodfellow, Shlens, and Szegedy (2015)",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "pages/slides/image_classification.html#adversarial-panda-1",
    "href": "pages/slides/image_classification.html#adversarial-panda-1",
    "title": "Image Classification",
    "section": "Adversarial Panda",
    "text": "Adversarial Panda\n\nSource: Goodfellow, Shlens, and Szegedy (2015)",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "pages/slides/image_classification.html#image-classification",
    "href": "pages/slides/image_classification.html#image-classification",
    "title": "Image Classification",
    "section": "Image Classification",
    "text": "Image Classification\n\n\n\nExample of Image Classification.",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "pages/slides/image_classification.html#image-classification-example",
    "href": "pages/slides/image_classification.html#image-classification-example",
    "title": "Image Classification",
    "section": "Image Classification: Example",
    "text": "Image Classification: Example\n\n\n\nExample of Image Classification (from Krizhevsky, Sutskever, and Hinton (2012)).",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "pages/slides/image_classification.html#image-classification-camera-traps",
    "href": "pages/slides/image_classification.html#image-classification-camera-traps",
    "title": "Image Classification",
    "section": "Image Classification: Camera Traps",
    "text": "Image Classification: Camera Traps\n\n\n\nExample images from camera traps.",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "pages/slides/image_classification.html#parametric-approach",
    "href": "pages/slides/image_classification.html#parametric-approach",
    "title": "Image Classification",
    "section": "Parametric Approach",
    "text": "Parametric Approach\nWith a parametric approach, we seek a model of the following form:\n\\[\\begin{equation}\n    \\hat{y}^{(i)} = f(\\theta, \\mathbf{x}^{(i)})\n\\end{equation}\\]\nThe model parameters \\(\\theta\\) define our model and must be learned with an algorithm.",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "pages/slides/image_classification.html#softmax-classifier",
    "href": "pages/slides/image_classification.html#softmax-classifier",
    "title": "Image Classification",
    "section": "Softmax Classifier",
    "text": "Softmax Classifier\nWe want to model the following probability:\n\\[\\begin{equation}\n    P(Y=\\mathbf{y}^{(i)}| X = \\mathbf{x}^{(i)})\n\\end{equation}\\]\nTo obtain a valid probability distribution, the untransformed outputs \\(\\mathbf{z}\\), also called logits, of a model are transformed with the Softmax function \\(\\sigma(\\mathbf{z})\\).",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "pages/slides/image_classification.html#softmax-transformation",
    "href": "pages/slides/image_classification.html#softmax-transformation",
    "title": "Image Classification",
    "section": "Softmax Transformation",
    "text": "Softmax Transformation\n\\[\\begin{equation}\n    P(Y = k| X = \\mathbf{x}^{(i)}) = \\sigma(\\mathbf{z})_k = \\frac{e^{z_k}}{\\sum_i^K e^{z_i}}\n\\end{equation}\\]",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "pages/slides/image_classification.html#logits-to-probabilities",
    "href": "pages/slides/image_classification.html#logits-to-probabilities",
    "title": "Image Classification",
    "section": "Logits to Probabilities",
    "text": "Logits to Probabilities\n\n\n\nLogits (left) to probabilities with the Softmax function.",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "pages/slides/image_classification.html#probabilities",
    "href": "pages/slides/image_classification.html#probabilities",
    "title": "Image Classification",
    "section": "Probabilities",
    "text": "Probabilities\n\nImage classifier with confidences.",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "pages/slides/image_classification.html#likelihood",
    "href": "pages/slides/image_classification.html#likelihood",
    "title": "Image Classification",
    "section": "Likelihood",
    "text": "Likelihood\nThe likelihood of a data point:\n\\[\\begin{equation}\n    P(Y=y^{(i)}| X = \\mathbf{x}^{(i)}) = f(\\theta, \\mathbf{x}^{(i)})\n\\end{equation}\\]\nThis is the modeled probability for the actually observed class \\(y^{(i)}\\).",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "pages/slides/image_classification.html#likelihood-for-multi-class-classification",
    "href": "pages/slides/image_classification.html#likelihood-for-multi-class-classification",
    "title": "Image Classification",
    "section": "Likelihood for Multi-Class Classification",
    "text": "Likelihood for Multi-Class Classification\nThe likelihood of a data point for multi-class classification:\n\\[\\begin{equation}\n    \\prod_{j=1}^K P(Y = j| X = \\mathbf{x}^{(i)})^{y^{(i)}_j}\n\\end{equation}\\]\nWhere \\(y^{(i)} \\in \\mathbb{R}^{K}\\) is a one-hot encoded vector, with the \\(1\\) at the true class.",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "pages/slides/image_classification.html#maximum-likelihood",
    "href": "pages/slides/image_classification.html#maximum-likelihood",
    "title": "Image Classification",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\nThe likelihood of an entire dataset:\n\\[\\begin{equation}\n    \\prod_{i=1}^N \\prod_{j=1}^K P(Y = j| X = \\mathbf{x}^{(i)})^{y^{(i)}_j}\n\\end{equation}\\]\nUnder the maximum likelihood approach, we seek the parameters \\(\\theta\\) that maximize the likelihood of observing the dataset.",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "pages/slides/image_classification.html#negative-log-likelihood",
    "href": "pages/slides/image_classification.html#negative-log-likelihood",
    "title": "Image Classification",
    "section": "Negative Log-Likelihood",
    "text": "Negative Log-Likelihood\nEquivalently, we can minimize the negative log likelihood:\n\\[\\begin{align}\n    L(\\mathbf{X}, \\mathbf{y}, \\theta) =& - \\log \\prod_{i=1}^N \\prod_{j=1}^K  P(Y = j| X = \\mathbf{x}^{(i)})^{y^{(i)}_j} \\\\\n    L(\\mathbf{X}, \\mathbf{y}, \\theta) =& -\\sum_{i=1}^N \\sum_{j=1}^K y^{(i)}_j \\log  P(Y = j| X = \\mathbf{x}^{(i)})\n\\end{align}\\]",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "pages/slides/image_classification.html#cross-entropy",
    "href": "pages/slides/image_classification.html#cross-entropy",
    "title": "Image Classification",
    "section": "Cross-Entropy",
    "text": "Cross-Entropy\nThe loss function derived with maximum likelihood can also be viewed from the perspective of cross-entropy between two discrete probability distributions.\n\\[\\begin{align}\n    CE = - \\sum_{x \\in X} p(x) \\log q(x) \\\\\n    CE = - \\sum_{i=1}^N \\sum_{j=1}^K y_j^{(i)} \\log \\hat{y}_j^{(i)}\n\\end{align}\\]",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "pages/slides/image_classification.html#cross-entropy-1",
    "href": "pages/slides/image_classification.html#cross-entropy-1",
    "title": "Image Classification",
    "section": "Cross-Entropy",
    "text": "Cross-Entropy\n\nTrue distribution (left) and predicted distribution (right).",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "pages/slides/image_classification.html#alexnet",
    "href": "pages/slides/image_classification.html#alexnet",
    "title": "Image Classification",
    "section": "AlexNet",
    "text": "AlexNet\n\nAlexNet Krizhevsky, Sutskever, and Hinton (2012)",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "pages/slides/image_classification.html#alexnet-1",
    "href": "pages/slides/image_classification.html#alexnet-1",
    "title": "Image Classification",
    "section": "AlexNet",
    "text": "AlexNet\n\nAlexNet Prince (2023)",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "pages/slides/image_classification.html#alexnet-table",
    "href": "pages/slides/image_classification.html#alexnet-table",
    "title": "Image Classification",
    "section": "AlexNet: Table",
    "text": "AlexNet: Table\n\nSource: Johnson (2019)",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "pages/slides/image_classification.html#vgg",
    "href": "pages/slides/image_classification.html#vgg",
    "title": "Image Classification",
    "section": "VGG",
    "text": "VGG\n\nVGG Simonyan and Zisserman (2015)",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "pages/slides/image_classification.html#vgg-1",
    "href": "pages/slides/image_classification.html#vgg-1",
    "title": "Image Classification",
    "section": "VGG",
    "text": "VGG\n\n\nVGG Prince (2023)",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "pages/slides/image_classification.html#vgg-2",
    "href": "pages/slides/image_classification.html#vgg-2",
    "title": "Image Classification",
    "section": "VGG",
    "text": "VGG\n\n\n\nSource: Johnson (2019)",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "pages/slides/image_classification.html#resnet",
    "href": "pages/slides/image_classification.html#resnet",
    "title": "Image Classification",
    "section": "ResNet",
    "text": "ResNet\n\nSource: He et al. (2016)\nTest error for deeper model is larger. Overfitting?",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "pages/slides/image_classification.html#resnet-1",
    "href": "pages/slides/image_classification.html#resnet-1",
    "title": "Image Classification",
    "section": "ResNet",
    "text": "ResNet\n\nSource: He et al. (2016)\nTraining error for deeper model is also larger? What is going on?!",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "pages/slides/image_classification.html#resnet-2",
    "href": "pages/slides/image_classification.html#resnet-2",
    "title": "Image Classification",
    "section": "ResNet",
    "text": "ResNet\n\n\n\nResNet He et al. (2016) (Image from Johnson (2019))",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "pages/slides/image_classification.html#resnet-3",
    "href": "pages/slides/image_classification.html#resnet-3",
    "title": "Image Classification",
    "section": "ResNet",
    "text": "ResNet\n\n\n\nFrom He et al. (2016)",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "pages/slides/image_classification.html#resnet-4",
    "href": "pages/slides/image_classification.html#resnet-4",
    "title": "Image Classification",
    "section": "ResNet",
    "text": "ResNet\n\n\n\nFrom Li et al. (2018)",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "pages/slides/image_classification.html#convnext",
    "href": "pages/slides/image_classification.html#convnext",
    "title": "Image Classification",
    "section": "ConvNext",
    "text": "ConvNext\n\n\n\n\n\n\n\n\n\n\nFigures from Liu et al. (2022).",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "pages/slides/image_classification.html#imagenet-performance",
    "href": "pages/slides/image_classification.html#imagenet-performance",
    "title": "Image Classification",
    "section": "ImageNet Performance",
    "text": "ImageNet Performance\n\nImage from Prince (2023)",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "pages/slides/image_classification.html#choosing-the-architecture",
    "href": "pages/slides/image_classification.html#choosing-the-architecture",
    "title": "Image Classification",
    "section": "Choosing the Architecture",
    "text": "Choosing the Architecture\n\nDon’t be a hero!\n\nTypically, ResNet-50 or ResNet-101 are good choices. However, there are also models that require significantly fewer parameters, such as Efficient Nets.",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "pages/slides/image_classification.html#pre-processing",
    "href": "pages/slides/image_classification.html#pre-processing",
    "title": "Image Classification",
    "section": "Pre-Processing",
    "text": "Pre-Processing\n\nResizing / Cropping to a fixed size\nScaling: from the range [0, 255] to the range [0, 1]\nNormalization: Often normalized along the color channels\n\nPyTorch Examples",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "pages/slides/image_classification.html#transfer-learning",
    "href": "pages/slides/image_classification.html#transfer-learning",
    "title": "Image Classification",
    "section": "Transfer Learning",
    "text": "Transfer Learning\nTransfer learning refers to the process of adapting a trained model that models Task A to Task B. Adapting pre-trained models often leads to better results and also reduces the number of training iterations.",
    "crumbs": [
      "Slides",
      "Image Classification"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#leaky-abstraction",
    "href": "pages/slides/practical.html#leaky-abstraction",
    "title": "Practical Considerations",
    "section": "Leaky Abstraction",
    "text": "Leaky Abstraction\n\nyour_data = # plug your awesome dataset here\nmodel = SuperCrossValidator(SuperDuper.fit, your_data, ResNet50, SGDOptimizer)",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#silent-failure",
    "href": "pages/slides/practical.html#silent-failure",
    "title": "Practical Considerations",
    "section": "Silent Failure",
    "text": "Silent Failure\nTraining neural networks fails silently!",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#get-to-know-the-data",
    "href": "pages/slides/practical.html#get-to-know-the-data",
    "title": "Practical Considerations",
    "section": "1) Get to Know the Data",
    "text": "1) Get to Know the Data\nThoroughly inspect the data!",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#camera-traps-errors",
    "href": "pages/slides/practical.html#camera-traps-errors",
    "title": "Practical Considerations",
    "section": "Camera Traps: Errors",
    "text": "Camera Traps: Errors\n\n\n\nExamples of images from camera traps.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#camera-traps-difficulties",
    "href": "pages/slides/practical.html#camera-traps-difficulties",
    "title": "Practical Considerations",
    "section": "Camera Traps: Difficulties",
    "text": "Camera Traps: Difficulties\n\n\n\nExamples of images from camera traps. Source: Beery, Van Horn, and Perona (2018)",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#rare-classes",
    "href": "pages/slides/practical.html#rare-classes",
    "title": "Practical Considerations",
    "section": "Rare Classes",
    "text": "Rare Classes\n\n\n\nAn image of a serval. Below are the model confidences.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#multiple-classes",
    "href": "pages/slides/practical.html#multiple-classes",
    "title": "Practical Considerations",
    "section": "Multiple Classes",
    "text": "Multiple Classes\n\nExamples of an image from a camera trap with different species.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#baselines-1",
    "href": "pages/slides/practical.html#baselines-1",
    "title": "Practical Considerations",
    "section": "2) Baselines",
    "text": "2) Baselines\nEvaluation pipeline, metrics, experiment tracking, and baseline model.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#ml-process",
    "href": "pages/slides/practical.html#ml-process",
    "title": "Practical Considerations",
    "section": "ML Process",
    "text": "ML Process\n\n\n\nThe components of a typical machine learning process. Source: Raschka and Mirjalili (2020)",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#experiment-tracking",
    "href": "pages/slides/practical.html#experiment-tracking",
    "title": "Practical Considerations",
    "section": "Experiment Tracking",
    "text": "Experiment Tracking\n\n\n\nWeights and Biases experiment tracking.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#baselines-2",
    "href": "pages/slides/practical.html#baselines-2",
    "title": "Practical Considerations",
    "section": "2) Baselines",
    "text": "2) Baselines\nEnsure reproducibility.\nimport torch\ntorch.manual_seed(0)",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#baselines-3",
    "href": "pages/slides/practical.html#baselines-3",
    "title": "Practical Considerations",
    "section": "2) Baselines",
    "text": "2) Baselines\nAvoid unnecessary techniques and complexities. Reduce error susceptibility.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#baselines-4",
    "href": "pages/slides/practical.html#baselines-4",
    "title": "Practical Considerations",
    "section": "2) Baselines",
    "text": "2) Baselines\nIf possible, use a human baseline. How good can the model be?",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#difficult-cases",
    "href": "pages/slides/practical.html#difficult-cases",
    "title": "Practical Considerations",
    "section": "Difficult Cases",
    "text": "Difficult Cases\n\nAn image from a camera trap that is difficult to classify. Here, annotators had a 96.6% agreement with experts.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#baselines-5",
    "href": "pages/slides/practical.html#baselines-5",
    "title": "Practical Considerations",
    "section": "2) Baselines",
    "text": "2) Baselines\nTrain an input-independent baseline. Is the model learning anything at all?",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#baselines-6",
    "href": "pages/slides/practical.html#baselines-6",
    "title": "Practical Considerations",
    "section": "2) Baselines",
    "text": "2) Baselines\nOverfit the model on a batch of data. Does the optimization work?",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#baselines-7",
    "href": "pages/slides/practical.html#baselines-7",
    "title": "Practical Considerations",
    "section": "2) Baselines",
    "text": "2) Baselines\nVisualize what goes into the model. Is my preprocessing working?\ny_hat = model(x)",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#fixed-sample-segmentation-example",
    "href": "pages/slides/practical.html#fixed-sample-segmentation-example",
    "title": "Practical Considerations",
    "section": "Fixed Sample: Segmentation Example",
    "text": "Fixed Sample: Segmentation Example\n\n\n\nExample of a segmentation problem: input on the left and output on the right.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#overfit-1",
    "href": "pages/slides/practical.html#overfit-1",
    "title": "Practical Considerations",
    "section": "3) Overfit",
    "text": "3) Overfit\nAt this point, you should have a good understanding of the dataset, high confidence in the evaluation pipeline, and initial baselines from simple models. Now, look for a model that performs well on the training set.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#overfit-2",
    "href": "pages/slides/practical.html#overfit-2",
    "title": "Practical Considerations",
    "section": "3) Overfit",
    "text": "3) Overfit\nLook for a good model architecture. Follow the principle “Don’t be a hero”. Prefer already implemented/established architectures.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#regularization-1",
    "href": "pages/slides/practical.html#regularization-1",
    "title": "Practical Considerations",
    "section": "4) Regularization",
    "text": "4) Regularization\nAt this point, you should have achieved good performance on the training set. Now, focus on the validation set.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#regularization-2",
    "href": "pages/slides/practical.html#regularization-2",
    "title": "Practical Considerations",
    "section": "4) Regularization",
    "text": "4) Regularization\nThe simplest measure to achieve better performance (and also reduce overfitting) is to collect more training data. However, this is often expensive!",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#learning-curve",
    "href": "pages/slides/practical.html#learning-curve",
    "title": "Practical Considerations",
    "section": "Learning Curve",
    "text": "Learning Curve\nIs it worth collecting more data?\n\nExample of a learning curve. X-axis: Performance, Y-axis: Number of training samples. Left panel with Gaussian Naive Bayes and right panel with Support Vector Classifier.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#regularization-3",
    "href": "pages/slides/practical.html#regularization-3",
    "title": "Practical Considerations",
    "section": "4) Regularization",
    "text": "4) Regularization\nAnother possibility is data augmentation. New data points are generated from existing ones by making random changes to the data. Typically, data points are augmented on-the-fly.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#data-augmentation-augly",
    "href": "pages/slides/practical.html#data-augmentation-augly",
    "title": "Practical Considerations",
    "section": "Data Augmentation: Augly",
    "text": "Data Augmentation: Augly\n\n\n\nAugLy",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#data-augmentation-albumentations",
    "href": "pages/slides/practical.html#data-augmentation-albumentations",
    "title": "Practical Considerations",
    "section": "Data Augmentation: Albumentations",
    "text": "Data Augmentation: Albumentations\n\nAlbumentations",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#data-augmentation-kornia",
    "href": "pages/slides/practical.html#data-augmentation-kornia",
    "title": "Practical Considerations",
    "section": "Data Augmentation: Kornia",
    "text": "Data Augmentation: Kornia\n\n\n\nKornia",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#data-augmentation-example",
    "href": "pages/slides/practical.html#data-augmentation-example",
    "title": "Practical Considerations",
    "section": "Data Augmentation: Example",
    "text": "Data Augmentation: Example\n\n\n\nData augmentation example.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#data-augmentation-synthetic-data",
    "href": "pages/slides/practical.html#data-augmentation-synthetic-data",
    "title": "Practical Considerations",
    "section": "Data Augmentation: Synthetic Data",
    "text": "Data Augmentation: Synthetic Data\n\n\n\nFrom Beery et al. (2020). Synthetic and semi-synthetic data.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#regularization-4",
    "href": "pages/slides/practical.html#regularization-4",
    "title": "Practical Considerations",
    "section": "4) Regularization",
    "text": "4) Regularization\nWith early stopping, a model is trained and periodically evaluated on a validation set, e.g., after each epoch. Training is stopped if no significant improvement is achieved after x evaluation cycles.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#early-stopping",
    "href": "pages/slides/practical.html#early-stopping",
    "title": "Practical Considerations",
    "section": "Early Stopping",
    "text": "Early Stopping\n\n\n\nSource: Link",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#regularization-5",
    "href": "pages/slides/practical.html#regularization-5",
    "title": "Practical Considerations",
    "section": "4) Regularization",
    "text": "4) Regularization\nEarly stopping in PyTorch.\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\n\nclass LitModel(LightningModule):\n    def validation_step(self, batch, batch_idx):\n        loss = ...\n        self.log(\"val_loss\", loss)\n\nmodel = LitModel()\ntrainer = Trainer(callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\")])\ntrainer.fit(model)",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#regularization-6",
    "href": "pages/slides/practical.html#regularization-6",
    "title": "Practical Considerations",
    "section": "4) Regularization",
    "text": "4) Regularization\nWith weight decay, a model can be regularized. The update step in gradient descent is modified.\n\\[\\begin{equation}\n\\theta_{t+1} = \\theta_t (1 - \\lambda) - \\eta \\nabla J(\\theta)\n\\end{equation}\\]\nWhere \\(t\\) is the iteration, \\(\\theta\\) the model parameters, \\(\\eta\\) the learning rate, and \\(\\lambda\\) the decay parameter.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#regularization-7",
    "href": "pages/slides/practical.html#regularization-7",
    "title": "Practical Considerations",
    "section": "4) Regularization",
    "text": "4) Regularization\nTransfer learning involves adapting a pre-trained model on a large dataset (e.g., ImageNet) to a new task. The last layer is removed and replaced according to the new task. The network is then further trained. Layers can be frozen (weights not updated) or fine-tuned (weights further trained).",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#transfer-learning",
    "href": "pages/slides/practical.html#transfer-learning",
    "title": "Practical Considerations",
    "section": "Transfer Learning",
    "text": "Transfer Learning\n\n\n\nSource: Johnson and Fouhey (2021)",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#regularization-8",
    "href": "pages/slides/practical.html#regularization-8",
    "title": "Practical Considerations",
    "section": "4) Regularization",
    "text": "4) Regularization\nIn PyTorch, you can freeze the parameters:\ndef set_parameter_requires_grad(model, feature_extracting):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#hyper-parameter-tuning",
    "href": "pages/slides/practical.html#hyper-parameter-tuning",
    "title": "Practical Considerations",
    "section": "5) Hyper-Parameter Tuning",
    "text": "5) Hyper-Parameter Tuning\nIn this step, different hyperparameters and architectures are systematically evaluated. Techniques such as grid search or random search can be used, with random search being preferred.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#hyper-parameter-tuning-1",
    "href": "pages/slides/practical.html#hyper-parameter-tuning-1",
    "title": "Practical Considerations",
    "section": "5) Hyper-Parameter Tuning",
    "text": "5) Hyper-Parameter Tuning\nParameterized architecture:\nclass Net(nn.Module):\n    def __init__(self, l1=120, l2=84):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, l1)\n        self.fc2 = nn.Linear(l1, l2)\n        self.fc3 = nn.Linear(l2, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#squeeze-out-the-juice",
    "href": "pages/slides/practical.html#squeeze-out-the-juice",
    "title": "Practical Considerations",
    "section": "6) Squeeze out the Juice",
    "text": "6) Squeeze out the Juice\nAfter finding the best architectures and hyperparameters, there are further ways to squeeze out more performance.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#squeeze-out-the-juice-1",
    "href": "pages/slides/practical.html#squeeze-out-the-juice-1",
    "title": "Practical Considerations",
    "section": "6) Squeeze out the Juice",
    "text": "6) Squeeze out the Juice\nModel ensembling.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#squeeze-out-the-juice-2",
    "href": "pages/slides/practical.html#squeeze-out-the-juice-2",
    "title": "Practical Considerations",
    "section": "6) Squeeze out the Juice",
    "text": "6) Squeeze out the Juice\nTrain longer.",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#double-descent",
    "href": "pages/slides/practical.html#double-descent",
    "title": "Practical Considerations",
    "section": "Double Descent",
    "text": "Double Descent\n\nSource: Nakkiran et al. (2019)",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#squeeze-out-the-juice-3",
    "href": "pages/slides/practical.html#squeeze-out-the-juice-3",
    "title": "Practical Considerations",
    "section": "6) Squeeze out the Juice",
    "text": "6) Squeeze out the Juice\nOther training techniques:\n\nSpecial optimizer (AdamW)\nComplex data augmentation techniques (Mixup, Cutmix, RandAugment)\nRegularization techniques (Stochastic Depth)\nLabel smoothing",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#huggingface",
    "href": "pages/slides/practical.html#huggingface",
    "title": "Practical Considerations",
    "section": "HuggingFace",
    "text": "HuggingFace\nHuggingFace",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#timm",
    "href": "pages/slides/practical.html#timm",
    "title": "Practical Considerations",
    "section": "timm",
    "text": "timm\nPyTorch Image Models (timm)",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/practical.html#links",
    "href": "pages/slides/practical.html#links",
    "title": "Practical Considerations",
    "section": "Links",
    "text": "Links\n\nDS-cookie cutter\nPyTorch Lightning\nHydra\nWeights & Biases\nNeptune AI\nVersion Control Systems for ML Projects",
    "crumbs": [
      "Slides",
      "Practical Considerations"
    ]
  },
  {
    "objectID": "pages/slides/representation_learning.html#overview",
    "href": "pages/slides/representation_learning.html#overview",
    "title": "Representation Learning",
    "section": "Overview",
    "text": "Overview\n\nMotivation & Introduction\nWhat is Representation Learning?\nDeep Image Representations\nComparing Representations\nPractical Applications",
    "crumbs": [
      "Slides",
      "Representation Learning"
    ]
  },
  {
    "objectID": "pages/slides/representation_learning.html#the-information-overkill",
    "href": "pages/slides/representation_learning.html#the-information-overkill",
    "title": "Representation Learning",
    "section": "The Information Overkill",
    "text": "The Information Overkill\n\nExample Image Source\nHow would you describe the photograph?",
    "crumbs": [
      "Slides",
      "Representation Learning"
    ]
  },
  {
    "objectID": "pages/slides/representation_learning.html#the-information-overkill-1",
    "href": "pages/slides/representation_learning.html#the-information-overkill-1",
    "title": "Representation Learning",
    "section": "The Information Overkill",
    "text": "The Information Overkill\n\nExample Image Source\n\nA photograph of an elephant facing the camera. The elephant has large tusks and ears and is standing in a flat savannah. Is is bright day light with a cloudy sky.\n\n\n\n\n\n\n\n\nRepresentation\nSize (bits)\nSize (KB)\nCompression Factor\n\n\n\n\nRaw RGB Image\n5,529,600\n675.0\n1×\n\n\nNatural Language (UTF-8)\n1,312\n0.16\n4215×\n\n\nNatural Language (ASCII)\n1,148\n0.14\n4817×\n\n\n\n\n\n\n\nKey insight: Massive compression while preserving semantically relevant information!",
    "crumbs": [
      "Slides",
      "Representation Learning"
    ]
  },
  {
    "objectID": "pages/slides/representation_learning.html#the-vastness-of-image-space",
    "href": "pages/slides/representation_learning.html#the-vastness-of-image-space",
    "title": "Representation Learning",
    "section": "The Vastness of Image Space",
    "text": "The Vastness of Image Space\n\nIllustrated are the set of all possible images and the subset of natural images\nTotal possible \\(256\\times256\\) RGB images: \\(255^{256 \\times 256 \\times 3}\\) ≈ infinite\n\n\nNatural images: (relatively) tiny subset of this space\n\n\nImplication: Pixel space is extremely inefficient for representing real images!",
    "crumbs": [
      "Slides",
      "Representation Learning"
    ]
  },
  {
    "objectID": "pages/slides/representation_learning.html#definition",
    "href": "pages/slides/representation_learning.html#definition",
    "title": "Representation Learning",
    "section": "Definition",
    "text": "Definition\n\nA representation is the output \\(\\mathbf{z} = f(\\mathbf{x})\\) produced by an encoder \\(f\\) applied to input \\(\\mathbf{x}\\)\n\n\n\nSemantic Coordindate System where distances \\(\\|\\mathbf{z}_i - \\mathbf{z}_j\\|_2\\) reflect meaningful relationships.\n\n\nWe focus on vector-valued (distributed) representations, not symbolic descriptions",
    "crumbs": [
      "Slides",
      "Representation Learning"
    ]
  },
  {
    "objectID": "pages/slides/representation_learning.html#intuition",
    "href": "pages/slides/representation_learning.html#intuition",
    "title": "Representation Learning",
    "section": "Intuition",
    "text": "Intuition\n\n\n\n\n\n\nElephant - Day 1\n\n\n\n\n\n\n\nElephant - Day 1\n\n\n\n\n\n\n\nElephant - Day 1\n\n\n\n\n\n\n\nElephant - Day 2\n\n\n\n\n\n\n\n\n\nGiraffe - Day\n\n\n\n\n\n\n\nRhino - Night\n\n\n\n\n\n\n\nElephant - Night\n\n\n\n\n\n\n\nCar - Day",
    "crumbs": [
      "Slides",
      "Representation Learning"
    ]
  },
  {
    "objectID": "pages/slides/representation_learning.html#exercise-2d-representation",
    "href": "pages/slides/representation_learning.html#exercise-2d-representation",
    "title": "Representation Learning",
    "section": "Exercise: 2D Representation",
    "text": "Exercise: 2D Representation\n\n\n\nEmpty 2-dimensional representation space for mapping image similarity\n\n\n\nYour task: How would you map these images to 2D space?\n\n\nHint: Similar images should be close, dissimilar far apart",
    "crumbs": [
      "Slides",
      "Representation Learning"
    ]
  },
  {
    "objectID": "pages/slides/representation_learning.html#what-makes-good-representations",
    "href": "pages/slides/representation_learning.html#what-makes-good-representations",
    "title": "Representation Learning",
    "section": "What Makes Good Representations?",
    "text": "What Makes Good Representations?\nInvariance & Selectivity Balance\n\nInvariance: \\(\\|f(\\mathbf{x}) - f(g(\\mathbf{x}))\\|_2 &lt; \\epsilon\\) for nuisance transform \\(g\\) (lighting, pose)\nSelectivity: \\(\\|f(\\mathbf{x}_i) - f(\\mathbf{x}_j)\\|_2 &gt; \\delta\\) for semantically different \\(\\mathbf{x}_i, \\mathbf{x}_j\\)\nTrade-off: Too much invariance → loss of discriminative details\nSweet spot: Preserve task-relevant variations, discard irrelevant ones",
    "crumbs": [
      "Slides",
      "Representation Learning"
    ]
  },
  {
    "objectID": "pages/slides/representation_learning.html#what-makes-good-representations-1",
    "href": "pages/slides/representation_learning.html#what-makes-good-representations-1",
    "title": "Representation Learning",
    "section": "What Makes Good Representations?",
    "text": "What Makes Good Representations?\nGeometric Structure Preservation\n\nSmooth manifold: Similar semantic concepts cluster in representation space\nComposability: \\(f(\\mathbf{x}_1) \\oplus f(\\mathbf{x}_2) \\approx f(\\text{combine}(\\mathbf{x}_1, \\mathbf{x}_2))\\) for vector operations\nInterpolability: Linear interpolation \\(\\alpha f(\\mathbf{x}_1) + (1-\\alpha) f(\\mathbf{x}_2)\\) yields meaningful intermediate concepts",
    "crumbs": [
      "Slides",
      "Representation Learning"
    ]
  },
  {
    "objectID": "pages/slides/representation_learning.html#what-makes-good-representations-2",
    "href": "pages/slides/representation_learning.html#what-makes-good-representations-2",
    "title": "Representation Learning",
    "section": "What Makes Good Representations?",
    "text": "What Makes Good Representations?\nTransfer Efficiency\n\nLow sample complexity: Few examples needed for downstream adaptation\nBroad applicability: Same \\(f(\\cdot)\\) works across multiple tasks/domains\nGraceful degradation: Performance degrades slowly with domain shift",
    "crumbs": [
      "Slides",
      "Representation Learning"
    ]
  },
  {
    "objectID": "pages/slides/representation_learning.html#what-makes-good-representations-3",
    "href": "pages/slides/representation_learning.html#what-makes-good-representations-3",
    "title": "Representation Learning",
    "section": "What Makes Good Representations?",
    "text": "What Makes Good Representations?\nComputational Practicality\n\nCompact dimensionality: \\(\\text{dim}(\\mathbf{z}) \\ll \\text{dim}(\\mathbf{x})\\) while preserving information\nFast computation: Forward pass \\(f(\\mathbf{x})\\) efficient for real-time applications\nStable training: Representation learning converges reliably",
    "crumbs": [
      "Slides",
      "Representation Learning"
    ]
  },
  {
    "objectID": "pages/slides/representation_learning.html#the-invariance-selectivity-trade-off",
    "href": "pages/slides/representation_learning.html#the-invariance-selectivity-trade-off",
    "title": "Representation Learning",
    "section": "The Invariance-Selectivity Trade-off",
    "text": "The Invariance-Selectivity Trade-off\n\nInvariance: \\(\\|f(\\mathbf{x}) - f(g(\\mathbf{x}))\\|_2 &lt; \\epsilon\\) (Similar output for nuisance transforms)\nSelectivity: \\(\\|f(\\mathbf{x}_i) - f(\\mathbf{x}_j)\\|_2 &gt; \\delta\\) (Different outputs for semantic differences)\nChallenge: Too much invariance → loss of discriminative details!\nSweet spot: Preserve task-relevant variations, discard nuisance factors",
    "crumbs": [
      "Slides",
      "Representation Learning"
    ]
  },
  {
    "objectID": "pages/slides/representation_learning.html#the-invariance-selectivity-trade-off-1",
    "href": "pages/slides/representation_learning.html#the-invariance-selectivity-trade-off-1",
    "title": "Representation Learning",
    "section": "The Invariance-Selectivity Trade-off",
    "text": "The Invariance-Selectivity Trade-off\n\n\nWhat function has maximal invariance?\nWhat function has maximal selectivity?",
    "crumbs": [
      "Slides",
      "Representation Learning"
    ]
  },
  {
    "objectID": "pages/slides/representation_learning.html#three-learning-paradigms",
    "href": "pages/slides/representation_learning.html#three-learning-paradigms",
    "title": "Representation Learning",
    "section": "Three Learning Paradigms",
    "text": "Three Learning Paradigms\n\nSupervised Learning: Learn from human labels (ImageNet)\nWeakly Supervised: Learn from noisy/partial labels (image-text pairs)\nSelf-Supervised: Learn from data structure (no labels needed)",
    "crumbs": [
      "Slides",
      "Representation Learning"
    ]
  },
  {
    "objectID": "pages/slides/representation_learning.html#supervised-learning",
    "href": "pages/slides/representation_learning.html#supervised-learning",
    "title": "Representation Learning",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\n\n\nEncoder + classifier trained with labeled data\nTrain on labeled data (e.g., ImageNet 1000 classes)\nCross-entropy loss on human-provided labels\nPenultimate layer embeddings = transferable features",
    "crumbs": [
      "Slides",
      "Representation Learning"
    ]
  },
  {
    "objectID": "pages/slides/representation_learning.html#weakly-supervised-clip",
    "href": "pages/slides/representation_learning.html#weakly-supervised-clip",
    "title": "Representation Learning",
    "section": "Weakly Supervised: CLIP",
    "text": "Weakly Supervised: CLIP\n\nFrom Radford et al. (2021)\n\nDual encoders align images with text captions\nUses image-text pairs (web-scale data)\nContrastive loss: maximize diagonal, minimize off-diagonal",
    "crumbs": [
      "Slides",
      "Representation Learning"
    ]
  },
  {
    "objectID": "pages/slides/representation_learning.html#self-supervised-learning-masked-autoencoding-mae",
    "href": "pages/slides/representation_learning.html#self-supervised-learning-masked-autoencoding-mae",
    "title": "Representation Learning",
    "section": "Self-Supervised Learning: Masked Autoencoding (MAE)",
    "text": "Self-Supervised Learning: Masked Autoencoding (MAE)\n\nFrom He et al. (2021)\n\nMask 75% of image patches random\nEncode only visible patches (efficient!)\nDecode with mask tokens inserted and reconstruct original pixels\n\n\n\nKey insight: To fill in gaps, model must learn global structure",
    "crumbs": [
      "Slides",
      "Representation Learning"
    ]
  },
  {
    "objectID": "pages/slides/representation_learning.html#self-supervised-learning-contrastive-learning",
    "href": "pages/slides/representation_learning.html#self-supervised-learning-contrastive-learning",
    "title": "Representation Learning",
    "section": "Self-Supervised Learning: Contrastive Learning",
    "text": "Self-Supervised Learning: Contrastive Learning\n\n\n\n\n\n\nFrom Chen et al. (2020)\n\n\n\n\n\n\n\nFrom Chen et al. (2020)\n\n\n\n\n\n\n\nPull positive pairs close together in embedding space\nPush negative pairs apart\nForm tight clusters of semantically similar samples",
    "crumbs": [
      "Slides",
      "Representation Learning"
    ]
  },
  {
    "objectID": "pages/slides/representation_learning.html#self-supervised-learning-self-distillation",
    "href": "pages/slides/representation_learning.html#self-supervised-learning-self-distillation",
    "title": "Representation Learning",
    "section": "Self-Supervised Learning: Self-Distillation",
    "text": "Self-Supervised Learning: Self-Distillation\n\nFrom @Caron et al. (2021)\n\nNo explicit negatives needed\nWorks with smaller batches\nEmergent semantic clustering\nProduces high-quality local features (patch tokens)",
    "crumbs": [
      "Slides",
      "Representation Learning"
    ]
  },
  {
    "objectID": "pages/slides/representation_learning.html#self-supervised-learning-self-distillation-1",
    "href": "pages/slides/representation_learning.html#self-supervised-learning-self-distillation-1",
    "title": "Representation Learning",
    "section": "Self-Supervised Learning: Self-Distillation",
    "text": "Self-Supervised Learning: Self-Distillation\n\n\nWhat are the consequences if model training uses random left-right flips?",
    "crumbs": [
      "Slides",
      "Representation Learning"
    ]
  },
  {
    "objectID": "pages/slides/representation_learning.html#dinov3",
    "href": "pages/slides/representation_learning.html#dinov3",
    "title": "Representation Learning",
    "section": "DINOv3",
    "text": "DINOv3",
    "crumbs": [
      "Slides",
      "Representation Learning"
    ]
  },
  {
    "objectID": "pages/slides/representation_learning.html#global-vs-local-representations",
    "href": "pages/slides/representation_learning.html#global-vs-local-representations",
    "title": "Representation Learning",
    "section": "Global vs Local Representations",
    "text": "Global vs Local Representations\nGlobal representations summarize an image into a single embedding\nLocal representations preserve spatially discriminative aspects",
    "crumbs": [
      "Slides",
      "Representation Learning"
    ]
  },
  {
    "objectID": "pages/slides/representation_learning.html#test-images-overview",
    "href": "pages/slides/representation_learning.html#test-images-overview",
    "title": "Representation Learning",
    "section": "Test Images Overview",
    "text": "Test Images Overview\n\n\n\n\n\n\ncar\n\n\n\n\n\n\n\neleph_d1\n\n\n\n\n\n\n\neleph_d2\n\n\n\n\n\n\n\neleph_n\n\n\n\n\n\n\n\n\n\ngiraffe\n\n\n\n\n\n\n\nrhino\n\n\n\n\n\n\n\neleph_brt\n\n\n\n\n\n\n\neleph_flip",
    "crumbs": [
      "Slides",
      "Representation Learning"
    ]
  },
  {
    "objectID": "pages/slides/representation_learning.html#comparisons",
    "href": "pages/slides/representation_learning.html#comparisons",
    "title": "Representation Learning",
    "section": "Comparisons",
    "text": "Comparisons\n\n\nPixel SpaceResNet-50CLIPDINOv2\n\n\n\n\n\n\n\n\nFigure 1: Raw pixel-level similarity fails to capture semantic relationships.\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Supervised CNN representations group semantically similar images.\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Language-aligned embeddings from CLIP show text-driven semantic structure.\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Self-distilled DINOv2 features reveal fine-grained semantic clusters.",
    "crumbs": [
      "Slides",
      "Representation Learning"
    ]
  },
  {
    "objectID": "pages/slides/representation_learning.html#comparison-summary",
    "href": "pages/slides/representation_learning.html#comparison-summary",
    "title": "Representation Learning",
    "section": "Comparison Summary",
    "text": "Comparison Summary\n\n\n\n\nModel\nSemantic\nInvariance\nUse Case\n\n\n\n\nPixel\n❌ Poor\n❌ None\nBaseline\n\n\nResNet\n⚠️ Moderate\n⚠️ Limited\nFast inference\n\n\nCLIP\n✅ Good\n✅ Strong\nZero-shot, V+L\n\n\nDINOv2\n✅ Excellent\n✅ Excellent\nFew-shot, dense tasks",
    "crumbs": [
      "Slides",
      "Representation Learning"
    ]
  },
  {
    "objectID": "pages/slides/representation_learning.html#best-practices",
    "href": "pages/slides/representation_learning.html#best-practices",
    "title": "Representation Learning",
    "section": "Best Practices",
    "text": "Best Practices\n\n✅ Start with k-NN baseline (diagnostic value)\n✅ Use linear probe before fine-tuning\n✅ Match invariances to task requirements\n✅ Choose global/local features based on task\n✅ Full fine-tuning is often overkill",
    "crumbs": [
      "Slides",
      "Representation Learning"
    ]
  },
  {
    "objectID": "pages/slides/representation_learning.html#model-selection-guide",
    "href": "pages/slides/representation_learning.html#model-selection-guide",
    "title": "Representation Learning",
    "section": "Model Selection Guide",
    "text": "Model Selection Guide\n\nDINOv3: Best for discriminative vision tasks, few-shot learning\nCLIP: Zero-shot classification, vision-language tasks\nSupervised CNNs: When computational efficiency is critical (however, small DINOv3 variants exist..)",
    "crumbs": [
      "Slides",
      "Representation Learning"
    ]
  },
  {
    "objectID": "pages/slides/representation_learning.html#global-vs-local-representations-1",
    "href": "pages/slides/representation_learning.html#global-vs-local-representations-1",
    "title": "Representation Learning",
    "section": "Global vs Local Representations",
    "text": "Global vs Local Representations\n\nGlobal Embeddings (e.g., [CLS] token)\n\nSingle vector per image\nUse for: Classification, retrieval, zero-shot\n\n\n\nLocal Embeddings (patch tokens)\n\nGrid of spatially-resolved features\nUse for: Segmentation, detection, dense prediction\n\n\n\nModern models provide both!",
    "crumbs": [
      "Slides",
      "Representation Learning"
    ]
  },
  {
    "objectID": "pages/slides/representation_learning.html#the-adaptation-hierarchy",
    "href": "pages/slides/representation_learning.html#the-adaptation-hierarchy",
    "title": "Representation Learning",
    "section": "The Adaptation Hierarchy",
    "text": "The Adaptation Hierarchy\n\nAlways start simple, add complexity only when needed!\n\n\nk-NN baseline (0 training) → Quick reality check\nLinear probe (freeze encoder) → Test feature quality\nLoRA/Adapters (efficient tuning) → Parameter-efficient\nPartial fine-tune (top layers) → Balanced approach\nFull fine-tune (all weights) → Last resort",
    "crumbs": [
      "Slides",
      "Representation Learning"
    ]
  },
  {
    "objectID": "pages/slides/representation_learning.html#strategy-1-k-nn-baseline",
    "href": "pages/slides/representation_learning.html#strategy-1-k-nn-baseline",
    "title": "Representation Learning",
    "section": "Strategy 1: k-NN Baseline",
    "text": "Strategy 1: k-NN Baseline\n\nNo training required!\n\n# Extract features with frozen pretrained model\nfeatures_train = model.encode(train_images)\nfeatures_test = model.encode(test_images)\n\n# Classify by nearest neighbors\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(features_train, train_labels)\npredictions = knn.predict(features_test)\n\n✅ Instant results, reveals representation quality",
    "crumbs": [
      "Slides",
      "Representation Learning"
    ]
  },
  {
    "objectID": "pages/slides/representation_learning.html#strategy-2-linear-probe",
    "href": "pages/slides/representation_learning.html#strategy-2-linear-probe",
    "title": "Representation Learning",
    "section": "Strategy 2: Linear Probe",
    "text": "Strategy 2: Linear Probe\n\nFreeze encoder, train only classification head\n\n# Freeze pretrained model\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Add trainable head\nhead = nn.Linear(embed_dim, num_classes)\n\n# Train only the head\noptimizer = AdamW(head.parameters(), lr=5e-4)\n\n✅ Fast, stable, minimal overfitting risk",
    "crumbs": [
      "Slides",
      "Representation Learning"
    ]
  },
  {
    "objectID": "pages/slides/representation_learning.html#strategy-3-loraadapters",
    "href": "pages/slides/representation_learning.html#strategy-3-loraadapters",
    "title": "Representation Learning",
    "section": "Strategy 3: LoRA/Adapters",
    "text": "Strategy 3: LoRA/Adapters\n\nAdd small adapter modules, train only those\n\nfrom peft import LoraConfig, get_peft_model\n\nconfig = LoraConfig(\n    r=16, lora_alpha=32,\n    target_modules=[\"query\",\"key\",\"value\"],\n)\nmodel = get_peft_model(model, config)\n\n✅ Much cheaper than full fine-tuning, adapts to task",
    "crumbs": [
      "Slides",
      "Representation Learning"
    ]
  },
  {
    "objectID": "pages/slides/representation_learning.html#strategy-4-partial-fine-tune",
    "href": "pages/slides/representation_learning.html#strategy-4-partial-fine-tune",
    "title": "Representation Learning",
    "section": "Strategy 4: Partial Fine-tune",
    "text": "Strategy 4: Partial Fine-tune\n\nUnfreeze top layers, keep early layers frozen\n\n# Freeze all parameters\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Unfreeze last 2 transformer blocks\nfor param in model.blocks[-2:].parameters():\n    param.requires_grad = True\n\n# Replace classification head\nmodel.head = nn.Linear(embed_dim, num_classes)\n\n✅ Adjusts high-level features while keeping general features",
    "crumbs": [
      "Slides",
      "Representation Learning"
    ]
  },
  {
    "objectID": "pages/slides/representation_learning.html#strategy-5-full-fine-tune",
    "href": "pages/slides/representation_learning.html#strategy-5-full-fine-tune",
    "title": "Representation Learning",
    "section": "Strategy 5: Full Fine-tune",
    "text": "Strategy 5: Full Fine-tune\n\nTrain all model weights - use as last resort!\n\n# Unfreeze everything\nfor param in model.parameters():\n    param.requires_grad = True\n\n# Use small learning rate to avoid catastrophic forgetting\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\n⚠️ Risk: Overfitting with limited data, expensive compute",
    "crumbs": [
      "Slides",
      "Representation Learning"
    ]
  }
]