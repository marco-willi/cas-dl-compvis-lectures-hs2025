
@article{Zeiler2014,
	title = {Visualizing and understanding convolutional networks},
	volume = {8689 LNCS},
	issn = {16113349},
	doi = {10.1007/978-3-319-10590-1_53},
	abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky {\textbackslash}etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
	number = {PART 1},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Zeiler, Matthew D. and Fergus, Rob},
	year = {2014},
	pmid = {26353135},
	note = {ISBN: 9783319105895},
	pages = {818--833},
	file = {PDF:D\:\\Literatur\\Zotero\\storage\\49J7GX9T\\visualizing_and_understanding_CNNs_Zeiler2013.pdf:application/pdf},
}

@article{Ren2017,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	volume = {39},
	issn = {01628828},
	doi = {10.1109/TPAMI.2016.2577031},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
	number = {6},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	year = {2017},
	pmid = {27295650},
	note = {ISBN: 0162-8828 VO - PP},
	keywords = {convolutional neural network, Object detection, region proposal},
	pages = {1137--1149},
	file = {PDF:D\:\\Literatur\\Zotero\\storage\\TJV26KCP\\faster_r_cnn_towards_real_time_object_detection_Ren2016.pdf:application/pdf},
}

@inproceedings{Beery2018,
	title = {Recognition in {Terra} {Incognita}},
	volume = {11220 LNCS},
	isbn = {978-3-030-01269-4},
	url = {http://arxiv.org/abs/1807.04975},
	doi = {10.1007/978-3-030-01270-0_28},
	abstract = {It is desirable for detection and classification algorithms to generalize to unfamiliar environments, but suitable benchmarks for quantitatively studying this phenomenon are not yet available. We present a dataset designed to measure recognition generalization to novel environments. The images in our dataset are harvested from twenty camera traps deployed to monitor animal populations. Camera traps are fixed at one location, hence the background changes little across images; capture is triggered automatically, hence there is no human bias. The challenge is learning recognition in a handful of locations, and generalizing animal detection and classification to new locations where no training data is available. In our experiments state-of-the-art algorithms show excellent performance when tested at the same location where they were trained. However, we find that generalization to new locations is poor, especially for classification systems.(The dataset is available at https://beerys.github.io/CaltechCameraTraps/ ).},
	booktitle = {Lecture {Notes} in {Computer} {Science} (including subseries {Lecture} {Notes} in {Artificial} {Intelligence} and {Lecture} {Notes} in {Bioinformatics})},
	author = {Beery, Sara and Van Horn, Grant and Perona, Pietro},
	month = jul,
	year = {2018},
	note = {ISSN: 16113349},
	keywords = {Benchmark, Dataset, Context, Domain adaptation, Recognition, Transfer learning},
	pages = {472--489},
	file = {PDF:D\:\\Literatur\\Zotero\\storage\\4FUWME2X\\Beery, van Horn, Perona - 2018 - Recognition in Terra Incognita.pdf:application/pdf},
}

@article{Yosinski2014,
	title = {How transferable are features in deep neural networks?},
	volume = {27},
	issn = {10495258},
	url = {http://arxiv.org/abs/1411.1792},
	doi = {10.1109/IJCNN.2016.7727519},
	abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.},
	author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
	month = nov,
	year = {2014},
	note = {ISBN: 978-1-5090-0620-5},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\XUET5AU5\\Yosinski et al. - 2014 - How transferable are features in deep neural netwo.pdf:application/pdf;arXiv.org Snapshot:D\:\\Literatur\\Zotero\\storage\\IC39XW87\\1411.html:text/html},
}

@article{Images2018,
	title = {Identifying animal species in camera trap images using deep learning and citizen science},
	volume = {10},
	issn = {2041-210X},
	url = {http://doi.wiley.com/10.1111/2041-210X.13099},
	doi = {10.1111/2041-210X.13099},
	abstract = {© 2018 The Authors. Methods in Ecology and Evolution © 2018 British Ecological Society Ecologists often study wildlife populations by deploying camera traps. Large datasets are generated using this approach which can be difficult for research teams to manually evaluate. Researchers increasingly enlist volunteers from the general public as citizen scientists to help classify images. The growing number of camera trap studies, however, makes it ever more challenging to find enough volunteers to process all projects in a timely manner. Advances in machine learning, especially deep learning, allow for accurate automatic image classification. By training models using existing datasets of images classified by citizen scientists and subsequent application of such models on new studies, human effort may be reduced substantially. The goals of this study were to (a) assess the accuracy of deep learning in classifying camera trap data, (b) investigate how to process datasets with only a few classified images that are generally difficult to model, and (c) apply a trained model on a live online citizen science project. Convolutional neural networks (CNNs) were used to differentiate among images of different animal species, images of humans or vehicles, and empty images (no animals, vehicles, or humans). We used four different camera trap datasets featuring a wide variety of species, different habitats, and a varying number of images. All datasets were labelled by citizen scientists on Zooniverse. Accuracies for identifying empty images across projects ranged between 91.2\% and 98.0\%, whereas accuracies for identifying specific species were between 88.7\% and 92.7\%. Transferring information from CNNs trained on large datasets (“transfer-learning”) was increasingly beneficial as the size of the training dataset decreased and raised accuracy by up to 10.3\%. Removing low-confidence predictions increased model accuracies to the level of citizen scientists. By combining a trained model with classifications from citizen scientists, human effort was reduced by 43\% while maintaining overall accuracy for a live experiment running on Zooniverse. Ecology researchers can significantly reduce image classification time and manual effort by combining citizen scientists and CNNs, enabling faster processing of data from large camera trap studies.},
	number = {1},
	journal = {Methods in Ecology and Evolution},
	author = {Willi, Marco and Pitman, Ross T. and Cardoso, Anabelle W. and Locke, Christina and Swanson, Alexandra and Boyer, Amy and Veldthuis, Marten and Fortson, Lucy},
	editor = {Gaggiotti, Oscar},
	month = jan,
	year = {2019},
	keywords = {deep learning, convolutional neural networks, citizen science, machine learning, 1, animal identification, c ti o n, camera trap, deep, i ntro d u, learning},
	pages = {80--91},
	file = {PDF:D\:\\Literatur\\Zotero\\storage\\MMSFL9LD\\Willi et al. - 2019 - Identifying animal species in camera trap images using deep learning and citizen science.pdf:application/pdf},
}

@article{Vaswani2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {https://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
	number = {Nips},
	journal = {Advances in neural information processing systems},
	author = {Vaswani, Ashish and Brain, Google and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
	pages = {5998--6008},
	file = {PDF:D\:\\Literatur\\Zotero\\storage\\DDFTXKAG\\Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf},
}

@inproceedings{He2016,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	isbn = {978-1-4673-8851-1},
	url = {http://arxiv.org/abs/1512.03385},
	doi = {10.1109/CVPR.2016.90},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = jun,
	year = {2016},
	pmid = {23554596},
	note = {ISSN: 15737721},
	keywords = {deep learning, Convolutional neural networks, denoising auto-encoder, image denoising, Image steganalysis, Residual learning},
	pages = {770--778},
	file = {PDF:D\:\\Literatur\\Zotero\\storage\\MQ4EP8N7\\He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf:application/pdf},
}

@article{Beery2020,
	title = {Synthetic examples improve generalization for rare classes},
	url = {https://arxiv.org/abs/1904.05916},
	doi = {10.1109/WACV45572.2020.9093570},
	abstract = {The ability to detect and classify rare occurrences in images has important applications - for example, counting rare and endangered species when studying biodiversity, or detecting infrequent traffic scenarios that pose a danger to self-driving cars. Few-shot learning is an open problem: current computer vision systems struggle to categorize objects they have seen only rarely during training, and collecting a sufficient number of training examples of rare events is often challenging and expensive, and sometimes outright impossible. We explore in depth an approach to this problem: complementing the few available training images with ad-hoc simulated data.Our testbed is animal species classification, which has a real-world long-tailed distribution. We present two natural world simulators, and analyze the effect of different axes of variation in simulation, such as pose, lighting, model, and simulation method, and we prescribe best practices for efficiently incorporating simulated data for real-world performance gain. Our experiments reveal that synthetic data can considerably reduce error rates for classes that are rare, that as the amount of simulated data is increased, accuracy on the target class improves, and that high variation of simulated data provides maximum performance gain.},
	journal = {Proceedings - 2020 IEEE Winter Conference on Applications of Computer Vision, WACV 2020},
	author = {Beery, Sara and Liu, Yang and Morris, Dan and Piavis, Jim and Kapoor, Ashish and Meister, Markus and Joshi, Neel and Perona, Pietro},
	year = {2020},
	note = {ISBN: 9781728165530},
	pages = {852--862},
	file = {PDF:D\:\\Literatur\\Zotero\\storage\\F3TUDKC9\\Beery et al. - 2020 - Synthetic examples improve generalization for rare classes.pdf:application/pdf},
}

@article{Redmon2016a,
	title = {You only look once: {Unified}, real-time object detection},
	volume = {2016-Decem},
	issn = {10636919},
	doi = {10.1109/CVPR.2016.91},
	abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.},
	journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
	year = {2016},
	note = {Publisher: IEEE
ISBN: 9781467388504},
	pages = {779--788},
	file = {PDF:D\:\\Literatur\\Zotero\\storage\\WERCUPVN\\Redmon et al. - 2016 - You only look once Unified, real-time object detection.pdf:application/pdf},
}

@article{nakkiran_deep_2019,
	title = {Deep double descent: {Where} bigger models and more data hurt},
	issn = {23318422},
	abstract = {We show that a variety of modern deep learning tasks exhibit a “double-descent” phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.},
	journal = {arXiv},
	author = {Nakkiran, Preetum and Barak, Boaz and Kaplun, Gal and Sutskever, Ilya and Bansal, Yamini and Yang, Tristan},
	year = {2019},
	pages = {1--24},
	file = {PDF:D\:\\Literatur\\Zotero\\storage\\G4E7T5K9\\Nakkiran et al. - 2019 - Deep double descent Where bigger models and more data hurt.pdf:application/pdf},
}

@article{dosovitskiy_image_2020,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	urldate = {2021-05-18},
	journal = {arXiv:2010.11929 [cs]},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.11929},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Transformers},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\7ZCRFYZ2\\Dosovitskiy et al. - 2020 - An Image is Worth 16x16 Words Transformers for Im.pdf:application/pdf;arXiv.org Snapshot:D\:\\Literatur\\Zotero\\storage\\TW2Z4R9P\\2010.html:text/html},
}

@article{radford_learning_2021,
	title = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
	url = {http://arxiv.org/abs/2103.00020},
	abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
	urldate = {2021-11-04},
	journal = {arXiv:2103.00020 [cs]},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	month = feb,
	year = {2021},
	note = {arXiv: 2103.00020},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, armasuisse, multimodal},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\XCW8R35W\\Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf:application/pdf;arXiv.org Snapshot:D\:\\Literatur\\Zotero\\storage\\KLDPMLKQ\\2103.html:text/html},
}

@article{liu_pay_2021,
	title = {Pay {Attention} to {MLPs}},
	url = {http://arxiv.org/abs/2105.08050},
	abstract = {Transformers have become one of the most important architectural innovations in deep learning and have enabled many breakthroughs over the past few years. Here we propose a simple network architecture, gMLP, based on MLPs with gating, and show that it can perform as well as Transformers in key language and vision applications. Our comparisons show that self-attention is not critical for Vision Transformers, as gMLP can achieve the same accuracy. For BERT, our model achieves parity with Transformers on pretraining perplexity and is better on some downstream NLP tasks. On finetuning tasks where gMLP performs worse, making the gMLP model substantially larger can close the gap with Transformers. In general, our experiments show that gMLP can scale as well as Transformers over increased data and compute.},
	urldate = {2021-11-14},
	journal = {arXiv:2105.08050 [cs]},
	author = {Liu, Hanxiao and Dai, Zihang and So, David R. and Le, Quoc V.},
	month = jun,
	year = {2021},
	note = {arXiv: 2105.08050},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language, Journal Club},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\97JWQYYE\\Liu et al. - 2021 - Pay Attention to MLPs.pdf:application/pdf;arXiv.org Snapshot:D\:\\Literatur\\Zotero\\storage\\WJWJRR9V\\2105.html:text/html},
}

@misc{liu_convnet_2022,
	title = {A {ConvNet} for the 2020s},
	url = {http://arxiv.org/abs/2201.03545},
	abstract = {The "Roaring 20s" of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model. A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (e.g., Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks. However, the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions. In this work, we reexamine the design spaces and test the limits of what a pure ConvNet can achieve. We gradually "modernize" a standard ResNet toward the design of a vision Transformer, and discover several key components that contribute to the performance difference along the way. The outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8\% ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection and ADE20K segmentation, while maintaining the simplicity and efficiency of standard ConvNets.},
	urldate = {2022-09-09},
	publisher = {arXiv},
	author = {Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
	month = mar,
	year = {2022},
	note = {arXiv:2201.03545 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\48RELPFX\\Liu et al. - 2022 - A ConvNet for the 2020s.pdf:application/pdf;arXiv.org Snapshot:D\:\\Literatur\\Zotero\\storage\\VHMGDIEW\\2201.html:text/html},
}

@misc{he_mask_2018,
	title = {Mask {R}-{CNN}},
	url = {http://arxiv.org/abs/1703.06870},
	abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
	urldate = {2022-09-15},
	publisher = {arXiv},
	author = {He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross},
	month = jan,
	year = {2018},
	note = {arXiv:1703.06870 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\GJ8FC6UE\\He et al. - 2018 - Mask R-CNN.pdf:application/pdf;arXiv.org Snapshot:D\:\\Literatur\\Zotero\\storage\\U469IN9E\\1703.html:text/html},
}

@misc{wu_detectron2_2019,
	title = {Detectron2},
	url = {https://github.com/facebookresearch/detectron2},
	author = {Wu, Yuxin and Kirillov, Alexander and Massa, Francisco and Lo, Wan-Yen and Girshick, Ross},
	year = {2019},
}

@inproceedings{krizhevsky_imagenet_2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {25},
	url = {https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C. J. and Bottou, L. and Weinberger, K. Q.},
	year = {2012},
	file = {Full Text:D\:\\Literatur\\Zotero\\storage\\KM5VV9WR\\Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf:application/pdf},
}

@misc{russakovsky_imagenet_2015,
	title = {{ImageNet} {Large} {Scale} {Visual} {Recognition} {Challenge}},
	url = {http://arxiv.org/abs/1409.0575},
	abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
	urldate = {2022-09-16},
	publisher = {arXiv},
	author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
	month = jan,
	year = {2015},
	note = {arXiv:1409.0575 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, I.4.8, I.5.2},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\UDBRKS73\\Russakovsky et al. - 2015 - ImageNet Large Scale Visual Recognition Challenge.pdf:application/pdf;arXiv.org Snapshot:D\:\\Literatur\\Zotero\\storage\\GC8864AW\\1409.html:text/html},
}

@misc{rombach_high-resolution_2022,
	title = {High-{Resolution} {Image} {Synthesis} with {Latent} {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2112.10752},
	abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .},
	urldate = {2022-09-16},
	publisher = {arXiv},
	author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
	month = apr,
	year = {2022},
	note = {arXiv:2112.10752 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\AMTBD8J5\\Rombach et al. - 2022 - High-Resolution Image Synthesis with Latent Diffus.pdf:application/pdf;arXiv.org Snapshot:D\:\\Literatur\\Zotero\\storage\\J9GSNHWP\\2112.html:text/html},
}

@misc{image_to_image_isola2018,
	title = {Image-to-{Image} {Translation} with {Conditional} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1611.07004},
	abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
	urldate = {2022-09-16},
	publisher = {arXiv},
	author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
	month = nov,
	year = {2018},
	note = {arXiv:1611.07004 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\APFQLJ77\\Isola et al. - 2018 - Image-to-Image Translation with Conditional Advers.pdf:application/pdf;arXiv.org Snapshot:D\:\\Literatur\\Zotero\\storage\\SVVDP59A\\1611.html:text/html},
}

@article{fukushima_neocognitron_1980,
	title = {Neocognitron: {A} self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position},
	volume = {36},
	issn = {0340-1200, 1432-0770},
	shorttitle = {Neocognitron},
	url = {http://link.springer.com/10.1007/BF00344251},
	doi = {10.1007/BF00344251},
	language = {en},
	number = {4},
	urldate = {2022-09-16},
	journal = {Biological Cybernetics},
	author = {Fukushima, Kunihiko},
	month = apr,
	year = {1980},
	pages = {193--202},
	file = {Full Text:D\:\\Literatur\\Zotero\\storage\\KGTYB34I\\Fukushima - 1980 - Neocognitron A self-organizing neural network mod.pdf:application/pdf},
}

@article{hubel_receptive_1959,
	title = {Receptive fields of single neurones in the cat's striate cortex},
	volume = {148},
	issn = {00223751},
	url = {https://onlinelibrary.wiley.com/doi/10.1113/jphysiol.1959.sp006308},
	doi = {10.1113/jphysiol.1959.sp006308},
	language = {en},
	number = {3},
	urldate = {2022-09-16},
	journal = {The Journal of Physiology},
	author = {Hubel, D. H. and Wiesel, T. N.},
	month = oct,
	year = {1959},
	pages = {574--591},
	file = {Full Text:D\:\\Literatur\\Zotero\\storage\\YY6NN5C7\\Hubel and Wiesel - 1959 - Receptive fields of single neurones in the cat's s.pdf:application/pdf},
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/323533a0},
	doi = {10.1038/323533a0},
	language = {en},
	number = {6088},
	urldate = {2022-09-16},
	journal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	month = oct,
	year = {1986},
	pages = {533--536},
}

@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {00189219},
	url = {http://ieeexplore.ieee.org/document/726791/},
	doi = {10.1109/5.726791},
	number = {11},
	urldate = {2022-09-16},
	journal = {Proceedings of the IEEE},
	author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	month = nov,
	year = {1998},
	pages = {2278--2324},
}

@inproceedings{deng_imagenet_2009,
	address = {Miami, FL},
	title = {{ImageNet}: {A} large-scale hierarchical image database},
	isbn = {978-1-4244-3992-8},
	shorttitle = {{ImageNet}},
	url = {https://ieeexplore.ieee.org/document/5206848/},
	doi = {10.1109/CVPR.2009.5206848},
	urldate = {2022-09-16},
	booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and {Kai Li} and {Li Fei-Fei}},
	month = jun,
	year = {2009},
	pages = {248--255},
}

@book{geron_hands-machine_2019,
	address = {Beijing Boston Farnham Sebastopol Tokyo},
	edition = {Second edition},
	series = {Covid-19 collection},
	title = {Hands-on machine learning with {Scikit}-{Learn}, {Keras}, and {TensorFlow}: concepts, tools, and techniques to build intelligent systems},
	isbn = {978-1-4920-3264-9},
	shorttitle = {Hands-on machine learning with {Scikit}-{Learn}, {Keras}, and {TensorFlow}},
	language = {eng},
	publisher = {O'Reilly},
	author = {Géron, Aurélien},
	year = {2019},
	file = {Table of Contents PDF:D\:\\Literatur\\Zotero\\storage\\DRSCT7XK\\Géron - 2019 - Hands-on machine learning with Scikit-Learn, Keras.pdf:application/pdf},
}

@book{raschka_python_2020,
	address = {Birmingham Mumbai},
	edition = {Second edition, fourth release,[fully revised and updated]},
	series = {Expert insight},
	title = {Python machine learning: machine learning and deep learning with {Python}, scikit-learn, and {TensorFlow}},
	isbn = {978-1-78712-593-3},
	shorttitle = {Python machine learning},
	language = {eng},
	publisher = {Packt Publishing},
	author = {Raschka, Sebastian and Mirjalili, Vahid},
	month = apr,
	year = {2020},
}

@misc{raschka_model_2020,
	title = {Model {Evaluation}, {Model} {Selection}, and {Algorithm} {Selection} in {Machine} {Learning}},
	url = {http://arxiv.org/abs/1811.12808},
	abstract = {The correct use of model evaluation, model selection, and algorithm selection techniques is vital in academic machine learning research as well as in many industrial settings. This article reviews different techniques that can be used for each of these three subtasks and discusses the main advantages and disadvantages of each technique with references to theoretical and empirical studies. Further, recommendations are given to encourage best yet feasible practices in research and applications of machine learning. Common methods such as the holdout method for model evaluation and selection are covered, which are not recommended when working with small datasets. Different flavors of the bootstrap technique are introduced for estimating the uncertainty of performance estimates, as an alternative to confidence intervals via normal approximation if bootstrapping is computationally feasible. Common cross-validation techniques such as leave-one-out cross-validation and k-fold cross-validation are reviewed, the bias-variance trade-off for choosing k is discussed, and practical tips for the optimal choice of k are given based on empirical evidence. Different statistical tests for algorithm comparisons are presented, and strategies for dealing with multiple comparisons such as omnibus tests and multiple-comparison corrections are discussed. Finally, alternative methods for algorithm selection, such as the combined F-test 5x2 cross-validation and nested cross-validation, are recommended for comparing machine learning algorithms when datasets are small.},
	urldate = {2022-09-30},
	publisher = {arXiv},
	author = {Raschka, Sebastian},
	month = nov,
	year = {2020},
	note = {arXiv:1811.12808 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\T4SZ8L6I\\Raschka - 2020 - Model Evaluation, Model Selection, and Algorithm S.pdf:application/pdf;arXiv.org Snapshot:D\:\\Literatur\\Zotero\\storage\\XXPUZCRG\\1811.html:text/html},
}

@book{vanderplas_python_2016,
	address = {Beijing Boston Farnham Sebastopol Tokyo},
	edition = {First edition},
	title = {Python data science handbook: essential tools for working with data},
	isbn = {978-1-4919-1205-8},
	shorttitle = {Python data science handbook},
	language = {eng},
	publisher = {O'Reilly},
	author = {VanderPlas, Jake},
	year = {2016},
}

@misc{noauthor_notitle_nodate,
	url = {https://youtu.be/IHZwWFHWa-w},
}

@misc{fleuret_deep_2018,
	title = {Deep {Learning} {Course} 14x050 of the {University} of {Geneva}, {Switzerland}},
	url = {https://fleuret.org/dlc/},
	author = {Fleuret, François},
	year = {2018},
}

@book{stevens_deep_2020,
	address = {Shelter Island, NY},
	title = {Deep learning with {PyTorch}},
	isbn = {978-1-61729-526-3},
	url = {https://isip.piconepress.com/courses/temple/ece_4822/resources/books/Deep-Learning-with-PyTorch.pdf},
	publisher = {Manning Publications Co},
	author = {Stevens, Eli and Antiga, Luca and Viehmann, Thomas},
	year = {2020},
	keywords = {Machine learning, Artificial intelligence, Neural networks (Computer science), Python (Computer program language)},
}

@misc{li_cs231n_2022,
	type = {Lecture {Notes}},
	title = {{CS231n} {Convolutional} {Neural} {Networks} for {Visual} {Recognition}},
	url = {https://cs231n.github.io},
	urldate = {2022-10-24},
	author = {Li, Fei-Fei},
	year = {2022},
}

@article{dumoulin_guide_2016,
	title = {A guide to convolution arithmetic for deep learning},
	journal = {ArXiv e-prints},
	author = {Dumoulin, Vincent and Visin, Francesco},
	month = mar,
	year = {2016},
	note = {\_eprint: 1603.07285},
}

@misc{yu_multi-scale_2016,
	title = {Multi-{Scale} {Context} {Aggregation} by {Dilated} {Convolutions}},
	url = {http://arxiv.org/abs/1511.07122},
	abstract = {State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy.},
	urldate = {2022-10-12},
	publisher = {arXiv},
	author = {Yu, Fisher and Koltun, Vladlen},
	month = apr,
	year = {2016},
	note = {arXiv:1511.07122 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\NHY76MJ8\\Yu and Koltun - 2016 - Multi-Scale Context Aggregation by Dilated Convolu.pdf:application/pdf;arXiv.org Snapshot:D\:\\Literatur\\Zotero\\storage\\BG3H3ZEV\\1511.html:text/html},
}

@misc{chollet_xception_2017,
	title = {Xception: {Deep} {Learning} with {Depthwise} {Separable} {Convolutions}},
	shorttitle = {Xception},
	url = {http://arxiv.org/abs/1610.02357},
	abstract = {We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.},
	urldate = {2022-10-12},
	publisher = {arXiv},
	author = {Chollet, François},
	month = apr,
	year = {2017},
	note = {arXiv:1610.02357 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\3CIPWDZ7\\Chollet - 2017 - Xception Deep Learning with Depthwise Separable C.pdf:application/pdf;arXiv.org Snapshot:D\:\\Literatur\\Zotero\\storage\\P45ZAEHH\\1610.html:text/html},
}

@misc{qiao_micro-batch_2020,
	title = {Micro-{Batch} {Training} with {Batch}-{Channel} {Normalization} and {Weight} {Standardization}},
	url = {http://arxiv.org/abs/1903.10520},
	abstract = {Batch Normalization (BN) has become an out-of-box technique to improve deep network training. However, its effectiveness is limited for micro-batch training, i.e., each GPU typically has only 1-2 images for training, which is inevitable for many computer vision tasks, e.g., object detection and semantic segmentation, constrained by memory consumption. To address this issue, we propose Weight Standardization (WS) and Batch-Channel Normalization (BCN) to bring two success factors of BN into micro-batch training: 1) the smoothing effects on the loss landscape and 2) the ability to avoid harmful elimination singularities along the training trajectory. WS standardizes the weights in convolutional layers to smooth the loss landscape by reducing the Lipschitz constants of the loss and the gradients; BCN combines batch and channel normalizations and leverages estimated statistics of the activations in convolutional layers to keep networks away from elimination singularities. We validate WS and BCN on comprehensive computer vision tasks, including image classification, object detection, instance segmentation, video recognition and semantic segmentation. All experimental results consistently show that WS and BCN improve micro-batch training significantly. Moreover, using WS and BCN with micro-batch training is even able to match or outperform the performances of BN with large-batch training.},
	urldate = {2022-10-13},
	publisher = {arXiv},
	author = {Qiao, Siyuan and Wang, Huiyu and Liu, Chenxi and Shen, Wei and Yuille, Alan},
	month = aug,
	year = {2020},
	note = {arXiv:1903.10520 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\JW6T9AAG\\Qiao et al. - 2020 - Micro-Batch Training with Batch-Channel Normalizat.pdf:application/pdf;arXiv.org Snapshot:D\:\\Literatur\\Zotero\\storage\\XZDF6NX9\\1903.html:text/html},
}

@misc{ba_layer_2016,
	title = {Layer {Normalization}},
	url = {http://arxiv.org/abs/1607.06450},
	abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
	urldate = {2022-10-13},
	publisher = {arXiv},
	author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
	month = jul,
	year = {2016},
	note = {arXiv:1607.06450 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\A8LRLNTJ\\Ba et al. - 2016 - Layer Normalization.pdf:application/pdf;arXiv.org Snapshot:D\:\\Literatur\\Zotero\\storage\\ARDHCYFA\\1607.html:text/html},
}

@misc{simonyan_very_2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	urldate = {2022-10-21},
	publisher = {arXiv},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = apr,
	year = {2015},
	note = {arXiv:1409.1556 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\WNYR4Q4R\\Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf:application/pdf;arXiv.org Snapshot:D\:\\Literatur\\Zotero\\storage\\9VB5IGF8\\1409.html:text/html},
}

@book{goodfellow_deep_2016,
	title = {Deep {Learning}},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
}

@misc{johnson_eecs_2019,
	type = {Lecture {Notes} / {Slides}},
	title = {{EECS} 498-007 / 598-005: {Deep} {Learning} for {Computer} {Vision}},
	url = {https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2019/},
	abstract = {Computer Vision has become ubiquitous in our society, with applications in search, image understanding, apps, mapping, medicine, drones, and self-driving cars. Core to many of these applications are visual recognition tasks such as image classification and object detection. Recent developments in neural network approaches have greatly advanced the performance of these state-of-the-art visual recognition systems. This course is a deep dive into details of neural-network based deep learning methods for computer vision. During this course, students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in computer vision. We will cover learning algorithms, neural network architectures, and practical engineering tricks for training and fine-tuning networks for visual recognition tasks.},
	author = {Johnson, Justin},
	year = {2019},
}

@misc{goodfellow_explaining_2015,
	title = {Explaining and {Harnessing} {Adversarial} {Examples}},
	url = {http://arxiv.org/abs/1412.6572},
	abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
	urldate = {2022-12-02},
	publisher = {arXiv},
	author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
	month = mar,
	year = {2015},
	note = {arXiv:1412.6572 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\VW9CYH47\\Goodfellow et al. - 2015 - Explaining and Harnessing Adversarial Examples.pdf:application/pdf;arXiv.org Snapshot:D\:\\Literatur\\Zotero\\storage\\UTRNRP5L\\1412.html:text/html},
}

@article{uijlings_selective_2013,
	title = {Selective {Search} for {Object} {Recognition}},
	volume = {104},
	issn = {0920-5691, 1573-1405},
	url = {http://link.springer.com/10.1007/s11263-013-0620-5},
	doi = {10.1007/s11263-013-0620-5},
	language = {en},
	number = {2},
	urldate = {2022-12-08},
	journal = {International Journal of Computer Vision},
	author = {Uijlings, J. R. R. and van de Sande, K. E. A. and Gevers, T. and Smeulders, A. W. M.},
	month = sep,
	year = {2013},
	pages = {154--171},
	file = {Submitted Version:D\:\\Literatur\\Zotero\\storage\\IP3SY3PJ\\Uijlings et al. - 2013 - Selective Search for Object Recognition.pdf:application/pdf},
}

@misc{girshick_rich_2014,
	title = {Rich feature hierarchies for accurate object detection and semantic segmentation},
	url = {http://arxiv.org/abs/1311.2524},
	abstract = {Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30\% relative to the previous best result on VOC 2012---achieving a mAP of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http://www.cs.berkeley.edu/{\textasciitilde}rbg/rcnn.},
	urldate = {2022-12-08},
	publisher = {arXiv},
	author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
	month = oct,
	year = {2014},
	note = {arXiv:1311.2524 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\77JPDV4B\\Girshick et al. - 2014 - Rich feature hierarchies for accurate object detec.pdf:application/pdf;arXiv.org Snapshot:D\:\\Literatur\\Zotero\\storage\\RFYQ5WDL\\1311.html:text/html},
}

@misc{girshick_fast_2015,
	title = {Fast {R}-{CNN}},
	url = {http://arxiv.org/abs/1504.08083},
	abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
	urldate = {2022-12-08},
	publisher = {arXiv},
	author = {Girshick, Ross},
	month = sep,
	year = {2015},
	note = {arXiv:1504.08083 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\X78U8HYK\\Girshick - 2015 - Fast R-CNN.pdf:application/pdf;arXiv.org Snapshot:D\:\\Literatur\\Zotero\\storage\\QB4R3MTR\\1504.html:text/html},
}

@misc{shelhamer_fully_2016,
	title = {Fully {Convolutional} {Networks} for {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/1605.06211},
	abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves improved segmentation of PASCAL VOC (30\% relative improvement to 67.2\% mean IU on 2012), NYUDv2, SIFT Flow, and PASCAL-Context, while inference takes one tenth of a second for a typical image.},
	urldate = {2022-12-13},
	publisher = {arXiv},
	author = {Shelhamer, Evan and Long, Jonathan and Darrell, Trevor},
	month = may,
	year = {2016},
	note = {arXiv:1605.06211 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\ACU4QGPE\\Shelhamer et al. - 2016 - Fully Convolutional Networks for Semantic Segmenta.pdf:application/pdf;arXiv.org Snapshot:D\:\\Literatur\\Zotero\\storage\\X8NRSDZH\\1605.html:text/html},
}

@misc{minaee_image_2020,
	title = {Image {Segmentation} {Using} {Deep} {Learning}: {A} {Survey}},
	shorttitle = {Image {Segmentation} {Using} {Deep} {Learning}},
	url = {http://arxiv.org/abs/2001.05566},
	abstract = {Image segmentation is a key topic in image processing and computer vision with applications such as scene understanding, medical image analysis, robotic perception, video surveillance, augmented reality, and image compression, among many others. Various algorithms for image segmentation have been developed in the literature. Recently, due to the success of deep learning models in a wide range of vision applications, there has been a substantial amount of works aimed at developing image segmentation approaches using deep learning models. In this survey, we provide a comprehensive review of the literature at the time of this writing, covering a broad spectrum of pioneering works for semantic and instance-level segmentation, including fully convolutional pixel-labeling networks, encoder-decoder architectures, multi-scale and pyramid based approaches, recurrent networks, visual attention models, and generative models in adversarial settings. We investigate the similarity, strengths and challenges of these deep learning models, examine the most widely used datasets, report performances, and discuss promising future research directions in this area.},
	urldate = {2022-12-13},
	publisher = {arXiv},
	author = {Minaee, Shervin and Boykov, Yuri and Porikli, Fatih and Plaza, Antonio and Kehtarnavaz, Nasser and Terzopoulos, Demetri},
	month = nov,
	year = {2020},
	note = {arXiv:2001.05566 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\9J4IKWQW\\Minaee et al. - 2020 - Image Segmentation Using Deep Learning A Survey.pdf:application/pdf;arXiv.org Snapshot:D\:\\Literatur\\Zotero\\storage\\EBTK3TUN\\2001.html:text/html},
}

@misc{ronneberger_u-net_2015,
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	shorttitle = {U-{Net}},
	url = {http://arxiv.org/abs/1505.04597},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	urldate = {2022-12-13},
	publisher = {arXiv},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	month = may,
	year = {2015},
	note = {arXiv:1505.04597 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\IW8HBWFE\\Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf:application/pdf;arXiv.org Snapshot:D\:\\Literatur\\Zotero\\storage\\L7A4XBCP\\1505.html:text/html},
}

@misc{noh_learning_2015,
	title = {Learning {Deconvolution} {Network} for {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/1505.04366},
	abstract = {We propose a novel semantic segmentation algorithm by learning a deconvolution network. We learn the network on top of the convolutional layers adopted from VGG 16-layer net. The deconvolution network is composed of deconvolution and unpooling layers, which identify pixel-wise class labels and predict segmentation masks. We apply the trained network to each proposal in an input image, and construct the final semantic segmentation map by combining the results from all proposals in a simple manner. The proposed algorithm mitigates the limitations of the existing methods based on fully convolutional networks by integrating deep deconvolution network and proposal-wise prediction; our segmentation method typically identifies detailed structures and handles objects in multiple scales naturally. Our network demonstrates outstanding performance in PASCAL VOC 2012 dataset, and we achieve the best accuracy (72.5\%) among the methods trained with no external data through ensemble with the fully convolutional network.},
	urldate = {2022-12-13},
	publisher = {arXiv},
	author = {Noh, Hyeonwoo and Hong, Seunghoon and Han, Bohyung},
	month = may,
	year = {2015},
	note = {arXiv:1505.04366 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\E8XEIJNG\\Noh et al. - 2015 - Learning Deconvolution Network for Semantic Segmen.pdf:application/pdf;arXiv.org Snapshot:D\:\\Literatur\\Zotero\\storage\\LM4LEPCE\\1505.html:text/html},
}

@misc{novikov_fully_2018,
	title = {Fully {Convolutional} {Architectures} for {Multi}-{Class} {Segmentation} in {Chest} {Radiographs}},
	url = {http://arxiv.org/abs/1701.08816},
	abstract = {The success of deep convolutional neural networks on image classification and recognition tasks has led to new applications in very diversified contexts, including the field of medical imaging. In this paper we investigate and propose neural network architectures for automated multi-class segmentation of anatomical organs in chest radiographs, namely for lungs, clavicles and heart. We address several open challenges including model overfitting, reducing number of parameters and handling of severely imbalanced data in CXR by fusing recent concepts in convolutional networks and adapting them to the segmentation problem task in CXR. We demonstrate that our architecture combining delayed subsampling, exponential linear units, highly restrictive regularization and a large number of high resolution low level abstract features outperforms state-of-the-art methods on all considered organs, as well as the human observer on lungs and heart. The models use a multi-class configuration with three target classes and are trained and tested on the publicly available JSRT database, consisting of 247 X-ray images the ground-truth masks for which are available in the SCR database. Our best performing model, trained with the loss function based on the Dice coefficient, reached mean Jaccard overlap scores of 95.0{\textbackslash}\% for lungs, 86.8{\textbackslash}\% for clavicles and 88.2{\textbackslash}\% for heart. This architecture outperformed the human observer results for lungs and heart.},
	urldate = {2022-12-16},
	publisher = {arXiv},
	author = {Novikov, Alexey A. and Lenis, Dimitrios and Major, David and Hladůvka, Jiri and Wimmer, Maria and Bühler, Katja},
	month = feb,
	year = {2018},
	note = {arXiv:1701.08816 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\NTLQF4LJ\\Novikov et al. - 2018 - Fully Convolutional Architectures for Multi-Class .pdf:application/pdf;arXiv.org Snapshot:D\:\\Literatur\\Zotero\\storage\\E7ZD2B2S\\1701.html:text/html},
}

@inproceedings{cordts_cityscapes_2016,
	title = {The {Cityscapes} {Dataset} for {Semantic} {Urban} {Scene} {Understanding}},
	url = {https://arxiv.org/pdf/1604.01685.pdf},
	booktitle = {Proc. of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
	year = {2016},
}

@misc{power_grokking_2022,
	title = {Grokking: {Generalization} {Beyond} {Overfitting} on {Small} {Algorithmic} {Datasets}},
	shorttitle = {Grokking},
	url = {http://arxiv.org/abs/2201.02177},
	abstract = {In this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efficiency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of "grokking" a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overfitting. We also study generalization as a function of dataset size and find that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the finite training dataset.},
	urldate = {2023-01-03},
	publisher = {arXiv},
	author = {Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
	month = jan,
	year = {2022},
	note = {arXiv:2201.02177 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\RY3DPYBZ\\Power et al. - 2022 - Grokking Generalization Beyond Overfitting on Sma.pdf:application/pdf;arXiv.org Snapshot:D\:\\Literatur\\Zotero\\storage\\4WPEXCKS\\2201.html:text/html},
}

@misc{johnson_eecs_2021,
	type = {Lecture {Notes} / {Slides}},
	title = {{EECS} 442: {Computer} {Vision}},
	url = {https://web.eecs.umich.edu/~justincj/teaching/eecs442/WI2021/},
	abstract = {This is an introduction to computer vision. Topics include: camera models, multi-view geometry, reconstruction, some low-level image processing, and high-level vision problems like object and scene recognition},
	author = {Johnson, Justin and Fouhey, David},
	year = {2021},
}

@book{prince_understanding_2023,
	title = {Understanding {Deep} {Learning}},
	url = {https://udlbook.github.io/udlbook/},
	publisher = {MIT Press},
	author = {Prince, Simon J. D.},
	year = {2023},
}

@article{phillips_speed_2015,
	title = {Speed of the {Human} {Brain}},
	url = {https://askabiologist.asu.edu/plosable/speed-human-brain},
	journal = {Ask A Biologist},
	author = {Phillips, Devin K},
	month = may,
	year = {2015},
}

@article{lee_unsupervised_2011,
	title = {Unsupervised learning of hierarchical representations with convolutional deep belief networks},
	volume = {54},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/2001269.2001295},
	doi = {10.1145/2001269.2001295},
	abstract = {There has been much interest in unsupervised learning of hierarchical generative models such as deep belief networks (DBNs); however, scaling such models to full-sized, high-dimensional images remains a difficult problem. To address this problem, we present the
              convolutional deep belief network
              , a hierarchical generative model that scales to realistic image sizes. This model is translation-invariant and supports efficient bottom-up and top-down probabilistic inference. Key to our approach is
              probabilistic max-pooling
              , a novel technique that shrinks the representations of higher layers in a probabilistically sound way. Our experiments show that the algorithm learns useful high-level visual features, such as object parts, from unlabeled images of objects and natural scenes. We demonstrate excellent performance on several visual recognition tasks and show that our model can perform hierarchical (bottom-up and top-down) inference over full-sized images.},
	language = {en},
	number = {10},
	urldate = {2023-03-24},
	journal = {Communications of the ACM},
	author = {Lee, Honglak and Grosse, Roger and Ranganath, Rajesh and Ng, Andrew Y.},
	month = oct,
	year = {2011},
	pages = {95--103},
}

@misc{austin_modern_2022,
	type = {Lecture {Notes} / {Slides}},
	title = {Modern {Computer} {Vision} and {Deep} {Learning} ({CS} 198-126)},
	url = {https://fluff-armadillo-037.notion.site/Modern-Computer-Vision-and-Deep-Learning-CS-198-126-b11006739378470fa67a9cf6594201e0},
	abstract = {Slide deck links will be linked on these entries as we go through the semester.},
	author = {Austin, Jake and Rajaraman, Arvind and Jain, Aryan and Viswanathan, Rohan and Alameddine, Ryan and Teo, Verona},
	year = {2022},
}

@misc{zou_object_2023,
	title = {Object {Detection} in 20 {Years}: {A} {Survey}},
	shorttitle = {Object {Detection} in 20 {Years}},
	url = {http://arxiv.org/abs/1905.05055},
	abstract = {Object detection, as of one the most fundamental and challenging problems in computer vision, has received great attention in recent years. Over the past two decades, we have seen a rapid technological evolution of object detection and its profound impact on the entire computer vision field. If we consider today's object detection technique as a revolution driven by deep learning, then back in the 1990s, we would see the ingenious thinking and long-term perspective design of early computer vision. This paper extensively reviews this fast-moving research field in the light of technical evolution, spanning over a quarter-century's time (from the 1990s to 2022). A number of topics have been covered in this paper, including the milestone detectors in history, detection datasets, metrics, fundamental building blocks of the detection system, speed-up techniques, and the recent state-of-the-art detection methods.},
	urldate = {2023-09-18},
	publisher = {arXiv},
	author = {Zou, Zhengxia and Chen, Keyan and Shi, Zhenwei and Guo, Yuhong and Ye, Jieping},
	month = jan,
	year = {2023},
	note = {arXiv:1905.05055 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\2RTGXU32\\Zou et al. - 2023 - Object Detection in 20 Years A Survey.pdf:application/pdf;arXiv.org Snapshot:D\:\\Literatur\\Zotero\\storage\\5RRQBJ8R\\1905.html:text/html},
}

@misc{hu_squeeze-and-excitation_2019,
	title = {Squeeze-and-{Excitation} {Networks}},
	url = {http://arxiv.org/abs/1709.01507},
	abstract = {The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the "Squeeze-and-Excitation" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251\%, surpassing the winning entry of 2016 by a relative improvement of {\textasciitilde}25\%. Models and code are available at https://github.com/hujie-frank/SENet.},
	urldate = {2023-09-23},
	publisher = {arXiv},
	author = {Hu, Jie and Shen, Li and Albanie, Samuel and Sun, Gang and Wu, Enhua},
	month = may,
	year = {2019},
	note = {arXiv:1709.01507 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\7XZLJX7M\\Hu et al. - 2019 - Squeeze-and-Excitation Networks.pdf:application/pdf;arXiv.org Snapshot:D\:\\Literatur\\Zotero\\storage\\W3IN4QUG\\1709.html:text/html},
}

@misc{li_visualizing_2018,
	title = {Visualizing the {Loss} {Landscape} of {Neural} {Nets}},
	url = {http://arxiv.org/abs/1712.09913},
	abstract = {Neural network training relies on our ability to find "good" minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effects on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple "filter normalization" method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.},
	urldate = {2023-09-23},
	publisher = {arXiv},
	author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
	month = nov,
	year = {2018},
	note = {arXiv:1712.09913 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\43CP8T4T\\Li et al. - 2018 - Visualizing the Loss Landscape of Neural Nets.pdf:application/pdf;arXiv.org Snapshot:D\:\\Literatur\\Zotero\\storage\\PXNU36J9\\1712.html:text/html},
}

@misc{woo_convnext_2023,
	title = {{ConvNeXt} {V2}: {Co}-designing and {Scaling} {ConvNets} with {Masked} {Autoencoders}},
	shorttitle = {{ConvNeXt} {V2}},
	url = {http://arxiv.org/abs/2301.00808},
	abstract = {Driven by improved architectures and better representation learning frameworks, the field of visual recognition has enjoyed rapid modernization and performance boost in the early 2020s. For example, modern ConvNets, represented by ConvNeXt, have demonstrated strong performance in various scenarios. While these models were originally designed for supervised learning with ImageNet labels, they can also potentially benefit from self-supervised learning techniques such as masked autoencoders (MAE). However, we found that simply combining these two approaches leads to subpar performance. In this paper, we propose a fully convolutional masked autoencoder framework and a new Global Response Normalization (GRN) layer that can be added to the ConvNeXt architecture to enhance inter-channel feature competition. This co-design of self-supervised learning techniques and architectural improvement results in a new model family called ConvNeXt V2, which significantly improves the performance of pure ConvNets on various recognition benchmarks, including ImageNet classification, COCO detection, and ADE20K segmentation. We also provide pre-trained ConvNeXt V2 models of various sizes, ranging from an efficient 3.7M-parameter Atto model with 76.7\% top-1 accuracy on ImageNet, to a 650M Huge model that achieves a state-of-the-art 88.9\% accuracy using only public training data.},
	urldate = {2023-09-23},
	publisher = {arXiv},
	author = {Woo, Sanghyun and Debnath, Shoubhik and Hu, Ronghang and Chen, Xinlei and Liu, Zhuang and Kweon, In So and Xie, Saining},
	month = jan,
	year = {2023},
	note = {arXiv:2301.00808 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\FFVKWB52\\Woo et al. - 2023 - ConvNeXt V2 Co-designing and Scaling ConvNets wit.pdf:application/pdf;arXiv.org Snapshot:D\:\\Literatur\\Zotero\\storage\\DUEHXC6Q\\2301.html:text/html},
}

@misc{zhou_objects_2019,
	title = {Objects as {Points}},
	url = {http://arxiv.org/abs/1904.07850},
	abstract = {Detection identifies objects as axis-aligned boxes in an image. Most successful object detectors enumerate a nearly exhaustive list of potential object locations and classify each. This is wasteful, inefficient, and requires additional post-processing. In this paper, we take a different approach. We model an object as a single point --- the center point of its bounding box. Our detector uses keypoint estimation to find center points and regresses to all other object properties, such as size, 3D location, orientation, and even pose. Our center point based approach, CenterNet, is end-to-end differentiable, simpler, faster, and more accurate than corresponding bounding box based detectors. CenterNet achieves the best speed-accuracy trade-off on the MS COCO dataset, with 28.1\% AP at 142 FPS, 37.4\% AP at 52 FPS, and 45.1\% AP with multi-scale testing at 1.4 FPS. We use the same approach to estimate 3D bounding box in the KITTI benchmark and human pose on the COCO keypoint dataset. Our method performs competitively with sophisticated multi-stage methods and runs in real-time.},
	urldate = {2023-09-23},
	publisher = {arXiv},
	author = {Zhou, Xingyi and Wang, Dequan and Krähenbühl, Philipp},
	month = apr,
	year = {2019},
	note = {arXiv:1904.07850 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\IW56QYV7\\Zhou et al. - 2019 - Objects as Points.pdf:application/pdf;arXiv.org Snapshot:D\:\\Literatur\\Zotero\\storage\\2QN7LA2V\\1904.html:text/html},
}

@misc{tian_fcos_2019,
	title = {{FCOS}: {Fully} {Convolutional} {One}-{Stage} {Object} {Detection}},
	shorttitle = {{FCOS}},
	url = {http://arxiv.org/abs/1904.01355},
	abstract = {We propose a fully convolutional one-stage object detector (FCOS) to solve object detection in a per-pixel prediction fashion, analogue to semantic segmentation. Almost all state-of-the-art object detectors such as RetinaNet, SSD, YOLOv3, and Faster R-CNN rely on pre-defined anchor boxes. In contrast, our proposed detector FCOS is anchor box free, as well as proposal free. By eliminating the predefined set of anchor boxes, FCOS completely avoids the complicated computation related to anchor boxes such as calculating overlapping during training. More importantly, we also avoid all hyper-parameters related to anchor boxes, which are often very sensitive to the final detection performance. With the only post-processing non-maximum suppression (NMS), FCOS with ResNeXt-64x4d-101 achieves 44.7\% in AP with single-model and single-scale testing, surpassing previous one-stage detectors with the advantage of being much simpler. For the first time, we demonstrate a much simpler and flexible detection framework achieving improved detection accuracy. We hope that the proposed FCOS framework can serve as a simple and strong alternative for many other instance-level tasks. Code is available at:Code is available at: https://tinyurl.com/FCOSv1},
	urldate = {2023-09-23},
	publisher = {arXiv},
	author = {Tian, Zhi and Shen, Chunhua and Chen, Hao and He, Tong},
	month = aug,
	year = {2019},
	note = {arXiv:1904.01355 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\MZHFB3U6\\Tian et al. - 2019 - FCOS Fully Convolutional One-Stage Object Detecti.pdf:application/pdf;arXiv.org Snapshot:D\:\\Literatur\\Zotero\\storage\\VC5TBIPQ\\1904.html:text/html},
}

@misc{carion_end_to_end_2020,
	title = {End-to-{End} {Object} {Detection} with {Transformers}},
	url = {http://arxiv.org/abs/2005.12872},
	abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
	urldate = {2023-09-23},
	publisher = {arXiv},
	author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
	month = may,
	year = {2020},
	note = {arXiv:2005.12872 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\SIZ29DWS\\Carion et al. - 2020 - End-to-End Object Detection with Transformers.pdf:application/pdf;arXiv.org Snapshot:D\:\\Literatur\\Zotero\\storage\\QE47XZ84\\2005.html:text/html},
}

@misc{kirillov_segment_2023,
	title = {Segment {Anything}},
	url = {http://arxiv.org/abs/2304.02643},
	abstract = {We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.},
	urldate = {2023-09-23},
	publisher = {arXiv},
	author = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Dollár, Piotr and Girshick, Ross},
	month = apr,
	year = {2023},
	note = {arXiv:2304.02643 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\2EYNNLPV\\Kirillov et al. - 2023 - Segment Anything.pdf:application/pdf;arXiv.org Snapshot:D\:\\Literatur\\Zotero\\storage\\QN5F5W37\\2304.html:text/html},
}

@misc{karras_alias-free_2021,
	title = {Alias-{Free} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/2106.12423},
	abstract = {We observe that despite their hierarchical convolutional nature, the synthesis process of typical generative adversarial networks depends on absolute pixel coordinates in an unhealthy manner. This manifests itself as, e.g., detail appearing to be glued to image coordinates instead of the surfaces of depicted objects. We trace the root cause to careless signal processing that causes aliasing in the generator network. Interpreting all signals in the network as continuous, we derive generally applicable, small architectural changes that guarantee that unwanted information cannot leak into the hierarchical synthesis process. The resulting networks match the FID of StyleGAN2 but differ dramatically in their internal representations, and they are fully equivariant to translation and rotation even at subpixel scales. Our results pave the way for generative models better suited for video and animation.},
	urldate = {2023-09-25},
	publisher = {arXiv},
	author = {Karras, Tero and Aittala, Miika and Laine, Samuli and Härkönen, Erik and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
	month = oct,
	year = {2021},
	note = {arXiv:2106.12423 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\EC5SJ7GD\\Karras et al. - 2021 - Alias-Free Generative Adversarial Networks.pdf:application/pdf;arXiv.org Snapshot:D\:\\Literatur\\Zotero\\storage\\X6RYWIAA\\2106.html:text/html},
}

@misc{pan_drag_2023,
	title = {Drag {Your} {GAN}: {Interactive} {Point}-based {Manipulation} on the {Generative} {Image} {Manifold}},
	shorttitle = {Drag {Your} {GAN}},
	url = {http://arxiv.org/abs/2305.10973},
	abstract = {Synthesizing visual content that meets users' needs often requires flexible and precise controllability of the pose, shape, expression, and layout of the generated objects. Existing approaches gain controllability of generative adversarial networks (GANs) via manually annotated training data or a prior 3D model, which often lack flexibility, precision, and generality. In this work, we study a powerful yet much less explored way of controlling GANs, that is, to "drag" any points of the image to precisely reach target points in a user-interactive manner, as shown in Fig.1. To achieve this, we propose DragGAN, which consists of two main components: 1) a feature-based motion supervision that drives the handle point to move towards the target position, and 2) a new point tracking approach that leverages the discriminative generator features to keep localizing the position of the handle points. Through DragGAN, anyone can deform an image with precise control over where pixels go, thus manipulating the pose, shape, expression, and layout of diverse categories such as animals, cars, humans, landscapes, etc. As these manipulations are performed on the learned generative image manifold of a GAN, they tend to produce realistic outputs even for challenging scenarios such as hallucinating occluded content and deforming shapes that consistently follow the object's rigidity. Both qualitative and quantitative comparisons demonstrate the advantage of DragGAN over prior approaches in the tasks of image manipulation and point tracking. We also showcase the manipulation of real images through GAN inversion.},
	urldate = {2023-09-25},
	publisher = {arXiv},
	author = {Pan, Xingang and Tewari, Ayush and Leimkühler, Thomas and Liu, Lingjie and Meka, Abhimitra and Theobalt, Christian},
	month = may,
	year = {2023},
	note = {arXiv:2305.10973 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\VVUN3GAW\\Pan et al. - 2023 - Drag Your GAN Interactive Point-based Manipulatio.pdf:application/pdf;arXiv.org Snapshot:D\:\\Literatur\\Zotero\\storage\\PA8WVY9P\\2305.html:text/html},
}

@misc{mildenhall_nerf_2020,
	title = {{NeRF}: {Representing} {Scenes} as {Neural} {Radiance} {Fields} for {View} {Synthesis}},
	shorttitle = {{NeRF}},
	url = {http://arxiv.org/abs/2003.08934},
	abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location \$(x,y,z)\$ and viewing direction \$({\textbackslash}theta, {\textbackslash}phi)\$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
	urldate = {2023-09-25},
	publisher = {arXiv},
	author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
	month = aug,
	year = {2020},
	note = {arXiv:2003.08934 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\GNBDWAPK\\Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf:application/pdf;arXiv.org Snapshot:D\:\\Literatur\\Zotero\\storage\\5XR2UDTV\\2003.html:text/html},
}

@inproceedings{viola_rapid_2001,
	address = {Kauai, HI, USA},
	title = {Rapid object detection using a boosted cascade of simple features},
	volume = {1},
	isbn = {978-0-7695-1272-3},
	url = {http://ieeexplore.ieee.org/document/990517/},
	doi = {10.1109/CVPR.2001.990517},
	urldate = {2023-10-06},
	booktitle = {Proceedings of the 2001 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}. {CVPR} 2001},
	publisher = {IEEE Comput. Soc},
	author = {Viola, P. and Jones, M.},
	year = {2001},
	pages = {I--511--I--518},
}

@article{jianbo_shi_normalized_2000,
	title = {Normalized cuts and image segmentation},
	volume = {22},
	issn = {01628828},
	url = {http://ieeexplore.ieee.org/document/868688/},
	doi = {10.1109/34.868688},
	number = {8},
	urldate = {2023-10-06},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {{Jianbo Shi} and Malik, J.},
	month = aug,
	year = {2000},
	pages = {888--905},
	file = {Submitted Version:D\:\\Literatur\\Zotero\\storage\\EEKN9535\\Jianbo Shi and Malik - 2000 - Normalized cuts and image segmentation.pdf:application/pdf},
}

@inproceedings{lowe_object_1999,
	address = {Kerkyra, Greece},
	title = {Object recognition from local scale-invariant features},
	isbn = {978-0-7695-0164-2},
	url = {http://ieeexplore.ieee.org/document/790410/},
	doi = {10.1109/ICCV.1999.790410},
	urldate = {2023-10-06},
	booktitle = {Proceedings of the {Seventh} {IEEE} {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Lowe, D.G.},
	year = {1999},
	pages = {1150--1157 vol.2},
}

@inproceedings{dalal_histograms_2005,
	address = {San Diego, CA, USA},
	title = {Histograms of {Oriented} {Gradients} for {Human} {Detection}},
	volume = {1},
	isbn = {978-0-7695-2372-9},
	url = {http://ieeexplore.ieee.org/document/1467360/},
	doi = {10.1109/CVPR.2005.177},
	urldate = {2023-10-06},
	booktitle = {2005 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR}'05)},
	publisher = {IEEE},
	author = {Dalal, N. and Triggs, B.},
	year = {2005},
	pages = {886--893},
	file = {Submitted Version:D\:\\Literatur\\Zotero\\storage\\MKIQ8S8M\\Dalal and Triggs - 2005 - Histograms of Oriented Gradients for Human Detecti.pdf:application/pdf},
}

@inproceedings{perronnin_large-scale_2010,
	address = {San Francisco, CA, USA},
	title = {Large-scale image retrieval with compressed {Fisher} vectors},
	isbn = {978-1-4244-6984-0},
	url = {http://ieeexplore.ieee.org/document/5540009/},
	doi = {10.1109/CVPR.2010.5540009},
	urldate = {2023-10-06},
	booktitle = {2010 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Perronnin, Florent and Liu, Yan and Sanchez, Jorge and Poirier, Herve},
	month = jun,
	year = {2010},
	pages = {3384--3391},
}

@article{canny_computational_1986,
	title = {A {Computational} {Approach} to {Edge} {Detection}},
	volume = {PAMI-8},
	issn = {0162-8828},
	url = {https://ieeexplore.ieee.org/document/4767851},
	doi = {10.1109/TPAMI.1986.4767851},
	number = {6},
	urldate = {2023-10-06},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Canny, John},
	month = nov,
	year = {1986},
	pages = {679--698},
}

@inproceedings{crall_hotspotter_2013,
	address = {Clearwater Beach, FL, USA},
	title = {{HotSpotter} \&\#x2014; {Patterned} species instance recognition},
	isbn = {978-1-4673-5054-9 978-1-4673-5053-2 978-1-4673-5052-5},
	url = {https://ieeexplore.ieee.org/document/6475023},
	doi = {10.1109/WACV.2013.6475023},
	urldate = {2023-10-06},
	booktitle = {2013 {IEEE} {Workshop} on {Applications} of {Computer} {Vision} ({WACV})},
	publisher = {IEEE},
	author = {Crall, Jonathan P. and Stewart, Charles V. and Berger-Wolf, Tanya Y. and Rubenstein, Daniel I. and Sundaresan, Siva R.},
	month = jan,
	year = {2013},
	pages = {230--237},
}

@misc{everingham_pascal_2007,
	title = {The {PASCAL} {Visual} {Object} {Classes} {Challenge} 2007 ({VOC2007}) {Results}},
	url = {http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html},
	author = {Everingham, M. and Van Gool, L. and Williams, C. K. I. and Winn, J. and Zisserman, A.},
	year = {2007},
}

@misc{johnson_eecs_2022,
	type = {Lecture {Notes} / {Slides}},
	title = {{EECS} 498.008 / 598.008 {Deep} {Learning} for {Computer} {Vision}},
	url = {https://web.eecs.umich.edu/~justincj/teaching/eecs498/WI2022/},
	author = {Johnson, Justin},
	year = {2022},
}

@misc{li_cs231n_2023,
	type = {Lecture {Notes}},
	title = {{CS231n} {Convolutional} {Neural} {Networks} for {Visual} {Recognition}},
	url = {http://cs231n.stanford.edu/schedule.html},
	urldate = {2022-10-24},
	author = {Li, Fei-Fei},
	year = {2023},
}

@misc{santurkar_how_2019,
	title = {How {Does} {Batch} {Normalization} {Help} {Optimization}?},
	url = {http://arxiv.org/abs/1805.11604},
	abstract = {Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm's effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers' input distributions during training to reduce the so-called "internal covariate shift". In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.},
	urldate = {2023-10-16},
	publisher = {arXiv},
	author = {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander},
	month = apr,
	year = {2019},
	note = {arXiv:1805.11604 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\6G8M7FXX\\Santurkar et al. - 2019 - How Does Batch Normalization Help Optimization.pdf:application/pdf;arXiv.org Snapshot:D\:\\Literatur\\Zotero\\storage\\UYL86U4Z\\1805.html:text/html},
}

@misc{brock_high-performance_2021,
	title = {High-{Performance} {Large}-{Scale} {Image} {Recognition} {Without} {Normalization}},
	url = {http://arxiv.org/abs/2102.06171},
	doi = {10.48550/arXiv.2102.06171},
	abstract = {Batch normalization is a key component of most image classification models, but it has many undesirable properties stemming from its dependence on the batch size and interactions between examples. Although recent work has succeeded in training deep ResNets without normalization layers, these models do not match the test accuracies of the best batch-normalized networks, and are often unstable for large learning rates or strong data augmentations. In this work, we develop an adaptive gradient clipping technique which overcomes these instabilities, and design a significantly improved class of Normalizer-Free ResNets. Our smaller models match the test accuracy of an EfficientNet-B7 on ImageNet while being up to 8.7x faster to train, and our largest models attain a new state-of-the-art top-1 accuracy of 86.5\%. In addition, Normalizer-Free models attain significantly better performance than their batch-normalized counterparts when finetuning on ImageNet after large-scale pre-training on a dataset of 300 million labeled images, with our best models obtaining an accuracy of 89.2\%. Our code is available at https://github.com/deepmind/ deepmind-research/tree/master/nfnets},
	urldate = {2023-10-29},
	publisher = {arXiv},
	author = {Brock, Andrew and De, Soham and Smith, Samuel L. and Simonyan, Karen},
	month = feb,
	year = {2021},
	note = {arXiv:2102.06171 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\HYY9SSL9\\Brock et al. - 2021 - High-Performance Large-Scale Image Recognition Without Normalization.pdf:application/pdf},
}

@misc{smith_convnets_2023,
	title = {{ConvNets} {Match} {Vision} {Transformers} at {Scale}},
	url = {http://arxiv.org/abs/2310.16764},
	abstract = {Many researchers believe that ConvNets perform well on small or moderately sized datasets, but are not competitive with Vision Transformers when given access to datasets on the web-scale. We challenge this belief by evaluating a performant ConvNet architecture pre-trained on JFT-4B, a large labelled dataset of images often used for training foundation models. We consider pre-training compute budgets between 0.4k and 110k TPU-v4 core compute hours, and train a series of networks of increasing depth and width from the NFNet model family. We observe a log-log scaling law between held out loss and compute budget. After fine-tuning on ImageNet, NFNets match the reported performance of Vision Transformers with comparable compute budgets. Our strongest fine-tuned model achieves a Top-1 accuracy of 90.4\%.},
	urldate = {2023-10-30},
	publisher = {arXiv},
	author = {Smith, Samuel L. and Brock, Andrew and Berrada, Leonard and De, Soham},
	month = oct,
	year = {2023},
	note = {arXiv:2310.16764 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\UEBY4T6T\\Smith et al. - 2023 - ConvNets Match Vision Transformers at Scale.pdf:application/pdf},
}

@article{srivastava_dropout_2014,
	title = {Dropout: {A} {Simple} {Way} to {Prevent} {Neural} {Networks} from {Overfitting}},
	volume = {15},
	issn = {1532-4435},
	abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
	number = {1},
	journal = {J. Mach. Learn. Res.},
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	month = jan,
	year = {2014},
	note = {Publisher: JMLR.org},
	keywords = {deep learning, neural networks, model combination, regularization},
	pages = {1929--1958},
}

@misc{kubilius_ventral_2017,
	title = {Ventral visual stream},
	url = {https://figshare.com/articles/figure/Ventral_visual_stream/106794},
	author = {Kubilius, Jonas},
	month = jan,
	year = {2017},
}

@article{barbhuiya_cnn_2021,
	title = {{CNN} based feature extraction and classification for sign language},
	volume = {80},
	issn = {1380-7501, 1573-7721},
	url = {https://link.springer.com/10.1007/s11042-020-09829-y},
	doi = {10.1007/s11042-020-09829-y},
	language = {en},
	number = {2},
	urldate = {2023-11-30},
	journal = {Multimedia Tools and Applications},
	author = {Barbhuiya, Abul Abbas and Karsh, Ram Kumar and Jain, Rahul},
	month = jan,
	year = {2021},
	pages = {3051--3069},
}

@article{llamas_classification_2017,
	title = {Classification of {Architectural} {Heritage} {Images} {Using} {Deep} {Learning} {Techniques}},
	volume = {7},
	issn = {2076-3417},
	url = {http://www.mdpi.com/2076-3417/7/10/992},
	doi = {10.3390/app7100992},
	language = {en},
	number = {10},
	urldate = {2023-11-30},
	journal = {Applied Sciences},
	author = {Llamas, Jose and M. Lerones, Pedro and Medina, Roberto and Zalama, Eduardo and Gómez-García-Bermejo, Jaime},
	month = sep,
	year = {2017},
	pages = {992},
	file = {Full Text:D\:\\Literatur\\Zotero\\storage\\459SRCPJ\\Llamas et al. - 2017 - Classification of Architectural Heritage Images Using Deep Learning Techniques.pdf:application/pdf},
}

@misc{lin_focal_2018,
	title = {Focal {Loss} for {Dense} {Object} {Detection}},
	url = {http://arxiv.org/abs/1708.02002},
	abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
	urldate = {2023-12-07},
	publisher = {arXiv},
	author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollár, Piotr},
	month = feb,
	year = {2018},
	note = {arXiv:1708.02002 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\LSWAHBJA\\Lin et al. - 2018 - Focal Loss for Dense Object Detection.pdf:application/pdf;arXiv.org Snapshot:D\:\\Literatur\\Zotero\\storage\\QXR284T5\\1708.html:text/html},
}

@misc{lin_feature_2017,
	title = {Feature {Pyramid} {Networks} for {Object} {Detection}},
	url = {http://arxiv.org/abs/1612.03144},
	abstract = {Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.},
	urldate = {2023-12-07},
	publisher = {arXiv},
	author = {Lin, Tsung-Yi and Dollár, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
	month = apr,
	year = {2017},
	note = {arXiv:1612.03144 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\9HT4J5WP\\Lin et al. - 2017 - Feature Pyramid Networks for Object Detection.pdf:application/pdf;arXiv.org Snapshot:D\:\\Literatur\\Zotero\\storage\\I6YPJELN\\1612.html:text/html},
}

@misc{tai_pca-aided_2017,
	title = {{PCA}-aided {Fully} {Convolutional} {Networks} for {Semantic} {Segmentation} of {Multi}-channel {fMRI}},
	url = {http://arxiv.org/abs/1610.01732},
	abstract = {Semantic segmentation of functional magnetic resonance imaging (fMRI) makes great sense for pathology diagnosis and decision system of medical robots. The multi-channel fMRI provides more information of the pathological features. But the increased amount of data causes complexity in feature detections. This paper proposes a principal component analysis (PCA)-aided fully convolutional network to particularly deal with multi-channel fMRI. We transfer the learned weights of contemporary classification networks to the segmentation task by fine-tuning. The results of the convolutional network are compared with various methods e.g. k-NN. A new labeling strategy is proposed to solve the semantic segmentation problem with unclear boundaries. Even with a small-sized training dataset, the test results demonstrate that our model outperforms other pathological feature detection methods. Besides, its forward inference only takes 90 milliseconds for a single set of fMRI data. To our knowledge, this is the first time to realize pixel-wise labeling of multi-channel magnetic resonance image using FCN.},
	urldate = {2023-12-15},
	publisher = {arXiv},
	author = {Tai, Lei and Ye, Haoyang and Ye, Qiong and Liu, Ming},
	month = jul,
	year = {2017},
	note = {arXiv:1610.01732 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\XU5DBEF8\\Tai et al. - 2017 - PCA-aided Fully Convolutional Networks for Semantic Segmentation of Multi-channel fMRI.pdf:application/pdf;arXiv.org Snapshot:D\:\\Literatur\\Zotero\\storage\\4UD8XTL2\\1610.html:text/html},
}

@book{chollet_deep_2021,
	address = {Shelter Island},
	edition = {Second edition},
	title = {Deep learning with {Python}},
	isbn = {978-1-61729-686-4},
	abstract = {Recent innovations in deep learning unlock exciting new software capabilities like automated language translation, image recognition, and more. Deep learning is quickly becoming essential knowledge for every software developer, and modern tools like Keras and TensorFlow put it within your reach-- even if you have no background in mathematics or data science. This book shows you how to get started. "Deep learning with Python, second edition" introduces the field of deep learning using Python and the powerful Keras library. In this revised and expanded new edition, Keras creator François Chollet offers insights for both novice and experienced machine learning practitioners. As you move through this book, you'll build your understanding through intuitive explanations, crisp illustrations, and clear examples. You'll quickly pick up the skills you need to start developing deep-learning applications.--},
	publisher = {Manning Publications},
	author = {Chollet, François},
	year = {2021},
	note = {OCLC: on1289290141},
	keywords = {Machine learning, Neural networks (Computer science), Python (Computer program language)},
}

@article{ma_segment_2024,
	title = {Segment anything in medical images},
	volume = {15},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-024-44824-z},
	doi = {10.1038/s41467-024-44824-z},
	abstract = {Abstract
            Medical image segmentation is a critical component in clinical practice, facilitating accurate diagnosis, treatment planning, and disease monitoring. However, existing methods, often tailored to specific modalities or disease types, lack generalizability across the diverse spectrum of medical image segmentation tasks. Here we present MedSAM, a foundation model designed for bridging this gap by enabling universal medical image segmentation. The model is developed on a large-scale medical image dataset with 1,570,263 image-mask pairs, covering 10 imaging modalities and over 30 cancer types. We conduct a comprehensive evaluation on 86 internal validation tasks and 60 external validation tasks, demonstrating better accuracy and robustness than modality-wise specialist models. By delivering accurate and efficient segmentation across a wide spectrum of tasks, MedSAM holds significant potential to expedite the evolution of diagnostic tools and the personalization of treatment plans.},
	language = {en},
	number = {1},
	urldate = {2024-09-16},
	journal = {Nature Communications},
	author = {Ma, Jun and He, Yuting and Li, Feifei and Han, Lin and You, Chenyu and Wang, Bo},
	month = jan,
	year = {2024},
	pages = {654},
	file = {Full Text:D\:\\Literatur\\Zotero\\storage\\YK4A68QH\\Ma et al. - 2024 - Segment anything in medical images.pdf:application/pdf},
}

@article{hani_minneapple_2020,
	title = {{MinneApple}: {A} {Benchmark} {Dataset} for {Apple} {Detection} and {Segmentation}},
	volume = {5},
	issn = {2377-3766, 2377-3774},
	shorttitle = {{MinneApple}},
	url = {http://arxiv.org/abs/1909.06441},
	doi = {10.1109/LRA.2020.2965061},
	abstract = {In this work, we present a new dataset to advance the state-of-the-art in fruit detection, segmentation, and counting in orchard environments. While there has been significant recent interest in solving these problems, the lack of a unified dataset has made it difficult to compare results. We hope to enable direct comparisons by providing a large variety of high-resolution images acquired in orchards, together with human annotations of the fruit on trees. The fruits are labeled using polygonal masks for each object instance to aid in precise object detection, localization, and segmentation. Additionally, we provide data for patch-based counting of clustered fruits. Our dataset contains over 41, 000 annotated object instances in 1000 images. We present a detailed overview of the dataset together with baseline performance analysis for bounding box detection, segmentation, and fruit counting as well as representative results for yield estimation. We make this dataset publicly available and host a CodaLab challenge to encourage comparison of results on a common dataset. To download the data and learn more about MinneApple please see the project website: http://rsn.cs.umn.edu/index.php/MinneApple. Up to date information is available online.},
	number = {2},
	urldate = {2024-09-16},
	journal = {IEEE Robotics and Automation Letters},
	author = {Häni, Nicolai and Roy, Pravakar and Isler, Volkan},
	month = apr,
	year = {2020},
	note = {arXiv:1909.06441 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {852--858},
	file = {arXiv Fulltext PDF:D\:\\Literatur\\Zotero\\storage\\93FIARWV\\Häni et al. - 2020 - MinneApple A Benchmark Dataset for Apple Detectio.pdf:application/pdf;arXiv.org Snapshot:D\:\\Literatur\\Zotero\\storage\\L497YTA9\\1909.html:text/html},
}

@techreport{breitenmoser-wursten_projekt_2024,
	title = {Das {Projekt} {LUNO} - {Abschlussbericht}},
	url = {https://portal.issn.org/#},
	number = {121},
	institution = {Stiftung KORA},
	author = {Breitenmoser-Würsten, Christine and Zimmermann, Fridolin and Vogt, Kristina and von Arx, Manuela and Signe, Sven and Stauffer, Christian},
	year = {2024},
	note = {3042-4488},
}

@misc{li_blip-2_2023,
	title = {{BLIP}-2: {Bootstrapping} {Language}-{Image} {Pre}-training with {Frozen} {Image} {Encoders} and {Large} {Language} {Models}},
	shorttitle = {{BLIP}-2},
	url = {http://arxiv.org/abs/2301.12597},
	abstract = {The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7\% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.},
	urldate = {2024-10-27},
	publisher = {arXiv},
	author = {Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
	month = jun,
	year = {2023},
	note = {arXiv:2301.12597 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:D\:\\Literatur\\Zotero\\storage\\DXD9Z7HC\\Li et al. - 2023 - BLIP-2 Bootstrapping Language-Image Pre-training .pdf:application/pdf;Snapshot:D\:\\Literatur\\Zotero\\storage\\L83IL324\\2301.html:text/html},
}

@misc{alayrac_flamingo_2022,
	title = {Flamingo: a {Visual} {Language} {Model} for {Few}-{Shot} {Learning}},
	shorttitle = {Flamingo},
	url = {http://arxiv.org/abs/2204.14198},
	abstract = {Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.},
	urldate = {2024-10-27},
	publisher = {arXiv},
	author = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katie and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob and Borgeaud, Sebastian and Brock, Andrew and Nematzadeh, Aida and Sharifzadeh, Sahand and Binkowski, Mikolaj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Karen},
	month = nov,
	year = {2022},
	note = {arXiv:2204.14198 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:D\:\\Literatur\\Zotero\\storage\\4WXM5CL5\\Alayrac et al. - 2022 - Flamingo a Visual Language Model for Few-Shot Lea.pdf:application/pdf;Snapshot:D\:\\Literatur\\Zotero\\storage\\PVIKXHZ3\\2204.html:text/html},
}

@article{araujo_computing_2019,
	title = {Computing {Receptive} {Fields} of {Convolutional} {Neural} {Networks}},
	volume = {4},
	issn = {2476-0757},
	url = {https://distill.pub/2019/computing-receptive-fields},
	doi = {10.23915/distill.00021},
	number = {11},
	urldate = {2024-10-29},
	journal = {Distill},
	author = {Araujo, Andr\&eacute; and Norris, Wade and Sim, Jack},
	month = nov,
	year = {2019},
	pages = {10.23915/distill.00021},
}

@misc{van_der_werff_cnn_2024,
	title = {{CNN} vs. {Vision} {Transformer}: {A} {Practitioner}'s {Guide} to {Selecting} the {Right} {Model}},
	url = {https://tobiasvanderwerff.github.io/2024/05/15/cnn-vs-vit.html},
	author = {van der Werff, Tobias},
	month = may,
	year = {2024},
}

@misc{naseer_intriguing_2021,
	title = {Intriguing {Properties} of {Vision} {Transformers}},
	url = {http://arxiv.org/abs/2105.10497},
	abstract = {Vision transformers (ViT) have demonstrated impressive performance across various machine vision problems. These models are based on multi-head self-attention mechanisms that can flexibly attend to a sequence of image patches to encode contextual cues. An important question is how such flexibility in attending image-wide context conditioned on a given patch can facilitate handling nuisances in natural images e.g., severe occlusions, domain shifts, spatial permutations, adversarial and natural perturbations. We systematically study this question via an extensive set of experiments encompassing three ViT families and comparisons with a high-performing convolutional neural network (CNN). We show and analyze the following intriguing properties of ViT: (a) Transformers are highly robust to severe occlusions, perturbations and domain shifts, e.g., retain as high as 60\% top-1 accuracy on ImageNet even after randomly occluding 80\% of the image content. (b) The robust performance to occlusions is not due to a bias towards local textures, and ViTs are significantly less biased towards textures compared to CNNs. When properly trained to encode shape-based features, ViTs demonstrate shape recognition capability comparable to that of human visual system, previously unmatched in the literature. (c) Using ViTs to encode shape representation leads to an interesting consequence of accurate semantic segmentation without pixel-level supervision. (d) Off-the-shelf features from a single ViT model can be combined to create a feature ensemble, leading to high accuracy rates across a range of classification datasets in both traditional and few-shot learning paradigms. We show effective features of ViTs are due to flexible and dynamic receptive fields possible via the self-attention mechanism.},
	urldate = {2024-10-30},
	publisher = {arXiv},
	author = {Naseer, Muzammal and Ranasinghe, Kanchana and Khan, Salman and Hayat, Munawar and Khan, Fahad Shahbaz and Yang, Ming-Hsuan},
	month = nov,
	year = {2021},
	note = {arXiv:2105.10497 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:D\:\\Literatur\\Zotero\\storage\\3B8HKM4D\\Naseer et al. - 2021 - Intriguing Properties of Vision Transformers.pdf:application/pdf;Snapshot:D\:\\Literatur\\Zotero\\storage\\6CQTEJAP\\2105.html:text/html},
}

@misc{pinto_impartial_2022,
	title = {An {Impartial} {Take} to the {CNN} vs {Transformer} {Robustness} {Contest}},
	url = {http://arxiv.org/abs/2207.11347},
	abstract = {Following the surge of popularity of Transformers in Computer Vision, several studies have attempted to determine whether they could be more robust to distribution shifts and provide better uncertainty estimates than Convolutional Neural Networks (CNNs). The almost unanimous conclusion is that they are, and it is often conjectured more or less explicitly that the reason of this supposed superiority is to be attributed to the self-attention mechanism. In this paper we perform extensive empirical analyses showing that recent state-of-the-art CNNs (particularly, ConvNeXt) can be as robust and reliable or even sometimes more than the current state-of-the-art Transformers. However, there is no clear winner. Therefore, although it is tempting to state the definitive superiority of one family of architectures over another, they seem to enjoy similar extraordinary performances on a variety of tasks while also suffering from similar vulnerabilities such as texture, background, and simplicity biases.},
	urldate = {2024-10-30},
	publisher = {arXiv},
	author = {Pinto, Francesco and Torr, Philip H. S. and Dokania, Puneet K.},
	month = jul,
	year = {2022},
	note = {arXiv:2207.11347 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:D\:\\Literatur\\Zotero\\storage\\2BVQBPVW\\Pinto et al. - 2022 - An Impartial Take to the CNN vs Transformer Robust.pdf:application/pdf;Snapshot:D\:\\Literatur\\Zotero\\storage\\GHDWLEK2\\2207.html:text/html},
}

@inproceedings{esser_scaling_2024,
	title = {Scaling {Rectified} {Flow} {Transformers} for {High}-{Resolution} {Image} {Synthesis}},
	url = {https://openreview.net/forum?id=FPnUhsQJ5B},
	booktitle = {Forty-first {International} {Conference} on {Machine} {Learning}},
	author = {Esser, Patrick and Kulal, Sumith and Blattmann, Andreas and Entezari, Rahim and Müller, Jonas and Saini, Harry and Levi, Yam and Lorenz, Dominik and Sauer, Axel and Boesel, Frederic and Podell, Dustin and Dockhorn, Tim and English, Zion and Rombach, Robin},
	year = {2024},
}

@misc{ho_denoising_2020,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	url = {http://arxiv.org/abs/2006.11239},
	doi = {10.48550/arXiv.2006.11239},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
	urldate = {2025-04-30},
	publisher = {arXiv},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	month = dec,
	year = {2020},
	note = {arXiv:2006.11239 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:D\:\\Literatur\\Zotero\\storage\\DLUNNSBI\\Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf:application/pdf;Snapshot:D\:\\Literatur\\Zotero\\storage\\7VWZEBSZ\\2006.html:text/html},
}

@misc{karras_style-based_2019,
	title = {A {Style}-{Based} {Generator} {Architecture} for {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1812.04948},
	doi = {10.48550/arXiv.1812.04948},
	abstract = {We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.},
	urldate = {2025-05-01},
	publisher = {arXiv},
	author = {Karras, Tero and Laine, Samuli and Aila, Timo},
	month = mar,
	year = {2019},
	note = {arXiv:1812.04948 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Preprint PDF:D\:\\Literatur\\Zotero\\storage\\FWKTTSJ6\\Karras et al. - 2019 - A Style-Based Generator Architecture for Generativ.pdf:application/pdf;Snapshot:D\:\\Literatur\\Zotero\\storage\\J8KVYIBX\\1812.html:text/html},
}

@misc{ledig_photo-realistic_2017,
	title = {Photo-{Realistic} {Single} {Image} {Super}-{Resolution} {Using} a {Generative} {Adversarial} {Network}},
	url = {http://arxiv.org/abs/1609.04802},
	doi = {10.48550/arXiv.1609.04802},
	abstract = {Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.},
	urldate = {2025-05-01},
	publisher = {arXiv},
	author = {Ledig, Christian and Theis, Lucas and Huszar, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and Shi, Wenzhe},
	month = may,
	year = {2017},
	note = {arXiv:1609.04802 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	file = {Preprint PDF:D\:\\Literatur\\Zotero\\storage\\JE6Y66VS\\Ledig et al. - 2017 - Photo-Realistic Single Image Super-Resolution Usin.pdf:application/pdf;Snapshot:D\:\\Literatur\\Zotero\\storage\\E9XLUY7I\\1609.html:text/html},
}

@misc{hessel_clipscore_2022,
	title = {{CLIPScore}: {A} {Reference}-free {Evaluation} {Metric} for {Image} {Captioning}},
	shorttitle = {{CLIPScore}},
	url = {http://arxiv.org/abs/2104.08718},
	doi = {10.48550/arXiv.2104.08718},
	abstract = {Image captioning has conventionally relied on reference-based automatic evaluations, where machine captions are compared against captions written by humans. This is in contrast to the reference-free manner in which humans assess caption quality. In this paper, we report the surprising empirical finding that CLIP (Radford et al., 2021), a cross-modal model pretrained on 400M image+caption pairs from the web, can be used for robust automatic evaluation of image captioning without the need for references. Experiments spanning several corpora demonstrate that our new reference-free metric, CLIPScore, achieves the highest correlation with human judgements, outperforming existing reference-based metrics like CIDEr and SPICE. Information gain experiments demonstrate that CLIPScore, with its tight focus on image-text compatibility, is complementary to existing reference-based metrics that emphasize text-text similarities. Thus, we also present a reference-augmented version, RefCLIPScore, which achieves even higher correlation. Beyond literal description tasks, several case studies reveal domains where CLIPScore performs well (clip-art images, alt-text rating), but also where it is relatively weaker in comparison to reference-based metrics, e.g., news captions that require richer contextual knowledge.},
	urldate = {2025-05-01},
	publisher = {arXiv},
	author = {Hessel, Jack and Holtzman, Ari and Forbes, Maxwell and Bras, Ronan Le and Choi, Yejin},
	month = mar,
	year = {2022},
	note = {arXiv:2104.08718 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language},
	file = {Preprint PDF:D\:\\Literatur\\Zotero\\storage\\VQWGHF2A\\Hessel et al. - 2022 - CLIPScore A Reference-free Evaluation Metric for .pdf:application/pdf;Snapshot:D\:\\Literatur\\Zotero\\storage\\4EISCGEV\\2104.html:text/html},
}

@misc{hu_lora_2021,
	title = {{LoRA}: {Low}-{Rank} {Adaptation} of {Large} {Language} {Models}},
	shorttitle = {{LoRA}},
	url = {http://arxiv.org/abs/2106.09685},
	doi = {10.48550/arXiv.2106.09685},
	abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
	urldate = {2025-05-01},
	publisher = {arXiv},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	month = oct,
	year = {2021},
	note = {arXiv:2106.09685 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:D\:\\Literatur\\Zotero\\storage\\IJWPSWGJ\\Hu et al. - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf:application/pdf;Snapshot:D\:\\Literatur\\Zotero\\storage\\RPT55F82\\2106.html:text/html},
}

@misc{alemohammad_self-consuming_2023,
	title = {Self-{Consuming} {Generative} {Models} {Go} {MAD}},
	url = {http://arxiv.org/abs/2307.01850},
	doi = {10.48550/arXiv.2307.01850},
	abstract = {Seismic advances in generative AI algorithms for imagery, text, and other data types has led to the temptation to use synthetic data to train next-generation models. Repeating this process creates an autophagous (self-consuming) loop whose properties are poorly understood. We conduct a thorough analytical and empirical analysis using state-of-the-art generative image models of three families of autophagous loops that differ in how fixed or fresh real training data is available through the generations of training and in whether the samples from previous generation models have been biased to trade off data quality versus diversity. Our primary conclusion across all scenarios is that without enough fresh real data in each generation of an autophagous loop, future generative models are doomed to have their quality (precision) or diversity (recall) progressively decrease. We term this condition Model Autophagy Disorder (MAD), making analogy to mad cow disease.},
	urldate = {2025-05-01},
	publisher = {arXiv},
	author = {Alemohammad, Sina and Casco-Rodriguez, Josue and Luzi, Lorenzo and Humayun, Ahmed Imtiaz and Babaei, Hossein and LeJeune, Daniel and Siahkoohi, Ali and Baraniuk, Richard G.},
	month = jul,
	year = {2023},
	note = {arXiv:2307.01850 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:D\:\\Literatur\\Zotero\\storage\\FRIBWSSY\\Alemohammad et al. - 2023 - Self-Consuming Generative Models Go MAD.pdf:application/pdf;Snapshot:D\:\\Literatur\\Zotero\\storage\\DREQ4HES\\2307.html:text/html},
}

@misc{esser_taming_2021,
	title = {Taming {Transformers} for {High}-{Resolution} {Image} {Synthesis}},
	url = {http://arxiv.org/abs/2012.09841},
	doi = {10.48550/arXiv.2012.09841},
	abstract = {Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers and obtain the state of the art among autoregressive models on class-conditional ImageNet. Code and pretrained models can be found at https://github.com/CompVis/taming-transformers .},
	urldate = {2025-05-02},
	publisher = {arXiv},
	author = {Esser, Patrick and Rombach, Robin and Ommer, Björn},
	month = jun,
	year = {2021},
	note = {arXiv:2012.09841 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:D\:\\Literatur\\Zotero\\storage\\R23XE89U\\Esser et al. - 2021 - Taming Transformers for High-Resolution Image Synt.pdf:application/pdf;Snapshot:D\:\\Literatur\\Zotero\\storage\\RTAC9VPM\\2012.html:text/html},
}

@misc{zhou_transfusion_2024,
	title = {Transfusion: {Predict} the {Next} {Token} and {Diffuse} {Images} with {One} {Multi}-{Modal} {Model}},
	shorttitle = {Transfusion},
	url = {http://arxiv.org/abs/2408.11039},
	doi = {10.48550/arXiv.2408.11039},
	abstract = {We introduce Transfusion, a recipe for training a multi-modal model over discrete and continuous data. Transfusion combines the language modeling loss function (next token prediction) with diffusion to train a single transformer over mixed-modality sequences. We pretrain multiple Transfusion models up to 7B parameters from scratch on a mixture of text and image data, establishing scaling laws with respect to a variety of uni- and cross-modal benchmarks. Our experiments show that Transfusion scales significantly better than quantizing images and training a language model over discrete image tokens. By introducing modality-specific encoding and decoding layers, we can further improve the performance of Transfusion models, and even compress each image to just 16 patches. We further demonstrate that scaling our Transfusion recipe to 7B parameters and 2T multi-modal tokens produces a model that can generate images and text on a par with similar scale diffusion models and language models, reaping the benefits of both worlds.},
	urldate = {2025-05-02},
	publisher = {arXiv},
	author = {Zhou, Chunting and Yu, Lili and Babu, Arun and Tirumala, Kushal and Yasunaga, Michihiro and Shamis, Leonid and Kahn, Jacob and Ma, Xuezhe and Zettlemoyer, Luke and Levy, Omer},
	month = aug,
	year = {2024},
	note = {arXiv:2408.11039 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:D\:\\Literatur\\Zotero\\storage\\KTKSSUUM\\Zhou et al. - 2024 - Transfusion Predict the Next Token and Diffuse Im.pdf:application/pdf;Snapshot:D\:\\Literatur\\Zotero\\storage\\2LS4LI65\\2408.html:text/html},
}

@misc{beery_6830068301_2024,
	type = {Lecture {Notes}},
	title = {6.8300/6.8301: {Advances} in {Computer} {Vision}},
	url = {https://advances-in-vision.github.io/index.html},
	author = {Beery, Last and He, Kaiming and Lukovic Konakovic, Mina and Sitzmann, Vincent},
	month = feb,
	year = {2024},
}

@misc{bengio_representation_2014,
	title = {Representation {Learning}: {A} {Review} and {New} {Perspectives}},
	shorttitle = {Representation {Learning}},
	url = {http://arxiv.org/abs/1206.5538},
	doi = {10.48550/arXiv.1206.5538},
	abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
	urldate = {2025-09-22},
	publisher = {arXiv},
	author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	month = apr,
	year = {2014},
	note = {arXiv:1206.5538 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:D\:\\Literatur\\Zotero\\storage\\XYBF6HCK\\Bengio et al. - 2014 - Representation Learning A Review and New Perspectives.pdf:application/pdf;Snapshot:D\:\\Literatur\\Zotero\\storage\\LNT3J45Z\\1206.html:text/html},
}

@misc{simeoni_dinov3_2025,
	title = {{DINOv3}},
	url = {http://arxiv.org/abs/2508.10104},
	doi = {10.48550/arXiv.2508.10104},
	abstract = {Self-supervised learning holds the promise of eliminating the need for manual data annotation, enabling models to scale effortlessly to massive datasets and larger architectures. By not being tailored to specific tasks or domains, this training paradigm has the potential to learn visual representations from diverse sources, ranging from natural to aerial images -- using a single algorithm. This technical report introduces DINOv3, a major milestone toward realizing this vision by leveraging simple yet effective strategies. First, we leverage the benefit of scaling both dataset and model size by careful data preparation, design, and optimization. Second, we introduce a new method called Gram anchoring, which effectively addresses the known yet unsolved issue of dense feature maps degrading during long training schedules. Finally, we apply post-hoc strategies that further enhance our models' flexibility with respect to resolution, model size, and alignment with text. As a result, we present a versatile vision foundation model that outperforms the specialized state of the art across a broad range of settings, without fine-tuning. DINOv3 produces high-quality dense features that achieve outstanding performance on various vision tasks, significantly surpassing previous self- and weakly-supervised foundation models. We also share the DINOv3 suite of vision models, designed to advance the state of the art on a wide spectrum of tasks and data by providing scalable solutions for diverse resource constraints and deployment scenarios.},
	urldate = {2025-09-25},
	publisher = {arXiv},
	author = {Siméoni, Oriane and Vo, Huy V. and Seitzer, Maximilian and Baldassarre, Federico and Oquab, Maxime and Jose, Cijo and Khalidov, Vasil and Szafraniec, Marc and Yi, Seungeun and Ramamonjisoa, Michaël and Massa, Francisco and Haziza, Daniel and Wehrstedt, Luca and Wang, Jianyuan and Darcet, Timothée and Moutakanni, Théo and Sentana, Leonel and Roberts, Claire and Vedaldi, Andrea and Tolan, Jamie and Brandt, John and Couprie, Camille and Mairal, Julien and Jégou, Hervé and Labatut, Patrick and Bojanowski, Piotr},
	month = aug,
	year = {2025},
	note = {arXiv:2508.10104 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:D\:\\Literatur\\Zotero\\storage\\LU28UPSB\\Siméoni et al. - 2025 - DINOv3.pdf:application/pdf;Snapshot:D\:\\Literatur\\Zotero\\storage\\YNSP3WVR\\2508.html:text/html},
}

@misc{oquab_dinov2_2024,
	title = {{DINOv2}: {Learning} {Robust} {Visual} {Features} without {Supervision}},
	shorttitle = {{DINOv2}},
	url = {http://arxiv.org/abs/2304.07193},
	doi = {10.48550/arXiv.2304.07193},
	abstract = {The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.},
	urldate = {2025-09-25},
	publisher = {arXiv},
	author = {Oquab, Maxime and Darcet, Timothée and Moutakanni, Théo and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and Assran, Mahmoud and Ballas, Nicolas and Galuba, Wojciech and Howes, Russell and Huang, Po-Yao and Li, Shang-Wen and Misra, Ishan and Rabbat, Michael and Sharma, Vasu and Synnaeve, Gabriel and Xu, Hu and Jegou, Hervé and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},
	month = feb,
	year = {2024},
	note = {arXiv:2304.07193 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:D\:\\Literatur\\Zotero\\storage\\UXNI8YMW\\Oquab et al. - 2024 - DINOv2 Learning Robust Visual Features without Supervision.pdf:application/pdf;Snapshot:D\:\\Literatur\\Zotero\\storage\\URZ6SKSN\\2304.html:text/html},
}

@misc{caron_emerging_2021,
	title = {Emerging {Properties} in {Self}-{Supervised} {Vision} {Transformers}},
	url = {http://arxiv.org/abs/2104.14294},
	doi = {10.48550/arXiv.2104.14294},
	abstract = {In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3\% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1\% top-1 on ImageNet in linear evaluation with ViT-Base.},
	urldate = {2025-10-07},
	author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and Jégou, Hervé and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
	month = may,
	year = {2021},
	note = {arXiv:2104.14294 [cs]},
	file = {Caron et al. - 2021 - Emerging Properties in Self-Supervised Vision Transformers.pdf:D\:\\Literatur\\Zotero\\storage\\BLAYWIEP\\Caron et al. - 2021 - Emerging Properties in Self-Supervised Vision Transformers.pdf:application/pdf},
}

@misc{loshchilov_decoupled_2019,
	title = {Decoupled {Weight} {Decay} {Regularization}},
	url = {http://arxiv.org/abs/1711.05101},
	doi = {10.48550/arXiv.1711.05101},
	abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by {\textbackslash}emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
	urldate = {2025-10-10},
	publisher = {arXiv},
	author = {Loshchilov, Ilya and Hutter, Frank},
	month = jan,
	year = {2019},
	note = {arXiv:1711.05101 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control},
	file = {Preprint PDF:D\:\\Literatur\\Zotero\\storage\\NDC9A8VT\\Loshchilov and Hutter - 2019 - Decoupled Weight Decay Regularization.pdf:application/pdf;Snapshot:D\:\\Literatur\\Zotero\\storage\\KTIZGZY9\\1711.html:text/html},
}

@article{torralba_using_2010,
	title = {Using the forest to see the trees: exploiting context for visual object detection and localization},
	volume = {53},
	number = {3},
	journal = {Communications of the ACM},
	author = {Torralba, Antonio and Murphy, Kevin P and Freeman, William T},
	year = {2010},
	note = {Publisher: ACM New York, NY, USA},
	pages = {107--114},
}

@article{xiang_structured_2024,
	title = {Structured {3D} {Latents} for {Scalable} and {Versatile} {3D} {Generation}},
	journal = {arXiv preprint arXiv:2412.01506},
	author = {Xiang, Jianfeng and Lv, Zelong and Xu, Sicheng and Deng, Yu and Wang, Ruicheng and Zhang, Bowen and Chen, Dong and Tong, Xin and Yang, Jiaolong},
	year = {2024},
}

@misc{kerbl_3d_2023,
	title = {{3D} {Gaussian} {Splatting} for {Real}-{Time} {Radiance} {Field} {Rendering}},
	url = {http://arxiv.org/abs/2308.04079},
	doi = {10.48550/arXiv.2308.04079},
	abstract = {Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates. We introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time ({\textgreater}= 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration, we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows realtime rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets.},
	urldate = {2025-10-17},
	publisher = {arXiv},
	author = {Kerbl, Bernhard and Kopanas, Georgios and Leimkühler, Thomas and Drettakis, George},
	month = aug,
	year = {2023},
	note = {arXiv:2308.04079 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
	file = {Preprint PDF:D\:\\Literatur\\Zotero\\storage\\2V89ZRTC\\Kerbl et al. - 2023 - 3D Gaussian Splatting for Real-Time Radiance Field Rendering.pdf:application/pdf;Snapshot:D\:\\Literatur\\Zotero\\storage\\Y8LP7HSW\\2308.html:text/html},
}

@misc{karaev_cotracker_2024,
	title = {{CoTracker}: {It} is {Better} to {Track} {Together}},
	shorttitle = {{CoTracker}},
	url = {http://arxiv.org/abs/2307.07635},
	doi = {10.48550/arXiv.2307.07635},
	abstract = {We introduce CoTracker, a transformer-based model that tracks a large number of 2D points in long video sequences. Differently from most existing approaches that track points independently, CoTracker tracks them jointly, accounting for their dependencies. We show that joint tracking significantly improves tracking accuracy and robustness, and allows CoTracker to track occluded points and points outside of the camera view. We also introduce several innovations for this class of trackers, including using token proxies that significantly improve memory efficiency and allow CoTracker to track 70k points jointly and simultaneously at inference on a single GPU. CoTracker is an online algorithm that operates causally on short windows. However, it is trained utilizing unrolled windows as a recurrent network, maintaining tracks for long periods of time even when points are occluded or leave the field of view. Quantitatively, CoTracker substantially outperforms prior trackers on standard point-tracking benchmarks.},
	urldate = {2025-10-17},
	publisher = {arXiv},
	author = {Karaev, Nikita and Rocco, Ignacio and Graham, Benjamin and Neverova, Natalia and Vedaldi, Andrea and Rupprecht, Christian},
	month = oct,
	year = {2024},
	note = {arXiv:2307.07635 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:D\:\\Literatur\\Zotero\\storage\\B3FELD4X\\Karaev et al. - 2024 - CoTracker It is Better to Track Together.pdf:application/pdf;Snapshot:D\:\\Literatur\\Zotero\\storage\\4332CPJR\\2307.html:text/html},
}

@misc{li_efficientformer_2022,
	title = {{EfficientFormer}: {Vision} {Transformers} at {MobileNet} {Speed}},
	shorttitle = {{EfficientFormer}},
	url = {http://arxiv.org/abs/2206.01191},
	doi = {10.48550/arXiv.2206.01191},
	abstract = {Vision Transformers (ViT) have shown rapid progress in computer vision tasks, achieving promising results on various benchmarks. However, due to the massive number of parameters and model design, {\textbackslash}textit\{e.g.\}, attention mechanism, ViT-based models are generally times slower than lightweight convolutional networks. Therefore, the deployment of ViT for real-time applications is particularly challenging, especially on resource-constrained hardware such as mobile devices. Recent efforts try to reduce the computation complexity of ViT through network architecture search or hybrid design with MobileNet block, yet the inference speed is still unsatisfactory. This leads to an important question: can transformers run as fast as MobileNet while obtaining high performance? To answer this, we first revisit the network architecture and operators used in ViT-based models and identify inefficient designs. Then we introduce a dimension-consistent pure transformer (without MobileNet blocks) as a design paradigm. Finally, we perform latency-driven slimming to get a series of final models dubbed EfficientFormer. Extensive experiments show the superiority of EfficientFormer in performance and speed on mobile devices. Our fastest model, EfficientFormer-L1, achieves \$79.2{\textbackslash}\%\$ top-1 accuracy on ImageNet-1K with only \$1.6\$ ms inference latency on iPhone 12 (compiled with CoreML), which runs as fast as MobileNetV2\${\textbackslash}times 1.4\$ (\$1.6\$ ms, \$74.7{\textbackslash}\%\$ top-1), and our largest model, EfficientFormer-L7, obtains \$83.3{\textbackslash}\%\$ accuracy with only \$7.0\$ ms latency. Our work proves that properly designed transformers can reach extremely low latency on mobile devices while maintaining high performance.},
	urldate = {2025-10-18},
	publisher = {arXiv},
	author = {Li, Yanyu and Yuan, Geng and Wen, Yang and Hu, Ju and Evangelidis, Georgios and Tulyakov, Sergey and Wang, Yanzhi and Ren, Jian},
	month = oct,
	year = {2022},
	note = {arXiv:2206.01191 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:D\:\\Literatur\\Zotero\\storage\\AXEW7DLG\\Li et al. - 2022 - EfficientFormer Vision Transformers at MobileNet Speed.pdf:application/pdf;Snapshot:D\:\\Literatur\\Zotero\\storage\\LJZVPCQZ\\2206.html:text/html},
}

@misc{vasu_fastvit_2023,
	title = {{FastViT}: {A} {Fast} {Hybrid} {Vision} {Transformer} using {Structural} {Reparameterization}},
	shorttitle = {{FastViT}},
	url = {http://arxiv.org/abs/2303.14189},
	doi = {10.48550/arXiv.2303.14189},
	abstract = {The recent amalgamation of transformer and convolutional designs has led to steady improvements in accuracy and efficiency of the models. In this work, we introduce FastViT, a hybrid vision transformer architecture that obtains the state-of-the-art latency-accuracy trade-off. To this end, we introduce a novel token mixing operator, RepMixer, a building block of FastViT, that uses structural reparameterization to lower the memory access cost by removing skip-connections in the network. We further apply train-time overparametrization and large kernel convolutions to boost accuracy and empirically show that these choices have minimal effect on latency. We show that - our model is 3.5x faster than CMT, a recent state-of-the-art hybrid transformer architecture, 4.9x faster than EfficientNet, and 1.9x faster than ConvNeXt on a mobile device for the same accuracy on the ImageNet dataset. At similar latency, our model obtains 4.2\% better Top-1 accuracy on ImageNet than MobileOne. Our model consistently outperforms competing architectures across several tasks -- image classification, detection, segmentation and 3D mesh regression with significant improvement in latency on both a mobile device and a desktop GPU. Furthermore, our model is highly robust to out-of-distribution samples and corruptions, improving over competing robust models. Code and models are available at https://github.com/apple/ml-fastvit.},
	urldate = {2025-10-18},
	publisher = {arXiv},
	author = {Vasu, Pavan Kumar Anasosalu and Gabriel, James and Zhu, Jeff and Tuzel, Oncel and Ranjan, Anurag},
	month = aug,
	year = {2023},
	note = {arXiv:2303.14189 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:D\:\\Literatur\\Zotero\\storage\\6ADA72D7\\Vasu et al. - 2023 - FastViT A Fast Hybrid Vision Transformer using Structural Reparameterization.pdf:application/pdf;Snapshot:D\:\\Literatur\\Zotero\\storage\\P2IGCMDQ\\2303.html:text/html},
}

@book{torralba_foundations_2024,
	series = {Adaptive {Computation} and {Machine} {Learning} series},
	title = {Foundations of {Computer} {Vision}},
	isbn = {978-0-262-37866-6},
	url = {https://mitpress.mit.edu/9780262048972/foundations-of-computer-vision/},
	publisher = {MIT Press},
	author = {Torralba, A. and Isola, P. and Freeman, W.T.},
	year = {2024},
	lccn = {2023024589},
}
