<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.24">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Marco Willi">

<title>6 - Practical – CAS Deep Learning - Computer Vision</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../lectures/recent_advances.html" rel="next">
<link href="../lectures/classification.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-a6e161b2431e1f94a14e0f5d32135a3c.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-e0449c57dc659bcd97f89e2c92d66f0b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-SCHRGR3LNM"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-SCHRGR3LNM', { 'anonymize_ip': true});
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../lectures/intro.html">Lectures</a></li><li class="breadcrumb-item"><a href="../lectures/practical.html">6 - Practical</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-center sidebar-header">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../assets/logo.webp" alt="" class="sidebar-logo light-content py-0 d-lg-inline d-none">
      <img src="../assets/logo.webp" alt="" class="sidebar-logo dark-content py-0 d-lg-inline d-none">
      </a>
      <div class="sidebar-tools-main">
    <a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2025" title="GitHub organization" class="quarto-navigation-tool px-1" aria-label="GitHub organization"><i class="bi bi-github"></i></a>
    <a href="" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-code-square"></i></a>
</div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Slides</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../slides/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Einführung Computer Vision mit Deep Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../slides/cnns.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Convolutional Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../slides/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../slides/practical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Practical Considerations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../slides/recent_advances.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Recent Advances</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/quiz.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Quiz &amp; Exam Prep</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Lectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1 - Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2 - Software &amp; Hardware for Deep Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/neural_networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3 - Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/cnns.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4 - Convolutional Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5 - Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/practical.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">6 - Practical</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/recent_advances.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">7 - Recent Advances</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Resources</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/literature.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Books</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/links.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Helpful Links &amp; Resources</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Mathematical Notation</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#a-recipe" id="toc-a-recipe" class="nav-link active" data-scroll-target="#a-recipe">A Recipe</a>
  <ul class="collapse">
  <li><a href="#become-familiar-with-the-data" id="toc-become-familiar-with-the-data" class="nav-link" data-scroll-target="#become-familiar-with-the-data">1) Become Familiar with the Data</a></li>
  <li><a href="#baselines" id="toc-baselines" class="nav-link" data-scroll-target="#baselines">2) Baselines</a></li>
  <li><a href="#overfit" id="toc-overfit" class="nav-link" data-scroll-target="#overfit">3) Overfit</a></li>
  <li><a href="#regularization" id="toc-regularization" class="nav-link" data-scroll-target="#regularization">4) Regularization</a></li>
  <li><a href="#hyper-parameter-tuning" id="toc-hyper-parameter-tuning" class="nav-link" data-scroll-target="#hyper-parameter-tuning">5) Hyper-Parameter Tuning</a></li>
  <li><a href="#squeeze-out-the-juice" id="toc-squeeze-out-the-juice" class="nav-link" data-scroll-target="#squeeze-out-the-juice">6) Squeeze out the Juice</a></li>
  </ul></li>
  <li><a href="#model-zoos-and-frameworks" id="toc-model-zoos-and-frameworks" class="nav-link" data-scroll-target="#model-zoos-and-frameworks">Model Zoos and Frameworks</a>
  <ul class="collapse">
  <li><a href="#hugging-face" id="toc-hugging-face" class="nav-link" data-scroll-target="#hugging-face">Hugging-Face</a></li>
  <li><a href="#timm" id="toc-timm" class="nav-link" data-scroll-target="#timm">timm</a></li>
  </ul></li>
  <li><a href="#code-setup-configuration" id="toc-code-setup-configuration" class="nav-link" data-scroll-target="#code-setup-configuration">Code Setup / Configuration</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2025/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../lectures/intro.html">Lectures</a></li><li class="breadcrumb-item"><a href="../lectures/practical.html">6 - Practical</a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">6 - Practical</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Marco Willi </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Institute for Data Science I4DS, FHNW
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  


</header>


<section id="a-recipe" class="level2">
<h2 class="anchored" data-anchor-id="a-recipe">A Recipe</h2>
<p>How to successfully train Deep Learning models? Below is a step-by-step recipe. This recipe is based in part on Andrej Karpathy’s blog post <a href="http://karpathy.github.io/2019/04/25/recipe/">A Recipe for Training Neural Networks</a>.</p>
<p>Karpathy observed that training neural networks with modern ML frameworks is highly abstracted, but <strong>these abstractions are “leaky”</strong>, meaning that without a deep understanding of internal mechanisms such as <em>back-propagation</em> and <em>gradient descent</em>, mistakes can occur. Therefore, one is not completely protected from understanding what happens within the abstraction.</p>
<div id="991dcda5" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>your_data <span class="op">=</span> <span class="co"># plug your awesome dataset here</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SuperCrossValidator(SuperDuper.fit, your_data, ResNet50, SGDOptimizer)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Moreover, faulty implementations or sub-optimal model training do not lead to an error message. This means <strong>training neural networks <em>fails silently</em></strong>. For example, the <em>learning rate</em> might be too high, causing the model to converge too early and achieve sub-optimal performance. Such issues would not be flagged with an error message but would require <em>hyper-parameter tuning</em> or other diagnostic plots.</p>
<p>For these reasons, Karpathy developed a recipe to avoid the most significant pitfalls.</p>
<section id="become-familiar-with-the-data" class="level3">
<h3 class="anchored" data-anchor-id="become-familiar-with-the-data">1) Become Familiar with the Data</h3>
<p>In the first step, the data to be modeled should be thoroughly inspected. Particularly for images, one can examine sample data points, their distribution, patterns, and anomalies. Corrupted images, skewed class distributions, duplicates, etc., may be found. Often, just by looking, one can gauge if modeling will work or be challenging, assess label quality, or determine how much downscaling images can tolerate.</p>
<div id="fig-practical-camera-traps-corrupt" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-practical-camera-traps-corrupt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/practical/camera_traps_corrupt.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-practical-camera-traps-corrupt-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Examples of images from camera traps. They may still be usable despite being distorted by heat.
</figcaption>
</figure>
</div>
<p><a href="#fig-practical-camera-traps-difficulties" class="quarto-xref">Figure&nbsp;2</a> shows typical challenging cases in modeling images from camera traps. Small ROIs suggest that the image resolution should not be excessively reduced. Typical resolutions like 224x224 might be insufficient for certain species.</p>
<div id="fig-practical-camera-traps-difficulties" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-practical-camera-traps-difficulties-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/practical/cam_trap_difficulties.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-practical-camera-traps-difficulties-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Examples of images from camera traps. Source: <span class="citation" data-cites="Beery2018">Beery, Van Horn, and Perona (<a href="#ref-Beery2018" role="doc-biblioref">2018</a>)</span>.
</figcaption>
</figure>
</div>
<p><a href="#fig-practical-camera-traps-dist" class="quarto-xref">Figure&nbsp;3</a> displays the distribution of a typical camera trap dataset. While some species are frequent, others are rare, making modeling difficult and requiring special techniques.</p>
<div id="fig-practical-camera-traps-dist" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-practical-camera-traps-dist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/practical/cam_trap_dist2.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-practical-camera-traps-dist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Species distribution in a typical camera trap dataset. “Deer” comprises about 65% of cases.
</figcaption>
</figure>
</div>
<p>Later in the modeling process, examining misclassified samples is crucial to understand where more data labeling is necessary. <a href="#fig-practical-camera-traps-serval" class="quarto-xref">Figure&nbsp;4</a> shows an example of a rare species that the model misclassified.</p>
<div id="fig-practical-camera-traps-serval" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-practical-camera-traps-serval-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/practical/wrong_predictions_serval.png" class="img-fluid figure-img" width="300">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-practical-camera-traps-serval-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: An image of a serval. Model confidences are displayed below.
</figcaption>
</figure>
</div>
<p>Sometimes, one may also encounter certain limitations, such as images that are ambiguous or belong to multiple classes. <a href="#fig-practical-camera-traps-multi" class="quarto-xref">Figure&nbsp;5</a> shows an example with a herd of animals containing multiple species (Thomson’s and Grant’s Gazelle). Here, consideration of the modeling approach’s suitability is necessary.</p>
<div id="fig-practical-camera-traps-multi" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-practical-camera-traps-multi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/practical/multi_species.JPG" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-practical-camera-traps-multi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Example of an image from a camera trap with various species.
</figcaption>
</figure>
</div>
</section>
<section id="baselines" class="level3">
<h3 class="anchored" data-anchor-id="baselines">2) Baselines</h3>
<p>After becoming familiar with the data, set up an evaluation pipeline. Define a metric to measure model performance and establish train/validation/test splits or cross-validation procedures. Also, implement or use tools for monitoring and logging training.</p>
<div id="fig-ml-prozess" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ml-prozess-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/practical/ml_process_raschka.png" class="img-fluid figure-img" width="800">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ml-prozess-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: The components of a typical machine learning process. Source: <span class="citation" data-cites="raschka_python_2020">Raschka and Mirjalili (<a href="#ref-raschka_python_2020" role="doc-biblioref">2020</a>)</span>.
</figcaption>
</figure>
</div>
<p>Next, use a simple model, which is unlikely to be incorrectly implemented, to test the pipeline. Visualize metrics, losses, and predictions, and test different hypotheses to verify pipeline accuracy.</p>
<div id="fig-practical-wb-example" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-practical-wb-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/practical/wb_example.png" class="img-fluid figure-img" width="500">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-practical-wb-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Weights and Biases experiment tracking.
</figcaption>
</figure>
</div>
<p><strong>Some Tips for This Step:</strong></p>
<p><strong>Ensure reproducibility</strong> by fixing the <em>random seed</em> so that results remain consistent. See <a href="https://pytorch.org/docs/stable/notes/randomness.html">PyTorch Reproducibility</a>.</p>
<div id="be281525" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">0</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p><strong>Avoid unnecessary techniques and complexities</strong>, such as <em>data augmentation</em>, which may introduce bugs that are hard to detect.</p>
<p><strong>Use a <em>human baseline</em> if possible</strong>. For certain tasks, labels may be subjective, with annotators potentially disagreeing on how to label a data point. This implies an <em>upper bound</em> on model performance, which can be estimated by having some data points annotated multiple times. Examples of such images are shown in <a href="#fig-practical-camera-trap-hartebeest" class="quarto-xref">Figure&nbsp;8</a> and <a href="#fig-practical-camera-trap-difficult" class="quarto-xref">Figure&nbsp;9</a>.</p>
<div id="fig-practical-camera-trap-hartebeest" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-practical-camera-trap-hartebeest-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/practical/hartebeest.gif" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-practical-camera-trap-hartebeest-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Example of a challenging species to classify (for amateurs). It could be either Hartebeest or Topi.
</figcaption>
</figure>
</div>
<div id="fig-practical-camera-trap-difficult" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-practical-camera-trap-difficult-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/practical/camera_trap_difficult.jpeg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-practical-camera-trap-difficult-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: A challenging image to classify from a camera trap dataset. In the <em>Snapshot Serengeti</em> dataset, amateur annotators achieved a 96.6% agreement rate with experts.
</figcaption>
</figure>
</div>
<p><strong>Train an input-independent baseline</strong>, setting all pixel values to the same value, such as zero. This helps determine if a simple model is learning anything at all.</p>
<p>Another option is to <strong>overfit the model on a batch of data</strong>. If everything is functioning correctly, one should achieve perfect train metrics/losses.</p>
<p>It is also helpful to <strong>visualize what goes into the model</strong>. This can help detect errors in <em>data augmentation</em> or <em>data preprocessing</em>. This should be done just before executing this line of code:</p>
<div id="4303855a" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>y_hat <span class="op">=</span> model(x)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Observing predictions from a fixed sample throughout training can provide insight into the dynamics of the process. In <em>object detection</em>, one might display bounding boxes and classification scores. <a href="#fig-practical-wb-comparison" class="quarto-xref">Figure&nbsp;10</a> shows an example of a segmentation problem: the input image is on the left, and the model’s segmentation is on the right.</p>
<div id="fig-practical-wb-comparison" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-practical-wb-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/practical/wb_comparison.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-practical-wb-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: Example of a segmentation problem: left is the input, right is the output.
</figcaption>
</figure>
</div>
</section>
<section id="overfit" class="level3">
<h3 class="anchored" data-anchor-id="overfit">3) Overfit</h3>
<p>At this point, you should have a good understanding of the dataset, confidence in the evaluation pipeline, and initial baselines for simple models. Additionally, you should have a sense of how well a human could perform on this task (if relevant). In this step, focus on achieving good performance on the training set, accepting overfitting temporarily. Next, focus on the validation set.</p>
<p>For model architecture, follow the principle “Don’t be a hero.” Use existing, well-established architectures, such as <em>ResNet-50</em> for <em>image classification</em>.</p>
<p>The choice of <a href="https://pytorch.org/docs/stable/optim.html">optimizer</a> should also be conservative. The <em>adam</em> optimizer is often effective and less sensitive to <em>hyper-parameters</em> like the <em>learning rate</em>.</p>
</section>
<section id="regularization" class="level3">
<h3 class="anchored" data-anchor-id="regularization">4) Regularization</h3>
<p>At this point, you should have achieved good performance on the training set. Now, shift focus to the validation set, as it is critical for estimating expected performance on new data. This often involves a reduction in training performance due to regularization.</p>
<p><strong>More Training Data</strong></p>
<p>The simplest way to improve performance (and reduce overfitting) is to collect more training data. If possible, this approach is more effective than other methods and helps a model generalize better. The benefit of additional data can be estimated by training several models on different training set sizes and observing performance changes (<em>learning curve</em>).</p>
<div id="fig-practical-learning-curve" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-practical-learning-curve-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/practical/learning_curve.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-practical-learning-curve-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: Example of a <em>learning curve</em>. X-axis is performance, Y-axis is the number of training samples. Left panel: Gaussian Naive Bayes; right panel: Support Vector Classifier.
</figcaption>
</figure>
</div>
<p><strong>Data Augmentation</strong></p>
<p>Another approach is <em>data augmentation</em>, generating new data points from existing ones by making random alterations. Data points are typically augmented <em>on-the-fly</em> rather than in advance. ML frameworks like PyTorch provide built-in transformations, and additional packages offer more complex augmentations.</p>
<p>For example, <a href="https://github.com/facebookresearch/AugLy">AugLy</a>, see <a href="#fig-practical-augly" class="quarto-xref">Figure&nbsp;12</a>.</p>
<div id="fig-practical-augly" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-practical-augly-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/practical/augly.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-practical-augly-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: AugLy
</figcaption>
</figure>
</div>
<p>Another option is <a href="https://github.com/albumentations-team/albumentations">Albumentations</a>, see <a href="#fig-practical-albumentations" class="quarto-xref">Figure&nbsp;13</a>.</p>
<div id="fig-practical-albumentations" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-practical-albumentations-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/practical/albumentations.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-practical-albumentations-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13: Albumentations
</figcaption>
</figure>
</div>
<p><a href="https://kornia.readthedocs.io/en/latest/applications/image_augmentations.html">Kornia</a> provides even more complex augmentations, see <a href="#fig-practical-kornia" class="quarto-xref">Figure&nbsp;14</a>.</p>
<div id="fig-practical-kornia" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-practical-kornia-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/practical/kornia.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-practical-kornia-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14: Kornia
</figcaption>
</figure>
</div>
<p><a href="#fig-practical-augmentation-rhino" class="quarto-xref">Figure&nbsp;15</a> shows an example of <em>data augmentation</em> with color changes, cropping, and rotations.</p>
<div id="fig-practical-augmentation-rhino" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-practical-augmentation-rhino-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/practical/augmentation_rhino.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-practical-augmentation-rhino-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15: Data augmentation example.
</figcaption>
</figure>
</div>
<p>Synthetic data can also be added to the training process, which can be successful. <a href="#fig-practical-synthetic-examples" class="quarto-xref">Figure&nbsp;16</a> shows an example where 3D models of animals were created using a game engine and placed either in synthetic landscapes or spliced into real camera trap images.</p>
<div id="fig-practical-synthetic-examples" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-practical-synthetic-examples-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/practical/synthetic_camera_trap_images.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-practical-synthetic-examples-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16: From <span class="citation" data-cites="Beery2020">Beery et al. (<a href="#ref-Beery2020" role="doc-biblioref">2020</a>)</span>. Synthetic and semi-synthetic data.
</figcaption>
</figure>
</div>
<p><strong>Early Stopping</strong></p>
<p><em>Early stopping</em> involves training a model and evaluating it on a validation set periodically (e.g., after each epoch). Training is halted if no significant improvement is observed after a set number of cycles, preventing overfitting.</p>
<div id="fig-practical-early-stopping" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-practical-early-stopping-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/practical/early_stopping.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-practical-early-stopping-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17: Source: <a href="https://www.oreilly.com/library/view/hands-on-transfer-learning/9781788831307/41172567-9482-4cad-ac87-1cfbd46026df.xhtml">Link</a>
</figcaption>
</figure>
</div>
<p>Frameworks like <a href="https://pytorch-lightning.readthedocs.io/en/stable/common/early_stopping.html"><code>pytorch-lightning</code></a> provide <code>Callbacks</code> to implement <em>early stopping</em>.</p>
<div id="862dff76" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pytorch_lightning.callbacks.early_stopping <span class="im">import</span> EarlyStopping</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LitModel(LightningModule):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> validation_step(<span class="va">self</span>, batch, batch_idx):</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> ...</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.log(<span class="st">"val_loss"</span>, loss)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LitModel()</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(callbacks<span class="op">=</span>[EarlyStopping(monitor<span class="op">=</span><span class="st">"val_loss"</span>, mode<span class="op">=</span><span class="st">"min"</span>)])</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>trainer.fit(model)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p><strong>Weight Decay</strong></p>
<p><em>Weight decay</em> is another common regularization technique. In gradient descent, weights are reduced by a factor <span class="math inline">\(\lambda\)</span> during each update. The modified update step is:</p>
<p>[ _{t+1} = _t (1 - ) - J() ]</p>
<p>where <span class="math inline">\(t\)</span> is the iteration, <span class="math inline">\(\theta\)</span> are the model parameters, and <span class="math inline">\(\eta\)</span> is the <em>learning rate</em>. This can be applied directly in PyTorch <a href="https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD">torch.optim.SGD</a>.</p>
<p><strong>Transfer Learning</strong></p>
<p>Transfer learning also has a regularizing effect and can reduce model training time. This approach typically involves using a pre-trained model on a large dataset (e.g., ImageNet), removing the last layer, and replacing it with a new one suited to the target task. Various layers can be frozen or fine-tuned.</p>
<div id="fig-practical-transfer-learning" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-practical-transfer-learning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/practical/transfer_learning.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-practical-transfer-learning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;18: Source: <span class="citation" data-cites="johnson_eecs_2021">Johnson and Fouhey (<a href="#ref-johnson_eecs_2021" role="doc-biblioref">2021</a>)</span>
</figcaption>
</figure>
</div>
<p>The following example code <a href="https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html">(PyTorch-Link)</a> shows how to freeze the parameters of a pre-trained model by setting <code>requires_grad = False</code>.</p>
<div id="f3f0ebc1" class="cell" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> set_parameter_requires_grad(model, feature_extracting):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> feature_extracting:</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> param <span class="kw">in</span> model.parameters():</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>            param.requires_grad <span class="op">=</span> <span class="va">False</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="hyper-parameter-tuning" class="level3">
<h3 class="anchored" data-anchor-id="hyper-parameter-tuning">5) Hyper-Parameter Tuning</h3>
<p>This step involves systematically evaluating various <em>hyper-parameters</em> and architectures. Techniques include <em>grid-search</em> or <em>random-search</em>, with <em>random-search</em> generally preferred.</p>
<p>Complete packages are available for this, such as <a href="https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html">Ray Tune</a>.</p>
<p>Hyper-parameter tuning requires configurable architecture implementations. Below is an example of configurable fully connected layers.</p>
<div id="1f383eb5" class="cell" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Net(nn.Module):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, l1<span class="op">=</span><span class="dv">120</span>, l2<span class="op">=</span><span class="dv">84</span>):</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(Net, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(<span class="dv">3</span>, <span class="dv">6</span>, <span class="dv">5</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pool <span class="op">=</span> nn.MaxPool2d(<span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(<span class="dv">6</span>, <span class="dv">16</span>, <span class="dv">5</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">16</span> <span class="op">*</span> <span class="dv">5</span> <span class="op">*</span> <span class="dv">5</span>, l1)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(l1, l2)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc3 <span class="op">=</span> nn.Linear(l2, <span class="dv">10</span>)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(F.relu(<span class="va">self</span>.conv1(x)))</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(F.relu(<span class="va">self</span>.conv2(x)))</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">16</span> <span class="op">*</span> <span class="dv">5</span> <span class="op">*</span> <span class="dv">5</span>)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc2(x))</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc3(x)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>For domain-specific problems, consult relevant literature for ideas on possible hyper-parameter variations and architectures (e.g., medical imaging or satellite images).</p>
</section>
<section id="squeeze-out-the-juice" class="level3">
<h3 class="anchored" data-anchor-id="squeeze-out-the-juice">6) Squeeze out the Juice</h3>
<p>Once the best architectures and hyper-parameters are identified, there are additional ways to maximize performance.</p>
<p>One is <em>ensembling</em>: training multiple models and combining their predictions often yields slightly better performance than a single model. However, this increases training and inference costs.</p>
<p>Surprising effects sometimes occur in deep learning, like <em>double descent</em> (<span class="citation" data-cites="nakkiran_deep_2019">Nakkiran et al. (<a href="#ref-nakkiran_deep_2019" role="doc-biblioref">2019</a>)</span>) or <em>grokking</em> (<span class="citation" data-cites="power_grokking_2022">Power et al. (<a href="#ref-power_grokking_2022" role="doc-biblioref">2022</a>)</span>). For instance, using highly complex models (an overfitting risk) can work well, and after a plateau, validation loss might improve if training continues. So it may be worthwhile to train a model longer (without <em>early stopping</em>).</p>
<div id="fig-practical-double-descent" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-practical-double-descent-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/practical/double_descent.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-practical-double-descent-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;19: Source: <span class="citation" data-cites="nakkiran_deep_2019">Nakkiran et al. (<a href="#ref-nakkiran_deep_2019" role="doc-biblioref">2019</a>)</span>. The X-axis represents model complexity, and the Y-axis shows test and train error. Test error decreases, increases (overfitting), and then decreases again (double descent).
</figcaption>
</figure>
</div>
<p>Additional techniques often yield small improvements. For example, <span class="citation" data-cites="liu_convnet_2022">Liu et al. (<a href="#ref-liu_convnet_2022" role="doc-biblioref">2022</a>)</span> reports that the following techniques improved accuracy by about 2.7%:</p>
<ul>
<li>Longer training</li>
<li>Specialized optimizer (AdamW)</li>
<li>Complex data augmentation techniques (Mixup, Cutmix, RandAugment)</li>
<li>Regularization techniques (Stochastic Depth)</li>
<li>Label smoothing</li>
</ul>
</section>
</section>
<section id="model-zoos-and-frameworks" class="level2">
<h2 class="anchored" data-anchor-id="model-zoos-and-frameworks">Model Zoos and Frameworks</h2>
<section id="hugging-face" class="level3">
<h3 class="anchored" data-anchor-id="hugging-face">Hugging-Face</h3>
<p><a href="https://huggingface.co/">HuggingFace</a> provides a wide range of models, datasets, and APIs. It offers easy-to-use Python APIs to integrate, use, and adapt models for PyTorch.</p>
</section>
<section id="timm" class="level3">
<h3 class="anchored" data-anchor-id="timm">timm</h3>
<p><a href="https://github.com/rwightman/pytorch-image-models">PyTorch Image Models (timm)</a> is a library with SOTA models, layers, and functions for using, reproducing, and adapting modern image models.</p>
</section>
</section>
<section id="code-setup-configuration" class="level2">
<h2 class="anchored" data-anchor-id="code-setup-configuration">Code Setup / Configuration</h2>
<p>Setting up a well-organized repository structure can significantly aid in model development. Key aspects include reproducibility, configurability, tracking, and logging experiments.</p>
<p>Suggested repository structures:</p>
<ul>
<li>DS-cookie cutter: <a href="https://github.com/drivendata/cookiecutter-data-science">DrivenData</a></li>
<li>PyTorch-Lightning example: <a href="https://github.com/PyTorchLightning/deep-learning-project-template">Lightning Template</a></li>
<li>Configurations: <a href="https://hydra.cc/docs/intro/">Hydra</a></li>
<li>Experiment tracking: <a href="https://wandb.ai/site">Weights &amp; Biases</a>, <a href="https://neptune.ai/experiment-tracking">Neptune AI</a></li>
<li>Version Control for ML projects: <a href="https://dvc.org/">DVC</a></li>
</ul>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-Beery2020" class="csl-entry" role="listitem">
Beery, Sara, Yang Liu, Dan Morris, Jim Piavis, Ashish Kapoor, Markus Meister, Neel Joshi, and Pietro Perona. 2020. <span>“Synthetic Examples Improve Generalization for Rare Classes.”</span> <em>Proceedings - 2020 IEEE Winter Conference on Applications of Computer Vision, WACV 2020</em>, 852–62. <a href="https://doi.org/10.1109/WACV45572.2020.9093570">https://doi.org/10.1109/WACV45572.2020.9093570</a>.
</div>
<div id="ref-Beery2018" class="csl-entry" role="listitem">
Beery, Sara, Grant Van Horn, and Pietro Perona. 2018. <span>“Recognition in <span>Terra</span> <span>Incognita</span>.”</span> In <em>Lecture <span>Notes</span> in <span>Computer</span> <span>Science</span> (Including Subseries <span>Lecture</span> <span>Notes</span> in <span>Artificial</span> <span>Intelligence</span> and <span>Lecture</span> <span>Notes</span> in <span>Bioinformatics</span>)</em>, 11220 LNCS:472–89. <a href="https://doi.org/10.1007/978-3-030-01270-0_28">https://doi.org/10.1007/978-3-030-01270-0_28</a>.
</div>
<div id="ref-johnson_eecs_2021" class="csl-entry" role="listitem">
Johnson, Justin, and David Fouhey. 2021. <span>“<span>EECS</span> 442: <span>Computer</span> <span>Vision</span>.”</span> Lecture {Notes} / {Slides}. <a href="https://web.eecs.umich.edu/~justincj/teaching/eecs442/WI2021/">https://web.eecs.umich.edu/~justincj/teaching/eecs442/WI2021/</a>.
</div>
<div id="ref-liu_convnet_2022" class="csl-entry" role="listitem">
Liu, Zhuang, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. 2022. <span>“A <span>ConvNet</span> for the 2020s.”</span> arXiv. <a href="http://arxiv.org/abs/2201.03545">http://arxiv.org/abs/2201.03545</a>.
</div>
<div id="ref-nakkiran_deep_2019" class="csl-entry" role="listitem">
Nakkiran, Preetum, Boaz Barak, Gal Kaplun, Ilya Sutskever, Yamini Bansal, and Tristan Yang. 2019. <span>“Deep Double Descent: <span>Where</span> Bigger Models and More Data Hurt.”</span> <em>arXiv</em>, 1–24.
</div>
<div id="ref-power_grokking_2022" class="csl-entry" role="listitem">
Power, Alethea, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. 2022. <span>“Grokking: <span>Generalization</span> <span>Beyond</span> <span>Overfitting</span> on <span>Small</span> <span>Algorithmic</span> <span>Datasets</span>.”</span> arXiv. <a href="http://arxiv.org/abs/2201.02177">http://arxiv.org/abs/2201.02177</a>.
</div>
<div id="ref-raschka_python_2020" class="csl-entry" role="listitem">
Raschka, Sebastian, and Vahid Mirjalili. 2020. <em>Python Machine Learning: Machine Learning and Deep Learning with <span>Python</span>, Scikit-Learn, and <span>TensorFlow</span></em>. Second edition, fourth release,[fully revised and updated]. Expert Insight. Birmingham Mumbai: Packt Publishing.
</div>
</div>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/marco-willi\.github\.io\/cas-dl-compvis-lectures-hs2025\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../lectures/classification.html" class="pagination-link" aria-label="5 - Image Classification">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">5 - Image Classification</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../lectures/recent_advances.html" class="pagination-link" aria-label="7 - Recent Advances">
        <span class="nav-page-text">7 - Recent Advances</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "6 - Practical"</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="an">params:</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co">   images_path: "/assets/images/practical/"</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>::: {.content-hidden}</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>{{&lt; include /assets/_macros.tex &gt;}}</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="fu">## A Recipe</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>How to successfully train Deep Learning models? Below is a step-by-step recipe. This recipe is based in part on Andrej Karpathy’s blog post <span class="co">[</span><span class="ot">A Recipe for Training Neural Networks</span><span class="co">](http://karpathy.github.io/2019/04/25/recipe/)</span>.</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>Karpathy observed that training neural networks with modern ML frameworks is highly abstracted, but **these abstractions are "leaky"**, meaning that without a deep understanding of internal mechanisms such as _back-propagation_ and _gradient descent_, mistakes can occur. Therefore, one is not completely protected from understanding what happens within the abstraction.</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>your_data <span class="op">=</span> <span class="co"># plug your awesome dataset here</span></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SuperCrossValidator(SuperDuper.fit, your_data, ResNet50, SGDOptimizer)</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>Moreover, faulty implementations or sub-optimal model training do not lead to an error message. This means **training neural networks _fails silently_**. For example, the _learning rate_ might be too high, causing the model to converge too early and achieve sub-optimal performance. Such issues would not be flagged with an error message but would require _hyper-parameter tuning_ or other diagnostic plots.</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>For these reasons, Karpathy developed a recipe to avoid the most significant pitfalls.</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a><span class="fu">### 1) Become Familiar with the Data</span></span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>In the first step, the data to be modeled should be thoroughly inspected. Particularly for images, one can examine sample data points, their distribution, patterns, and anomalies. Corrupted images, skewed class distributions, duplicates, etc., may be found. Often, just by looking, one can gauge if modeling will work or be challenging, assess label quality, or determine how much downscaling images can tolerate.</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>::: {#fig-practical-camera-traps-corrupt}</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a><span class="al">![ ]({{&lt; meta params.images_path &gt;}}camera_traps_corrupt.jpg)</span>{width=600}</span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>Examples of images from camera traps. They may still be usable despite being distorted by heat.</span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a>@fig-practical-camera-traps-difficulties shows typical challenging cases in modeling images from camera traps. Small ROIs suggest that the image resolution should not be excessively reduced. Typical resolutions like 224x224 might be insufficient for certain species.</span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a>::: {#fig-practical-camera-traps-difficulties}</span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a><span class="al">![ ]({{&lt; meta params.images_path &gt;}}cam_trap_difficulties.png)</span>{width=600}</span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a>Examples of images from camera traps. Source: @Beery2018.</span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a>@fig-practical-camera-traps-dist displays the distribution of a typical camera trap dataset. While some species are frequent, others are rare, making modeling difficult and requiring special techniques.</span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a>::: {#fig-practical-camera-traps-dist}</span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a><span class="al">![ ]({{&lt; meta params.images_path &gt;}}cam_trap_dist2.jpg)</span>{width=600}</span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a>Species distribution in a typical camera trap dataset. "Deer" comprises about 65% of cases.</span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a>Later in the modeling process, examining misclassified samples is crucial to understand where more data labeling is necessary. @fig-practical-camera-traps-serval shows an example of a rare species that the model misclassified.</span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a>::: {#fig-practical-camera-traps-serval}</span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a><span class="al">![ ]({{&lt; meta params.images_path &gt;}}wrong_predictions_serval.png)</span>{width=300}</span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a>An image of a serval. Model confidences are displayed below.</span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true" tabindex="-1"></a>Sometimes, one may also encounter certain limitations, such as images that are ambiguous or belong to multiple classes. @fig-practical-camera-traps-multi shows an example with a herd of animals containing multiple species (Thomson's and Grant's Gazelle). Here, consideration of the modeling approach's suitability is necessary.</span>
<span id="cb7-69"><a href="#cb7-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-70"><a href="#cb7-70" aria-hidden="true" tabindex="-1"></a>::: {#fig-practical-camera-traps-multi}</span>
<span id="cb7-71"><a href="#cb7-71" aria-hidden="true" tabindex="-1"></a><span class="al">![ ]({{&lt; meta params.images_path &gt;}}multi_species.JPG)</span>{width=600}</span>
<span id="cb7-72"><a href="#cb7-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-73"><a href="#cb7-73" aria-hidden="true" tabindex="-1"></a>Example of an image from a camera trap with various species.</span>
<span id="cb7-74"><a href="#cb7-74" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-75"><a href="#cb7-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-76"><a href="#cb7-76" aria-hidden="true" tabindex="-1"></a><span class="fu">### 2) Baselines</span></span>
<span id="cb7-77"><a href="#cb7-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-78"><a href="#cb7-78" aria-hidden="true" tabindex="-1"></a>After becoming familiar with the data, set up an evaluation pipeline. Define a metric to measure model performance and establish train/validation/test splits or cross-validation procedures. Also, implement or use tools for monitoring and logging training.</span>
<span id="cb7-79"><a href="#cb7-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-80"><a href="#cb7-80" aria-hidden="true" tabindex="-1"></a>::: {#fig-ml-prozess}</span>
<span id="cb7-81"><a href="#cb7-81" aria-hidden="true" tabindex="-1"></a><span class="al">![ ]({{&lt; meta params.images_path &gt;}}ml_process_raschka.png)</span>{width=800}</span>
<span id="cb7-82"><a href="#cb7-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-83"><a href="#cb7-83" aria-hidden="true" tabindex="-1"></a>The components of a typical machine learning process. Source: @raschka_python_2020.</span>
<span id="cb7-84"><a href="#cb7-84" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-85"><a href="#cb7-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-86"><a href="#cb7-86" aria-hidden="true" tabindex="-1"></a>Next, use a simple model, which is unlikely to be incorrectly implemented, to test the pipeline. Visualize metrics, losses, and predictions, and test different hypotheses to verify pipeline accuracy.</span>
<span id="cb7-87"><a href="#cb7-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-88"><a href="#cb7-88" aria-hidden="true" tabindex="-1"></a>::: {#fig-practical-wb-example}</span>
<span id="cb7-89"><a href="#cb7-89" aria-hidden="true" tabindex="-1"></a><span class="al">![ ]({{&lt; meta params.images_path &gt;}}wb_example.png)</span>{width=500}</span>
<span id="cb7-90"><a href="#cb7-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-91"><a href="#cb7-91" aria-hidden="true" tabindex="-1"></a>Weights and Biases experiment tracking.</span>
<span id="cb7-92"><a href="#cb7-92" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-93"><a href="#cb7-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-94"><a href="#cb7-94" aria-hidden="true" tabindex="-1"></a>**Some Tips for This Step:**</span>
<span id="cb7-95"><a href="#cb7-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-96"><a href="#cb7-96" aria-hidden="true" tabindex="-1"></a>**Ensure reproducibility** by fixing the _random seed_ so that results remain consistent. See <span class="co">[</span><span class="ot">PyTorch Reproducibility</span><span class="co">](https://pytorch.org/docs/stable/notes/randomness.html)</span>.</span>
<span id="cb7-97"><a href="#cb7-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-100"><a href="#cb7-100" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-101"><a href="#cb7-101" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb7-102"><a href="#cb7-102" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb7-103"><a href="#cb7-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-104"><a href="#cb7-104" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb7-105"><a href="#cb7-105" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">0</span>)</span>
<span id="cb7-106"><a href="#cb7-106" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-107"><a href="#cb7-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-108"><a href="#cb7-108" aria-hidden="true" tabindex="-1"></a>**Avoid unnecessary techniques and complexities**, such as _data augmentation_, which may introduce bugs that are hard to detect.</span>
<span id="cb7-109"><a href="#cb7-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-110"><a href="#cb7-110" aria-hidden="true" tabindex="-1"></a>**Use a _human baseline_ if possible**. For certain tasks, labels may be subjective, with annotators potentially disagreeing on how to label a data point. This implies an _upper bound_ on model performance, which can be estimated by having some data points annotated multiple times. Examples of such images are shown in @fig-practical-camera-trap-hartebeest and @fig-practical-camera-trap-difficult.</span>
<span id="cb7-111"><a href="#cb7-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-112"><a href="#cb7-112" aria-hidden="true" tabindex="-1"></a>::: {#fig-practical-camera-trap-hartebeest}</span>
<span id="cb7-113"><a href="#cb7-113" aria-hidden="true" tabindex="-1"></a><span class="al">![ ]({{&lt; meta params.images_path &gt;}}hartebeest.gif)</span>{width=600}</span>
<span id="cb7-114"><a href="#cb7-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-115"><a href="#cb7-115" aria-hidden="true" tabindex="-1"></a>Example of a challenging species to classify (for amateurs). It could be either Hartebeest or Topi.</span>
<span id="cb7-116"><a href="#cb7-116" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-117"><a href="#cb7-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-118"><a href="#cb7-118" aria-hidden="true" tabindex="-1"></a>::: {#fig-practical-camera-trap-difficult}</span>
<span id="cb7-119"><a href="#cb7-119" aria-hidden="true" tabindex="-1"></a><span class="al">![ ]({{&lt; meta params.images_path &gt;}}camera_trap_difficult.jpeg)</span>{width=600}</span>
<span id="cb7-120"><a href="#cb7-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-121"><a href="#cb7-121" aria-hidden="true" tabindex="-1"></a>A challenging image to classify from a camera trap dataset. In the _Snapshot Serengeti_ dataset, amateur annotators achieved a 96.6% agreement rate with experts.</span>
<span id="cb7-122"><a href="#cb7-122" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-123"><a href="#cb7-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-124"><a href="#cb7-124" aria-hidden="true" tabindex="-1"></a>**Train an input-independent baseline**, setting all pixel values to the same value, such as zero. This helps determine if a simple model is learning anything at all.</span>
<span id="cb7-125"><a href="#cb7-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-126"><a href="#cb7-126" aria-hidden="true" tabindex="-1"></a>Another option is to **overfit the model on a batch of data**. If everything is functioning correctly, one should achieve perfect train metrics/losses.</span>
<span id="cb7-127"><a href="#cb7-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-128"><a href="#cb7-128" aria-hidden="true" tabindex="-1"></a>It is also helpful to **visualize what goes into the model**. This can help detect errors in _data augmentation_ or _data preprocessing_. This should be done just before executing this line of code:</span>
<span id="cb7-129"><a href="#cb7-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-132"><a href="#cb7-132" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-133"><a href="#cb7-133" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb7-134"><a href="#cb7-134" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb7-135"><a href="#cb7-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-136"><a href="#cb7-136" aria-hidden="true" tabindex="-1"></a>y_hat <span class="op">=</span> model(x)</span>
<span id="cb7-137"><a href="#cb7-137" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-138"><a href="#cb7-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-139"><a href="#cb7-139" aria-hidden="true" tabindex="-1"></a>Observing predictions from a fixed sample throughout training can provide insight into the dynamics of the process. In _object detection_, one might display bounding boxes and classification scores. @fig-practical-wb-comparison shows an example of a segmentation problem: the input image is on the left, and the model’s segmentation is on the right.</span>
<span id="cb7-140"><a href="#cb7-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-141"><a href="#cb7-141" aria-hidden="true" tabindex="-1"></a>::: {#fig-practical-wb-comparison}</span>
<span id="cb7-142"><a href="#cb7-142" aria-hidden="true" tabindex="-1"></a><span class="al">![ ]({{&lt; meta params.images_path &gt;}}wb_comparison.jpg)</span>{width=600}</span>
<span id="cb7-143"><a href="#cb7-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-144"><a href="#cb7-144" aria-hidden="true" tabindex="-1"></a>Example of a segmentation problem: left is the input, right is the output.</span>
<span id="cb7-145"><a href="#cb7-145" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-146"><a href="#cb7-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-147"><a href="#cb7-147" aria-hidden="true" tabindex="-1"></a><span class="fu">### 3) Overfit</span></span>
<span id="cb7-148"><a href="#cb7-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-149"><a href="#cb7-149" aria-hidden="true" tabindex="-1"></a>At this point, you should have a good understanding of the dataset, confidence in the evaluation pipeline, and initial baselines for simple models. Additionally, you should have a sense of how well a human could perform on this task (if relevant). In this step, focus on achieving good performance on the training set, accepting overfitting temporarily. Next, focus on the validation set.</span>
<span id="cb7-150"><a href="#cb7-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-151"><a href="#cb7-151" aria-hidden="true" tabindex="-1"></a>For model architecture, follow the principle "Don't be a hero." Use existing, well-established architectures, such as _ResNet-50_ for _image classification_.</span>
<span id="cb7-152"><a href="#cb7-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-153"><a href="#cb7-153" aria-hidden="true" tabindex="-1"></a>The choice of <span class="co">[</span><span class="ot">optimizer</span><span class="co">](https://pytorch.org/docs/stable/optim.html)</span> should also be conservative. The _adam_ optimizer is often effective and less sensitive to _hyper-parameters_ like the _learning rate_.</span>
<span id="cb7-154"><a href="#cb7-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-155"><a href="#cb7-155" aria-hidden="true" tabindex="-1"></a><span class="fu">### 4) Regularization</span></span>
<span id="cb7-156"><a href="#cb7-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-157"><a href="#cb7-157" aria-hidden="true" tabindex="-1"></a>At this point, you should have achieved good performance on the training set. Now, shift focus to the validation set, as it is critical for estimating expected performance on new data. This often involves a reduction in training performance due to regularization.</span>
<span id="cb7-158"><a href="#cb7-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-159"><a href="#cb7-159" aria-hidden="true" tabindex="-1"></a>**More Training Data**</span>
<span id="cb7-160"><a href="#cb7-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-161"><a href="#cb7-161" aria-hidden="true" tabindex="-1"></a>The simplest way to improve performance (and reduce overfitting) is to collect more training data. If possible, this approach is more effective than other methods and helps a model generalize better. The benefit of additional data can be estimated by training several models on different training set sizes and observing performance changes (_learning curve_).</span>
<span id="cb7-162"><a href="#cb7-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-163"><a href="#cb7-163" aria-hidden="true" tabindex="-1"></a>::: {#fig-practical-learning-curve}</span>
<span id="cb7-164"><a href="#cb7-164" aria-hidden="true" tabindex="-1"></a><span class="al">![ ]({{&lt; meta params.images_path &gt;}}learning_curve.jpg)</span>{width=600}</span>
<span id="cb7-165"><a href="#cb7-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-166"><a href="#cb7-166" aria-hidden="true" tabindex="-1"></a>Example of a _learning curve_. X-axis is performance, Y-axis is the number of training samples. Left panel: Gaussian Naive Bayes; right panel: Support Vector Classifier.</span>
<span id="cb7-167"><a href="#cb7-167" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-168"><a href="#cb7-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-169"><a href="#cb7-169" aria-hidden="true" tabindex="-1"></a>**Data Augmentation**</span>
<span id="cb7-170"><a href="#cb7-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-171"><a href="#cb7-171" aria-hidden="true" tabindex="-1"></a>Another approach is _data augmentation_, generating new data points from existing ones by making random alterations. Data points are typically augmented _on-the-fly_ rather than in advance. ML frameworks like PyTorch provide built-in transformations, and additional packages offer more complex augmentations.</span>
<span id="cb7-172"><a href="#cb7-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-173"><a href="#cb7-173" aria-hidden="true" tabindex="-1"></a>For example, <span class="co">[</span><span class="ot">AugLy</span><span class="co">](https://github.com/facebookresearch/AugLy)</span>, see @fig-practical-augly.</span>
<span id="cb7-174"><a href="#cb7-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-175"><a href="#cb7-175" aria-hidden="true" tabindex="-1"></a>::: {#fig-practical-augly}</span>
<span id="cb7-176"><a href="#cb7-176" aria-hidden="true" tabindex="-1"></a><span class="al">![ ]({{&lt; meta params.images_path &gt;}}augly.jpg)</span>{width=600}</span>
<span id="cb7-177"><a href="#cb7-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-178"><a href="#cb7-178" aria-hidden="true" tabindex="-1"></a>AugLy</span>
<span id="cb7-179"><a href="#cb7-179" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-180"><a href="#cb7-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-181"><a href="#cb7-181" aria-hidden="true" tabindex="-1"></a>Another option is <span class="co">[</span><span class="ot">Albumentations</span><span class="co">](https://github.com/albumentations-team/albumentations)</span>, see @fig-practical-albumentations.</span>
<span id="cb7-182"><a href="#cb7-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-183"><a href="#cb7-183" aria-hidden="true" tabindex="-1"></a>::: {#fig-practical-albumentations}</span>
<span id="cb7-184"><a href="#cb7-184" aria-hidden="true" tabindex="-1"></a><span class="al">![ ]({{&lt; meta params.images_path &gt;}}albumentations.jpg)</span>{width=600}</span>
<span id="cb7-185"><a href="#cb7-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-186"><a href="#cb7-186" aria-hidden="true" tabindex="-1"></a>Albumentations</span>
<span id="cb7-187"><a href="#cb7-187" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-188"><a href="#cb7-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-189"><a href="#cb7-189" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Kornia</span><span class="co">](https://kornia.readthedocs.io/en/latest/applications/image_augmentations.html)</span> provides even more complex augmentations, see @fig-practical-kornia.</span>
<span id="cb7-190"><a href="#cb7-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-191"><a href="#cb7-191" aria-hidden="true" tabindex="-1"></a>::: {#fig-practical-kornia}</span>
<span id="cb7-192"><a href="#cb7-192" aria-hidden="true" tabindex="-1"></a><span class="al">![ ]({{&lt; meta params.images_path &gt;}}kornia.png)</span>{width=600}</span>
<span id="cb7-193"><a href="#cb7-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-194"><a href="#cb7-194" aria-hidden="true" tabindex="-1"></a>Kornia</span>
<span id="cb7-195"><a href="#cb7-195" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-196"><a href="#cb7-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-197"><a href="#cb7-197" aria-hidden="true" tabindex="-1"></a>@fig-practical-augmentation-rhino shows an example of _data augmentation_ with color changes, cropping, and rotations.</span>
<span id="cb7-198"><a href="#cb7-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-199"><a href="#cb7-199" aria-hidden="true" tabindex="-1"></a>::: {#fig-practical-augmentation-rhino}</span>
<span id="cb7-200"><a href="#cb7-200" aria-hidden="true" tabindex="-1"></a><span class="al">![ ]({{&lt; meta params.images_path &gt;}}augmentation_rhino.png)</span>{width=600}</span>
<span id="cb7-201"><a href="#cb7-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-202"><a href="#cb7-202" aria-hidden="true" tabindex="-1"></a>Data augmentation example.</span>
<span id="cb7-203"><a href="#cb7-203" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-204"><a href="#cb7-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-205"><a href="#cb7-205" aria-hidden="true" tabindex="-1"></a>Synthetic data can also be added to the training process, which can be successful. @fig-practical-synthetic-examples shows an example where 3D models of animals were created using a game engine and placed either in synthetic landscapes or spliced into real camera trap images.</span>
<span id="cb7-206"><a href="#cb7-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-207"><a href="#cb7-207" aria-hidden="true" tabindex="-1"></a>::: {#fig-practical-synthetic-examples}</span>
<span id="cb7-208"><a href="#cb7-208" aria-hidden="true" tabindex="-1"></a><span class="al">![ ]({{&lt; meta params.images_path &gt;}}synthetic_camera_trap_images.jpg)</span>{width=600}</span>
<span id="cb7-209"><a href="#cb7-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-210"><a href="#cb7-210" aria-hidden="true" tabindex="-1"></a>From @Beery2020. Synthetic and semi-synthetic data.</span>
<span id="cb7-211"><a href="#cb7-211" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-212"><a href="#cb7-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-213"><a href="#cb7-213" aria-hidden="true" tabindex="-1"></a>**Early Stopping**</span>
<span id="cb7-214"><a href="#cb7-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-215"><a href="#cb7-215" aria-hidden="true" tabindex="-1"></a>_Early stopping_ involves training a model and evaluating it on a validation set periodically (e.g., after each epoch). Training is halted if no significant improvement is observed after a set number of cycles, preventing overfitting.</span>
<span id="cb7-216"><a href="#cb7-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-217"><a href="#cb7-217" aria-hidden="true" tabindex="-1"></a>::: {#fig-practical-early-stopping}</span>
<span id="cb7-218"><a href="#cb7-218" aria-hidden="true" tabindex="-1"></a><span class="al">![ ]({{&lt; meta params.images_path &gt;}}early_stopping.png)</span>{width=600}</span>
<span id="cb7-219"><a href="#cb7-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-220"><a href="#cb7-220" aria-hidden="true" tabindex="-1"></a>Source: <span class="co">[</span><span class="ot">Link</span><span class="co">](https://www.oreilly.com/library/view/hands-on-transfer-learning/9781788831307/41172567-9482-4cad-ac87-1cfbd46026df.xhtml)</span></span>
<span id="cb7-221"><a href="#cb7-221" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-222"><a href="#cb7-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-223"><a href="#cb7-223" aria-hidden="true" tabindex="-1"></a>Frameworks like <span class="co">[</span><span class="ot">`pytorch-lightning`</span><span class="co">](https://pytorch-lightning.readthedocs.io/en/stable/common/early_stopping.html)</span> provide <span class="in">`Callbacks`</span> to implement _early stopping_.</span>
<span id="cb7-224"><a href="#cb7-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-227"><a href="#cb7-227" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-228"><a href="#cb7-228" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb7-229"><a href="#cb7-229" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb7-230"><a href="#cb7-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-231"><a href="#cb7-231" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pytorch_lightning.callbacks.early_stopping <span class="im">import</span> EarlyStopping</span>
<span id="cb7-232"><a href="#cb7-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-233"><a href="#cb7-233" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LitModel(LightningModule):</span>
<span id="cb7-234"><a href="#cb7-234" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> validation_step(<span class="va">self</span>, batch, batch_idx):</span>
<span id="cb7-235"><a href="#cb7-235" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> ...</span>
<span id="cb7-236"><a href="#cb7-236" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.log(<span class="st">"val_loss"</span>, loss)</span>
<span id="cb7-237"><a href="#cb7-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-238"><a href="#cb7-238" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LitModel()</span>
<span id="cb7-239"><a href="#cb7-239" aria-hidden="true" tabindex="-1"></a>trainer <span class="op">=</span> Trainer(callbacks<span class="op">=</span>[EarlyStopping(monitor<span class="op">=</span><span class="st">"val_loss"</span>, mode<span class="op">=</span><span class="st">"min"</span>)])</span>
<span id="cb7-240"><a href="#cb7-240" aria-hidden="true" tabindex="-1"></a>trainer.fit(model)</span>
<span id="cb7-241"><a href="#cb7-241" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-242"><a href="#cb7-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-243"><a href="#cb7-243" aria-hidden="true" tabindex="-1"></a>**Weight Decay**</span>
<span id="cb7-244"><a href="#cb7-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-245"><a href="#cb7-245" aria-hidden="true" tabindex="-1"></a>_Weight decay_ is another common regularization technique. In gradient descent, weights are reduced by a factor $\lambda$ during each update. The modified update step is:</span>
<span id="cb7-246"><a href="#cb7-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-247"><a href="#cb7-247" aria-hidden="true" tabindex="-1"></a><span class="sc">\[</span></span>
<span id="cb7-248"><a href="#cb7-248" aria-hidden="true" tabindex="-1"></a>\theta_{t+1} = \theta_t (1 - \lambda) - \eta \nabla J(\theta)</span>
<span id="cb7-249"><a href="#cb7-249" aria-hidden="true" tabindex="-1"></a><span class="sc">\]</span></span>
<span id="cb7-250"><a href="#cb7-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-251"><a href="#cb7-251" aria-hidden="true" tabindex="-1"></a>where $t$ is the iteration, $\theta$ are the model parameters, and $\eta$ is the _learning rate_. This can be applied directly in PyTorch <span class="co">[</span><span class="ot">torch.optim.SGD</span><span class="co">](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD)</span>.</span>
<span id="cb7-252"><a href="#cb7-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-253"><a href="#cb7-253" aria-hidden="true" tabindex="-1"></a>**Transfer Learning**</span>
<span id="cb7-254"><a href="#cb7-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-255"><a href="#cb7-255" aria-hidden="true" tabindex="-1"></a>Transfer learning also has a regularizing effect and can reduce model training time. This approach typically involves using a pre-trained model on a large dataset (e.g., ImageNet), removing the last layer, and replacing it with a new one suited to the target task. Various layers can be frozen or fine-tuned.</span>
<span id="cb7-256"><a href="#cb7-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-257"><a href="#cb7-257" aria-hidden="true" tabindex="-1"></a>::: {#fig-practical-transfer-learning}</span>
<span id="cb7-258"><a href="#cb7-258" aria-hidden="true" tabindex="-1"></a><span class="al">![ ]({{&lt; meta params.images_path &gt;}}transfer_learning.jpg)</span>{width=600}</span>
<span id="cb7-259"><a href="#cb7-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-260"><a href="#cb7-260" aria-hidden="true" tabindex="-1"></a>Source: @johnson_eecs_2021</span>
<span id="cb7-261"><a href="#cb7-261" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-262"><a href="#cb7-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-263"><a href="#cb7-263" aria-hidden="true" tabindex="-1"></a>The following example code <span class="co">[</span><span class="ot">(PyTorch-Link)</span><span class="co">](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html)</span> shows how to freeze the parameters of a pre-trained model by setting <span class="in">`requires_grad = False`</span>.</span>
<span id="cb7-264"><a href="#cb7-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-267"><a href="#cb7-267" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-268"><a href="#cb7-268" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb7-269"><a href="#cb7-269" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb7-270"><a href="#cb7-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-271"><a href="#cb7-271" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> set_parameter_requires_grad(model, feature_extracting):</span>
<span id="cb7-272"><a href="#cb7-272" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> feature_extracting:</span>
<span id="cb7-273"><a href="#cb7-273" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> param <span class="kw">in</span> model.parameters():</span>
<span id="cb7-274"><a href="#cb7-274" aria-hidden="true" tabindex="-1"></a>            param.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb7-275"><a href="#cb7-275" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-276"><a href="#cb7-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-277"><a href="#cb7-277" aria-hidden="true" tabindex="-1"></a><span class="fu">### 5) Hyper-Parameter Tuning</span></span>
<span id="cb7-278"><a href="#cb7-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-279"><a href="#cb7-279" aria-hidden="true" tabindex="-1"></a>This step involves systematically evaluating various _hyper-parameters_ and architectures. Techniques include _grid-search_ or _random-search_, with _random-search_ generally preferred.</span>
<span id="cb7-280"><a href="#cb7-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-281"><a href="#cb7-281" aria-hidden="true" tabindex="-1"></a>Complete packages are available for this, such as <span class="co">[</span><span class="ot">Ray Tune</span><span class="co">](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html)</span>.</span>
<span id="cb7-282"><a href="#cb7-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-283"><a href="#cb7-283" aria-hidden="true" tabindex="-1"></a>Hyper-parameter tuning requires configurable architecture implementations. Below is an example of configurable fully connected layers.</span>
<span id="cb7-284"><a href="#cb7-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-287"><a href="#cb7-287" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-288"><a href="#cb7-288" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb7-289"><a href="#cb7-289" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb7-290"><a href="#cb7-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-291"><a href="#cb7-291" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Net(nn.Module):</span>
<span id="cb7-292"><a href="#cb7-292" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, l1<span class="op">=</span><span class="dv">120</span>, l2<span class="op">=</span><span class="dv">84</span>):</span>
<span id="cb7-293"><a href="#cb7-293" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(Net, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb7-294"><a href="#cb7-294" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(<span class="dv">3</span>, <span class="dv">6</span>, <span class="dv">5</span>)</span>
<span id="cb7-295"><a href="#cb7-295" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pool <span class="op">=</span> nn.MaxPool2d(<span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb7-296"><a href="#cb7-296" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(<span class="dv">6</span>, <span class="dv">16</span>, <span class="dv">5</span>)</span>
<span id="cb7-297"><a href="#cb7-297" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">16</span> <span class="op">*</span> <span class="dv">5</span> <span class="op">*</span> <span class="dv">5</span>, l1)</span>
<span id="cb7-298"><a href="#cb7-298" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(l1, l2)</span>
<span id="cb7-299"><a href="#cb7-299" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc3 <span class="op">=</span> nn.Linear(l2, <span class="dv">10</span>)</span>
<span id="cb7-300"><a href="#cb7-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-301"><a href="#cb7-301" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb7-302"><a href="#cb7-302" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(F.relu(<span class="va">self</span>.conv1(x)))</span>
<span id="cb7-303"><a href="#cb7-303" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(F.relu(<span class="va">self</span>.conv2(x)))</span>
<span id="cb7-304"><a href="#cb7-304" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">16</span> <span class="op">*</span> <span class="dv">5</span> <span class="op">*</span> <span class="dv">5</span>)</span>
<span id="cb7-305"><a href="#cb7-305" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb7-306"><a href="#cb7-306" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc2(x))</span>
<span id="cb7-307"><a href="#cb7-307" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc3(x)</span>
<span id="cb7-308"><a href="#cb7-308" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb7-309"><a href="#cb7-309" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-310"><a href="#cb7-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-311"><a href="#cb7-311" aria-hidden="true" tabindex="-1"></a>For domain-specific problems, consult relevant literature for ideas on possible hyper-parameter variations and architectures (e.g., medical imaging or satellite images).</span>
<span id="cb7-312"><a href="#cb7-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-313"><a href="#cb7-313" aria-hidden="true" tabindex="-1"></a><span class="fu">### 6) Squeeze out the Juice</span></span>
<span id="cb7-314"><a href="#cb7-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-315"><a href="#cb7-315" aria-hidden="true" tabindex="-1"></a>Once the best architectures and hyper-parameters are identified, there are additional ways to maximize performance.</span>
<span id="cb7-316"><a href="#cb7-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-317"><a href="#cb7-317" aria-hidden="true" tabindex="-1"></a>One is _ensembling_: training multiple models and combining their predictions often yields slightly better performance than a single model. However, this increases training and inference costs.</span>
<span id="cb7-318"><a href="#cb7-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-319"><a href="#cb7-319" aria-hidden="true" tabindex="-1"></a>Surprising effects sometimes occur in deep learning, like _double descent_ (@nakkiran_deep_2019) or _grokking_ (@power_grokking_2022). For instance, using highly complex models (an overfitting risk) can work well, and after a plateau, validation loss might improve if training continues. So it may be worthwhile to train a model longer (without _early stopping_).</span>
<span id="cb7-320"><a href="#cb7-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-321"><a href="#cb7-321" aria-hidden="true" tabindex="-1"></a>::: {#fig-practical-double-descent}</span>
<span id="cb7-322"><a href="#cb7-322" aria-hidden="true" tabindex="-1"></a><span class="al">![ ]({{&lt; meta params.images_path &gt;}}double_descent.jpg)</span>{width=600}</span>
<span id="cb7-323"><a href="#cb7-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-324"><a href="#cb7-324" aria-hidden="true" tabindex="-1"></a>Source: @nakkiran_deep_2019. The X-axis represents model complexity, and the Y-axis shows test and train error. Test error decreases, increases (overfitting), and then decreases again (double descent).</span>
<span id="cb7-325"><a href="#cb7-325" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-326"><a href="#cb7-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-327"><a href="#cb7-327" aria-hidden="true" tabindex="-1"></a>Additional techniques often yield small improvements. For example, @liu_convnet_2022 reports that the following techniques improved accuracy by about 2.7%:</span>
<span id="cb7-328"><a href="#cb7-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-329"><a href="#cb7-329" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Longer training</span>
<span id="cb7-330"><a href="#cb7-330" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Specialized optimizer (AdamW)</span>
<span id="cb7-331"><a href="#cb7-331" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Complex data augmentation techniques (Mixup, Cutmix, RandAugment)</span>
<span id="cb7-332"><a href="#cb7-332" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Regularization techniques (Stochastic Depth)</span>
<span id="cb7-333"><a href="#cb7-333" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Label smoothing</span>
<span id="cb7-334"><a href="#cb7-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-335"><a href="#cb7-335" aria-hidden="true" tabindex="-1"></a><span class="fu">## Model Zoos and Frameworks</span></span>
<span id="cb7-336"><a href="#cb7-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-337"><a href="#cb7-337" aria-hidden="true" tabindex="-1"></a><span class="fu">### Hugging-Face</span></span>
<span id="cb7-338"><a href="#cb7-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-339"><a href="#cb7-339" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">HuggingFace</span><span class="co">](https://huggingface.co/)</span> provides a wide range of models, datasets, and APIs. It offers easy-to-use Python APIs to integrate, use, and adapt models for PyTorch.</span>
<span id="cb7-340"><a href="#cb7-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-341"><a href="#cb7-341" aria-hidden="true" tabindex="-1"></a><span class="fu">### timm</span></span>
<span id="cb7-342"><a href="#cb7-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-343"><a href="#cb7-343" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">PyTorch Image Models (timm)</span><span class="co">](https://github.com/rwightman/pytorch-image-models)</span> is a library with SOTA models, layers, and functions for using, reproducing, and adapting modern image models.</span>
<span id="cb7-344"><a href="#cb7-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-345"><a href="#cb7-345" aria-hidden="true" tabindex="-1"></a><span class="fu">## Code Setup / Configuration</span></span>
<span id="cb7-346"><a href="#cb7-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-347"><a href="#cb7-347" aria-hidden="true" tabindex="-1"></a>Setting up a well-organized repository structure can significantly aid in model development. Key aspects include reproducibility, configurability, tracking, and logging experiments.</span>
<span id="cb7-348"><a href="#cb7-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-349"><a href="#cb7-349" aria-hidden="true" tabindex="-1"></a>Suggested repository structures:</span>
<span id="cb7-350"><a href="#cb7-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-351"><a href="#cb7-351" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>DS-cookie cutter: <span class="co">[</span><span class="ot">DrivenData</span><span class="co">](https://github.com/drivendata/cookiecutter-data-science)</span></span>
<span id="cb7-352"><a href="#cb7-352" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>PyTorch-Lightning example: <span class="co">[</span><span class="ot">Lightning Template</span><span class="co">](https://github.com/PyTorchLightning/deep-learning-project-template)</span></span>
<span id="cb7-353"><a href="#cb7-353" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Configurations: <span class="co">[</span><span class="ot">Hydra</span><span class="co">](https://hydra.cc/docs/intro/)</span></span>
<span id="cb7-354"><a href="#cb7-354" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Experiment tracking: <span class="co">[</span><span class="ot">Weights &amp; Biases</span><span class="co">](https://wandb.ai/site)</span>, <span class="co">[</span><span class="ot">Neptune AI</span><span class="co">](https://neptune.ai/experiment-tracking)</span></span>
<span id="cb7-355"><a href="#cb7-355" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Version Control for ML projects: <span class="co">[</span><span class="ot">DVC</span><span class="co">](https://dvc.org/)</span></span>
<span id="cb7-356"><a href="#cb7-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-357"><a href="#cb7-357" aria-hidden="true" tabindex="-1"></a><span class="fu">## References</span></span>
<span id="cb7-358"><a href="#cb7-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-359"><a href="#cb7-359" aria-hidden="true" tabindex="-1"></a>::: {#refs}</span>
<span id="cb7-360"><a href="#cb7-360" aria-hidden="true" tabindex="-1"></a>:::</span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© Copyright 2024, Marco Willi</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2025/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This page is built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>