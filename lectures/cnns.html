<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.24">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Marco Willi">

<title>4 - Convolutional Neural Networks – CAS Deep Learning - Computer Vision</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../lectures/classification.html" rel="next">
<link href="../lectures/neural_networks.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-a6e161b2431e1f94a14e0f5d32135a3c.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-7365f4c1a6be05280eba792c58017f0b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-SCHRGR3LNM"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-SCHRGR3LNM', { 'anonymize_ip': true});
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../lectures/intro.html">Lectures</a></li><li class="breadcrumb-item"><a href="../lectures/cnns.html">4 - Convolutional Neural Networks</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-center sidebar-header">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../assets/logo.webp" alt="" class="sidebar-logo light-content py-0 d-lg-inline d-none">
      <img src="../assets/logo.webp" alt="" class="sidebar-logo dark-content py-0 d-lg-inline d-none">
      </a>
      <div class="sidebar-tools-main">
    <a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2025" title="GitHub organization" class="quarto-navigation-tool px-1" aria-label="GitHub organization"><i class="bi bi-github"></i></a>
    <a href="" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-code-square"></i></a>
</div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Slides</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../slides/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Einführung Computer Vision mit Deep Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../slides/cnns.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Convolutional Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../slides/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../slides/practical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Practical Considerations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../slides/recent_advances.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Recent Advances</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/quiz.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Quiz &amp; Exam Prep</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Lectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1 - Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2 - Software &amp; Hardware for Deep Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/neural_networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3 - Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/cnns.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">4 - Convolutional Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5 - Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/practical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6 - Practical</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/recent_advances.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">7 - Recent Advances</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Resources</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/literature.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Books</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/links.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Helpful Links &amp; Resources</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Mathematical Notation</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction-motivation" id="toc-introduction-motivation" class="nav-link active" data-scroll-target="#introduction-motivation">Introduction &amp; Motivation</a>
  <ul class="collapse">
  <li><a href="#mlps-and-images" id="toc-mlps-and-images" class="nav-link" data-scroll-target="#mlps-and-images">MLPs and Images</a></li>
  <li><a href="#invariance-and-equivariance" id="toc-invariance-and-equivariance" class="nav-link" data-scroll-target="#invariance-and-equivariance">Invariance and Equivariance</a></li>
  </ul></li>
  <li><a href="#convolutional-layers" id="toc-convolutional-layers" class="nav-link" data-scroll-target="#convolutional-layers">Convolutional Layers</a>
  <ul class="collapse">
  <li><a href="#convolutional-neural-networks" id="toc-convolutional-neural-networks" class="nav-link" data-scroll-target="#convolutional-neural-networks">Convolutional Neural Networks</a></li>
  <li><a href="#convolution-operation" id="toc-convolution-operation" class="nav-link" data-scroll-target="#convolution-operation">Convolution Operation</a></li>
  <li><a href="#padding-stride-kernel-size-dilation" id="toc-padding-stride-kernel-size-dilation" class="nav-link" data-scroll-target="#padding-stride-kernel-size-dilation">Padding, Stride, Kernel Size, Dilation</a></li>
  <li><a href="#calculations-spatial-dimensionality-of-activation-maps" id="toc-calculations-spatial-dimensionality-of-activation-maps" class="nav-link" data-scroll-target="#calculations-spatial-dimensionality-of-activation-maps">Calculations: Spatial Dimensionality of Activation Maps</a></li>
  <li><a href="#calculations-number-of-weights-in-a-convolutional-layer" id="toc-calculations-number-of-weights-in-a-convolutional-layer" class="nav-link" data-scroll-target="#calculations-number-of-weights-in-a-convolutional-layer">Calculations: Number of Weights in a Convolutional Layer</a></li>
  </ul></li>
  <li><a href="#properties" id="toc-properties" class="nav-link" data-scroll-target="#properties">Properties</a>
  <ul class="collapse">
  <li><a href="#local-sparse-connectivity-parameter-sharing" id="toc-local-sparse-connectivity-parameter-sharing" class="nav-link" data-scroll-target="#local-sparse-connectivity-parameter-sharing">Local (Sparse) Connectivity &amp; Parameter Sharing</a></li>
  <li><a href="#translation-invariance-equivariance" id="toc-translation-invariance-equivariance" class="nav-link" data-scroll-target="#translation-invariance-equivariance">Translation Invariance / Equivariance</a></li>
  <li><a href="#sec-cnn-receptive-field" id="toc-sec-cnn-receptive-field" class="nav-link" data-scroll-target="#sec-cnn-receptive-field">Stacking &amp; Receptive Field</a></li>
  </ul></li>
  <li><a href="#more-layers-operations" id="toc-more-layers-operations" class="nav-link" data-scroll-target="#more-layers-operations">More Layers &amp; Operations</a>
  <ul class="collapse">
  <li><a href="#downsampling" id="toc-downsampling" class="nav-link" data-scroll-target="#downsampling">Downsampling</a></li>
  <li><a href="#sec-cnn-upsampling" id="toc-sec-cnn-upsampling" class="nav-link" data-scroll-target="#sec-cnn-upsampling">Upsampling</a></li>
  <li><a href="#changing-the-number-of-channels" id="toc-changing-the-number-of-channels" class="nav-link" data-scroll-target="#changing-the-number-of-channels">Changing the number of channels</a></li>
  <li><a href="#global-average-pooling" id="toc-global-average-pooling" class="nav-link" data-scroll-target="#global-average-pooling">Global Average Pooling</a></li>
  <li><a href="#other-layers" id="toc-other-layers" class="nav-link" data-scroll-target="#other-layers">Other Layers</a></li>
  </ul></li>
  <li><a href="#pytorch-examples" id="toc-pytorch-examples" class="nav-link" data-scroll-target="#pytorch-examples">PyTorch Examples</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2025/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../lectures/intro.html">Lectures</a></li><li class="breadcrumb-item"><a href="../lectures/cnns.html">4 - Convolutional Neural Networks</a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">4 - Convolutional Neural Networks</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Marco Willi </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Institute for Data Science I4DS, FHNW
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  


</header>


<section id="introduction-motivation" class="level2">
<h2 class="anchored" data-anchor-id="introduction-motivation">Introduction &amp; Motivation</h2>
<p>Images have the following relevant properties when modelling them:</p>
<ul>
<li>High-Dimensional: An RGB image of size <span class="math inline">\(224 \times 224\)</span> (height, width) has = <span class="math inline">\(150'528\)</span> values.</li>
<li>Locality: Nearby pixels are statistically related</li>
<li>Stability under transformations: Interpretation of an image does not change under many geomoetric transformations.</li>
</ul>
<div id="fig-cnn-example-rgb-image" class="quarto-float quarto-figure quarto-figure-center anchored" width="600">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-example-rgb-image-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/cows2_small.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-example-rgb-image-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: <a href="https://unsplash.com/de/fotos/braune-schafe-auf-grunem-grasfeld-tagsuber-olonUwUrmQk">Image Source</a>
</figcaption>
</figure>
</div>
<p>We will see that CNNs are much better suited to model images than MLPs. Unlike MLPs, CNNs explicitly assume that inputs (e.g., pixels) that are close together need to be considered together and that information is locally correlated. This allows certain properties to be embedded in the architecture of CNNs (inductive biases) to define models much more efficiently (with fewer parameters).</p>
<section id="mlps-and-images" class="level3">
<h3 class="anchored" data-anchor-id="mlps-and-images">MLPs and Images</h3>
<p>The input to an MLP is a vector <span class="math inline">\(\mathbf{x}^{(i)}\)</span>, which is transformed through multiple hidden layers to the output layer. Each hidden layer has a certain number of neurons, each connected to all neurons in the previous layer (fully-connected layers). The fully connected layers can only process 1-D vectors. Therefore, images <span class="math inline">\(\in \mathbb{R}^{H \times W \times C}\)</span> must be flattened into 1-D vectors <span class="math inline">\(\in \mathbb{R}^p\)</span>. Here, <span class="math inline">\(p= H \times W \times C\)</span>. This causes MLPs to become very large (having many learnable parameters) when applied to high-dimensional inputs such as images. See <a href="#fig-cnn-mlp" class="quarto-xref">Figure&nbsp;2</a> for an illustration.</p>
<div id="fig-cnn-mlp" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-mlp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/mlp_images_example.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-mlp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Illustration of an MLP applied to an image.
</figcaption>
</figure>
</div>
<p>In the CIFAR-10 dataset, which consists of very small images of 32x32x3 (height, width, colors), a single neuron in the first hidden layer has 32 * 32 * 3 = 3,072 weights to learn (see <a href="#fig-cnn-mlp-images" class="quarto-xref">Figure&nbsp;3</a>).</p>
<div id="fig-cnn-mlp-images" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-mlp-images-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/mlp_images.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-mlp-images-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Source: <span class="citation" data-cites="johnson_eecs_2019">Johnson (<a href="#ref-johnson_eecs_2019" role="doc-biblioref">2019</a>)</span>
</figcaption>
</figure>
</div>
<!-- :::{#fig-cnn-spatial-structure-mlp}

![](/assets/images/cnns/mlp-spatial-structure.png){width=600}

Source: @li_cs231n_2023
::: -->
<p>For larger images, which are often encountered in practice, the number of weights is correspondingly much larger. Many neurons are also used, further increasing the number of parameters, leading to overfitting, and making learning the weights more difficult.</p>
<p>The dimensionality of weight matrix <span class="math inline">\(\mathbf{W}\)</span> scales with input size. <span class="math inline">\(\mathbf{W} \in \mathbb{R}^{d \times k}\)</span>, while <span class="math inline">\(d\)</span> the dimensionality of the inputs, and <span class="math inline">\(k\)</span> the number of neurons in the first hidden layer. The columns of the weight matrix <span class="math inline">\(\mathbf{W}\)</span> (in the first hidden layer) can be visualized to learn what kind pattern a specific neuron has learned. See <a href="#fig-cnn-mlp-example-weights" class="quarto-xref">Figure&nbsp;4</a>.</p>
<div id="fig-cnn-mlp-example-weights" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-mlp-example-weights-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/mlp_images_example_weights.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-mlp-example-weights-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Illustration of weight matrix <span class="math inline">\(\mathbf{W}\)</span> and how the weights of a single neuron can be reshaped into the input image dimensionality.
</figcaption>
</figure>
</div>
<p>Often the patterns we want to learn are not stationary. Ideally we want to recognize them at all positions, particularly under translations of the inputs, e.g.&nbsp;<span class="math inline">\(g(\mathbf{x})\)</span> where <span class="math inline">\(g()\)</span> is a spatial translation. Figure <a href="#fig-cnns-mlp-pattern-shift" class="quarto-xref">Figure&nbsp;6</a> illustrated the problem.</p>
<div id="fig-cnns-mlp-pattern-shift" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnns-mlp-pattern-shift-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/mlp_images_example_pattern_shifts.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnns-mlp-pattern-shift-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Illustration of different inputs with the same pattern but shifted spatially.
</figcaption>
</figure>
</div>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Question</strong></p>
<p>How do MLPs deal with such pattern shifts?</p>
</div>
</div>
</div>
<div id="fig-cnns-mlp-pattern-shift" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnns-mlp-pattern-shift-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/mlp_images_examples_permutation.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnns-mlp-pattern-shift-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Inputs transformed with a permuation matrix.
</figcaption>
</figure>
</div>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Question</strong></p>
<p>Can an MLP successfully learn patterns in images <span class="math inline">\(\mathbf{x}\)</span> that are permuted with a permutation matrix <span class="math inline">\(\mathbf{P}\)</span>, i.e.&nbsp;<span class="math inline">\(f(P(\mathbf{x}))\)</span>?</p>
</div>
</div>
</div>
<p>To summarize MLPs on images:</p>
<ul>
<li>High-Dimensional: The size of an MLP scales with the input dimensionality which might blow beyond memory and compute budgets.</li>
<li>Locality: MLPs have no notion of locality and thus can’t exploit this inherent bias in natural images.</li>
<li>Stability under transformations: MLPs need to learn position-dependent patterns which is very inefficient.</li>
</ul>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Demonstration</strong></p>
<p><a href="https://adamharley.com/nn_vis/mlp/2d.html">https://adamharley.com/nn_vis/mlp/2d.html</a></p>
</div>
</div>
</div>
<p>A single neuron in a CNN is only connected to a small portion (local connectivity) of the image (see <a href="#fig-cnn-cnn-spatial" class="quarto-xref">Figure&nbsp;7</a>). As a result, the neurons have far fewer parameters than in an MLP. The 2-D structure of the image is also preserved, meaning they do not need to be flattened as in an MLP. This exploits the property of images that certain features, such as edges and corners, are relevant throughout the image. By convolving the neurons across the entire input, the same feature can be detected by a neuron throughout the image. In an MLP, a specific feature would need to be relearned at each position.</p>
<div id="fig-cnn-cnn-spatial" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-cnn-spatial-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/cnn_spatial.jpg" class="img-fluid figure-img" width="200">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-cnn-spatial-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Source: <span class="citation" data-cites="johnson_eecs_2019">Johnson (<a href="#ref-johnson_eecs_2019" role="doc-biblioref">2019</a>)</span>
</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>CNNs are used not only for image data but also for data with spatial dependencies/local structures. This includes not only images but also time series, videos, audio, and text. The key is that signals that are spatially close together should be interpreted together.</p>
</div>
</div>
<p>What properties do we want?</p>
</section>
<section id="invariance-and-equivariance" class="level3">
<h3 class="anchored" data-anchor-id="invariance-and-equivariance">Invariance and Equivariance</h3>
<p>For many tasks small variations in the input should either <strong>not</strong> change the model output (invariance) or should change the output <strong>in tandem</strong> with the input changes (equivariance).</p>
<p>A function <span class="math inline">\(f(\mathbf{x})\)</span> is <strong>invariant</strong> to a transformation <span class="math inline">\(g(\mathbf{x})\)</span> if:</p>
<p><span class="math display">\[\begin{align}
f(g(\mathbf{x})) = f(\mathbf{x})
\end{align}\]</span></p>
<p>A function <span class="math inline">\(f(\mathbf{x})\)</span> (such as a layer in a neural network) of an image <span class="math inline">\(\mathbf{x}\)</span> is <strong>equivariant</strong> with respect to a transformation <span class="math inline">\(g(\mathbf{\mathbf{x}})\)</span> if:</p>
<p><span class="math display">\[\begin{align}
f(g(\mathbf{x})) = g(f(\mathbf{x}))
\end{align}\]</span></p>
<p>Example where invariance is required:</p>
<div id="fig-cnns-invariance-example" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnns-invariance-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/cows2_small_invariance_collage.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnns-invariance-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: When objects in the input translate spatially, the output (in this case the classification of the images) does not change. The model <span class="math inline">\(f(\mathbf{x})\)</span> is thus invariant to spatial translations.
</figcaption>
</figure>
</div>
<p>Example where equivariance is required:</p>
<div id="fig-cnns-equivariance-example" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnns-equivariance-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/cows2_small_detection_collage.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnns-equivariance-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: When objects in the input (first row) translate spatially, the detections (bounding boxes) change accordingly (bottom row). The model <span class="math inline">\(f(\mathbf{x})\)</span> that produces the bounding boxes is thus equivariant with respect to spatial translations.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="convolutional-layers" class="level2">
<h2 class="anchored" data-anchor-id="convolutional-layers">Convolutional Layers</h2>
<section id="convolutional-neural-networks" class="level3">
<h3 class="anchored" data-anchor-id="convolutional-neural-networks">Convolutional Neural Networks</h3>
<p>CNNs consist of a sequence of different layers. Each layer transforms activations from the previous layer into new activations through a differentiable operation. Below we look at the main layer types: convolutional layers, pooling layers, activation layers, and fully connected layers. Arranged in a specific sequence, this is referred to as the architecture of the model.</p>
<p><a href="#fig-cnn-convnet" class="quarto-xref">Figure&nbsp;10</a> shows an example architecture. The activation maps of the various layers are shown, representing the corresponding outputs of the layers.</p>
<div id="fig-cnn-convnet" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-convnet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/convnet.jpeg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-convnet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: The activations of a ConvNet architecture are shown. The input image is on the left and the predictions on the right. Source: <span class="citation" data-cites="li_cs231n_2022">Li (<a href="#ref-li_cs231n_2022" role="doc-biblioref">2022</a>)</span>.
</figcaption>
</figure>
</div>
<p>Sometimes different layers are combined and referred to as a block. For example, the combination of a convolutional layer followed by an activation layer and a pooling layer is often used. This would be a CONV-ACT-POOL block.</p>
</section>
<section id="convolution-operation" class="level3">
<h3 class="anchored" data-anchor-id="convolution-operation">Convolution Operation</h3>
<p>Convolutional layers are the main layers in CNNs responsible for extracting visual features. The weights of a convolutional layer consist of a set of learnable filters. Each filter is typically small along the spatial dimensions (height, width) relative to the input but extends over the entire input depth. A typical filter in the first layer, for example, has the dimension <span class="math inline">\(7 \times 7 \times 3\)</span> (7 pixels along height/width and 3 along the color channels). During the forward pass, the filters are convolved along height/width over the input. At each position, the dot product (when considering the input and filter as 1-D vectors) between the filter and input is calculated. This produces a 2-D activation map representing the filter’s expression at each position in the input. Intuitively, the CNN learns filters corresponding to typical visual patterns, such as edges and colors. A set of <span class="math inline">\(K\)</span> filters produces activation maps with a depth of <span class="math inline">\(K\)</span>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Filter and kernel are sometimes used synonymously. Here, we differentiate by considering a filter as 3-dimensional (CxHxW) and a kernel as 2-dimensional (HxW). A filter consists of C kernels.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Convolution in deep learning is typically implemented as cross-correlation.</p>
<p><span class="math display">\[\begin{equation}
S(i, j) = (K * I)(i, j) =\sum_m\sum_n I(i + m, j + n)K(m, n)
\end{equation}\]</span></p>
<p><span class="math inline">\(I\)</span> is the input (for example an image), <span class="math inline">\(K\)</span> is the kernel (typically smaller than <span class="math inline">\(I\)</span>) and <span class="math inline">\(b\)</span> is a bias term which is being added to the weighted sum.</p>
<p>If <span class="math inline">\(I\)</span> is an RGB image (in the first layer of a CNN for example), the kernel <span class="math inline">\(K\)</span> would have dimensionality <span class="math inline">\(3 \times K \times K\)</span> (assuming a square kernel). More generally we learn kernels of the dimensionality <span class="math inline">\(C_{in} \times K \times K\)</span>.</p>
<p>Multiple kernels, let’s say <span class="math inline">\(C_o\)</span> kernels, can be grouped together: <span class="math inline">\(C_o \times C_{in} \times K \times K\)</span>.</p>
<p>We often refer to such tensors as filters or filter banks.</p>
</div>
</div>
<p>Convolution on RGB images is nicely illusrated in <a href="#fig-cnns-intro-example" class="quarto-xref">Figure&nbsp;11</a>. We can see that the filter is multiplied element-wise with the corresponding values in the image, the result is summed top produce one single value in the output.</p>
<div id="fig-cnns-intro-example" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnns-intro-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../assets/images/cnns/convolution_prince_rgb.png" class="img-fluid figure-img"></p>
<figcaption>Source: <span class="citation" data-cites="prince_understanding_2023">Prince (<a href="#ref-prince_understanding_2023" role="doc-biblioref">2023</a>)</span></figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-cnns-intro-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11
</figcaption>
</figure>
</div>
<p>The following figures show further step-by-step illustration of applying convolutions on input activations that produce multiple output activation maps.</p>
<div id="fig-cnn-conv-one-number" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-conv-one-number-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/cnn_conv_one_number.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-conv-one-number-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: Source: <span class="citation" data-cites="johnson_eecs_2019">Johnson (<a href="#ref-johnson_eecs_2019" role="doc-biblioref">2019</a>)</span>
</figcaption>
</figure>
</div>
<div id="fig-cnn-conv-activation-map" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-conv-activation-map-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/conv_activation_map.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-conv-activation-map-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13: Source: <span class="citation" data-cites="johnson_eecs_2019">Johnson (<a href="#ref-johnson_eecs_2019" role="doc-biblioref">2019</a>)</span>
</figcaption>
</figure>
</div>
<div id="fig-cnn-conv-activation-map2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-conv-activation-map2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/conv_activation_map2.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-conv-activation-map2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14: Source: <span class="citation" data-cites="johnson_eecs_2019">Johnson (<a href="#ref-johnson_eecs_2019" role="doc-biblioref">2019</a>)</span>
</figcaption>
</figure>
</div>
<div id="fig-cnn-conv-activation-map3" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-conv-activation-map3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/conv_activation_map3.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-conv-activation-map3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15: Source: <span class="citation" data-cites="johnson_eecs_2019">Johnson (<a href="#ref-johnson_eecs_2019" role="doc-biblioref">2019</a>)</span>
</figcaption>
</figure>
</div>
<p>The data is processed in mini-batches, i.e., multiple images at once, as shown in <a href="#fig-cnn-conv-activation-map4" class="quarto-xref">Figure&nbsp;16</a>.</p>
<div id="fig-cnn-conv-activation-map4" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-conv-activation-map4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/conv_activation_map4.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-conv-activation-map4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16: Source: <span class="citation" data-cites="johnson_eecs_2019">Johnson (<a href="#ref-johnson_eecs_2019" role="doc-biblioref">2019</a>)</span>
</figcaption>
</figure>
</div>
</section>
<section id="padding-stride-kernel-size-dilation" class="level3">
<h3 class="anchored" data-anchor-id="padding-stride-kernel-size-dilation">Padding, Stride, Kernel Size, Dilation</h3>
<p>To define a convolution, various hyperparameters need to be set. Some of the most important ones are:</p>
<ul>
<li>Padding: How much padding is added to the input?</li>
<li>Stride: What is the step size (stride) of the kernel?</li>
<li>Kernel-Size: What is the kernel size?</li>
<li>Dilation: What is the dilation rate?</li>
</ul>
<p><strong>Stride</strong> determines how the filters are convolved over the input activations, essentially the step size. If the stride is 1, the filter moves one pixel at a time to compute the next activation (see <a href="#fig-cnn-stride1" class="quarto-xref">Figure&nbsp;17</a>). If the stride is greater, e.g., 2, it moves two pixels at a time, making the activation maps smaller in width and height (see <a href="#fig-cnn-stride2" class="quarto-xref">Figure&nbsp;18</a>).</p>
<div id="fig-cnn-stride1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-stride1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/stride1_example.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-cnn-stride1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17
</figcaption>
</figure>
</div>
<div id="fig-cnn-stride2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-stride2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/stride2_example.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-cnn-stride2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;18
</figcaption>
</figure>
</div>
<p><strong>Padding</strong> refers to adding (typically) zeros to the border of the input activations before performing the convolution. This can be useful to ensure, for example, that the spatial dimensions of the activation maps are identical to those of the input activations. This is essential for segmentation tasks. <a href="#fig-cnn-padding" class="quarto-xref">Figure&nbsp;19</a> illustrates how padding helps maintain the spatial resolution when padding the input activations.</p>
<div id="fig-cnn-padding" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-padding-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/padding_example_result.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-padding-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;19: Left: Input (Yellow) with Zero-Padding (white border), Middle: Filter, Right: Output.
</figcaption>
</figure>
</div>
<p><strong>Kernel-size</strong> determines the spatial extend from which a value in the output activation map migh aggregate values. It is the receptive field of an output value with respect to the input values. <a href="#fig-cnn-kernel-size3x3" class="quarto-xref">Figure&nbsp;20</a> and <a href="#fig-cnn-kernel-size5x5" class="quarto-xref">Figure&nbsp;21</a> show convolutions with different kernel sizes. Note that the amount of padding increases with kernel-size. Kernels usually have an odd number of pixels in height and width to make sure they have a center pixel. Also, the number of parameters increases with kernel-size <span class="math inline">\((C_{in} \times k \times k) + C_{out}\)</span>, which is often an important consideration.</p>
<div id="fig-cnn-kernel-size3x3" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-kernel-size3x3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/kernel_size_example_3x3.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-kernel-size3x3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;20: Convolution with zero-padding and <span class="math inline">\(3 \times 3\)</span> kernel that produces activations maps with unchanged spatial resolution.
</figcaption>
</figure>
</div>
<div id="fig-cnn-kernel-size5x5" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-kernel-size5x5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/kernel_size_example_5x5.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-kernel-size5x5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;21: Convolution with zero-padding and <span class="math inline">\(5 \times 5\)</span> kernel that produces activations maps with unchanged spatial resolution.
</figcaption>
</figure>
</div>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Question</strong></p>
<p>What is the difference between using 3x3 vs 5x5 kernels? What is the same?</p>
</div>
</div>
</div>
<p><strong>Dilation</strong> specifies how much the kernel is spread out and thus directly influences the receptive field. <a href="#fig-cnn-dilation" class="quarto-xref">Figure&nbsp;22</a> shows how it works.</p>
<div id="fig-cnn-dilation" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-dilation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/dilation_example.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-dilation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;22: Illustration of how a kernel with dilation=2 (middle) is applied to the input activations (left) to produce activations (right). Note the reduced spatial dimensionality of the output activations.
</figcaption>
</figure>
</div>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Question</strong></p>
<p>Why would we use dilation <span class="math inline">\(d \gt 1\)</span>?</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><span class="citation" data-cites="dumoulin_guide_2016">Dumoulin and Visin (<a href="#ref-dumoulin_guide_2016" role="doc-biblioref">2016</a>)</span> has created some animations for better understanding of convolutions and published them here: <a href="https://github.com/vdumoulin/conv_arithmetic">https://github.com/vdumoulin/conv_arithmetic</a>.</p>
<div id="fig-cnn-stride-and-padding-gif3" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-stride-and-padding-gif3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/no_padding_no_strides.gif" class="img-fluid figure-img" width="200">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-stride-and-padding-gif3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;23: Convolving a 3x3 kernel over a 5x5 input without padding and with stride 1x1. Source <span class="citation" data-cites="dumoulin_guide_2016">Dumoulin and Visin (<a href="#ref-dumoulin_guide_2016" role="doc-biblioref">2016</a>)</span>
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="calculations-spatial-dimensionality-of-activation-maps" class="level3">
<h3 class="anchored" data-anchor-id="calculations-spatial-dimensionality-of-activation-maps">Calculations: Spatial Dimensionality of Activation Maps</h3>
<p>You can calculate the dimensionality of the activation maps with the following formulas:</p>
<ul>
<li><span class="math inline">\(i\)</span>: Side length of the input activations (assumption: square inputs)</li>
<li><span class="math inline">\(k\)</span>: Kernel size (assumption: square kernel)</li>
<li><span class="math inline">\(o\)</span>: Side length of the output activation maps</li>
<li><span class="math inline">\(s\)</span>: Stride (assumption: same stride along spatial dimensions)</li>
<li><span class="math inline">\(p\)</span>: Number of paddings on each side (assumption: same number of paddings along spatial dimensions)</li>
<li><span class="math inline">\(d\)</span>: Dilation rate.</li>
</ul>
<p><span class="math display">\[\begin{equation}
o = \left\lfloor \frac{i + 2p - k-(k-1)(d-1)}{s} \right\rfloor + 1
\end{equation}\]</span></p>
<p>Note: If dilation is 1 <span class="math inline">\((k-1)(d-1)\)</span> equals 0.</p>
</section>
<section id="calculations-number-of-weights-in-a-convolutional-layer" class="level3">
<h3 class="anchored" data-anchor-id="calculations-number-of-weights-in-a-convolutional-layer">Calculations: Number of Weights in a Convolutional Layer</h3>
<p>You can calculate the total number of weights in a convolutional layer with the following formula:</p>
<ul>
<li><span class="math inline">\(k\)</span>: Kernel size (assumption: square kernel, so kernel has dimensions <span class="math inline">\(k \times k\)</span>)</li>
<li><span class="math inline">\(C_{in}\)</span>: Number of input channels</li>
<li><span class="math inline">\(C_{out}\)</span>: Number of output channels</li>
</ul>
<p><span class="math display">\[\begin{equation}
\text{Total Weights} = (k \times k) \times C_{in} \times C_{out}
\end{equation}\]</span></p>
<p>Each filter has a size of <span class="math inline">\(k \times k \times C_{in}\)</span>, and there are <span class="math inline">\(C_{out}\)</span> filters in total, resulting in <span class="math inline">\((k \times k \times C_{in}) \times C_{out}\)</span> weights.</p>
<p><strong>Biases:</strong> If each output channel has a bias term, add an additional <span class="math inline">\(C_{out}\)</span> weights for the biases.</p>
<p><span class="math display">\[\begin{equation}
\text{Total Parameters} = (k \times k \times C_{in} \times C_{out}) + C_{out}
\end{equation}\]</span></p>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Question</strong></p>
<p><strong>Scenario:</strong></p>
<ul>
<li>Input activations: <span class="math inline">\(3 \times 32 \times 32\)</span> (C, H, W)</li>
<li>Convolution: 10 filters with: <span class="math inline">\(k=5  \times 5\)</span>, <span class="math inline">\(s=1\)</span>, <span class="math inline">\(p=2\)</span>, <span class="math inline">\(d=1\)</span></li>
</ul>
<p><strong>Questions:</strong></p>
<ol type="1">
<li><p>What is the size of the activation map <span class="math inline">\(C_{out}, H_{out}, W_{out}\)</span> ?</p></li>
<li><p>How many weights / parameters defines this operation?</p></li>
</ol>
</div>
</div>
</div>
</section>
</section>
<section id="properties" class="level2">
<h2 class="anchored" data-anchor-id="properties">Properties</h2>
<section id="local-sparse-connectivity-parameter-sharing" class="level3">
<h3 class="anchored" data-anchor-id="local-sparse-connectivity-parameter-sharing">Local (Sparse) Connectivity &amp; Parameter Sharing</h3>
<p>Fully connected layers are, as discussed, impractical when working with high-dimensional inputs like images. If all neurons in a layer were connected to all previous neurons, the number of parameters to be estimated would increase massively, which is inefficient and leads to overfitting. Each neuron is therefore only connected to a local region of the input volume. The spatial extent of this region is a hyperparameter and is called the receptive field of a neuron (also kernel size) on the input volume. The connections along the depth (C) extend over the entire depth of the input volume. The connections are therefore local along the spatial dimensions (width and height) but complete along the depth.</p>
<p>Parameter sharing in convolutional layers is used to reduce the number of parameters. Since the filters are convolved over the inputs, the individual weights of the filters are identical over the spatial extent of the input volume. One of the main assumptions behind CNNs is the following: If it is useful to learn a specific (visual) feature at a certain position, then it is probably useful at other positions as well. In other words: If I learn filters that detect edges, corners, or cats, then it is a reasonable assumption that I want to do this throughout the image.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Sometimes parameter sharing does not make sense. This can be the case, for example, if we have centered structures in the images. Then you might want to learn position-dependent features. An example is images of faces that have been centered, where you might want to learn filters that detect the mouth only in the lower middle area (locally connected layers).</p>
</div>
</div>
<p>The following output shows the number of parameters in an MLP and a CNN (each with two hidden layers) on the CIFAR10 dataset.</p>
<div id="aab25a97" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchinfo</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MLP(nn.Module):</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.flatten <span class="op">=</span> nn.Flatten()</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layer1 <span class="op">=</span> nn.Linear(<span class="dv">3</span> <span class="op">*</span> <span class="dv">32</span> <span class="op">*</span> <span class="dv">32</span>, <span class="dv">64</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layer2 <span class="op">=</span> nn.Linear(<span class="dv">64</span>, <span class="dv">32</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_layer <span class="op">=</span> nn.Linear(<span class="dv">32</span>, <span class="dv">10</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.flatten(x)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.hidden_layer1(x))</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.hidden_layer2(x))</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.output_layer(x)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>net <span class="op">=</span> MLP()</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(torchinfo.summary(net, input_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">32</span>, <span class="dv">32</span>)))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
MLP                                      [1, 10]                   --
├─Flatten: 1-1                           [1, 3072]                 --
├─Linear: 1-2                            [1, 64]                   196,672
├─Linear: 1-3                            [1, 32]                   2,080
├─Linear: 1-4                            [1, 10]                   330
==========================================================================================
Total params: 199,082
Trainable params: 199,082
Non-trainable params: 0
Total mult-adds (Units.MEGABYTES): 0.20
==========================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 0.00
Params size (MB): 0.80
Estimated Total Size (MB): 0.81
==========================================================================================</code></pre>
</div>
</div>
<div id="39e069f0" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchinfo</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CNN(nn.Module):</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(<span class="dv">3</span>, <span class="dv">16</span>, <span class="dv">7</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(<span class="dv">16</span>, <span class="dv">16</span>, <span class="dv">3</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.flatten <span class="op">=</span> nn.Flatten()</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_layer <span class="op">=</span> nn.Linear(<span class="dv">16</span> <span class="op">*</span> <span class="dv">8</span> <span class="op">*</span> <span class="dv">8</span> , <span class="dv">10</span>)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.conv1(x))</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.conv2(x))</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.flatten(x)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.output_layer(x)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>cnn <span class="op">=</span> CNN()</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(torchinfo.summary(cnn, input_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">32</span>, <span class="dv">32</span>)))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
CNN                                      [1, 10]                   --
├─Conv2d: 1-1                            [1, 16, 16, 16]           2,368
├─Conv2d: 1-2                            [1, 16, 8, 8]             2,320
├─Flatten: 1-3                           [1, 1024]                 --
├─Linear: 1-4                            [1, 10]                   10,250
==========================================================================================
Total params: 14,938
Trainable params: 14,938
Non-trainable params: 0
Total mult-adds (Units.MEGABYTES): 0.76
==========================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 0.04
Params size (MB): 0.06
Estimated Total Size (MB): 0.11
==========================================================================================</code></pre>
</div>
</div>
<div id="fig-cnn-linear-transf-calc" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-linear-transf-calc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/linear_transf.png" class="img-fluid figure-img" width="800">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-linear-transf-calc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;24: Input in 2-D (top left), the flattened version of it (bottom left), expected output (right), and unknown transformation (middle).
</figcaption>
</figure>
</div>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Question</strong></p>
<p>How should the linear transformation be defined to obtain the desired result? How many parameters are needed? How could this be done with a convolution?</p>
</div>
</div>
</div>
</section>
<section id="translation-invariance-equivariance" class="level3">
<h3 class="anchored" data-anchor-id="translation-invariance-equivariance">Translation Invariance / Equivariance</h3>
<p>Translation invariant is a function that produces the same value under translations <span class="math inline">\(g()\)</span> of the input <span class="math inline">\(x\)</span>:</p>
<p><span class="math display">\[\begin{equation}
f(g(x))=f(x)
\end{equation}\]</span></p>
<p>Translation equivariant is a function that produces the same value under translations <span class="math inline">\(g()\)</span> of the input <span class="math inline">\(x\)</span>, provided that it is also shifted by <span class="math inline">\(g()\)</span>:</p>
<p><span class="math display">\[\begin{equation}
f(g(x))=g(f(x))
\end{equation}\]</span></p>
<p>Convolutions are translation equivariant, as illustrated well in the following example:</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/qoWAFBYOtoU?start=50" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<!-- <iframe width="560" height="315" src="https://www.youtube.com/embed/qoWAFBYOtoU?start=50" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
 -->
</section>
<section id="sec-cnn-receptive-field" class="level3">
<h3 class="anchored" data-anchor-id="sec-cnn-receptive-field">Stacking &amp; Receptive Field</h3>
<p>Multiple convolutions can be executed in sequence (stacking). Each convolution is performed on the activation maps of another previous convolution. <a href="#fig-cnn-conv-stacking" class="quarto-xref">Figure&nbsp;25</a> illustrates the result.</p>
<div id="fig-cnn-conv-stacking" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-conv-stacking-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/conv_stacking.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-conv-stacking-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;25: Source: <span class="citation" data-cites="johnson_eecs_2019">Johnson (<a href="#ref-johnson_eecs_2019" role="doc-biblioref">2019</a>)</span>
</figcaption>
</figure>
</div>
<p>A convolution is therefore not only performed directly on the input (e.g., images) but is generally defined on inputs of dimensionality <span class="math inline">\(H \times W \times C\)</span>! (There are also variants in higher dimensions.)</p>
<p>However, non-linear activation functions must be used between the convolutions. Otherwise, the stacked convolution can be expressed with a simple convolution (similar to an MLP, which can be expressed with a linear transformation without activation functions).</p>
<p>The receptive field defines which inputs influence the activations of a neuron. <a href="#fig-cnn-receptive-field-1" class="quarto-xref">Figure&nbsp;26</a> to <a href="#fig-cnn-receptive-field-4" class="quarto-xref">Figure&nbsp;29</a> illustrate the concept.</p>
<div id="fig-cnn-receptive-field-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-receptive-field-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/receptive_field1.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-receptive-field-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;26: Receptive field with a 5x5 kernel.
</figcaption>
</figure>
</div>
<div id="fig-cnn-receptive-field-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-receptive-field-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/receptive_field2.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-receptive-field-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;27: Receptive field with a 5x5 kernel and two layers.
</figcaption>
</figure>
</div>
<div id="fig-cnn-receptive-field-3" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-receptive-field-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/receptive_field3.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-receptive-field-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;28: Receptive field with a 5x5 kernel and two layers.
</figcaption>
</figure>
</div>
<div id="fig-cnn-receptive-field-4" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-receptive-field-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/receptive_field4.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-receptive-field-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29: Receptive field with a 5x5 kernel and two layers. The output activation at the center (right) aggregates values over the union of all blue boxes in the left figure and has thus a receptive field of <span class="math inline">\(9 \times 9\)</span> with respect to the input activations.
</figcaption>
</figure>
</div>
<p>The receptive field <span class="math inline">\(r_0\)</span> of a convolutional neural network can be calculated as follows:</p>
<ol type="1">
<li>Define the recurrence relation:</li>
</ol>
<p><span class="math display">\[\begin{equation}
r_{l-1} = s_l \cdot r_l + (k_l - s_l)
\end{equation}\]</span></p>
<ul>
<li><span class="math inline">\(r_l\)</span>: Receptive field at layer <span class="math inline">\(l\)</span>.</li>
<li><span class="math inline">\(s_l\)</span>: Stride of layer <span class="math inline">\(l\)</span>.</li>
<li><span class="math inline">\(k_l\)</span>: Kernel size of layer <span class="math inline">\(l\)</span>.</li>
</ul>
<ol start="2" type="1">
<li>Solving this recurrence relation, we get the closed-form solution for the receptive field at the input layer <span class="math inline">\(r_0\)</span>:</li>
</ol>
<p><span class="math display">\[\begin{equation}
r_0 = \sum_{l=1}^L \left( (k_l - 1) \prod_{i=1}^{l-1} s_i \right) + 1
\end{equation}\]</span></p>
<ul>
<li><span class="math inline">\(L\)</span>: Total number of layers.</li>
<li><span class="math inline">\(k_l\)</span>: Kernel size at layer <span class="math inline">\(l\)</span>.</li>
<li><span class="math inline">\(s_i\)</span>: Stride at layer <span class="math inline">\(i\)</span>, from layer 1 up to <span class="math inline">\(l-1\)</span>.</li>
</ul>
<p><span class="citation" data-cites="araujo_computing_2019">Araujo, Norris, and Sim (<a href="#ref-araujo_computing_2019" role="doc-biblioref">2019</a>)</span> have written a nice article about receptive fields and how to calculate them.</p>
</section>
</section>
<section id="more-layers-operations" class="level2">
<h2 class="anchored" data-anchor-id="more-layers-operations">More Layers &amp; Operations</h2>
<section id="downsampling" class="level3">
<h3 class="anchored" data-anchor-id="downsampling">Downsampling</h3>
<p>Spatial downsampling is often a desired property of convolutional layers. We can achieve that by choosing a stride <span class="math inline">\(\gt 1\)</span>. However, there are other options, such as pooling layers.</p>
<div id="fig-cnn-downsampling-pooling" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-downsampling-pooling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<p><img src="../assets/images/cnns/downsampling_with_pooling.jpg" class="img-fluid figure-img"></p>
<ol type="a">
<li>Sub-Sampling, b) Max Pooling, c) Average Pooling . Source: <span class="citation" data-cites="prince_understanding_2023">Prince (<a href="#ref-prince_understanding_2023" role="doc-biblioref">2023</a>)</span></li>
</ol>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-cnn-downsampling-pooling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;30
</figcaption>
</figure>
</div>
</section>
<section id="sec-cnn-upsampling" class="level3">
<h3 class="anchored" data-anchor-id="sec-cnn-upsampling">Upsampling</h3>
<p>Sometimes we need to increase the spatial dimensionality of our activation maps.</p>
<div id="fig-cnn-upsampling" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-upsampling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<p><img src="../assets/images/cnns/upsampling_options.jpg" class="img-fluid figure-img"></p>
<ol type="a">
<li>Duplicate, b) Max-Unpooling, c) Bilinear Interpolation . Source: <span class="citation" data-cites="prince_understanding_2023">Prince (<a href="#ref-prince_understanding_2023" role="doc-biblioref">2023</a>)</span></li>
</ol>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-cnn-upsampling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;31
</figcaption>
</figure>
</div>
<p>Upsampling can also be learned with Transposed Convolutions.</p>
<div id="fig-cnn-transposed-convs" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-transposed-convs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/transposed_convolution.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-cnn-transposed-convs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;32
</figcaption>
</figure>
</div>
</section>
<section id="changing-the-number-of-channels" class="level3">
<h3 class="anchored" data-anchor-id="changing-the-number-of-channels">Changing the number of channels</h3>
<p>1x1 convolutions have a kernel size of 1x1 and thus no spatial extent. These layers are often used in CNNs to change the number (<span class="math inline">\(C\)</span>) of activation maps with few parameters. For example, activation maps of dimensionality (<span class="math inline">\(C \times H \times W\)</span>) can be changed to a volume of (<span class="math inline">\(C2 \times H \times W\)</span>) with <span class="math inline">\(C2 * (C + 1)\)</span>. This can be useful, for example, to save parameters before more complex layers or at the end of the CNN to adjust the size of the activation maps to the number of classes to be modeled (for classification problems) or to reduce to 3 color channels (<span class="math inline">\(C2=3\)</span>) for image generation models. <a href="#fig-cnn-1x1-conv" class="quarto-xref">Figure&nbsp;33</a> shows an example.</p>
<div id="fig-cnn-1x1-conv" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-1x1-conv-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/1x1_conv_example.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-1x1-conv-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;33: Source: <span class="citation" data-cites="prince_understanding_2023">Prince (<a href="#ref-prince_understanding_2023" role="doc-biblioref">2023</a>)</span>
</figcaption>
</figure>
</div>
</section>
<section id="global-average-pooling" class="level3">
<h3 class="anchored" data-anchor-id="global-average-pooling">Global Average Pooling</h3>
<p>Global Average Pooling is often an important component. It computes the average of the activations along the depth dimension, reducing activation maps from (C x H x W) to (C x 1 x 1). This is useful for directly modeling logits in a classification problem with C classes, enabling architectures that completely eliminate fully-connected layers.</p>
<div id="fig-cnn-global-avg-pooling" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-global-avg-pooling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/global_average_pooling.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-global-avg-pooling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;34: Global Average pooling, input (left) and output (right).
</figcaption>
</figure>
</div>
</section>
<section id="other-layers" class="level3">
<h3 class="anchored" data-anchor-id="other-layers">Other Layers</h3>
<p>There are many other types of layers, also there are more flavours of convolutional layers. Here are some honorable mentions</p>
<section id="depthwise-separable-convolutions" class="level4">
<h4 class="anchored" data-anchor-id="depthwise-separable-convolutions">Depthwise Separable Convolutions</h4>
<p>Depthwise separable convolutions are a way to further reduce the number of parameters in convolutional layers. Instead of extending filters over the entire depth of the input activations, a separate filter (kernel) is used for each input channel, with the dimensionality (<span class="math inline">\(1 \times K \times K\)</span>). <a href="#fig-cnn-depthwise" class="quarto-xref">Figure&nbsp;35</a> shows an example. Subsequently, 1x1 convolutions are used to combine information across the input channels. See <a href="#fig-cnn-depthwise-separabel" class="quarto-xref">Figure&nbsp;36</a> for a comparison of ‘normal’ convolutions and depthwise separable convolutions. Since 1x1 convolutions require fewer parameters, activation maps can be generated with fewer parameters.</p>
<div id="fig-cnn-depthwise" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-depthwise-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/depthwise.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-depthwise-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;35: Source: <a href="https://paperswithcode.com/method/depthwise-convolution">https://paperswithcode.com/method/depthwise-convolution</a>
</figcaption>
</figure>
</div>
<div id="fig-cnn-depthwise-separabel" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-depthwise-separabel-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/cnns/depthwise_separabel.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-depthwise-separabel-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;36: Source: <span class="citation" data-cites="yu_multi-scale_2016">Yu and Koltun (<a href="#ref-yu_multi-scale_2016" role="doc-biblioref">2016</a>)</span>
</figcaption>
</figure>
</div>
</section>
</section>
</section>
<section id="pytorch-examples" class="level2">
<h2 class="anchored" data-anchor-id="pytorch-examples">PyTorch Examples</h2>
<div id="528be547" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> functional <span class="im">as</span> F</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchshow <span class="im">as</span> ts</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="430711ed" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co">#img = Image.open({{&lt; meta params.images_path &gt;}}'cat.jpg')</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>image_path <span class="op">=</span> <span class="st">"../assets/images/cnns/cat.jpg"</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> Image.<span class="bu">open</span>(image_path)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>img</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<div>
<figure class="figure">
<p><img src="cnns_files/figure-html/cell-5-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="1e6ade9d" class="cell" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="bu">filter</span> <span class="op">=</span> torch.tensor(</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    [   [[<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>]], <span class="co"># R</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>        [[<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>]], <span class="co"># G</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>        [[<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>]], <span class="co"># B</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    ]).unsqueeze(<span class="dv">0</span>).<span class="bu">float</span>()</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>ts.show(<span class="bu">filter</span>, show_axis<span class="op">=</span><span class="va">False</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="cnns_files/figure-html/cell-6-output-1.png" width="470" height="470" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="fa58325d" class="cell" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="bu">input</span> <span class="op">=</span> torch.tensor(np.array(img)).unsqueeze(<span class="dv">0</span>).permute(<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">2</span>).<span class="bu">float</span>() <span class="co"># (N, C, H, W)</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="bu">input</span> <span class="op">/=</span> <span class="fl">255.0</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="bu">input</span> <span class="op">-=</span> <span class="fl">1.0</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> F.conv2d(<span class="bu">input</span>, <span class="bu">filter</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">0</span>, dilation<span class="op">=</span><span class="dv">1</span>, groups<span class="op">=</span><span class="dv">1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="52eba558" class="cell" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>ts.show(result)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>/opt/hostedtoolcache/Python/3.12.11/x64/lib/python3.12/site-packages/torchshow/visualization.py:385: UserWarning:

Original input range is not 0-1 when using grayscale mode. Auto-rescaling it to 0-1 by default.
</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="cnns_files/figure-html/cell-8-output-2.png" width="621" height="470" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>2D-Convolution:</p>
<div id="8fda4968" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> F.conv2d(<span class="bu">input</span>, <span class="bu">filter</span>, stride<span class="op">=</span><span class="dv">6</span>, padding<span class="op">=</span><span class="dv">0</span>, dilation<span class="op">=</span><span class="dv">1</span>, groups<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>ts.show(result)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="cnns_files/figure-html/cell-9-output-1.png" width="618" height="470" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Transposed convolution:</p>
<div id="77c103c8" class="cell" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> F.conv2d(<span class="bu">input</span>, <span class="bu">filter</span>, stride<span class="op">=</span><span class="dv">6</span>, padding<span class="op">=</span><span class="dv">0</span>, dilation<span class="op">=</span><span class="dv">1</span>, groups<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> F.conv_transpose2d(result, weight<span class="op">=</span>torch.ones_like(<span class="bu">filter</span>))</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>ts.show(result)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="cnns_files/figure-html/cell-10-output-1.png" width="616" height="470" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Max-Pooling:</p>
<div id="e45aed19" class="cell" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> F.max_pool2d(<span class="bu">input</span>, kernel_size<span class="op">=</span><span class="dv">8</span>, stride<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>ts.show(result)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="cnns_files/figure-html/cell-11-output-1.png" width="621" height="470" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-araujo_computing_2019" class="csl-entry" role="listitem">
Araujo, Andr&amp;eacute;, Wade Norris, and Jack Sim. 2019. <span>“Computing <span>Receptive</span> <span>Fields</span> of <span>Convolutional</span> <span>Neural</span> <span>Networks</span>.”</span> <em>Distill</em> 4 (11): 10.23915/distill.00021. <a href="https://doi.org/10.23915/distill.00021">https://doi.org/10.23915/distill.00021</a>.
</div>
<div id="ref-dumoulin_guide_2016" class="csl-entry" role="listitem">
Dumoulin, Vincent, and Francesco Visin. 2016. <span>“A Guide to Convolution Arithmetic for Deep Learning.”</span> <em>ArXiv e-Prints</em>, March.
</div>
<div id="ref-johnson_eecs_2019" class="csl-entry" role="listitem">
Johnson, Justin. 2019. <span>“<span>EECS</span> 498-007 / 598-005: <span>Deep</span> <span>Learning</span> for <span>Computer</span> <span>Vision</span>.”</span> Lecture {Notes} / {Slides}. <a href="https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2019/">https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2019/</a>.
</div>
<div id="ref-li_cs231n_2022" class="csl-entry" role="listitem">
Li, Fei-Fei. 2022. <span>“<span>CS231n</span> <span>Convolutional</span> <span>Neural</span> <span>Networks</span> for <span>Visual</span> <span>Recognition</span>.”</span> Lecture {Notes}. <a href="https://cs231n.github.io">https://cs231n.github.io</a>.
</div>
<div id="ref-prince_understanding_2023" class="csl-entry" role="listitem">
Prince, Simon J. D. 2023. <em>Understanding <span>Deep</span> <span>Learning</span></em>. MIT Press. <a href="https://udlbook.github.io/udlbook/">https://udlbook.github.io/udlbook/</a>.
</div>
<div id="ref-yu_multi-scale_2016" class="csl-entry" role="listitem">
Yu, Fisher, and Vladlen Koltun. 2016. <span>“Multi-<span>Scale</span> <span>Context</span> <span>Aggregation</span> by <span>Dilated</span> <span>Convolutions</span>.”</span> arXiv. <a href="http://arxiv.org/abs/1511.07122">http://arxiv.org/abs/1511.07122</a>.
</div>
</div>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/marco-willi\.github\.io\/cas-dl-compvis-lectures-hs2025\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../lectures/neural_networks.html" class="pagination-link" aria-label="3 - Neural Networks">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">3 - Neural Networks</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../lectures/classification.html" class="pagination-link" aria-label="5 - Image Classification">
        <span class="nav-page-text">5 - Image Classification</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb14" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "4 - Convolutional Neural Networks"</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="an">params:</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co">   images_path: "/assets/images/cnns/"</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>::: {.content-hidden}</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>{{&lt; include /assets/_macros.tex &gt;}}</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction &amp; Motivation</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>Images have the following relevant properties when modelling them:</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>High-Dimensional: An RGB image of size $224 \times 224$  (height, width) has = $150'528$ values.</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Locality: Nearby pixels are statistically related</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Stability under transformations: Interpretation of an image does not change under many geomoetric transformations.</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-example-rgb-image width=600}</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a><span class="al">![ ]({{&lt; meta params.images_path &gt;}}cows2_small.png)</span></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Image Source</span><span class="co">](https://unsplash.com/de/fotos/braune-schafe-auf-grunem-grasfeld-tagsuber-olonUwUrmQk)</span></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>We will see that CNNs are much better suited to model images than MLPs. Unlike MLPs, CNNs explicitly assume that inputs (e.g., pixels) that are close together need to be considered together and that information is locally correlated. This allows certain properties to be embedded in the architecture of CNNs (inductive biases) to define models much more efficiently (with fewer parameters).</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a><span class="fu">### MLPs and Images</span></span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>The input to an MLP is a vector $\vect{x}^{(i)}$, which is transformed through multiple hidden layers to the output layer. Each hidden layer has a certain number of neurons, each connected to all neurons in the previous layer (fully-connected layers). The fully connected layers can only process 1-D vectors. Therefore, images $\in \mathbb{R}^{H \times W \times C}$ must be flattened into 1-D vectors $\in \mathbb{R}^p$. Here, $p= H \times W \times C$. This causes MLPs to become very large (having many learnable parameters) when applied to high-dimensional inputs such as images. See @fig-cnn-mlp for an illustration.</span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-mlp}</span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a><span class="al">![ ]({{&lt; meta params.images_path &gt;}}mlp_images_example.png)</span></span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a>Illustration of an MLP applied to an image.</span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a> In the CIFAR-10 dataset, which consists of very small images of 32x32x3 (height, width, colors), a single neuron in the first hidden layer has 32 * 32 * 3 = 3,072 weights to learn (see @fig-cnn-mlp-images).</span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-mlp-images}</span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}mlp_images.jpg)</span>{width=600}</span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a>Source: @johnson_eecs_2019</span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-54"><a href="#cb14-54" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- :::{#fig-cnn-spatial-structure-mlp}</span></span>
<span id="cb14-55"><a href="#cb14-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-56"><a href="#cb14-56" aria-hidden="true" tabindex="-1"></a><span class="co">![]({{&lt; meta params.images_path &gt;}}mlp-spatial-structure.png){width=600}</span></span>
<span id="cb14-57"><a href="#cb14-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-58"><a href="#cb14-58" aria-hidden="true" tabindex="-1"></a><span class="co">Source: @li_cs231n_2023</span></span>
<span id="cb14-59"><a href="#cb14-59" aria-hidden="true" tabindex="-1"></a><span class="co">::: --&gt;</span></span>
<span id="cb14-60"><a href="#cb14-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-61"><a href="#cb14-61" aria-hidden="true" tabindex="-1"></a>For larger images, which are often encountered in practice, the number of weights is correspondingly much larger. Many neurons are also used, further increasing the number of parameters, leading to overfitting, and making learning the weights more difficult.</span>
<span id="cb14-62"><a href="#cb14-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-63"><a href="#cb14-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-64"><a href="#cb14-64" aria-hidden="true" tabindex="-1"></a>The dimensionality of weight matrix $\m{W}$ scales with input size.  $\m{W} \in \mathbb{R}^{d \times k}$, while $d$ the dimensionality of the inputs, and $k$ the number of neurons in the first hidden layer. The columns of the weight matrix $\m{W}$ (in the first hidden layer) can be visualized to learn what kind pattern a specific neuron has learned. See @fig-cnn-mlp-example-weights.</span>
<span id="cb14-65"><a href="#cb14-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-66"><a href="#cb14-66" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-mlp-example-weights}</span>
<span id="cb14-67"><a href="#cb14-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-68"><a href="#cb14-68" aria-hidden="true" tabindex="-1"></a><span class="al">![ ]({{&lt; meta params.images_path &gt;}}mlp_images_example_weights.png)</span></span>
<span id="cb14-69"><a href="#cb14-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-70"><a href="#cb14-70" aria-hidden="true" tabindex="-1"></a>Illustration of weight matrix $\m{W}$ and how the weights of a single neuron can be reshaped into the input image dimensionality.</span>
<span id="cb14-71"><a href="#cb14-71" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-72"><a href="#cb14-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-73"><a href="#cb14-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-74"><a href="#cb14-74" aria-hidden="true" tabindex="-1"></a>Often the patterns we want to learn are not stationary. Ideally we want to recognize them at all positions, particularly under translations of the inputs, e.g. $g(\vect{x})$ where $g()$ is a spatial translation. Figure @fig-cnns-mlp-pattern-shift illustrated the problem.</span>
<span id="cb14-75"><a href="#cb14-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-76"><a href="#cb14-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-77"><a href="#cb14-77" aria-hidden="true" tabindex="-1"></a>::: {#fig-cnns-mlp-pattern-shift}</span>
<span id="cb14-78"><a href="#cb14-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-79"><a href="#cb14-79" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}mlp_images_example_pattern_shifts.png)</span></span>
<span id="cb14-80"><a href="#cb14-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-81"><a href="#cb14-81" aria-hidden="true" tabindex="-1"></a>Illustration of different inputs with the same pattern but shifted spatially.</span>
<span id="cb14-82"><a href="#cb14-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-83"><a href="#cb14-83" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-84"><a href="#cb14-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-85"><a href="#cb14-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-86"><a href="#cb14-86" aria-hidden="true" tabindex="-1"></a>::: {.callout-note appearance="simple"}</span>
<span id="cb14-87"><a href="#cb14-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-88"><a href="#cb14-88" aria-hidden="true" tabindex="-1"></a>**Question**</span>
<span id="cb14-89"><a href="#cb14-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-90"><a href="#cb14-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-91"><a href="#cb14-91" aria-hidden="true" tabindex="-1"></a>How do MLPs deal with such pattern shifts?</span>
<span id="cb14-92"><a href="#cb14-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-93"><a href="#cb14-93" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-94"><a href="#cb14-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-95"><a href="#cb14-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-96"><a href="#cb14-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-97"><a href="#cb14-97" aria-hidden="true" tabindex="-1"></a>::: {#fig-cnns-mlp-pattern-shift}</span>
<span id="cb14-98"><a href="#cb14-98" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}mlp_images_examples_permutation.png)</span></span>
<span id="cb14-99"><a href="#cb14-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-100"><a href="#cb14-100" aria-hidden="true" tabindex="-1"></a>Inputs transformed with a permuation matrix.</span>
<span id="cb14-101"><a href="#cb14-101" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-102"><a href="#cb14-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-103"><a href="#cb14-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-104"><a href="#cb14-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-105"><a href="#cb14-105" aria-hidden="true" tabindex="-1"></a>::: {.callout-note appearance="simple"}</span>
<span id="cb14-106"><a href="#cb14-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-107"><a href="#cb14-107" aria-hidden="true" tabindex="-1"></a>**Question**</span>
<span id="cb14-108"><a href="#cb14-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-109"><a href="#cb14-109" aria-hidden="true" tabindex="-1"></a>Can an MLP successfully learn patterns in images $\vect{x}$ that are permuted with a permutation matrix $\m{P}$, i.e. $f(P(\vect{x}))$?</span>
<span id="cb14-110"><a href="#cb14-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-111"><a href="#cb14-111" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-112"><a href="#cb14-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-113"><a href="#cb14-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-114"><a href="#cb14-114" aria-hidden="true" tabindex="-1"></a>To summarize MLPs on images:</span>
<span id="cb14-115"><a href="#cb14-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-116"><a href="#cb14-116" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>High-Dimensional: The size of an MLP scales with the input dimensionality which might blow beyond memory and compute budgets.</span>
<span id="cb14-117"><a href="#cb14-117" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Locality: MLPs have no notion of locality and thus can't exploit this inherent bias in natural images.</span>
<span id="cb14-118"><a href="#cb14-118" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Stability under transformations: MLPs need to learn position-dependent patterns which is very inefficient.</span>
<span id="cb14-119"><a href="#cb14-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-120"><a href="#cb14-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-121"><a href="#cb14-121" aria-hidden="true" tabindex="-1"></a>::: {.callout-note appearance="simple"}</span>
<span id="cb14-122"><a href="#cb14-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-123"><a href="#cb14-123" aria-hidden="true" tabindex="-1"></a>**Demonstration**</span>
<span id="cb14-124"><a href="#cb14-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-125"><a href="#cb14-125" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">https://adamharley.com/nn_vis/mlp/2d.html</span><span class="co">](https://adamharley.com/nn_vis/mlp/2d.html)</span></span>
<span id="cb14-126"><a href="#cb14-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-127"><a href="#cb14-127" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-128"><a href="#cb14-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-129"><a href="#cb14-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-130"><a href="#cb14-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-131"><a href="#cb14-131" aria-hidden="true" tabindex="-1"></a>A single neuron in a CNN is only connected to a small portion (local connectivity) of the image (see @fig-cnn-cnn-spatial). As a result, the neurons have far fewer parameters than in an MLP. The 2-D structure of the image is also preserved, meaning they do not need to be flattened as in an MLP. This exploits the property of images that certain features, such as edges and corners, are relevant throughout the image. By convolving the neurons across the entire input, the same feature can be detected by a neuron throughout the image. In an MLP, a specific feature would need to be relearned at each position.</span>
<span id="cb14-132"><a href="#cb14-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-133"><a href="#cb14-133" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-cnn-spatial}</span>
<span id="cb14-134"><a href="#cb14-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-135"><a href="#cb14-135" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}cnn_spatial.jpg)</span>{width=200}</span>
<span id="cb14-136"><a href="#cb14-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-137"><a href="#cb14-137" aria-hidden="true" tabindex="-1"></a>Source: @johnson_eecs_2019</span>
<span id="cb14-138"><a href="#cb14-138" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-139"><a href="#cb14-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-140"><a href="#cb14-140" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb14-141"><a href="#cb14-141" aria-hidden="true" tabindex="-1"></a>CNNs are used not only for image data but also for data with spatial dependencies/local structures. This includes not only images but also time series, videos, audio, and text. The key is that signals that are spatially close together should be interpreted together.</span>
<span id="cb14-142"><a href="#cb14-142" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-143"><a href="#cb14-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-144"><a href="#cb14-144" aria-hidden="true" tabindex="-1"></a>What properties do we want?</span>
<span id="cb14-145"><a href="#cb14-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-146"><a href="#cb14-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-147"><a href="#cb14-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-148"><a href="#cb14-148" aria-hidden="true" tabindex="-1"></a><span class="fu">### Invariance and Equivariance</span></span>
<span id="cb14-149"><a href="#cb14-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-150"><a href="#cb14-150" aria-hidden="true" tabindex="-1"></a>For many tasks small variations in the input should either **not** change the model output (invariance) or should change the output **in tandem** with the input changes (equivariance).</span>
<span id="cb14-151"><a href="#cb14-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-152"><a href="#cb14-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-153"><a href="#cb14-153" aria-hidden="true" tabindex="-1"></a>A function $f(\vect{x})$ is **invariant** to a transformation $g(\vect{x})$ if:</span>
<span id="cb14-154"><a href="#cb14-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-155"><a href="#cb14-155" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb14-156"><a href="#cb14-156" aria-hidden="true" tabindex="-1"></a>f(g(\vect{x})) = f(\vect{x})</span>
<span id="cb14-157"><a href="#cb14-157" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb14-158"><a href="#cb14-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-159"><a href="#cb14-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-160"><a href="#cb14-160" aria-hidden="true" tabindex="-1"></a>A function $f(\vect{x})$ (such as a layer in a neural network) of an image $\vect{x}$ is **equivariant** with respect to a transformation $g(\vect{\vect{x}})$ if:</span>
<span id="cb14-161"><a href="#cb14-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-162"><a href="#cb14-162" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb14-163"><a href="#cb14-163" aria-hidden="true" tabindex="-1"></a>f(g(\vect{x})) = g(f(\vect{x}))</span>
<span id="cb14-164"><a href="#cb14-164" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb14-165"><a href="#cb14-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-166"><a href="#cb14-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-167"><a href="#cb14-167" aria-hidden="true" tabindex="-1"></a>Example where invariance is required:</span>
<span id="cb14-168"><a href="#cb14-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-169"><a href="#cb14-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-170"><a href="#cb14-170" aria-hidden="true" tabindex="-1"></a>::: {#fig-cnns-invariance-example}</span>
<span id="cb14-171"><a href="#cb14-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-172"><a href="#cb14-172" aria-hidden="true" tabindex="-1"></a><span class="al">![ ]({{&lt; meta params.images_path &gt;}}cows2_small_invariance_collage.png)</span></span>
<span id="cb14-173"><a href="#cb14-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-174"><a href="#cb14-174" aria-hidden="true" tabindex="-1"></a>When objects in the input translate spatially, the output (in this case the classification of the images) does not change. The model  $f(\vect{x})$ is thus invariant to spatial translations.</span>
<span id="cb14-175"><a href="#cb14-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-176"><a href="#cb14-176" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-177"><a href="#cb14-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-178"><a href="#cb14-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-179"><a href="#cb14-179" aria-hidden="true" tabindex="-1"></a>Example where equivariance is required:</span>
<span id="cb14-180"><a href="#cb14-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-181"><a href="#cb14-181" aria-hidden="true" tabindex="-1"></a>::: {#fig-cnns-equivariance-example}</span>
<span id="cb14-182"><a href="#cb14-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-183"><a href="#cb14-183" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}cows2_small_detection_collage.png)</span></span>
<span id="cb14-184"><a href="#cb14-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-185"><a href="#cb14-185" aria-hidden="true" tabindex="-1"></a>When objects in the input (first row) translate spatially, the detections (bounding boxes) change accordingly (bottom row). The model  $f(\vect{x})$ that produces the bounding boxes is thus equivariant with respect to spatial translations.</span>
<span id="cb14-186"><a href="#cb14-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-187"><a href="#cb14-187" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-188"><a href="#cb14-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-189"><a href="#cb14-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-190"><a href="#cb14-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-191"><a href="#cb14-191" aria-hidden="true" tabindex="-1"></a><span class="fu">## Convolutional Layers</span></span>
<span id="cb14-192"><a href="#cb14-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-193"><a href="#cb14-193" aria-hidden="true" tabindex="-1"></a><span class="fu">### Convolutional Neural Networks</span></span>
<span id="cb14-194"><a href="#cb14-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-195"><a href="#cb14-195" aria-hidden="true" tabindex="-1"></a>CNNs consist of a sequence of different layers. Each layer transforms activations from the previous layer into new activations through a differentiable operation. Below we look at the main layer types: convolutional layers, pooling layers, activation layers, and fully connected layers. Arranged in a specific sequence, this is referred to as the architecture of the model.</span>
<span id="cb14-196"><a href="#cb14-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-197"><a href="#cb14-197" aria-hidden="true" tabindex="-1"></a>@fig-cnn-convnet shows an example architecture. The activation maps of the various layers are shown, representing the corresponding outputs of the layers.</span>
<span id="cb14-198"><a href="#cb14-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-199"><a href="#cb14-199" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-convnet}</span>
<span id="cb14-200"><a href="#cb14-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-201"><a href="#cb14-201" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}convnet.jpeg)</span>{width=600}</span>
<span id="cb14-202"><a href="#cb14-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-203"><a href="#cb14-203" aria-hidden="true" tabindex="-1"></a>The activations of a ConvNet architecture are shown. The input image is on the left and the predictions on the right. Source: @li_cs231n_2022.</span>
<span id="cb14-204"><a href="#cb14-204" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-205"><a href="#cb14-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-206"><a href="#cb14-206" aria-hidden="true" tabindex="-1"></a>Sometimes different layers are combined and referred to as a block. For example, the combination of a convolutional layer followed by an activation layer and a pooling layer is often used. This would be a CONV-ACT-POOL block.</span>
<span id="cb14-207"><a href="#cb14-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-208"><a href="#cb14-208" aria-hidden="true" tabindex="-1"></a><span class="fu">### Convolution Operation</span></span>
<span id="cb14-209"><a href="#cb14-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-210"><a href="#cb14-210" aria-hidden="true" tabindex="-1"></a>Convolutional layers are the main layers in CNNs responsible for extracting visual features. The weights of a convolutional layer consist of a set of learnable filters. Each filter is typically small along the spatial dimensions (height, width) relative to the input but extends over the entire input depth. A typical filter in the first layer, for example, has the dimension $7 \times 7 \times 3$ (7 pixels along height/width and 3 along the color channels). During the forward pass, the filters are convolved along height/width over the input. At each position, the dot product (when considering the input and filter as 1-D vectors) between the filter and input is calculated. This produces a 2-D activation map representing the filter's expression at each position in the input. Intuitively, the CNN learns filters corresponding to typical visual patterns, such as edges and colors. A set of $K$ filters produces activation maps with a depth of $K$.</span>
<span id="cb14-211"><a href="#cb14-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-212"><a href="#cb14-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-213"><a href="#cb14-213" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb14-214"><a href="#cb14-214" aria-hidden="true" tabindex="-1"></a>Filter and kernel are sometimes used synonymously. Here, we differentiate by considering a filter as 3-dimensional (CxHxW) and a kernel as 2-dimensional (HxW). A filter consists of C kernels.</span>
<span id="cb14-215"><a href="#cb14-215" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-216"><a href="#cb14-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-217"><a href="#cb14-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-218"><a href="#cb14-218" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb14-219"><a href="#cb14-219" aria-hidden="true" tabindex="-1"></a>Convolution in deep learning is typically implemented as cross-correlation.</span>
<span id="cb14-220"><a href="#cb14-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-221"><a href="#cb14-221" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb14-222"><a href="#cb14-222" aria-hidden="true" tabindex="-1"></a>S(i, j) = (K * I)(i, j) =\sum_m\sum_n I(i + m, j + n)K(m, n)</span>
<span id="cb14-223"><a href="#cb14-223" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb14-224"><a href="#cb14-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-225"><a href="#cb14-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-226"><a href="#cb14-226" aria-hidden="true" tabindex="-1"></a>$I$ is the input (for example an image), $K$ is the kernel (typically smaller than $I$) and $b$ is a bias term which is being added to the weighted sum.</span>
<span id="cb14-227"><a href="#cb14-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-228"><a href="#cb14-228" aria-hidden="true" tabindex="-1"></a>If $I$ is an RGB image (in the first layer of a CNN for example), the kernel $K$ would have dimensionality $3 \times K \times K$ (assuming a square kernel). More generally we learn kernels of the dimensionality $C_{in} \times K \times K$.</span>
<span id="cb14-229"><a href="#cb14-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-230"><a href="#cb14-230" aria-hidden="true" tabindex="-1"></a>Multiple kernels, let's say $C_o$ kernels, can be grouped together: $C_o \times C_{in} \times K \times K$.</span>
<span id="cb14-231"><a href="#cb14-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-232"><a href="#cb14-232" aria-hidden="true" tabindex="-1"></a>We often refer to such tensors as filters or filter banks.</span>
<span id="cb14-233"><a href="#cb14-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-234"><a href="#cb14-234" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-235"><a href="#cb14-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-236"><a href="#cb14-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-237"><a href="#cb14-237" aria-hidden="true" tabindex="-1"></a>Convolution on RGB images is nicely illusrated in @fig-cnns-intro-example. We can see that the filter is multiplied element-wise with the corresponding values in the image, the result is summed top produce one single value in the output.</span>
<span id="cb14-238"><a href="#cb14-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-239"><a href="#cb14-239" aria-hidden="true" tabindex="-1"></a>::: {#fig-cnns-intro-example}</span>
<span id="cb14-240"><a href="#cb14-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-241"><a href="#cb14-241" aria-hidden="true" tabindex="-1"></a><span class="al">![Source:  @prince_understanding_2023]({{&lt; meta params.images_path &gt;}}convolution_prince_rgb.png)</span></span>
<span id="cb14-242"><a href="#cb14-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-243"><a href="#cb14-243" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-244"><a href="#cb14-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-245"><a href="#cb14-245" aria-hidden="true" tabindex="-1"></a>The following figures show further step-by-step illustration of applying convolutions on input activations that produce multiple output activation maps.</span>
<span id="cb14-246"><a href="#cb14-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-247"><a href="#cb14-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-248"><a href="#cb14-248" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-conv-one-number}</span>
<span id="cb14-249"><a href="#cb14-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-250"><a href="#cb14-250" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}cnn_conv_one_number.jpg)</span>{width=600}</span>
<span id="cb14-251"><a href="#cb14-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-252"><a href="#cb14-252" aria-hidden="true" tabindex="-1"></a>Source: @johnson_eecs_2019</span>
<span id="cb14-253"><a href="#cb14-253" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-254"><a href="#cb14-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-255"><a href="#cb14-255" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-conv-activation-map}</span>
<span id="cb14-256"><a href="#cb14-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-257"><a href="#cb14-257" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}conv_activation_map.jpg)</span>{width=600}</span>
<span id="cb14-258"><a href="#cb14-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-259"><a href="#cb14-259" aria-hidden="true" tabindex="-1"></a>Source: @johnson_eecs_2019</span>
<span id="cb14-260"><a href="#cb14-260" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-261"><a href="#cb14-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-262"><a href="#cb14-262" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-conv-activation-map2}</span>
<span id="cb14-263"><a href="#cb14-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-264"><a href="#cb14-264" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}conv_activation_map2.jpg)</span>{width=600}</span>
<span id="cb14-265"><a href="#cb14-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-266"><a href="#cb14-266" aria-hidden="true" tabindex="-1"></a>Source: @johnson_eecs_2019</span>
<span id="cb14-267"><a href="#cb14-267" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-268"><a href="#cb14-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-269"><a href="#cb14-269" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-conv-activation-map3}</span>
<span id="cb14-270"><a href="#cb14-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-271"><a href="#cb14-271" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}conv_activation_map3.jpg)</span>{width=600}</span>
<span id="cb14-272"><a href="#cb14-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-273"><a href="#cb14-273" aria-hidden="true" tabindex="-1"></a>Source: @johnson_eecs_2019</span>
<span id="cb14-274"><a href="#cb14-274" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-275"><a href="#cb14-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-276"><a href="#cb14-276" aria-hidden="true" tabindex="-1"></a>The data is processed in mini-batches, i.e., multiple images at once, as shown in @fig-cnn-conv-activation-map4.</span>
<span id="cb14-277"><a href="#cb14-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-278"><a href="#cb14-278" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-conv-activation-map4}</span>
<span id="cb14-279"><a href="#cb14-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-280"><a href="#cb14-280" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}conv_activation_map4.jpg)</span>{width=600}</span>
<span id="cb14-281"><a href="#cb14-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-282"><a href="#cb14-282" aria-hidden="true" tabindex="-1"></a>Source: @johnson_eecs_2019</span>
<span id="cb14-283"><a href="#cb14-283" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-284"><a href="#cb14-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-285"><a href="#cb14-285" aria-hidden="true" tabindex="-1"></a><span class="fu">### Padding, Stride, Kernel Size, Dilation</span></span>
<span id="cb14-286"><a href="#cb14-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-287"><a href="#cb14-287" aria-hidden="true" tabindex="-1"></a>To define a convolution, various hyperparameters need to be set. Some of the most important ones are:</span>
<span id="cb14-288"><a href="#cb14-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-289"><a href="#cb14-289" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Padding: How much padding is added to the input?</span>
<span id="cb14-290"><a href="#cb14-290" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Stride: What is the step size (stride) of the kernel?</span>
<span id="cb14-291"><a href="#cb14-291" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Kernel-Size: What is the kernel size?</span>
<span id="cb14-292"><a href="#cb14-292" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Dilation: What is the dilation rate?</span>
<span id="cb14-293"><a href="#cb14-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-294"><a href="#cb14-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-295"><a href="#cb14-295" aria-hidden="true" tabindex="-1"></a>**Stride** determines how the filters are convolved over the input activations, essentially the step size. If the stride is 1, the filter moves one pixel at a time to compute the next activation (see @fig-cnn-stride1). If the stride is greater, e.g., 2, it moves two pixels at a time, making the activation maps smaller in width and height (see @fig-cnn-stride2).</span>
<span id="cb14-296"><a href="#cb14-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-297"><a href="#cb14-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-298"><a href="#cb14-298" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-stride1}</span>
<span id="cb14-299"><a href="#cb14-299" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}stride1_example.jpg)</span></span>
<span id="cb14-300"><a href="#cb14-300" aria-hidden="true" tabindex="-1"></a>Convolution with stride (1, 1)</span>
<span id="cb14-301"><a href="#cb14-301" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-302"><a href="#cb14-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-303"><a href="#cb14-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-304"><a href="#cb14-304" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-stride2}</span>
<span id="cb14-305"><a href="#cb14-305" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}stride2_example.jpg)</span></span>
<span id="cb14-306"><a href="#cb14-306" aria-hidden="true" tabindex="-1"></a>Convolution with stride (2, 2)</span>
<span id="cb14-307"><a href="#cb14-307" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-308"><a href="#cb14-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-309"><a href="#cb14-309" aria-hidden="true" tabindex="-1"></a>**Padding** refers to adding (typically) zeros to the border of the input activations before performing the convolution. This can be useful to ensure, for example, that the spatial dimensions of the activation maps are identical to those of the input activations. This is essential for segmentation tasks. @fig-cnn-padding illustrates how padding helps maintain the spatial resolution when padding the input activations.</span>
<span id="cb14-310"><a href="#cb14-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-311"><a href="#cb14-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-312"><a href="#cb14-312" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-padding}</span>
<span id="cb14-313"><a href="#cb14-313" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}padding_example_result.jpg)</span></span>
<span id="cb14-314"><a href="#cb14-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-315"><a href="#cb14-315" aria-hidden="true" tabindex="-1"></a>Left: Input (Yellow) with Zero-Padding (white border), Middle: Filter, Right: Output.</span>
<span id="cb14-316"><a href="#cb14-316" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-317"><a href="#cb14-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-318"><a href="#cb14-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-319"><a href="#cb14-319" aria-hidden="true" tabindex="-1"></a>**Kernel-size** determines the spatial extend from which a value in the output activation map migh aggregate values. It is the receptive field of an output value with respect to the input values. @fig-cnn-kernel-size3x3 and @fig-cnn-kernel-size5x5 show convolutions with different kernel sizes. Note that the amount of padding increases with kernel-size. Kernels usually have an odd number of pixels in height and width to make sure they have a center pixel. Also, the number of parameters increases with kernel-size $(C_{in} \times k \times k) + C_{out}$, which is often an important consideration.</span>
<span id="cb14-320"><a href="#cb14-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-321"><a href="#cb14-321" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-kernel-size3x3}</span>
<span id="cb14-322"><a href="#cb14-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-323"><a href="#cb14-323" aria-hidden="true" tabindex="-1"></a><span class="al">![ ]({{&lt; meta params.images_path &gt;}}kernel_size_example_3x3.jpg)</span></span>
<span id="cb14-324"><a href="#cb14-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-325"><a href="#cb14-325" aria-hidden="true" tabindex="-1"></a>Convolution with zero-padding and $3 \times 3$ kernel that produces activations maps with unchanged spatial resolution.</span>
<span id="cb14-326"><a href="#cb14-326" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-327"><a href="#cb14-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-328"><a href="#cb14-328" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-kernel-size5x5}</span>
<span id="cb14-329"><a href="#cb14-329" aria-hidden="true" tabindex="-1"></a><span class="al">![ ]({{&lt; meta params.images_path &gt;}}kernel_size_example_5x5.jpg)</span></span>
<span id="cb14-330"><a href="#cb14-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-331"><a href="#cb14-331" aria-hidden="true" tabindex="-1"></a>Convolution with zero-padding and $5 \times 5$ kernel that produces activations maps with unchanged spatial resolution.</span>
<span id="cb14-332"><a href="#cb14-332" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-333"><a href="#cb14-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-334"><a href="#cb14-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-335"><a href="#cb14-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-336"><a href="#cb14-336" aria-hidden="true" tabindex="-1"></a>::: {.callout-note appearance="simple"}</span>
<span id="cb14-337"><a href="#cb14-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-338"><a href="#cb14-338" aria-hidden="true" tabindex="-1"></a>**Question**</span>
<span id="cb14-339"><a href="#cb14-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-340"><a href="#cb14-340" aria-hidden="true" tabindex="-1"></a>What is the difference between using 3x3 vs 5x5 kernels? What is the same?</span>
<span id="cb14-341"><a href="#cb14-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-342"><a href="#cb14-342" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-343"><a href="#cb14-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-344"><a href="#cb14-344" aria-hidden="true" tabindex="-1"></a>**Dilation** specifies how much the kernel is spread out and thus directly influences the receptive field. @fig-cnn-dilation shows how it works.</span>
<span id="cb14-345"><a href="#cb14-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-346"><a href="#cb14-346" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-dilation}</span>
<span id="cb14-347"><a href="#cb14-347" aria-hidden="true" tabindex="-1"></a><span class="al">![ ]({{&lt; meta params.images_path &gt;}}dilation_example.jpg)</span></span>
<span id="cb14-348"><a href="#cb14-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-349"><a href="#cb14-349" aria-hidden="true" tabindex="-1"></a>Illustration of how a kernel with dilation=2 (middle) is applied to the input activations (left) to produce activations (right). Note the reduced spatial dimensionality of the output activations.</span>
<span id="cb14-350"><a href="#cb14-350" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-351"><a href="#cb14-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-352"><a href="#cb14-352" aria-hidden="true" tabindex="-1"></a>::: {.callout-note appearance="simple"}</span>
<span id="cb14-353"><a href="#cb14-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-354"><a href="#cb14-354" aria-hidden="true" tabindex="-1"></a>**Question**</span>
<span id="cb14-355"><a href="#cb14-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-356"><a href="#cb14-356" aria-hidden="true" tabindex="-1"></a>Why would we use dilation $d \gt 1$?</span>
<span id="cb14-357"><a href="#cb14-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-358"><a href="#cb14-358" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-359"><a href="#cb14-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-360"><a href="#cb14-360" aria-hidden="true" tabindex="-1"></a>::: {.callout-note appearance="simple"}</span>
<span id="cb14-361"><a href="#cb14-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-362"><a href="#cb14-362" aria-hidden="true" tabindex="-1"></a>@dumoulin_guide_2016 has created some animations for better understanding of convolutions and published them here: <span class="co">[</span><span class="ot">https://github.com/vdumoulin/conv_arithmetic</span><span class="co">](https://github.com/vdumoulin/conv_arithmetic)</span>.</span>
<span id="cb14-363"><a href="#cb14-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-364"><a href="#cb14-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-365"><a href="#cb14-365" aria-hidden="true" tabindex="-1"></a>::: {.content-hidden unless-format="html"}</span>
<span id="cb14-366"><a href="#cb14-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-367"><a href="#cb14-367" aria-hidden="true" tabindex="-1"></a><span class="al">![Convolving a 3x3 kernel over a 5x5 input without padding and with stride 1x1. Source @dumoulin_guide_2016]({{&lt; meta params.images_path &gt;}}no_padding_no_strides.gif)</span>{#fig-cnn-stride-and-padding-gif3 width=200}</span>
<span id="cb14-368"><a href="#cb14-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-369"><a href="#cb14-369" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-370"><a href="#cb14-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-371"><a href="#cb14-371" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-372"><a href="#cb14-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-373"><a href="#cb14-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-374"><a href="#cb14-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-375"><a href="#cb14-375" aria-hidden="true" tabindex="-1"></a><span class="fu">### Calculations: Spatial Dimensionality of Activation Maps</span></span>
<span id="cb14-376"><a href="#cb14-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-377"><a href="#cb14-377" aria-hidden="true" tabindex="-1"></a>You can calculate the dimensionality of the activation maps with the following formulas:</span>
<span id="cb14-378"><a href="#cb14-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-379"><a href="#cb14-379" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$i$: Side length of the input activations (assumption: square inputs)</span>
<span id="cb14-380"><a href="#cb14-380" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$k$: Kernel size (assumption: square kernel)</span>
<span id="cb14-381"><a href="#cb14-381" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$o$: Side length of the output activation maps</span>
<span id="cb14-382"><a href="#cb14-382" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$s$: Stride (assumption: same stride along spatial dimensions)</span>
<span id="cb14-383"><a href="#cb14-383" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$p$: Number of paddings on each side (assumption: same number of paddings along spatial dimensions)</span>
<span id="cb14-384"><a href="#cb14-384" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$d$: Dilation rate.</span>
<span id="cb14-385"><a href="#cb14-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-386"><a href="#cb14-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-387"><a href="#cb14-387" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb14-388"><a href="#cb14-388" aria-hidden="true" tabindex="-1"></a>o = \left\lfloor \frac{i + 2p - k-(k-1)(d-1)}{s} \right\rfloor + 1</span>
<span id="cb14-389"><a href="#cb14-389" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb14-390"><a href="#cb14-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-391"><a href="#cb14-391" aria-hidden="true" tabindex="-1"></a>Note: If dilation is 1 $(k-1)(d-1)$ equals 0.</span>
<span id="cb14-392"><a href="#cb14-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-393"><a href="#cb14-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-394"><a href="#cb14-394" aria-hidden="true" tabindex="-1"></a><span class="fu">### Calculations: Number of Weights in a Convolutional Layer</span></span>
<span id="cb14-395"><a href="#cb14-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-396"><a href="#cb14-396" aria-hidden="true" tabindex="-1"></a>You can calculate the total number of weights in a convolutional layer with the following formula:</span>
<span id="cb14-397"><a href="#cb14-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-398"><a href="#cb14-398" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$k$: Kernel size (assumption: square kernel, so kernel has dimensions $k \times k$)</span>
<span id="cb14-399"><a href="#cb14-399" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$C_{in}$: Number of input channels</span>
<span id="cb14-400"><a href="#cb14-400" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$C_{out}$: Number of output channels</span>
<span id="cb14-401"><a href="#cb14-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-402"><a href="#cb14-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-403"><a href="#cb14-403" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb14-404"><a href="#cb14-404" aria-hidden="true" tabindex="-1"></a>\text{Total Weights} = (k \times k) \times C_{in} \times C_{out}</span>
<span id="cb14-405"><a href="#cb14-405" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb14-406"><a href="#cb14-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-407"><a href="#cb14-407" aria-hidden="true" tabindex="-1"></a>Each filter has a size of $k \times k \times C_{in}$, and there are $C_{out}$ filters in total, resulting in $(k \times k \times C_{in}) \times C_{out}$ weights.</span>
<span id="cb14-408"><a href="#cb14-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-409"><a href="#cb14-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-410"><a href="#cb14-410" aria-hidden="true" tabindex="-1"></a>**Biases:**</span>
<span id="cb14-411"><a href="#cb14-411" aria-hidden="true" tabindex="-1"></a>If each output channel has a bias term, add an additional $C_{out}$ weights for the biases.</span>
<span id="cb14-412"><a href="#cb14-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-413"><a href="#cb14-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-414"><a href="#cb14-414" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb14-415"><a href="#cb14-415" aria-hidden="true" tabindex="-1"></a>\text{Total Parameters} = (k \times k \times C_{in} \times C_{out}) + C_{out}</span>
<span id="cb14-416"><a href="#cb14-416" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb14-417"><a href="#cb14-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-418"><a href="#cb14-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-419"><a href="#cb14-419" aria-hidden="true" tabindex="-1"></a>::: {.callout-note appearance="simple"}</span>
<span id="cb14-420"><a href="#cb14-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-421"><a href="#cb14-421" aria-hidden="true" tabindex="-1"></a>**Question**</span>
<span id="cb14-422"><a href="#cb14-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-423"><a href="#cb14-423" aria-hidden="true" tabindex="-1"></a>**Scenario:**</span>
<span id="cb14-424"><a href="#cb14-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-425"><a href="#cb14-425" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Input activations: $3 \times 32 \times 32$ (C, H, W)</span>
<span id="cb14-426"><a href="#cb14-426" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Convolution: 10 filters with: $k=5  \times 5$, $s=1$, $p=2$, $d=1$</span>
<span id="cb14-427"><a href="#cb14-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-428"><a href="#cb14-428" aria-hidden="true" tabindex="-1"></a>**Questions:**</span>
<span id="cb14-429"><a href="#cb14-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-430"><a href="#cb14-430" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>What is the size of the activation map  $C_{out}, H_{out}, W_{out}$ ?</span>
<span id="cb14-431"><a href="#cb14-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-432"><a href="#cb14-432" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>How many weights / parameters defines this operation?</span>
<span id="cb14-433"><a href="#cb14-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-434"><a href="#cb14-434" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-435"><a href="#cb14-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-436"><a href="#cb14-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-437"><a href="#cb14-437" aria-hidden="true" tabindex="-1"></a><span class="fu">## Properties</span></span>
<span id="cb14-438"><a href="#cb14-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-439"><a href="#cb14-439" aria-hidden="true" tabindex="-1"></a><span class="fu">### Local (Sparse) Connectivity &amp; Parameter Sharing</span></span>
<span id="cb14-440"><a href="#cb14-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-441"><a href="#cb14-441" aria-hidden="true" tabindex="-1"></a>Fully connected layers are, as discussed, impractical when working with high-dimensional inputs like images. If all neurons in a layer were connected to all previous neurons, the number of parameters to be estimated would increase massively, which is inefficient and leads to overfitting. Each neuron is therefore only connected to a local region of the input volume. The spatial extent of this region is a hyperparameter and is called the receptive field of a neuron (also kernel size) on the input volume. The connections along the depth (C) extend over the entire depth of the input volume. The connections are therefore local along the spatial dimensions (width and height) but complete along the depth.</span>
<span id="cb14-442"><a href="#cb14-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-443"><a href="#cb14-443" aria-hidden="true" tabindex="-1"></a>Parameter sharing in convolutional layers is used to reduce the number of parameters. Since the filters are convolved over the inputs, the individual weights of the filters are identical over the spatial extent of the input volume. One of the main assumptions behind CNNs is the following: If it is useful to learn a specific (visual) feature at a certain position, then it is probably useful at other positions as well. In other words: If I learn filters that detect edges, corners, or cats, then it is a reasonable assumption that I want to do this throughout the image.</span>
<span id="cb14-444"><a href="#cb14-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-445"><a href="#cb14-445" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb14-446"><a href="#cb14-446" aria-hidden="true" tabindex="-1"></a>Sometimes parameter sharing does not make sense. This can be the case, for example, if we have centered structures in the images. Then you might want to learn position-dependent features. An example is images of faces that have been centered, where you might want to learn filters that detect the mouth only in the lower middle area (locally connected layers).</span>
<span id="cb14-447"><a href="#cb14-447" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-448"><a href="#cb14-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-449"><a href="#cb14-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-450"><a href="#cb14-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-451"><a href="#cb14-451" aria-hidden="true" tabindex="-1"></a>The following output shows the number of parameters in an MLP and a CNN (each with two hidden layers) on the CIFAR10 dataset.</span>
<span id="cb14-452"><a href="#cb14-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-455"><a href="#cb14-455" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-456"><a href="#cb14-456" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb14-457"><a href="#cb14-457" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-458"><a href="#cb14-458" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb14-459"><a href="#cb14-459" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb14-460"><a href="#cb14-460" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb14-461"><a href="#cb14-461" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchinfo</span>
<span id="cb14-462"><a href="#cb14-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-463"><a href="#cb14-463" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MLP(nn.Module):</span>
<span id="cb14-464"><a href="#cb14-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-465"><a href="#cb14-465" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb14-466"><a href="#cb14-466" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb14-467"><a href="#cb14-467" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.flatten <span class="op">=</span> nn.Flatten()</span>
<span id="cb14-468"><a href="#cb14-468" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layer1 <span class="op">=</span> nn.Linear(<span class="dv">3</span> <span class="op">*</span> <span class="dv">32</span> <span class="op">*</span> <span class="dv">32</span>, <span class="dv">64</span>)</span>
<span id="cb14-469"><a href="#cb14-469" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layer2 <span class="op">=</span> nn.Linear(<span class="dv">64</span>, <span class="dv">32</span>)</span>
<span id="cb14-470"><a href="#cb14-470" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_layer <span class="op">=</span> nn.Linear(<span class="dv">32</span>, <span class="dv">10</span>)</span>
<span id="cb14-471"><a href="#cb14-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-472"><a href="#cb14-472" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb14-473"><a href="#cb14-473" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.flatten(x)</span>
<span id="cb14-474"><a href="#cb14-474" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.hidden_layer1(x))</span>
<span id="cb14-475"><a href="#cb14-475" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.hidden_layer2(x))</span>
<span id="cb14-476"><a href="#cb14-476" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.output_layer(x)</span>
<span id="cb14-477"><a href="#cb14-477" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb14-478"><a href="#cb14-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-479"><a href="#cb14-479" aria-hidden="true" tabindex="-1"></a>net <span class="op">=</span> MLP()</span>
<span id="cb14-480"><a href="#cb14-480" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(torchinfo.summary(net, input_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">32</span>, <span class="dv">32</span>)))</span>
<span id="cb14-481"><a href="#cb14-481" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-482"><a href="#cb14-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-485"><a href="#cb14-485" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-486"><a href="#cb14-486" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb14-487"><a href="#cb14-487" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-488"><a href="#cb14-488" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb14-489"><a href="#cb14-489" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb14-490"><a href="#cb14-490" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb14-491"><a href="#cb14-491" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchinfo</span>
<span id="cb14-492"><a href="#cb14-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-493"><a href="#cb14-493" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CNN(nn.Module):</span>
<span id="cb14-494"><a href="#cb14-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-495"><a href="#cb14-495" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb14-496"><a href="#cb14-496" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb14-497"><a href="#cb14-497" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(<span class="dv">3</span>, <span class="dv">16</span>, <span class="dv">7</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb14-498"><a href="#cb14-498" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(<span class="dv">16</span>, <span class="dv">16</span>, <span class="dv">3</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-499"><a href="#cb14-499" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.flatten <span class="op">=</span> nn.Flatten()</span>
<span id="cb14-500"><a href="#cb14-500" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_layer <span class="op">=</span> nn.Linear(<span class="dv">16</span> <span class="op">*</span> <span class="dv">8</span> <span class="op">*</span> <span class="dv">8</span> , <span class="dv">10</span>)</span>
<span id="cb14-501"><a href="#cb14-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-502"><a href="#cb14-502" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb14-503"><a href="#cb14-503" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.conv1(x))</span>
<span id="cb14-504"><a href="#cb14-504" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.conv2(x))</span>
<span id="cb14-505"><a href="#cb14-505" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.flatten(x)</span>
<span id="cb14-506"><a href="#cb14-506" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.output_layer(x)</span>
<span id="cb14-507"><a href="#cb14-507" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb14-508"><a href="#cb14-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-509"><a href="#cb14-509" aria-hidden="true" tabindex="-1"></a>cnn <span class="op">=</span> CNN()</span>
<span id="cb14-510"><a href="#cb14-510" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(torchinfo.summary(cnn, input_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">32</span>, <span class="dv">32</span>)))</span>
<span id="cb14-511"><a href="#cb14-511" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-512"><a href="#cb14-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-513"><a href="#cb14-513" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-linear-transf-calc}</span>
<span id="cb14-514"><a href="#cb14-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-515"><a href="#cb14-515" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}linear_transf.png)</span>{width=800}</span>
<span id="cb14-516"><a href="#cb14-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-517"><a href="#cb14-517" aria-hidden="true" tabindex="-1"></a>Input in 2-D (top left), the flattened version of it (bottom left), expected output (right), and unknown transformation (middle).</span>
<span id="cb14-518"><a href="#cb14-518" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-519"><a href="#cb14-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-520"><a href="#cb14-520" aria-hidden="true" tabindex="-1"></a>::: {.callout-note appearance="simple"}</span>
<span id="cb14-521"><a href="#cb14-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-522"><a href="#cb14-522" aria-hidden="true" tabindex="-1"></a>**Question**</span>
<span id="cb14-523"><a href="#cb14-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-524"><a href="#cb14-524" aria-hidden="true" tabindex="-1"></a>How should the linear transformation be defined to obtain the desired result? How many parameters are needed? How could this be done with a convolution?</span>
<span id="cb14-525"><a href="#cb14-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-526"><a href="#cb14-526" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-527"><a href="#cb14-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-528"><a href="#cb14-528" aria-hidden="true" tabindex="-1"></a><span class="fu">### Translation Invariance / Equivariance</span></span>
<span id="cb14-529"><a href="#cb14-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-530"><a href="#cb14-530" aria-hidden="true" tabindex="-1"></a>Translation invariant is a function that produces the same value under translations $g()$ of the input $x$:</span>
<span id="cb14-531"><a href="#cb14-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-532"><a href="#cb14-532" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb14-533"><a href="#cb14-533" aria-hidden="true" tabindex="-1"></a>f(g(x))=f(x)</span>
<span id="cb14-534"><a href="#cb14-534" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb14-535"><a href="#cb14-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-536"><a href="#cb14-536" aria-hidden="true" tabindex="-1"></a>Translation equivariant is a function that produces the same value under translations $g()$ of the input $x$, provided that it is also shifted by $g()$:</span>
<span id="cb14-537"><a href="#cb14-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-538"><a href="#cb14-538" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb14-539"><a href="#cb14-539" aria-hidden="true" tabindex="-1"></a>f(g(x))=g(f(x))</span>
<span id="cb14-540"><a href="#cb14-540" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb14-541"><a href="#cb14-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-542"><a href="#cb14-542" aria-hidden="true" tabindex="-1"></a>Convolutions are translation equivariant, as illustrated well in the following example:</span>
<span id="cb14-543"><a href="#cb14-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-544"><a href="#cb14-544" aria-hidden="true" tabindex="-1"></a>{{&lt; video https://www.youtube.com/embed/qoWAFBYOtoU start="50" &gt;}}</span>
<span id="cb14-545"><a href="#cb14-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-546"><a href="#cb14-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-547"><a href="#cb14-547" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- &lt;iframe width="560" height="315" src="https://www.youtube.com/embed/qoWAFBYOtoU?start=50" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;</span></span>
<span id="cb14-548"><a href="#cb14-548" aria-hidden="true" tabindex="-1"></a><span class="co"> --&gt;</span></span>
<span id="cb14-549"><a href="#cb14-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-550"><a href="#cb14-550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-551"><a href="#cb14-551" aria-hidden="true" tabindex="-1"></a><span class="fu">### Stacking &amp; Receptive Field  {#sec-cnn-receptive-field}</span></span>
<span id="cb14-552"><a href="#cb14-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-553"><a href="#cb14-553" aria-hidden="true" tabindex="-1"></a>Multiple convolutions can be executed in sequence (stacking). Each convolution is performed on the activation maps of another previous convolution. @fig-cnn-conv-stacking illustrates the result.</span>
<span id="cb14-554"><a href="#cb14-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-555"><a href="#cb14-555" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-conv-stacking}</span>
<span id="cb14-556"><a href="#cb14-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-557"><a href="#cb14-557" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}conv_stacking.jpg)</span>{width=600}</span>
<span id="cb14-558"><a href="#cb14-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-559"><a href="#cb14-559" aria-hidden="true" tabindex="-1"></a>Source: @johnson_eecs_2019</span>
<span id="cb14-560"><a href="#cb14-560" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-561"><a href="#cb14-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-562"><a href="#cb14-562" aria-hidden="true" tabindex="-1"></a>A convolution is therefore not only performed directly on the input (e.g., images) but is generally defined on inputs of dimensionality $H \times W \times C$! (There are also variants in higher dimensions.)</span>
<span id="cb14-563"><a href="#cb14-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-564"><a href="#cb14-564" aria-hidden="true" tabindex="-1"></a>However, non-linear activation functions must be used between the convolutions. Otherwise, the stacked convolution can be expressed with a simple convolution (similar to an MLP, which can be expressed with a linear transformation without activation functions).</span>
<span id="cb14-565"><a href="#cb14-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-566"><a href="#cb14-566" aria-hidden="true" tabindex="-1"></a>The receptive field defines which inputs influence the activations of a neuron. @fig-cnn-receptive-field-1 to @fig-cnn-receptive-field-4 illustrate the concept.</span>
<span id="cb14-567"><a href="#cb14-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-568"><a href="#cb14-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-569"><a href="#cb14-569" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-receptive-field-1}</span>
<span id="cb14-570"><a href="#cb14-570" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}receptive_field1.jpg)</span></span>
<span id="cb14-571"><a href="#cb14-571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-572"><a href="#cb14-572" aria-hidden="true" tabindex="-1"></a>Receptive field with a 5x5 kernel.</span>
<span id="cb14-573"><a href="#cb14-573" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-574"><a href="#cb14-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-575"><a href="#cb14-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-576"><a href="#cb14-576" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-receptive-field-2}</span>
<span id="cb14-577"><a href="#cb14-577" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}receptive_field2.jpg)</span></span>
<span id="cb14-578"><a href="#cb14-578" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-579"><a href="#cb14-579" aria-hidden="true" tabindex="-1"></a>Receptive field with a 5x5 kernel and two layers.</span>
<span id="cb14-580"><a href="#cb14-580" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-581"><a href="#cb14-581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-582"><a href="#cb14-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-583"><a href="#cb14-583" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-receptive-field-3}</span>
<span id="cb14-584"><a href="#cb14-584" aria-hidden="true" tabindex="-1"></a><span class="al">![ ]({{&lt; meta params.images_path &gt;}}receptive_field3.jpg)</span></span>
<span id="cb14-585"><a href="#cb14-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-586"><a href="#cb14-586" aria-hidden="true" tabindex="-1"></a>Receptive field with a 5x5 kernel and two layers.</span>
<span id="cb14-587"><a href="#cb14-587" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-588"><a href="#cb14-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-589"><a href="#cb14-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-590"><a href="#cb14-590" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-receptive-field-4}</span>
<span id="cb14-591"><a href="#cb14-591" aria-hidden="true" tabindex="-1"></a><span class="al">![ ]({{&lt; meta params.images_path &gt;}}receptive_field4.jpg)</span></span>
<span id="cb14-592"><a href="#cb14-592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-593"><a href="#cb14-593" aria-hidden="true" tabindex="-1"></a>Receptive field with a 5x5 kernel and two layers. The output activation at the center (right) aggregates values over the union of all blue boxes in the left figure and has thus a receptive field of $9 \times 9$ with respect to the input activations.</span>
<span id="cb14-594"><a href="#cb14-594" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-595"><a href="#cb14-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-596"><a href="#cb14-596" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-597"><a href="#cb14-597" aria-hidden="true" tabindex="-1"></a>The receptive field $r_0$ of a convolutional neural network can be calculated as follows:</span>
<span id="cb14-598"><a href="#cb14-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-599"><a href="#cb14-599" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Define the recurrence relation:</span>
<span id="cb14-600"><a href="#cb14-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-601"><a href="#cb14-601" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb14-602"><a href="#cb14-602" aria-hidden="true" tabindex="-1"></a>r_{l-1} = s_l \cdot r_l + (k_l - s_l)</span>
<span id="cb14-603"><a href="#cb14-603" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb14-604"><a href="#cb14-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-605"><a href="#cb14-605" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$r_l$: Receptive field at layer $l$.</span>
<span id="cb14-606"><a href="#cb14-606" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$s_l$: Stride of layer $l$.</span>
<span id="cb14-607"><a href="#cb14-607" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$k_l$: Kernel size of layer $l$.</span>
<span id="cb14-608"><a href="#cb14-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-609"><a href="#cb14-609" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Solving this recurrence relation, we get the closed-form solution for the receptive field at the input layer $r_0$:</span>
<span id="cb14-610"><a href="#cb14-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-611"><a href="#cb14-611" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb14-612"><a href="#cb14-612" aria-hidden="true" tabindex="-1"></a>r_0 = \sum_{l=1}^L \left( (k_l - 1) \prod_{i=1}^{l-1} s_i \right) + 1</span>
<span id="cb14-613"><a href="#cb14-613" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb14-614"><a href="#cb14-614" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-615"><a href="#cb14-615" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$L$: Total number of layers.</span>
<span id="cb14-616"><a href="#cb14-616" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$k_l$: Kernel size at layer $l$.</span>
<span id="cb14-617"><a href="#cb14-617" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$s_i$: Stride at layer $i$, from layer 1 up to $l-1$.</span>
<span id="cb14-618"><a href="#cb14-618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-619"><a href="#cb14-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-620"><a href="#cb14-620" aria-hidden="true" tabindex="-1"></a>@araujo_computing_2019 have written a nice article about receptive fields and how to calculate them.</span>
<span id="cb14-621"><a href="#cb14-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-622"><a href="#cb14-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-623"><a href="#cb14-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-624"><a href="#cb14-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-625"><a href="#cb14-625" aria-hidden="true" tabindex="-1"></a><span class="fu">## More Layers &amp; Operations</span></span>
<span id="cb14-626"><a href="#cb14-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-627"><a href="#cb14-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-628"><a href="#cb14-628" aria-hidden="true" tabindex="-1"></a><span class="fu">### Downsampling</span></span>
<span id="cb14-629"><a href="#cb14-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-630"><a href="#cb14-630" aria-hidden="true" tabindex="-1"></a>Spatial downsampling is often a desired property of convolutional layers. We can achieve that by choosing a stride $\gt 1$. However, there are other options, such as pooling layers.</span>
<span id="cb14-631"><a href="#cb14-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-632"><a href="#cb14-632" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-downsampling-pooling}</span>
<span id="cb14-633"><a href="#cb14-633" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}downsampling_with_pooling.jpg)</span></span>
<span id="cb14-634"><a href="#cb14-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-635"><a href="#cb14-635" aria-hidden="true" tabindex="-1"></a>a) Sub-Sampling, b) Max Pooling, c) Average Pooling . Source:  @prince_understanding_2023</span>
<span id="cb14-636"><a href="#cb14-636" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-637"><a href="#cb14-637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-638"><a href="#cb14-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-639"><a href="#cb14-639" aria-hidden="true" tabindex="-1"></a><span class="fu">### Upsampling {#sec-cnn-upsampling}</span></span>
<span id="cb14-640"><a href="#cb14-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-641"><a href="#cb14-641" aria-hidden="true" tabindex="-1"></a>Sometimes we need to increase the spatial dimensionality of our activation maps.</span>
<span id="cb14-642"><a href="#cb14-642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-643"><a href="#cb14-643" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-644"><a href="#cb14-644" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-upsampling}</span>
<span id="cb14-645"><a href="#cb14-645" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}upsampling_options.jpg)</span></span>
<span id="cb14-646"><a href="#cb14-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-647"><a href="#cb14-647" aria-hidden="true" tabindex="-1"></a>a) Duplicate, b) Max-Unpooling, c) Bilinear Interpolation . Source:  @prince_understanding_2023</span>
<span id="cb14-648"><a href="#cb14-648" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-649"><a href="#cb14-649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-650"><a href="#cb14-650" aria-hidden="true" tabindex="-1"></a>Upsampling can also be learned with Transposed Convolutions.</span>
<span id="cb14-651"><a href="#cb14-651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-652"><a href="#cb14-652" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-transposed-convs}</span>
<span id="cb14-653"><a href="#cb14-653" aria-hidden="true" tabindex="-1"></a><span class="al">![ ]({{&lt; meta params.images_path &gt;}}transposed_convolution.jpg)</span></span>
<span id="cb14-654"><a href="#cb14-654" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-655"><a href="#cb14-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-656"><a href="#cb14-656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-657"><a href="#cb14-657" aria-hidden="true" tabindex="-1"></a><span class="fu">### Changing the number of channels</span></span>
<span id="cb14-658"><a href="#cb14-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-659"><a href="#cb14-659" aria-hidden="true" tabindex="-1"></a>1x1 convolutions have a kernel size of 1x1 and thus no spatial extent. These layers are often used in CNNs to change the number ($C$) of activation maps with few parameters. For example, activation maps of dimensionality ($C \times H \times W$) can be changed to a volume of ($C2 \times H \times W$) with $C2 * (C + 1)$. This can be useful, for example, to save parameters before more complex layers or at the end of the CNN to adjust the size of the activation maps to the number of classes to be modeled (for classification problems) or to reduce to 3 color channels ($C2=3$) for image generation models. @fig-cnn-1x1-conv shows an example.</span>
<span id="cb14-660"><a href="#cb14-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-661"><a href="#cb14-661" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-1x1-conv}</span>
<span id="cb14-662"><a href="#cb14-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-663"><a href="#cb14-663" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}1x1_conv_example.jpg)</span></span>
<span id="cb14-664"><a href="#cb14-664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-665"><a href="#cb14-665" aria-hidden="true" tabindex="-1"></a>Source: @prince_understanding_2023</span>
<span id="cb14-666"><a href="#cb14-666" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-667"><a href="#cb14-667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-668"><a href="#cb14-668" aria-hidden="true" tabindex="-1"></a><span class="fu">### Global Average Pooling</span></span>
<span id="cb14-669"><a href="#cb14-669" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-670"><a href="#cb14-670" aria-hidden="true" tabindex="-1"></a>Global Average Pooling is often an important component. It computes the average of the activations along the depth dimension, reducing activation maps from (C x H x W) to (C x 1 x 1). This is useful for directly modeling logits in a classification problem with C classes, enabling architectures that completely eliminate fully-connected layers.</span>
<span id="cb14-671"><a href="#cb14-671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-672"><a href="#cb14-672" aria-hidden="true" tabindex="-1"></a>::: {#fig-cnn-global-avg-pooling}</span>
<span id="cb14-673"><a href="#cb14-673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-674"><a href="#cb14-674" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}global_average_pooling.jpg)</span></span>
<span id="cb14-675"><a href="#cb14-675" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-676"><a href="#cb14-676" aria-hidden="true" tabindex="-1"></a>Global Average pooling, input (left) and output (right).</span>
<span id="cb14-677"><a href="#cb14-677" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-678"><a href="#cb14-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-679"><a href="#cb14-679" aria-hidden="true" tabindex="-1"></a><span class="fu">### Other Layers</span></span>
<span id="cb14-680"><a href="#cb14-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-681"><a href="#cb14-681" aria-hidden="true" tabindex="-1"></a>There are many other types of layers, also there are more flavours of convolutional layers. Here are some honorable mentions</span>
<span id="cb14-682"><a href="#cb14-682" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-683"><a href="#cb14-683" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-684"><a href="#cb14-684" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Depthwise Separable Convolutions</span></span>
<span id="cb14-685"><a href="#cb14-685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-686"><a href="#cb14-686" aria-hidden="true" tabindex="-1"></a>Depthwise separable convolutions are a way to further reduce the number of parameters in convolutional layers. Instead of extending filters over the entire depth of the input activations, a separate filter (kernel) is used for each input channel, with the dimensionality ($1 \times K \times K$). @fig-cnn-depthwise shows an example. Subsequently, 1x1 convolutions are used to combine information across the input channels. See @fig-cnn-depthwise-separabel for a comparison of 'normal' convolutions and depthwise separable convolutions. Since 1x1 convolutions require fewer parameters, activation maps can be generated with fewer parameters.</span>
<span id="cb14-687"><a href="#cb14-687" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-688"><a href="#cb14-688" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-depthwise}</span>
<span id="cb14-689"><a href="#cb14-689" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-690"><a href="#cb14-690" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}depthwise.png)</span>{width=600}</span>
<span id="cb14-691"><a href="#cb14-691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-692"><a href="#cb14-692" aria-hidden="true" tabindex="-1"></a>Source: <span class="co">[</span><span class="ot">https://paperswithcode.com/method/depthwise-convolution</span><span class="co">](https://paperswithcode.com/method/depthwise-convolution)</span></span>
<span id="cb14-693"><a href="#cb14-693" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-694"><a href="#cb14-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-695"><a href="#cb14-695" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-depthwise-separabel}</span>
<span id="cb14-696"><a href="#cb14-696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-697"><a href="#cb14-697" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}depthwise_separabel.png)</span>{width=600}</span>
<span id="cb14-698"><a href="#cb14-698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-699"><a href="#cb14-699" aria-hidden="true" tabindex="-1"></a>Source: @yu_multi-scale_2016</span>
<span id="cb14-700"><a href="#cb14-700" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-701"><a href="#cb14-701" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-702"><a href="#cb14-702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-703"><a href="#cb14-703" aria-hidden="true" tabindex="-1"></a><span class="fu">## PyTorch Examples</span></span>
<span id="cb14-704"><a href="#cb14-704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-707"><a href="#cb14-707" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-708"><a href="#cb14-708" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb14-709"><a href="#cb14-709" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-710"><a href="#cb14-710" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb14-711"><a href="#cb14-711" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb14-712"><a href="#cb14-712" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> functional <span class="im">as</span> F</span>
<span id="cb14-713"><a href="#cb14-713" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchshow <span class="im">as</span> ts</span>
<span id="cb14-714"><a href="#cb14-714" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb14-715"><a href="#cb14-715" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb14-716"><a href="#cb14-716" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-717"><a href="#cb14-717" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-720"><a href="#cb14-720" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-721"><a href="#cb14-721" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb14-722"><a href="#cb14-722" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-723"><a href="#cb14-723" aria-hidden="true" tabindex="-1"></a><span class="co">#img = Image.open({{&lt; meta params.images_path &gt;}}'cat.jpg')</span></span>
<span id="cb14-724"><a href="#cb14-724" aria-hidden="true" tabindex="-1"></a>image_path <span class="op">=</span> <span class="st">"../assets/images/cnns/cat.jpg"</span></span>
<span id="cb14-725"><a href="#cb14-725" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> Image.<span class="bu">open</span>(image_path)</span>
<span id="cb14-726"><a href="#cb14-726" aria-hidden="true" tabindex="-1"></a>img</span>
<span id="cb14-727"><a href="#cb14-727" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-728"><a href="#cb14-728" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-731"><a href="#cb14-731" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-732"><a href="#cb14-732" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb14-733"><a href="#cb14-733" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-734"><a href="#cb14-734" aria-hidden="true" tabindex="-1"></a><span class="bu">filter</span> <span class="op">=</span> torch.tensor(</span>
<span id="cb14-735"><a href="#cb14-735" aria-hidden="true" tabindex="-1"></a>    [   [[<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>]], <span class="co"># R</span></span>
<span id="cb14-736"><a href="#cb14-736" aria-hidden="true" tabindex="-1"></a>        [[<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>]], <span class="co"># G</span></span>
<span id="cb14-737"><a href="#cb14-737" aria-hidden="true" tabindex="-1"></a>        [[<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>]], <span class="co"># B</span></span>
<span id="cb14-738"><a href="#cb14-738" aria-hidden="true" tabindex="-1"></a>    ]).unsqueeze(<span class="dv">0</span>).<span class="bu">float</span>()</span>
<span id="cb14-739"><a href="#cb14-739" aria-hidden="true" tabindex="-1"></a>ts.show(<span class="bu">filter</span>, show_axis<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb14-740"><a href="#cb14-740" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-741"><a href="#cb14-741" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-744"><a href="#cb14-744" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-745"><a href="#cb14-745" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb14-746"><a href="#cb14-746" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-747"><a href="#cb14-747" aria-hidden="true" tabindex="-1"></a><span class="bu">input</span> <span class="op">=</span> torch.tensor(np.array(img)).unsqueeze(<span class="dv">0</span>).permute(<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">2</span>).<span class="bu">float</span>() <span class="co"># (N, C, H, W)</span></span>
<span id="cb14-748"><a href="#cb14-748" aria-hidden="true" tabindex="-1"></a><span class="bu">input</span> <span class="op">/=</span> <span class="fl">255.0</span></span>
<span id="cb14-749"><a href="#cb14-749" aria-hidden="true" tabindex="-1"></a><span class="bu">input</span> <span class="op">-=</span> <span class="fl">1.0</span></span>
<span id="cb14-750"><a href="#cb14-750" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> F.conv2d(<span class="bu">input</span>, <span class="bu">filter</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">0</span>, dilation<span class="op">=</span><span class="dv">1</span>, groups<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-751"><a href="#cb14-751" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-752"><a href="#cb14-752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-755"><a href="#cb14-755" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-756"><a href="#cb14-756" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb14-757"><a href="#cb14-757" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-758"><a href="#cb14-758" aria-hidden="true" tabindex="-1"></a>ts.show(result)</span>
<span id="cb14-759"><a href="#cb14-759" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-760"><a href="#cb14-760" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-761"><a href="#cb14-761" aria-hidden="true" tabindex="-1"></a>2D-Convolution:</span>
<span id="cb14-762"><a href="#cb14-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-765"><a href="#cb14-765" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-766"><a href="#cb14-766" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb14-767"><a href="#cb14-767" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-768"><a href="#cb14-768" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> F.conv2d(<span class="bu">input</span>, <span class="bu">filter</span>, stride<span class="op">=</span><span class="dv">6</span>, padding<span class="op">=</span><span class="dv">0</span>, dilation<span class="op">=</span><span class="dv">1</span>, groups<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-769"><a href="#cb14-769" aria-hidden="true" tabindex="-1"></a>ts.show(result)</span>
<span id="cb14-770"><a href="#cb14-770" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-771"><a href="#cb14-771" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-772"><a href="#cb14-772" aria-hidden="true" tabindex="-1"></a>Transposed convolution:</span>
<span id="cb14-773"><a href="#cb14-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-776"><a href="#cb14-776" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-777"><a href="#cb14-777" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb14-778"><a href="#cb14-778" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-779"><a href="#cb14-779" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> F.conv2d(<span class="bu">input</span>, <span class="bu">filter</span>, stride<span class="op">=</span><span class="dv">6</span>, padding<span class="op">=</span><span class="dv">0</span>, dilation<span class="op">=</span><span class="dv">1</span>, groups<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-780"><a href="#cb14-780" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> F.conv_transpose2d(result, weight<span class="op">=</span>torch.ones_like(<span class="bu">filter</span>))</span>
<span id="cb14-781"><a href="#cb14-781" aria-hidden="true" tabindex="-1"></a>ts.show(result)</span>
<span id="cb14-782"><a href="#cb14-782" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-783"><a href="#cb14-783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-784"><a href="#cb14-784" aria-hidden="true" tabindex="-1"></a>Max-Pooling:</span>
<span id="cb14-785"><a href="#cb14-785" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-788"><a href="#cb14-788" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-789"><a href="#cb14-789" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb14-790"><a href="#cb14-790" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-791"><a href="#cb14-791" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> F.max_pool2d(<span class="bu">input</span>, kernel_size<span class="op">=</span><span class="dv">8</span>, stride<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb14-792"><a href="#cb14-792" aria-hidden="true" tabindex="-1"></a>ts.show(result)</span>
<span id="cb14-793"><a href="#cb14-793" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-794"><a href="#cb14-794" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-795"><a href="#cb14-795" aria-hidden="true" tabindex="-1"></a><span class="fu">## References</span></span>
<span id="cb14-796"><a href="#cb14-796" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-797"><a href="#cb14-797" aria-hidden="true" tabindex="-1"></a>::: {#refs}</span>
<span id="cb14-798"><a href="#cb14-798" aria-hidden="true" tabindex="-1"></a>:::</span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© Copyright 2024, Marco Willi</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2025/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This page is built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>