<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.24">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Marco Willi">

<title>1 - Introduction – CAS Deep Learning - Computer Vision</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../lectures/frameworks.html" rel="next">
<link href="../pages/quiz.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-a6e161b2431e1f94a14e0f5d32135a3c.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-7365f4c1a6be05280eba792c58017f0b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="../site_libs/quarto-contrib/videojs/video.min.js"></script>
<link href="../site_libs/quarto-contrib/videojs/video-js.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-SCHRGR3LNM"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-SCHRGR3LNM', { 'anonymize_ip': true});
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../lectures/intro.html">Lectures</a></li><li class="breadcrumb-item"><a href="../lectures/intro.html">1 - Introduction</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-center sidebar-header">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../assets/logo.webp" alt="" class="sidebar-logo light-content py-0 d-lg-inline d-none">
      <img src="../assets/logo.webp" alt="" class="sidebar-logo dark-content py-0 d-lg-inline d-none">
      </a>
      <div class="sidebar-tools-main">
    <a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2025" title="GitHub organization" class="quarto-navigation-tool px-1" aria-label="GitHub organization"><i class="bi bi-github"></i></a>
    <a href="" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-code-square"></i></a>
</div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Slides</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../slides/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Einführung Computer Vision mit Deep Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../slides/cnns.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Convolutional Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../slides/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../slides/practical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Practical Considerations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../slides/recent_advances.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Recent Advances</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/quiz.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Quiz &amp; Exam Prep</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Lectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/intro.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">1 - Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2 - Software &amp; Hardware for Deep Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/neural_networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3 - Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/cnns.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4 - Convolutional Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5 - Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/practical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6 - Practical</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/recent_advances.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">7 - Recent Advances</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Resources</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/literature.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Books</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/links.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Helpful Links &amp; Resources</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Mathematical Notation</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#motivation" id="toc-motivation" class="nav-link active" data-scroll-target="#motivation">Motivation</a></li>
  <li><a href="#computer-vision-tasks" id="toc-computer-vision-tasks" class="nav-link" data-scroll-target="#computer-vision-tasks">Computer Vision Tasks</a>
  <ul class="collapse">
  <li><a href="#image-classification" id="toc-image-classification" class="nav-link" data-scroll-target="#image-classification">Image Classification</a></li>
  <li><a href="#object-detection" id="toc-object-detection" class="nav-link" data-scroll-target="#object-detection">Object Detection</a></li>
  <li><a href="#segmentation" id="toc-segmentation" class="nav-link" data-scroll-target="#segmentation">Segmentation</a></li>
  <li><a href="#keypoint-detection" id="toc-keypoint-detection" class="nav-link" data-scroll-target="#keypoint-detection">Keypoint Detection</a></li>
  <li><a href="#image-generation" id="toc-image-generation" class="nav-link" data-scroll-target="#image-generation">Image Generation</a></li>
  <li><a href="#further-tasks" id="toc-further-tasks" class="nav-link" data-scroll-target="#further-tasks">Further tasks</a></li>
  </ul></li>
  <li><a href="#challenges" id="toc-challenges" class="nav-link" data-scroll-target="#challenges">Challenges</a>
  <ul class="collapse">
  <li><a href="#semantic-gap" id="toc-semantic-gap" class="nav-link" data-scroll-target="#semantic-gap">Semantic Gap</a></li>
  <li><a href="#viewpoint" id="toc-viewpoint" class="nav-link" data-scroll-target="#viewpoint">Viewpoint</a></li>
  <li><a href="#deformation" id="toc-deformation" class="nav-link" data-scroll-target="#deformation">Deformation</a></li>
  <li><a href="#illumination" id="toc-illumination" class="nav-link" data-scroll-target="#illumination">Illumination</a></li>
  <li><a href="#background" id="toc-background" class="nav-link" data-scroll-target="#background">Background</a></li>
  <li><a href="#occlusion" id="toc-occlusion" class="nav-link" data-scroll-target="#occlusion">Occlusion</a></li>
  <li><a href="#intra-class-variation" id="toc-intra-class-variation" class="nav-link" data-scroll-target="#intra-class-variation">Intra-Class Variation</a></li>
  <li><a href="#context-dependence" id="toc-context-dependence" class="nav-link" data-scroll-target="#context-dependence">Context Dependence</a></li>
  </ul></li>
  <li><a href="#software" id="toc-software" class="nav-link" data-scroll-target="#software">Software</a>
  <ul class="collapse">
  <li><a href="#pipelines" id="toc-pipelines" class="nav-link" data-scroll-target="#pipelines">Pipelines</a></li>
  <li><a href="#image-analysis-libraries" id="toc-image-analysis-libraries" class="nav-link" data-scroll-target="#image-analysis-libraries">Image Analysis Libraries</a></li>
  </ul></li>
  <li><a href="#machine-learning" id="toc-machine-learning" class="nav-link" data-scroll-target="#machine-learning">Machine Learning</a>
  <ul class="collapse">
  <li><a href="#machine-learning-process" id="toc-machine-learning-process" class="nav-link" data-scroll-target="#machine-learning-process">Machine Learning Process</a></li>
  <li><a href="#models" id="toc-models" class="nav-link" data-scroll-target="#models">Models</a></li>
  <li><a href="#optimization" id="toc-optimization" class="nav-link" data-scroll-target="#optimization">Optimization</a></li>
  <li><a href="#model-selection" id="toc-model-selection" class="nav-link" data-scroll-target="#model-selection">Model Selection</a></li>
  <li><a href="#ml-with-classical-computer-vision" id="toc-ml-with-classical-computer-vision" class="nav-link" data-scroll-target="#ml-with-classical-computer-vision">ML with Classical Computer Vision</a></li>
  </ul></li>
  <li><a href="#deep-learning" id="toc-deep-learning" class="nav-link" data-scroll-target="#deep-learning">Deep Learning</a>
  <ul class="collapse">
  <li><a href="#deep-learning-history" id="toc-deep-learning-history" class="nav-link" data-scroll-target="#deep-learning-history">Deep Learning History</a></li>
  <li><a href="#hubel-and-wiesel" id="toc-hubel-and-wiesel" class="nav-link" data-scroll-target="#hubel-and-wiesel">Hubel and Wiesel</a></li>
  <li><a href="#neocognitron" id="toc-neocognitron" class="nav-link" data-scroll-target="#neocognitron">Neocognitron</a></li>
  <li><a href="#backpropagation" id="toc-backpropagation" class="nav-link" data-scroll-target="#backpropagation">Backpropagation</a></li>
  <li><a href="#convolutional-neural-networks-cnns" id="toc-convolutional-neural-networks-cnns" class="nav-link" data-scroll-target="#convolutional-neural-networks-cnns">Convolutional Neural Networks (CNNs)</a></li>
  <li><a href="#other-architectures-in-computer-vision" id="toc-other-architectures-in-computer-vision" class="nav-link" data-scroll-target="#other-architectures-in-computer-vision">Other Architectures in Computer Vision</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2025/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../lectures/intro.html">Lectures</a></li><li class="breadcrumb-item"><a href="../lectures/intro.html">1 - Introduction</a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">1 - Introduction</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Marco Willi </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Institute for Data Science I4DS, FHNW
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  


</header>


<section id="motivation" class="level2">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<p>Deep Learning models have played a transformative role in Computer Vision over the last decade, significantly enhancing and expanding the capabilities to process visual data.</p>
<p>The integration of Deep Learning in Computer Vision has massively improved the accuracy and efficiency of visual recognition, classification, and analysis. This has opened up new possibilities in applications such as automated driving, facial recognition, and medical image analysis. These models are now embedded in a wide variety of products and services.</p>
<p>Biologists might need to analyse images from camera traps for species identification, see <a href="#fig-intro-kora-lynx" class="quarto-xref">Figure&nbsp;1</a>.</p>
<div id="fig-intro-kora-lynx" class="quarto-float quarto-figure quarto-figure-center anchored" width="400">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-kora-lynx-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../assets/images/intro/kora_lynx.png" class="img-fluid figure-img"></p>
<figcaption>Source: <span class="citation" data-cites="breitenmoser-wursten_projekt_2024">Breitenmoser-Würsten et al. (<a href="#ref-breitenmoser-wursten_projekt_2024" role="doc-biblioref">2024</a>)</span></figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-kora-lynx-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Image from a camera trap.
</figcaption>
</figure>
</div>
<p>Recent advances in generative deep learning enables anybody to create realistic looking synthetic images (see <a href="#fig-intro-synthetic-images" class="quarto-xref">Figure&nbsp;2</a>). This can be used for many applications, positive and negative. Such images might be used to create misinformation which is one reason researchers are trying to build models to detect them.</p>
<div id="fig-intro-synthetic-images" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-synthetic-images-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../assets/images/intro/sid_trump.jpg" class="img-fluid figure-img"></p>
<figcaption><a href="https://x.com/TheInfiniteDude/status/1637211568692932608">Source</a></figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../assets/images/intro/sid_pope.jpg" class="img-fluid figure-img"></p>
<figcaption><a href="https://x.com/cryptomattk/status/1687908457880367104">Source</a></figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-synthetic-images-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Examples of synthetic images.
</figcaption>
</figure>
</div>
<!-- ![Camera Trap Image Analysis](/assets/images/intro/camtrap.jpg){width=400} -->
<p><a href="#fig-intro-google-lens" class="quarto-xref">Figure&nbsp;3</a> shows an application where one can take a photo of an object of interest and let a model identify it, as well as search additional information about it. Photos can also be used for translation tasks whereby models detect letters (optical character recognition) and then translate it (using language models).</p>
<div id="fig-intro-google-lens" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-google-lens-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../assets/images/intro/google_lens_classification.png" class="img-fluid figure-img"></p>
<figcaption>Identification &amp; Search</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../assets/images/intro/google_lens_ocr.png" class="img-fluid figure-img"></p>
<figcaption>Translation</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-intro-google-lens-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3
</figcaption>
</figure>
</div>
<p><a href="https://search.google/ways-to-search/lens/">Google Lens</a></p>
<p>One area with high commercial interest and high competition is the development of self-driving capabilities. <a href="#fig-intro-self-driving" class="quarto-xref">Figure&nbsp;4</a> shows a demonstration of the capabilities required towards self-driving, such as understanding a scene and identifying any objects in it.</p>
<div id="fig-intro-self-driving" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-self-driving-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video"><video id="video_shortcode_videojs_video1" width="800" class="video-js vjs-default-skin " controls="" preload="auto" data-setup="{}" title=""><source src="https://storage.googleapis.com/waymo-uploads/files/site-animations/waymo-driver/cameras.webm"></video></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-self-driving-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: <a href="https://waymo.com/waymo-driver/">Example from Waymo</a>.
</figcaption>
</figure>
</div>
<p>Face recognition rechnology, such as Apple’s Face ID (<a href="#fig-intro-face-id" class="quarto-xref">Figure&nbsp;5</a>), has become ubiquitious. In this case depth information is derived from infrared light.</p>
<div id="fig-intro-face-id" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-face-id-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video"><iframe data-external="1" src="https://www.youtube.com/embed/z-t1h0Y8vuM?si=qnEOYDmqyv8zGvMV?start=50" width="800" height="600" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-face-id-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: <a href="https://support.apple.com/en-us/102381">Example from Apple Face ID</a>
</figcaption>
</figure>
</div>
<p>Computer vision has also been very useful in agricultur. <a href="#fig-intro-minneapple" class="quarto-xref">Figure&nbsp;6</a> shows an application where apples can be counted from image data. Furthermore, drones might be used to estimate crop yield, type and health.</p>
<div id="fig-intro-minneapple" class="quarto-float quarto-figure quarto-figure-center anchored" width="400">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-minneapple-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../assets/images/intro/minneapple.png" class="img-fluid figure-img"></p>
<figcaption>Example from <span class="citation" data-cites="hani_minneapple_2020">Häni, Roy, and Isler (<a href="#ref-hani_minneapple_2020" role="doc-biblioref">2020</a>)</span></figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-intro-minneapple-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6
</figcaption>
</figure>
</div>
<p>Deep learning has also been applied extensively in the medical area. In particular, many imaging technologies, such as MRI or CT, can be analysed with computer vision techniques. <a href="#fig-intro-sam" class="quarto-xref">Figure&nbsp;7</a> illustrates a generic segmentation model which can be used to identify organs and other anatomical structures on pixel-level.</p>
<div id="fig-intro-sam" class="quarto-float quarto-figure quarto-figure-center anchored" width="400">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-sam-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../assets/images/intro/medsam.png" class="img-fluid figure-img"></p>
<figcaption>Example from <span class="citation" data-cites="ma_segment_2024">Ma et al. (<a href="#ref-ma_segment_2024" role="doc-biblioref">2024</a>)</span>.</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-intro-sam-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7
</figcaption>
</figure>
</div>
<p>Photo-editing has been brought to new levels of sophistication with AI-based capabilities. <a href="#fig-intro-magic" class="quarto-xref">Figure&nbsp;8</a> shows an impressive example from the Google Magic Editor. Maybe even a bit too good?</p>
<div id="fig-intro-magic" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-magic-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video"><video id="video_shortcode_videojs_video2" width="800" class="video-js vjs-default-skin " controls="" preload="auto" data-setup="{}" title=""><source src="https://storage.googleapis.com/gweb-mobius-cdn/photos/uploads/6e54ed750f84538fd052b31818127f1e4df5711c.compressed.mp4"></video></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-magic-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: <a href="https://www.google.com/intl/en/photos/editing/">Example from Google Magic Editor</a>
</figcaption>
</figure>
</div>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Question</strong></p>
<p>What steps do you think the model in <a href="#fig-intro-magic" class="quarto-xref">Figure&nbsp;8</a> performs?</p>
</div>
</div>
</div>
<p>The increasingly better models and the ability to run them quickly and resource-efficiently on mobile devices have enabled such applications. <a href="#fig-intro-tensor-phone" class="quarto-xref">Figure&nbsp;9</a> shows that special chips have been developed to process images with Deep Learning models quickly.</p>
<div id="fig-intro-tensor-phone" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-tensor-phone-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/intro/tensor_phone.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-tensor-phone-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: From <a href="https://store.google.com/intl/en/ideas/pixel-camera-features/">Link</a>.
</figcaption>
</figure>
</div>
</section>
<section id="computer-vision-tasks" class="level2">
<h2 class="anchored" data-anchor-id="computer-vision-tasks">Computer Vision Tasks</h2>
<section id="image-classification" class="level3">
<h3 class="anchored" data-anchor-id="image-classification">Image Classification</h3>
<p>In image classification, an image is assigned to a predefined set of classes. In multi-class classification, there are <span class="math inline">\(\gt 2\)</span> classes; in binary classification, there are 2 classes; and in multi-label classification, there are multiple sets of classes. <a href="#fig-intro-image_clssifier" class="quarto-xref">Figure&nbsp;10</a> shows an example from the paper by <span class="citation" data-cites="krizhevsky_imagenet_2012">Krizhevsky, Sutskever, and Hinton (<a href="#ref-krizhevsky_imagenet_2012" role="doc-biblioref">2012</a>)</span> (multi-class classification), which achieved the best results in the ImageNet competition in 2012 and demonstrated the effectiveness of CNNs.</p>
<div id="fig-intro-image_clssifier" class="quarto-float quarto-figure quarto-figure-center anchored" width="600">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-image_clssifier-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/intro/image_classification_example.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-image_clssifier-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: Multi-Class Image Classification Beispiel (aus <span class="citation" data-cites="krizhevsky_imagenet_2012">Krizhevsky, Sutskever, and Hinton (<a href="#ref-krizhevsky_imagenet_2012" role="doc-biblioref">2012</a>)</span>).
</figcaption>
</figure>
</div>
</section>
<section id="object-detection" class="level3">
<h3 class="anchored" data-anchor-id="object-detection">Object Detection</h3>
<p>Object detection involves locating and recognizing (multiple) objects in an image. <a href="#fig-intro-yolo-example" class="quarto-xref">Figure&nbsp;11</a> shows an example from the paper by <span class="citation" data-cites="Redmon2016a">Redmon et al. (<a href="#ref-Redmon2016a" role="doc-biblioref">2016</a>)</span>. Each object is localized with a bounding box and assigned an object class.</p>
<div id="fig-intro-yolo-example" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-yolo-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/intro/yolo_object_detection_example.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-yolo-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: Object Detection example (from <span class="citation" data-cites="Redmon2016a">Redmon et al. (<a href="#ref-Redmon2016a" role="doc-biblioref">2016</a>)</span>). Bounding boxes localize the objects, indicating the most likely class and confidence for each object.
</figcaption>
</figure>
</div>
</section>
<section id="segmentation" class="level3">
<h3 class="anchored" data-anchor-id="segmentation">Segmentation</h3>
<p>In segmentation, individual pixels are assigned to specific objects (instance segmentation) or classes (semantic segmentation). <a href="#fig-mask-rcnn-object-segmentation-example" class="quarto-xref">Figure&nbsp;12</a> shows an example of object segmentation (instance segmentation) from the paper by <span class="citation" data-cites="he_mask_2018">He et al. (<a href="#ref-he_mask_2018" role="doc-biblioref">2018</a>)</span>, where individual objects are detected and precisely localized (segmented) at the pixel level.</p>
<div id="fig-mask-rcnn-object-segmentation-example" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mask-rcnn-object-segmentation-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/intro/mask_rcnn_object_segmentation_example.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mask-rcnn-object-segmentation-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: Object Segmentation example (from <span class="citation" data-cites="he_mask_2018">He et al. (<a href="#ref-he_mask_2018" role="doc-biblioref">2018</a>)</span>).
</figcaption>
</figure>
</div>
<p>The following video shows an example of semantic segmentation:</p>
<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/wfObVKKKJkE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="">
</iframe>
</section>
<section id="keypoint-detection" class="level3">
<h3 class="anchored" data-anchor-id="keypoint-detection">Keypoint Detection</h3>
<p>In keypoint detection, keypoints of people are localized. People must be detected and their keypoints (joints) localized. <a href="#fig-mask-rcnn-keypoint-detection-example" class="quarto-xref">Figure&nbsp;13</a> shows an example of keypoint detection from the paper by <span class="citation" data-cites="he_mask_2018">He et al. (<a href="#ref-he_mask_2018" role="doc-biblioref">2018</a>)</span>, where</p>
<p>individual people are detected and their joints localized. This can be used to transfer movements from a person to an avatar or to recognize activities of people (action recognition).</p>
<div id="fig-mask-rcnn-keypoint-detection-example" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mask-rcnn-keypoint-detection-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/intro/mask_rcnn_keypoint_detection_example.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mask-rcnn-keypoint-detection-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13: Keypoint Detection example (from <span class="citation" data-cites="he_mask_2018">He et al. (<a href="#ref-he_mask_2018" role="doc-biblioref">2018</a>)</span>).
</figcaption>
</figure>
</div>
</section>
<section id="image-generation" class="level3">
<h3 class="anchored" data-anchor-id="image-generation">Image Generation</h3>
<p>There are various applications where models transform input images into specific output images (image-to-image or image translation) or generate completely new images (image generation). Below some example applications.</p>
<section id="image-manipulation" class="level4">
<h4 class="anchored" data-anchor-id="image-manipulation">Image Manipulation</h4>
<div id="fig-intro-image-manipulation" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-image-manipulation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video"><video id="video_shortcode_videojs_video3" width="800" class="video-js vjs-default-skin " controls="" preload="auto" data-setup="{}" title=""><source src="https://vcai.mpi-inf.mpg.de/projects/DragGAN/data/DragGAN.mp4"></video></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-image-manipulation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14: <a href="https://vcai.mpi-inf.mpg.de/projects/DragGAN/">Source: Link</a>, DragGAN by <span class="citation" data-cites="pan_drag_2023">Pan et al. (<a href="#ref-pan_drag_2023" role="doc-biblioref">2023</a>)</span>
</figcaption>
</figure>
</div>
</section>
<section id="image-translation" class="level4">
<h4 class="anchored" data-anchor-id="image-translation">Image Translation</h4>
<p><a href="#fig-intro-pix2pix-example" class="quarto-xref">Figure&nbsp;15</a> shows an example of image generation from the paper by <span class="citation" data-cites="image_to_image_isola2018">Isola et al. (<a href="#ref-image_to_image_isola2018" role="doc-biblioref">2018</a>)</span>, where images are generated conditioned on image inputs (translated). This can be used, for example, to convert a sketch of an object into a photorealistic depiction of it.</p>
<div id="fig-intro-pix2pix-example" class="quarto-float quarto-figure quarto-figure-center anchored" width="600">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-pix2pix-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/intro/pix2pix_examples.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-pix2pix-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15: Image Generation example (from <span class="citation" data-cites="image_to_image_isola2018">Isola et al. (<a href="#ref-image_to_image_isola2018" role="doc-biblioref">2018</a>)</span>).
</figcaption>
</figure>
</div>
</section>
<section id="image-super-resolution" class="level4">
<h4 class="anchored" data-anchor-id="image-super-resolution">Image Super Resolution</h4>
<p>In the gaming industry, Deep Learning is used to generate high-resolution images, scaling low-resolution images efficiently (image super resolution), as shown in <a href="#fig-intro-dssl" class="quarto-xref">Figure&nbsp;16</a>. This allows for higher frame rates.</p>
<div id="fig-intro-dssl" class="quarto-float quarto-figure quarto-figure-center anchored" width="600">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-dssl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/intro/dssl.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-dssl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16: Nvidia DLSS: <a href="https://images.nvidia.com/aem-dam/Solutions/geforce/news/control-nvidia-dlss-2-0-update/deliver-us-the-moon-nvidia-dlss-2-0-performance-boost.png">Link</a>
</figcaption>
</figure>
</div>
</section>
<section id="image-colorization" class="level4">
<h4 class="anchored" data-anchor-id="image-colorization">Image Colorization</h4>
<p>Image colorization can be learned with Deep Learning. This involves transforming a black-and-white image into an RGB image. <a href="#fig-intro-colorization-example" class="quarto-xref">Figure&nbsp;17</a> shows an example.</p>
<div id="fig-intro-colorization-example" class="quarto-float quarto-figure quarto-figure-center anchored" width="600">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-colorization-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/intro/colorization_example.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-colorization-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17: Norwegian Bride (est late 1890s) from DeOldify: <a href="https://github.com/jantic/DeOldify">Link</a>
</figcaption>
</figure>
</div>
</section>
<section id="view-synthesis" class="level4">
<h4 class="anchored" data-anchor-id="view-synthesis">View Synthesis</h4>
<p>In view synthesis, views of certain scenes are generated from models. Neural Radiance Fields (NeRFs) are simple models that can generate new views from known viewpoints and their images. <a href="#fig-intro-nerf" class="quarto-xref">Figure&nbsp;18</a> shows the data on which such a model is trained and what can be generated with it.</p>
<div id="fig-intro-nerf" class="quarto-float quarto-figure quarto-figure-center anchored" width="600">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-nerf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/intro/nerf.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-nerf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;18: Neural Radiance Fields - example (from <span class="citation" data-cites="mildenhall_nerf_2020">Mildenhall et al. (<a href="#ref-mildenhall_nerf_2020" role="doc-biblioref">2020</a>)</span>).
</figcaption>
</figure>
</div>
</section>
<section id="unconditional-generation" class="level4">
<h4 class="anchored" data-anchor-id="unconditional-generation">Unconditional Generation</h4>
<p>In unconditional image generation, data (images) are generated that resemble those in the training data. Here, you have no direct control over the model’s output. However, you can often make changes to generated images or interpolate between data points. <a href="#fig-intro-stylegan3" class="quarto-xref">Figure&nbsp;19</a> shows generated images from a model trained on portrait images of people.</p>
<div id="fig-intro-stylegan3" class="quarto-float quarto-figure quarto-figure-center anchored" width="600">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-stylegan3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/intro/stylegan3_examples.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-stylegan3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;19: StyleGan3 (from <span class="citation" data-cites="karras_alias-free_2021">Karras et al. (<a href="#ref-karras_alias-free_2021" role="doc-biblioref">2021</a>)</span>).
</figcaption>
</figure>
</div>
</section>
<section id="text-to-image" class="level4">
<h4 class="anchored" data-anchor-id="text-to-image">Text to Image</h4>
<p><a href="#fig-stable-diffusion-example" class="quarto-xref">Figure&nbsp;20</a> shows an example of image generation from the paper by <span class="citation" data-cites="rombach_high-resolution_2022">Rombach et al. (<a href="#ref-rombach_high-resolution_2022" role="doc-biblioref">2022</a>)</span>, where images are generated conditioned on text inputs.</p>
<div id="fig-stable-diffusion-example" class="quarto-float quarto-figure quarto-figure-center anchored" width="600">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-stable-diffusion-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/intro/stable_diffusion_example.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-stable-diffusion-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;20: Image Generation example (from <span class="citation" data-cites="rombach_high-resolution_2022">Rombach et al. (<a href="#ref-rombach_high-resolution_2022" role="doc-biblioref">2022</a>)</span>.)
</figcaption>
</figure>
</div>
<p>On <a href="https://civitai.com/">civitai</a>, there are numerous examples and models to admire or download.</p>
</section>
</section>
<section id="further-tasks" class="level3">
<h3 class="anchored" data-anchor-id="further-tasks">Further tasks</h3>
<p>There are many more computer vision tasks. The following list is not exhaustive:</p>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>Image Classification</li>
<li>Object Detection (and Tracking)</li>
<li>Image Segmentation
<ul>
<li>Semantic Segmentation</li>
<li>Instance Segmentation</li>
</ul></li>
<li>Optical Character Recognition (OCR)</li>
<li>Pose Estimation</li>
<li>Facial Recognition</li>
<li>Action Recognition</li>
</ul>
</div><div class="column" style="width:50%;">
<ul>
<li>Image Generation
<ul>
<li>Style Transfer</li>
<li>Image Inpainting</li>
<li>Super-Resolution</li>
<li>Text-to-Image (and more)</li>
</ul></li>
<li>Image Captioning</li>
<li>3D Reconstruction</li>
<li>Image Retrieval</li>
</ul>
</div>
</div>
</section>
</section>
<section id="challenges" class="level2">
<h2 class="anchored" data-anchor-id="challenges">Challenges</h2>
<p>We will now explore some challenges that must be overcome when analyzing images with machine learning models.</p>
<section id="semantic-gap" class="level3">
<h3 class="anchored" data-anchor-id="semantic-gap">Semantic Gap</h3>
<p>The semantic gap refers to the discrepancy between low-level information that can be extracted from an image and the interpretation of an image by a viewer. Simply put: an image often consists of millions of pixels whose information must be condensed to ultimately derive semantically meaningful information. This is an extremely complex task.</p>
<div id="fig-cat-number-grid" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cat-number-grid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/intro/semantic_gap.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cat-number-grid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;21: Illustration of the semantic gap.
</figcaption>
</figure>
</div>
</section>
<section id="viewpoint" class="level3">
<h3 class="anchored" data-anchor-id="viewpoint">Viewpoint</h3>
<p>The meaning of the image does not change with the viewpoint, but the pixels do.</p>
<div id="fig-intro-challenge-viewpoint" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-challenge-viewpoint-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/intro/viewpoint.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-challenge-viewpoint-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;22: <a href="http://cs231n.stanford.edu/">Source</a>
</figcaption>
</figure>
</div>
</section>
<section id="deformation" class="level3">
<h3 class="anchored" data-anchor-id="deformation">Deformation</h3>
<p>Objects are often flexible and appear in different shapes and poses.</p>
<div id="fig-challenge-deformation" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-challenge-deformation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/intro/challenge_deformation.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-challenge-deformation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;23: <a href="http://cs231n.stanford.edu/">Source</a>
</figcaption>
</figure>
</div>
</section>
<section id="illumination" class="level3">
<h3 class="anchored" data-anchor-id="illumination">Illumination</h3>
<p>Changes in illumination affect pixel values and the visibility of objects.</p>
<div id="fig-challenge-illumination" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-challenge-illumination-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/intro/challenge_illumination.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-challenge-illumination-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;24: <a href="http://cs231n.stanford.edu/">Source</a>
</figcaption>
</figure>
</div>
</section>
<section id="background" class="level3">
<h3 class="anchored" data-anchor-id="background">Background</h3>
<p>Background pixels can resemble objects and make their exact delineation or visibility more difficult.</p>
<div id="fig-challenge-background" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-challenge-background-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/intro/challenge_background.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-challenge-background-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;25: <a href="http://cs231n.stanford.edu/">Source</a>
</figcaption>
</figure>
</div>
</section>
<section id="occlusion" class="level3">
<h3 class="anchored" data-anchor-id="occlusion">Occlusion</h3>
<p>Objects are not always fully visible, which can make their detection more difficult.</p>
<div id="fig-challenge-occlusion" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-challenge-occlusion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/intro/challenge_occlusion.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-challenge-occlusion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;26: <a href="http://cs231n.stanford.edu/">Source</a>
</figcaption>
</figure>
</div>
</section>
<section id="intra-class-variation" class="level3">
<h3 class="anchored" data-anchor-id="intra-class-variation">Intra-Class Variation</h3>
<p>Objects of the same class can exhibit large intra-class variability.</p>
<div id="fig-challenge-intra-class" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-challenge-intra-class-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/intro/challenge_intra_class_variation.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-challenge-intra-class-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;27: <a href="https://www.maxpixel.net/Cat-Kittens-Free-Float-Kitten-Rush-Cat-Puppy-555822">Source</a>
</figcaption>
</figure>
</div>
</section>
<section id="context-dependence" class="level3">
<h3 class="anchored" data-anchor-id="context-dependence">Context Dependence</h3>
<p><a href="#fig-tiger-context" class="quarto-xref">Figure&nbsp;28</a> shows that context information can be important to correctly classify an object.</p>
<div id="fig-tiger-context" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tiger-context-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/intro/tiger_context.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tiger-context-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;28: Context <a href="https://www.linkedin.com/posts/ralph-aboujaoude-diaz-40838313_technology-artificialintelligence-computervision-activity-6912446088364875776-h-Iq?utm_source=linkedin_share&amp;utm_medium=member_desktop_web">Source</a>
</figcaption>
</figure>
</div>
</section>
</section>
<section id="software" class="level2">
<h2 class="anchored" data-anchor-id="software">Software</h2>
<section id="pipelines" class="level3">
<h3 class="anchored" data-anchor-id="pipelines">Pipelines</h3>
<p>Various models can also be combined into pipelines. One example is <a href="https://github.com/IDEA-Research/Grounded-Segment-Anything">Grounded-Segment-Anything</a>, shown in <a href="#fig-intro-grounded-segment-anything" class="quarto-xref">Figure&nbsp;29</a>. Object detection models that process text queries are used to detect objects. These detections are used by a segmentation model to segment the target object. This segmentation is then used in a text-to-image model to make the desired change at the correct location.</p>
<div id="fig-intro-grounded-segment-anything" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-grounded-segment-anything-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/intro/grounded_segment_anything.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-grounded-segment-anything-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29: Example from Grounded-Segment-Anything <a href="https://github.com/IDEA-Research/Grounded-Segment-Anything">Link</a>
</figcaption>
</figure>
</div>
</section>
<section id="image-analysis-libraries" class="level3">
<h3 class="anchored" data-anchor-id="image-analysis-libraries">Image Analysis Libraries</h3>
<p>There are numerous open-source libraries that provide pre-trained models to handle the tasks mentioned above. Therefore, you do not always have to train a model yourself. <a href="#fig-detectron-example" class="quarto-xref">Figure&nbsp;30</a> shows the capabilities of Detectron 2, an object detection library from Facebook, which can also be used for other tasks like segmentation.</p>
<div id="fig-detectron-example" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-detectron-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/intro/detectron2_animation.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-detectron-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;30: Example from Facebook’s Detectron Library <a href="https://github.com/facebookresearch/detectron2">Link</a>
</figcaption>
</figure>
</div>
<p>Hugging Face is also well-known. It offers numerous models and datasets for various computer vision questions like object detection, segmentation, and classification.</p>
</section>
</section>
<section id="machine-learning" class="level2">
<h2 class="anchored" data-anchor-id="machine-learning">Machine Learning</h2>
<p>We follow a data-driven approach in machine learning to solve various tasks. Typically, the process involves:</p>
<ul>
<li>Collecting a dataset of images and their labels.</li>
<li>Using a machine learning algorithm to train a model that learns to associate images with labels.</li>
<li>Evaluating/applying the model on new data.</li>
</ul>
<div id="1abb9c2d" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(images, labels):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a> <span class="co">""" Train a Model """</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a> <span class="co"># Fit Model here</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a> <span class="cf">return</span> model</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict(test_images, model):</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a> <span class="co">""" Predict """</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a> predictions <span class="op">=</span> model(test_images)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a> <span class="cf">return</span> predictions</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Question</strong></p>
<p>How would you train a model for super resolution? The task of the model would be to scale low-resolution images to high-resolution images with the best possible quality.</p>
</div>
</div>
</div>
<section id="machine-learning-process" class="level3">
<h3 class="anchored" data-anchor-id="machine-learning-process">Machine Learning Process</h3>
<p>When modeling data, one often follows certain process steps: acquiring data, preparing it, training multiple models, selecting the most suitable model, estimating its future performance, and finally deploying it in production. <a href="#fig-intro-ml-pipeline" class="quarto-xref">Figure&nbsp;31</a> illustrates this process graphically.</p>
<div id="fig-intro-ml-pipeline" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-ml-pipeline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/intro/python_ml.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-ml-pipeline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;31: Machine Learning Pipeline (Source: <span class="citation" data-cites="raschka_python_2020">Raschka and Mirjalili (<a href="#ref-raschka_python_2020" role="doc-biblioref">2020</a>)</span>)
</figcaption>
</figure>
</div>
<p>At the core of a machine learning application is typically a mathematical model, which is fitted to a dataset so that it can then be used for prediction (in supervised learning). We often refer to ‘models’, meaning the mathematical description of the dataset.</p>
</section>
<section id="models" class="level3">
<h3 class="anchored" data-anchor-id="models">Models</h3>
<p>A model is typically described as a function of a data point, generating an output <span class="math inline">\(\hat{y}\)</span>:</p>
<p><span class="math display">\[\begin{align*}
f(\mathbf{x}^{(i)}) = \hat{y}^{(i)}
\end{align*}\]</span></p>
<p>Most models have parameters or coefficients that describe the model. The entirety of all parameters is denoted by <span class="math inline">\(\theta\)</span>.</p>
<p><span class="math display">\[\begin{align*}
f_{\theta}(\mathbf{x}^{(i)}) \text{ or } f(\theta, \mathbf{x}^{(i)})
\end{align*}\]</span></p>
<p>For simplicity, we often omit <span class="math inline">\(\theta\)</span>: <span class="math inline">\(f(\mathbf{x}^{(i)})\)</span></p>
</section>
<section id="optimization" class="level3">
<h3 class="anchored" data-anchor-id="optimization">Optimization</h3>
<p>The coefficients are fitted to a training dataset through an optimization procedure.</p>
<p>The optimization procedure can often be influenced by additional factors, called hyperparameters (<span class="math inline">\(\alpha, \lambda, \dots\)</span>). These cannot be directly optimized.</p>
<p>The function/quantity to be optimized is usually called the cost function, i.e., cost function (other terms include objective function, loss function, etc.). We use <span class="math inline">\(J(\cdot)\)</span> to denote the cost function. Often, the cost function is also referred to as the loss function <span class="math inline">\(L(\cdot)\)</span>. We use <span class="math inline">\(l(\cdot)\)</span> for the per-sample loss, i.e., the computation of the cost function on a single sample.</p>
<p>Our goal is to find a model (and its parameters) that minimizes the cost function:</p>
<p><span class="math display">\[\begin{equation*}
\mathsf{argmin}_{\theta, \lambda} J\Big(f_{\theta, \lambda}(\mathbf{X}), \mathbf{y}\Big)
\end{equation*}\]</span></p>
<p>Usually, preprocessing of variables precedes the learning of</p>
<p>the coefficients. Forms of preprocessing include standardizing, normalizing, feature encoding, dimensionality reduction, and more. This preprocessing also affects the optimization procedure and can be considered hyperparameters.</p>
</section>
<section id="model-selection" class="level3">
<h3 class="anchored" data-anchor-id="model-selection">Model Selection</h3>
<p>Model selection is one of the most important and complex components of the machine learning process. This step involves comparing multiple models and selecting the “best” model for the task to be modeled. Which model is the “best” must be defined based on a metric that measures the model’s performance.</p>
<p>If we calculate the value of the metric on the training dataset, our model is usually too optimistic about its general performance. This is because the data points in the training dataset were directly used to optimize the cost function, and the model coefficients are thus optimally adjusted to them. New data points, for which predictions are to be made, could not have been used for optimization. Therefore, a dataset is usually divided into a training set and a test set. The model is trained with the training set and its performance is measured on the test set. When comparing many models, it is advisable to compare them on a separate validation set (see <a href="#fig-intro-train-test-split" class="quarto-xref">Figure&nbsp;32</a>) and evaluate only the best model on the test set. This makes the estimate on the test set more accurate.</p>
<div id="fig-intro-train-test-split" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-train-test-split-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/intro/train_test_split.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-train-test-split-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;32: Train-Test Split to select and evaluate models.
</figcaption>
</figure>
</div>
</section>
<section id="ml-with-classical-computer-vision" class="level3">
<h3 class="anchored" data-anchor-id="ml-with-classical-computer-vision">ML with Classical Computer Vision</h3>
<p>Images typically have very high dimensionality. For example, an RGB image with a resolution of <span class="math inline">\(800 \times 600\)</span> has a dimensionality of <span class="math inline">\(800 \times 600 \times 3 = 1,440,000\)</span>. Classical machine learning algorithms often struggle with such high dimensionalities:</p>
<ul>
<li>They are very slow or require a lot of memory.</li>
<li>They cannot exploit the 2-D structure of images.</li>
<li>They are very sensitive to slight changes in images (e.g., rotations).</li>
<li>They can easily overfit, as the number of features is close to the number of observations (training set).</li>
</ul>
<p>When modeling images with (classical) machine learning algorithms, methods from (classical) computer vision are often used. With the help of such methods, features can be extracted from images, and the algorithms can learn on these features. This avoids modeling high-dimensional raw data. <a href="#fig-intro-color-histograms" class="quarto-xref">Figure&nbsp;33</a>, <a href="#fig-intro-hog-features" class="quarto-xref">Figure&nbsp;34</a>, and <a href="#fig-intro-bow-features" class="quarto-xref">Figure&nbsp;35</a> show various feature extraction methods.</p>
<p><a href="#fig-intro-color-histograms" class="quarto-xref">Figure&nbsp;33</a> shows that, for example, the distribution over the color spectrum can be extracted from an image. This could be an important feature.</p>
<div id="fig-intro-color-histograms" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-color-histograms-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/intro/color_histogram.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-color-histograms-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;33: Color Histograms as Features (Source: <span class="citation" data-cites="johnson_eecs_2022">Johnson (<a href="#ref-johnson_eecs_2022" role="doc-biblioref">2022</a>)</span>)
</figcaption>
</figure>
</div>
<p><a href="#fig-intro-hog-features" class="quarto-xref">Figure&nbsp;34</a> shows that techniques like Histogram of Oriented Gradients (HOG) <span class="citation" data-cites="dalal_histograms_2005">Dalal and Triggs (<a href="#ref-dalal_histograms_2005" role="doc-biblioref">2005</a>)</span> can be used to extract structures from images. Such features were successfully used for pedestrian detection <span class="citation" data-cites="dalal_histograms_2005">Dalal and Triggs (<a href="#ref-dalal_histograms_2005" role="doc-biblioref">2005</a>)</span>.</p>
<div id="fig-intro-hog-features" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-hog-features-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/intro/hog_features.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-hog-features-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;34: HOG as Features (Source: <span class="citation" data-cites="johnson_eecs_2022">Johnson (<a href="#ref-johnson_eecs_2022" role="doc-biblioref">2022</a>)</span>)
</figcaption>
</figure>
</div>
<p><a href="#fig-intro-bow-features" class="quarto-xref">Figure&nbsp;35</a> shows another feature variant. Visual patches can be extracted from a dataset, clustered, and then used as descriptors.</p>
<div id="fig-intro-bow-features" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-bow-features-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/intro/bag_of_words_features.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-bow-features-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;35: Bag of (visual) words Features (Source: <span class="citation" data-cites="johnson_eecs_2022">Johnson (<a href="#ref-johnson_eecs_2022" role="doc-biblioref">2022</a>)</span>)
</figcaption>
</figure>
</div>
<p>Finally, all features can be combined, often more is better, as shown in <a href="#fig-intro-concat-features" class="quarto-xref">Figure&nbsp;36</a>.</p>
<div id="fig-intro-concat-features" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-concat-features-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/intro/features_concat.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-concat-features-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;36: Image Features (Source: <span class="citation" data-cites="johnson_eecs_2022">Johnson (<a href="#ref-johnson_eecs_2022" role="doc-biblioref">2022</a>)</span>)
</figcaption>
</figure>
</div>
<p>Depending on the parameterization, HOG descriptors, for example, can be very large for images. The resulting feature vector from <a href="#fig-intro-concat-features" class="quarto-xref">Figure&nbsp;36</a> can still be reduced in dimensionality, e.g., with Principal Component Analysis.</p>
<p><a href="#fig-intro-cifar10" class="quarto-xref">Figure&nbsp;37</a> shows the CIFAR10 dataset. A well-known dataset for testing models, consisting of 10 classes with 6,000 images each.</p>
<div id="fig-intro-cifar10" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-cifar10-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/intro/cifar10.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-cifar10-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;37: CIFAR10 Dataset <a href="https://www.cs.toronto.edu/~kriz/cifar.html">Source</a>
</figcaption>
</figure>
</div>
</section>
</section>
<section id="deep-learning" class="level2">
<h2 class="anchored" data-anchor-id="deep-learning">Deep Learning</h2>
<p>To accelerate research in computer vision and make progress more measurable, the PASCAL VOC Challenges were introduced <span class="citation" data-cites="everingham_pascal_2007">Everingham et al. (<a href="#ref-everingham_pascal_2007" role="doc-biblioref">2007</a>)</span>. These involved various tasks, such as detecting objects in photographs (<a href="#fig-intro-pascal" class="quarto-xref">Figure&nbsp;38</a>).</p>
<div id="fig-intro-pascal" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-pascal-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/intro/pascal_voc_2007.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-pascal-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;38: Images/illustrations from <a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2007/">Link</a> and <span class="citation" data-cites="johnson_eecs_2022">Johnson (<a href="#ref-johnson_eecs_2022" role="doc-biblioref">2022</a>)</span>. On the left, you see object annotations in images, and on the right, you see the development of Mean Average Precision over the years.
</figcaption>
</figure>
</div>
<p>The easy availability of images on the internet has made it possible to collect increasingly larger datasets. <a href="https://www.image-net.org/">ImageNet</a> is such a very large, hierarchically annotated image dataset <span class="citation" data-cites="deng_imagenet_2009">Deng et al. (<a href="#ref-deng_imagenet_2009" role="doc-biblioref">2009</a>)</span> with over 1.4 million images, categorized into 1,000 object classes. <a href="#fig-intro-imagenet" class="quarto-xref">Figure&nbsp;39</a> illustrates the dataset.</p>
<div id="fig-intro-imagenet" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-imagenet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/intro/imagenet.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-imagenet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;39: <a href="https://www.image-net.org/">ImageNet</a>, <a href="https://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_full_1k.jpg">Image Source</a>, details in <span class="citation" data-cites="deng_imagenet_2009">Deng et al. (<a href="#ref-deng_imagenet_2009" role="doc-biblioref">2009</a>)</span>
</figcaption>
</figure>
</div>
<p>Since 2010, challenges have been regularly conducted on the ImageNet dataset <span class="citation" data-cites="russakovsky_imagenet_2015">Russakovsky et al. (<a href="#ref-russakovsky_imagenet_2015" role="doc-biblioref">2015</a>)</span>, such as image classification and object detection. <a href="#fig-intro-imagenet-perf" class="quarto-xref">Figure&nbsp;40</a> shows the development of the error rate over time.</p>
<div id="fig-intro-imagenet-perf" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-imagenet-perf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/intro/image_net_perf.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-imagenet-perf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;40: Source: <span class="citation" data-cites="johnson_eecs_2022">Johnson (<a href="#ref-johnson_eecs_2022" role="doc-biblioref">2022</a>)</span>
</figcaption>
</figure>
</div>
<p>In 2011, a team won <span class="citation" data-cites="perronnin_large-scale_2010">Perronnin et al. (<a href="#ref-perronnin_large-scale_2010" role="doc-biblioref">2010</a>)</span> by combining various (classical) feature extraction methods with machine learning. They used, among other things, SIFT features to train SVMs.</p>
<p>In 2012, a drastic reduction in the error rate was achieved in the ImageNet competition. This development marked the end of classical computer vision methods in many areas. Krizhevsky et al. <span class="citation" data-cites="krizhevsky_imagenet_2012">Krizhevsky, Sutskever, and Hinton (<a href="#ref-krizhevsky_imagenet_2012" role="doc-biblioref">2012</a>)</span> impressively demonstrated the potential of neural networks in 2012. They implemented a convolutional neural network (CNN) with multiple layers, the so-called AlexNet architecture, as shown in <a href="#fig-intro-alexnet" class="quarto-xref">Figure&nbsp;41</a>.</p>
<div id="fig-intro-alexnet" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-alexnet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/intro/alexnet.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-alexnet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;41: Alexnet <span class="citation" data-cites="krizhevsky_imagenet_2012">Krizhevsky, Sutskever, and Hinton (<a href="#ref-krizhevsky_imagenet_2012" role="doc-biblioref">2012</a>)</span>.
</figcaption>
</figure>
</div>
<p>While classical computer vision trains a machine learning model on features extracted with hand-crafted algorithms <a href="#fig-intro-classical-ml" class="quarto-xref">Figure&nbsp;42</a>, the development is increasingly moving towards end-to-end learning. In this approach, one avoids as much as possible static/hand-designed components and learns everything, including feature extraction, with machine learning <a href="#fig-intro-end-to-end" class="quarto-xref">Figure&nbsp;43</a>.</p>
<div id="fig-intro-classical-ml" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-classical-ml-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/intro/classical_ml.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-classical-ml-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;42: Illustration from <span class="citation" data-cites="johnson_eecs_2022">Johnson (<a href="#ref-johnson_eecs_2022" role="doc-biblioref">2022</a>)</span>
</figcaption>
</figure>
</div>
<div id="fig-intro-end-to-end" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-end-to-end-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/intro/end-to-end-ml.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-end-to-end-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;43: Illustration from <span class="citation" data-cites="johnson_eecs_2022">Johnson (<a href="#ref-johnson_eecs_2022" role="doc-biblioref">2022</a>)</span>
</figcaption>
</figure>
</div>
<p>Deep learning-based approaches have several advantages over classical machine learning methods:</p>
<ul>
<li>Automatic feature extraction: no manual feature extraction procedures are needed.</li>
<li>Hierarchical features: these are particularly valuable for processing and understanding visual data.</li>
<li>Generalization: with more training data, deep learning methods generalize better.</li>
<li>End-to-end learning: this approach allows many problems to be modeled similarly.</li>
<li>Robustness to variability: certain models are naturally invariant to</li>
</ul>
<p>transformations like translations, scalings, etc. - Adaptability and transferability: deep learning models can often be easily adapted (transfer learning) and can create good models even with little data.</p>
<section id="deep-learning-history" class="level3">
<h3 class="anchored" data-anchor-id="deep-learning-history">Deep Learning History</h3>
<p>We will now explore the most important milestones in deep learning for image analysis.</p>
</section>
<section id="hubel-and-wiesel" class="level3">
<h3 class="anchored" data-anchor-id="hubel-and-wiesel">Hubel and Wiesel</h3>
<p><span class="citation" data-cites="hubel_receptive_1959">Hubel and Wiesel (<a href="#ref-hubel_receptive_1959" role="doc-biblioref">1959</a>)</span> showed in experiments on cats that there are complex cells/neurons in the visual cortex that only respond to certain patterns. In addition to these complex cells, there are also simple cells that only respond to lines at a certain angle.</p>
<div id="fig-huber-wiesel" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-huber-wiesel-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/intro/huber_wiesel_cat_experiment.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-huber-wiesel-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;44: Illustration <a href="https://link.springer.com/chapter/10.1007/978-3-030-28954-6_4/figures/1">Source</a>
</figcaption>
</figure>
</div>
</section>
<section id="neocognitron" class="level3">
<h3 class="anchored" data-anchor-id="neocognitron">Neocognitron</h3>
<p><span class="citation" data-cites="fukushima_neocognitron_1980">Fukushima (<a href="#ref-fukushima_neocognitron_1980" role="doc-biblioref">1980</a>)</span> defined a model of a neural network that can recognize visual patterns. It already has the hierarchical structure of a modern convolutional neural network and is inspired by biological neural networks, particularly from the insights of <span class="citation" data-cites="hubel_receptive_1959">Hubel and Wiesel (<a href="#ref-hubel_receptive_1959" role="doc-biblioref">1959</a>)</span>.</p>
<div id="fig-neocognitron" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-neocognitron-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/intro/neocogitron.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-neocognitron-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;45: The Neocognitron <span class="citation" data-cites="fukushima_neocognitron_1980">Fukushima (<a href="#ref-fukushima_neocognitron_1980" role="doc-biblioref">1980</a>)</span>.
</figcaption>
</figure>
</div>
</section>
<section id="backpropagation" class="level3">
<h3 class="anchored" data-anchor-id="backpropagation">Backpropagation</h3>
<p><span class="citation" data-cites="rumelhart_learning_1986">Rumelhart, Hinton, and Williams (<a href="#ref-rumelhart_learning_1986" role="doc-biblioref">1986</a>)</span> introduced the backpropagation algorithm in the context of neural networks. This algorithm is used to train modern neural networks: it finds the parameters of an artificial neural network (ANN) to solve a specific task. Backpropagation is based on the chain rule from calculus and is also important for other machine learning models.</p>
<div id="fig-rumalhart-backprop" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rumalhart-backprop-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/intro/rumelhart_backprop.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rumalhart-backprop-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;46: Backpropagation in neural networks <span class="citation" data-cites="rumelhart_learning_1986">Rumelhart, Hinton, and Williams (<a href="#ref-rumelhart_learning_1986" role="doc-biblioref">1986</a>)</span>.
</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>A good video on backpropagation: <a href="https://www.youtube.com/watch?v=tIeHLnjs5U8">3Blue1Brown Backpropagation Calculus</a></p>
</div>
</div>
</section>
<section id="convolutional-neural-networks-cnns" class="level3">
<h3 class="anchored" data-anchor-id="convolutional-neural-networks-cnns">Convolutional Neural Networks (CNNs)</h3>
<p><span class="citation" data-cites="lecun_gradient-based_1998">Lecun et al. (<a href="#ref-lecun_gradient-based_1998" role="doc-biblioref">1998</a>)</span> implemented convolutional neural networks (CNNs) to recognize handwritten digits. It is specialized for the 2-D structure of the input data. They trained a model very similar to modern CNNs, as shown in <a href="#fig-lecun-cnn" class="quarto-xref">Figure&nbsp;47</a>.</p>
<div id="fig-lecun-cnn" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lecun-cnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/intro/lecun_cnn.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lecun-cnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;47: Modern CNN <span class="citation" data-cites="lecun_gradient-based_1998">Lecun et al. (<a href="#ref-lecun_gradient-based_1998" role="doc-biblioref">1998</a>)</span>.
</figcaption>
</figure>
</div>
<p>CNNs became extremely popular after winning the <a href="https://www.image-net.org/">ImageNet</a> competition. <span class="citation" data-cites="krizhevsky_imagenet_2012">Krizhevsky, Sutskever, and Hinton (<a href="#ref-krizhevsky_imagenet_2012" role="doc-biblioref">2012</a>)</span> implemented a CNN with multiple layers, the so-called AlexNet architecture, as shown in <a href="#fig-alexnet" class="quarto-xref">Figure&nbsp;48</a>.</p>
<div id="fig-alexnet" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-alexnet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/intro/alexnet.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-alexnet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;48: Alexnet <span class="citation" data-cites="krizhevsky_imagenet_2012">Krizhevsky, Sutskever, and Hinton (<a href="#ref-krizhevsky_imagenet_2012" role="doc-biblioref">2012</a>)</span>.
</figcaption>
</figure>
</div>
<p>Since the breakthrough in 2012, CNNs have been used for increasingly complex tasks and further developed. Well-known are, for example, the <a href="https://cocodataset.org/workshop/coco-lvis-eccv-2020.html">COCO Challenges</a>, with various tasks.</p>
</section>
<section id="other-architectures-in-computer-vision" class="level3">
<h3 class="anchored" data-anchor-id="other-architectures-in-computer-vision">Other Architectures in Computer Vision</h3>
<p>CNNs are still in use today (2024). Meanwhile, there are alternative architectures, such as transformer-based models (<span class="citation" data-cites="dosovitskiy_image_2020">Dosovitskiy et al. (<a href="#ref-dosovitskiy_image_2020" role="doc-biblioref">2020</a>)</span>), which are extremely successful in language modeling, or multilayer perceptron-based architectures (<span class="citation" data-cites="liu_pay_2021">Liu et al. (<a href="#ref-liu_pay_2021" role="doc-biblioref">2021</a>)</span>). However, it has been shown that CNNs are still competitive and sometimes superior to alternative architectures (see <span class="citation" data-cites="woo_convnext_2023">Woo et al. (<a href="#ref-woo_convnext_2023" role="doc-biblioref">2023</a>)</span>). The question of which architecture type will prevail is open. Currently, it seems that CNNs and transformer-based models perform similarily (<span class="citation" data-cites="smith_convnets_2023">Smith et al. (<a href="#ref-smith_convnets_2023" role="doc-biblioref">2023</a>)</span>).</p>
</section>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-breitenmoser-wursten_projekt_2024" class="csl-entry" role="listitem">
Breitenmoser-Würsten, Christine, Fridolin Zimmermann, Kristina Vogt, Manuela von Arx, Sven Signe, and Christian Stauffer. 2024. <span>“Das <span>Projekt</span> <span>LUNO</span> - <span>Abschlussbericht</span>.”</span> 121. Stiftung KORA. <a href="https://portal.issn.org/#">https://portal.issn.org/#</a>.
</div>
<div id="ref-dalal_histograms_2005" class="csl-entry" role="listitem">
Dalal, N., and B. Triggs. 2005. <span>“Histograms of <span>Oriented</span> <span>Gradients</span> for <span>Human</span> <span>Detection</span>.”</span> In <em>2005 <span>IEEE</span> <span>Computer</span> <span>Society</span> <span>Conference</span> on <span>Computer</span> <span>Vision</span> and <span>Pattern</span> <span>Recognition</span> (<span>CVPR</span>’05)</em>, 1:886–93. San Diego, CA, USA: IEEE. <a href="https://doi.org/10.1109/CVPR.2005.177">https://doi.org/10.1109/CVPR.2005.177</a>.
</div>
<div id="ref-deng_imagenet_2009" class="csl-entry" role="listitem">
Deng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. <span>“<span>ImageNet</span>: <span>A</span> Large-Scale Hierarchical Image Database.”</span> In <em>2009 <span>IEEE</span> <span>Conference</span> on <span>Computer</span> <span>Vision</span> and <span>Pattern</span> <span>Recognition</span></em>, 248–55. Miami, FL: IEEE. <a href="https://doi.org/10.1109/CVPR.2009.5206848">https://doi.org/10.1109/CVPR.2009.5206848</a>.
</div>
<div id="ref-dosovitskiy_image_2020" class="csl-entry" role="listitem">
Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2020. <span>“An <span>Image</span> Is <span>Worth</span> 16x16 <span>Words</span>: <span>Transformers</span> for <span>Image</span> <span>Recognition</span> at <span>Scale</span>.”</span> <em>arXiv:2010.11929 [Cs]</em>, October. <a href="http://arxiv.org/abs/2010.11929">http://arxiv.org/abs/2010.11929</a>.
</div>
<div id="ref-everingham_pascal_2007" class="csl-entry" role="listitem">
Everingham, M., L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. 2007. <span>“The <span>PASCAL</span> <span>Visual</span> <span>Object</span> <span>Classes</span> <span>Challenge</span> 2007 (<span>VOC2007</span>) <span>Results</span>.”</span> <a href="http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html">http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html</a>.
</div>
<div id="ref-fukushima_neocognitron_1980" class="csl-entry" role="listitem">
Fukushima, Kunihiko. 1980. <span>“Neocognitron: <span>A</span> Self-Organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position.”</span> <em>Biological Cybernetics</em> 36 (4): 193–202. <a href="https://doi.org/10.1007/BF00344251">https://doi.org/10.1007/BF00344251</a>.
</div>
<div id="ref-hani_minneapple_2020" class="csl-entry" role="listitem">
Häni, Nicolai, Pravakar Roy, and Volkan Isler. 2020. <span>“<span>MinneApple</span>: <span>A</span> <span>Benchmark</span> <span>Dataset</span> for <span>Apple</span> <span>Detection</span> and <span>Segmentation</span>.”</span> <em>IEEE Robotics and Automation Letters</em> 5 (2): 852–58. <a href="https://doi.org/10.1109/LRA.2020.2965061">https://doi.org/10.1109/LRA.2020.2965061</a>.
</div>
<div id="ref-he_mask_2018" class="csl-entry" role="listitem">
He, Kaiming, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. 2018. <span>“Mask <span>R</span>-<span>CNN</span>.”</span> arXiv. <a href="http://arxiv.org/abs/1703.06870">http://arxiv.org/abs/1703.06870</a>.
</div>
<div id="ref-hubel_receptive_1959" class="csl-entry" role="listitem">
Hubel, D. H., and T. N. Wiesel. 1959. <span>“Receptive Fields of Single Neurones in the Cat’s Striate Cortex.”</span> <em>The Journal of Physiology</em> 148 (3): 574–91. <a href="https://doi.org/10.1113/jphysiol.1959.sp006308">https://doi.org/10.1113/jphysiol.1959.sp006308</a>.
</div>
<div id="ref-image_to_image_isola2018" class="csl-entry" role="listitem">
Isola, Phillip, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. 2018. <span>“Image-to-<span>Image</span> <span>Translation</span> with <span>Conditional</span> <span>Adversarial</span> <span>Networks</span>.”</span> arXiv. <a href="http://arxiv.org/abs/1611.07004">http://arxiv.org/abs/1611.07004</a>.
</div>
<div id="ref-johnson_eecs_2022" class="csl-entry" role="listitem">
Johnson, Justin. 2022. <span>“<span>EECS</span> 498.008 / 598.008 <span>Deep</span> <span>Learning</span> for <span>Computer</span> <span>Vision</span>.”</span> Lecture {Notes} / {Slides}. <a href="https://web.eecs.umich.edu/~justincj/teaching/eecs498/WI2022/">https://web.eecs.umich.edu/~justincj/teaching/eecs498/WI2022/</a>.
</div>
<div id="ref-karras_alias-free_2021" class="csl-entry" role="listitem">
Karras, Tero, Miika Aittala, Samuli Laine, Erik Härkönen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. 2021. <span>“Alias-<span>Free</span> <span>Generative</span> <span>Adversarial</span> <span>Networks</span>.”</span> arXiv. <a href="http://arxiv.org/abs/2106.12423">http://arxiv.org/abs/2106.12423</a>.
</div>
<div id="ref-krizhevsky_imagenet_2012" class="csl-entry" role="listitem">
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. <span>“<span>ImageNet</span> <span>Classification</span> with <span>Deep</span> <span>Convolutional</span> <span>Neural</span> <span>Networks</span>.”</span> In <em>Advances in <span>Neural</span> <span>Information</span> <span>Processing</span> <span>Systems</span></em>, edited by F. Pereira, C. J. Burges, L. Bottou, and K. Q. Weinberger. Vol. 25. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf</a>.
</div>
<div id="ref-lecun_gradient-based_1998" class="csl-entry" role="listitem">
Lecun, Y., L. Bottou, Y. Bengio, and P. Haffner. 1998. <span>“Gradient-Based Learning Applied to Document Recognition.”</span> <em>Proceedings of the IEEE</em> 86 (11): 2278–2324. <a href="https://doi.org/10.1109/5.726791">https://doi.org/10.1109/5.726791</a>.
</div>
<div id="ref-liu_pay_2021" class="csl-entry" role="listitem">
Liu, Hanxiao, Zihang Dai, David R. So, and Quoc V. Le. 2021. <span>“Pay <span>Attention</span> to <span>MLPs</span>.”</span> <em>arXiv:2105.08050 [Cs]</em>, June. <a href="http://arxiv.org/abs/2105.08050">http://arxiv.org/abs/2105.08050</a>.
</div>
<div id="ref-ma_segment_2024" class="csl-entry" role="listitem">
Ma, Jun, Yuting He, Feifei Li, Lin Han, Chenyu You, and Bo Wang. 2024. <span>“Segment Anything in Medical Images.”</span> <em>Nature Communications</em> 15 (1): 654. <a href="https://doi.org/10.1038/s41467-024-44824-z">https://doi.org/10.1038/s41467-024-44824-z</a>.
</div>
<div id="ref-mildenhall_nerf_2020" class="csl-entry" role="listitem">
Mildenhall, Ben, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. 2020. <span>“<span>NeRF</span>: <span>Representing</span> <span>Scenes</span> as <span>Neural</span> <span>Radiance</span> <span>Fields</span> for <span>View</span> <span>Synthesis</span>.”</span> arXiv. <a href="http://arxiv.org/abs/2003.08934">http://arxiv.org/abs/2003.08934</a>.
</div>
<div id="ref-pan_drag_2023" class="csl-entry" role="listitem">
Pan, Xingang, Ayush Tewari, Thomas Leimkühler, Lingjie Liu, Abhimitra Meka, and Christian Theobalt. 2023. <span>“Drag <span>Your</span> <span>GAN</span>: <span>Interactive</span> <span>Point</span>-Based <span>Manipulation</span> on the <span>Generative</span> <span>Image</span> <span>Manifold</span>.”</span> arXiv. <a href="http://arxiv.org/abs/2305.10973">http://arxiv.org/abs/2305.10973</a>.
</div>
<div id="ref-perronnin_large-scale_2010" class="csl-entry" role="listitem">
Perronnin, Florent, Yan Liu, Jorge Sanchez, and Herve Poirier. 2010. <span>“Large-Scale Image Retrieval with Compressed <span>Fisher</span> Vectors.”</span> In <em>2010 <span>IEEE</span> <span>Computer</span> <span>Society</span> <span>Conference</span> on <span>Computer</span> <span>Vision</span> and <span>Pattern</span> <span>Recognition</span></em>, 3384–91. San Francisco, CA, USA: IEEE. <a href="https://doi.org/10.1109/CVPR.2010.5540009">https://doi.org/10.1109/CVPR.2010.5540009</a>.
</div>
<div id="ref-raschka_python_2020" class="csl-entry" role="listitem">
Raschka, Sebastian, and Vahid Mirjalili. 2020. <em>Python Machine Learning: Machine Learning and Deep Learning with <span>Python</span>, Scikit-Learn, and <span>TensorFlow</span></em>. Second edition, fourth release,[fully revised and updated]. Expert Insight. Birmingham Mumbai: Packt Publishing.
</div>
<div id="ref-Redmon2016a" class="csl-entry" role="listitem">
Redmon, Joseph, Santosh Divvala, Ross Girshick, and Ali Farhadi. 2016. <span>“You Only Look Once: <span>Unified</span>, Real-Time Object Detection.”</span> <em>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</em> 2016-Decem: 779–88. <a href="https://doi.org/10.1109/CVPR.2016.91">https://doi.org/10.1109/CVPR.2016.91</a>.
</div>
<div id="ref-rombach_high-resolution_2022" class="csl-entry" role="listitem">
Rombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. <span>“High-<span>Resolution</span> <span>Image</span> <span>Synthesis</span> with <span>Latent</span> <span>Diffusion</span> <span>Models</span>.”</span> arXiv. <a href="http://arxiv.org/abs/2112.10752">http://arxiv.org/abs/2112.10752</a>.
</div>
<div id="ref-rumelhart_learning_1986" class="csl-entry" role="listitem">
Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. 1986. <span>“Learning Representations by Back-Propagating Errors.”</span> <em>Nature</em> 323 (6088): 533–36. <a href="https://doi.org/10.1038/323533a0">https://doi.org/10.1038/323533a0</a>.
</div>
<div id="ref-russakovsky_imagenet_2015" class="csl-entry" role="listitem">
Russakovsky, Olga, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, et al. 2015. <span>“<span>ImageNet</span> <span>Large</span> <span>Scale</span> <span>Visual</span> <span>Recognition</span> <span>Challenge</span>.”</span> arXiv. <a href="http://arxiv.org/abs/1409.0575">http://arxiv.org/abs/1409.0575</a>.
</div>
<div id="ref-smith_convnets_2023" class="csl-entry" role="listitem">
Smith, Samuel L., Andrew Brock, Leonard Berrada, and Soham De. 2023. <span>“<span>ConvNets</span> <span>Match</span> <span>Vision</span> <span>Transformers</span> at <span>Scale</span>.”</span> arXiv. <a href="http://arxiv.org/abs/2310.16764">http://arxiv.org/abs/2310.16764</a>.
</div>
<div id="ref-woo_convnext_2023" class="csl-entry" role="listitem">
Woo, Sanghyun, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, and Saining Xie. 2023. <span>“<span>ConvNeXt</span> <span>V2</span>: <span>Co</span>-Designing and <span>Scaling</span> <span>ConvNets</span> with <span>Masked</span> <span>Autoencoders</span>.”</span> arXiv. <a href="http://arxiv.org/abs/2301.00808">http://arxiv.org/abs/2301.00808</a>.
</div>
</div>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/marco-willi\.github\.io\/cas-dl-compvis-lectures-hs2025\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../pages/quiz.html" class="pagination-link" aria-label="Quiz &amp; Exam Prep">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Quiz &amp; Exam Prep</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../lectures/frameworks.html" class="pagination-link" aria-label="2 - Software &amp; Hardware for Deep Learning">
        <span class="nav-page-text">2 - Software &amp; Hardware for Deep Learning</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "1 - Introduction"</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="an">params:</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co">   images_path: "/assets/images/intro/"</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="fu">## Motivation</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>Deep Learning models have played a transformative role in Computer Vision over the last decade, significantly enhancing and expanding the capabilities to process visual data.</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>The integration of Deep Learning in Computer Vision has massively improved the accuracy and efficiency of visual recognition, classification, and analysis. This has opened up new possibilities in applications such as automated driving, facial recognition, and medical image analysis. These models are now embedded in a wide variety of products and services.</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>Biologists might need to analyse images from camera traps for species identification, see @fig-intro-kora-lynx.</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-kora-lynx width=400}</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="al">![Source: @breitenmoser-wursten_projekt_2024]({{&lt; meta params.images_path &gt;}}kora_lynx.png)</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>Image from a camera trap.</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>Recent advances in generative deep learning enables anybody to create realistic looking synthetic images (see @fig-intro-synthetic-images). This can be used for many applications, positive and negative. Such images might be used to create misinformation which is one reason researchers are trying to build models to detect them.</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-synthetic-images layout-ncol=2}</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>![<span class="co">[</span><span class="ot">Source</span><span class="co">](https://x.com/TheInfiniteDude/status/1637211568692932608)</span>]({{&lt; meta params.images_path &gt;}}sid_trump.jpg)</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>![<span class="co">[</span><span class="ot">Source</span><span class="co">](https://x.com/cryptomattk/status/1687908457880367104)</span>]({{&lt; meta params.images_path &gt;}}sid_pope.jpg)</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>Examples of synthetic images.</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ![Camera Trap Image Analysis]({{&lt; meta params.images_path &gt;}}camtrap.jpg){width=400} --&gt;</span></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>@fig-intro-google-lens shows an application where one can take a photo of an object of interest and let a model identify it, as well as search additional information about it. Photos can also be used for translation tasks whereby models detect letters (optical character recognition) and then translate it (using language models).</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-google-lens layout-ncol=2}</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Identification &amp; Search</span><span class="co">]</span>({{&lt; meta params.images_path &gt;}}google_lens_classification.png</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a><span class="al">![Translation]({{&lt; meta params.images_path &gt;}}google_lens_ocr.png)</span></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Google Lens</span><span class="co">](https://search.google/ways-to-search/lens/)</span></span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>One area with high commercial interest and high competition is the development of self-driving capabilities. @fig-intro-self-driving shows a demonstration of the capabilities required towards self-driving, such as understanding a scene and identifying any objects in it.</span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-self-driving}</span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>{{&lt; video https://storage.googleapis.com/waymo-uploads/files/site-animations/waymo-driver/cameras.webm width=800 &gt;}}</span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Example from Waymo</span><span class="co">](https://waymo.com/waymo-driver/)</span>.</span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>Face recognition rechnology, such as Apple's Face ID (@fig-intro-face-id), has become  ubiquitious. In this case depth information is derived from infrared light.</span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-face-id}</span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a>{{&lt; video https://www.youtube.com/embed/z-t1h0Y8vuM?si=qnEOYDmqyv8zGvMV start="50" width=800 height=600 &gt;}}</span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Example from Apple Face ID</span><span class="co">](https://support.apple.com/en-us/102381)</span></span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a>Computer vision has also been very useful in agricultur. @fig-intro-minneapple shows an application where apples can be counted from image data. Furthermore, drones might be used to estimate crop yield, type and health.</span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-minneapple width=400}</span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a><span class="al">![Example from @hani_minneapple_2020]({{&lt; meta params.images_path &gt;}}minneapple.png)</span></span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a>Deep learning has also been applied extensively in the medical area. In particular, many imaging technologies, such as MRI or CT, can be analysed with computer vision techniques. @fig-intro-sam illustrates a generic segmentation model which can be used to identify organs and other anatomical structures on pixel-level.</span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-83"><a href="#cb2-83" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-sam width=400}</span>
<span id="cb2-84"><a href="#cb2-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-85"><a href="#cb2-85" aria-hidden="true" tabindex="-1"></a><span class="al">![Example from @ma_segment_2024.]({{&lt; meta params.images_path &gt;}}medsam.png)</span></span>
<span id="cb2-86"><a href="#cb2-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-87"><a href="#cb2-87" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-88"><a href="#cb2-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-89"><a href="#cb2-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-90"><a href="#cb2-90" aria-hidden="true" tabindex="-1"></a>Photo-editing has been brought to new levels of sophistication with AI-based capabilities. @fig-intro-magic shows an impressive example from the Google Magic Editor. Maybe even a bit too good?</span>
<span id="cb2-91"><a href="#cb2-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-92"><a href="#cb2-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-93"><a href="#cb2-93" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-magic}</span>
<span id="cb2-94"><a href="#cb2-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-95"><a href="#cb2-95" aria-hidden="true" tabindex="-1"></a>{{&lt; video https://storage.googleapis.com/gweb-mobius-cdn/photos/uploads/6e54ed750f84538fd052b31818127f1e4df5711c.compressed.mp4 width=800 &gt;}}</span>
<span id="cb2-96"><a href="#cb2-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-97"><a href="#cb2-97" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Example from Google Magic Editor</span><span class="co">](https://www.google.com/intl/en/photos/editing/)</span></span>
<span id="cb2-98"><a href="#cb2-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-99"><a href="#cb2-99" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-100"><a href="#cb2-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-101"><a href="#cb2-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-102"><a href="#cb2-102" aria-hidden="true" tabindex="-1"></a>::: {.callout-note appearance="simple"}</span>
<span id="cb2-103"><a href="#cb2-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-104"><a href="#cb2-104" aria-hidden="true" tabindex="-1"></a>**Question**</span>
<span id="cb2-105"><a href="#cb2-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-106"><a href="#cb2-106" aria-hidden="true" tabindex="-1"></a>What steps do you think the model in @fig-intro-magic performs?</span>
<span id="cb2-107"><a href="#cb2-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-108"><a href="#cb2-108" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-109"><a href="#cb2-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-110"><a href="#cb2-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-111"><a href="#cb2-111" aria-hidden="true" tabindex="-1"></a>The increasingly better models and the ability to run them quickly and resource-efficiently on mobile devices have enabled such applications. @fig-intro-tensor-phone shows that special chips have been developed to process images with Deep Learning models quickly.</span>
<span id="cb2-112"><a href="#cb2-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-113"><a href="#cb2-113" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-tensor-phone}</span>
<span id="cb2-114"><a href="#cb2-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-115"><a href="#cb2-115" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}tensor_phone.png)</span>{width=600}</span>
<span id="cb2-116"><a href="#cb2-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-117"><a href="#cb2-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-118"><a href="#cb2-118" aria-hidden="true" tabindex="-1"></a>From <span class="co">[</span><span class="ot">Link</span><span class="co">](https://store.google.com/intl/en/ideas/pixel-camera-features/)</span>.</span>
<span id="cb2-119"><a href="#cb2-119" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-120"><a href="#cb2-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-121"><a href="#cb2-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-122"><a href="#cb2-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-123"><a href="#cb2-123" aria-hidden="true" tabindex="-1"></a><span class="fu">## Computer Vision Tasks</span></span>
<span id="cb2-124"><a href="#cb2-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-125"><a href="#cb2-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-126"><a href="#cb2-126" aria-hidden="true" tabindex="-1"></a><span class="fu">### Image Classification</span></span>
<span id="cb2-127"><a href="#cb2-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-128"><a href="#cb2-128" aria-hidden="true" tabindex="-1"></a>In image classification, an image is assigned to a predefined set of classes. In multi-class classification, there are $\gt 2$ classes; in binary classification, there are 2 classes; and in multi-label classification, there are multiple sets of classes. @fig-intro-image_clssifier shows an example from the paper by @krizhevsky_imagenet_2012 (multi-class classification), which achieved the best results in the ImageNet competition in 2012 and demonstrated the effectiveness of CNNs.</span>
<span id="cb2-129"><a href="#cb2-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-130"><a href="#cb2-130" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-image_clssifier width=600}</span>
<span id="cb2-131"><a href="#cb2-131" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}image_classification_example.png)</span></span>
<span id="cb2-132"><a href="#cb2-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-133"><a href="#cb2-133" aria-hidden="true" tabindex="-1"></a>Multi-Class Image Classification Beispiel (aus @krizhevsky_imagenet_2012).</span>
<span id="cb2-134"><a href="#cb2-134" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-135"><a href="#cb2-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-136"><a href="#cb2-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-137"><a href="#cb2-137" aria-hidden="true" tabindex="-1"></a><span class="fu">### Object Detection</span></span>
<span id="cb2-138"><a href="#cb2-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-139"><a href="#cb2-139" aria-hidden="true" tabindex="-1"></a>Object detection involves locating and recognizing (multiple) objects in an image. @fig-intro-yolo-example shows an example from the paper by @Redmon2016a. Each object is localized with a bounding box and assigned an object class.</span>
<span id="cb2-140"><a href="#cb2-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-141"><a href="#cb2-141" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-yolo-example}</span>
<span id="cb2-142"><a href="#cb2-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-143"><a href="#cb2-143" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}yolo_object_detection_example.png)</span>{width=600}</span>
<span id="cb2-144"><a href="#cb2-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-145"><a href="#cb2-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-146"><a href="#cb2-146" aria-hidden="true" tabindex="-1"></a>Object Detection example (from @Redmon2016a). Bounding boxes localize the objects, indicating the most likely class and confidence for each object.</span>
<span id="cb2-147"><a href="#cb2-147" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-148"><a href="#cb2-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-149"><a href="#cb2-149" aria-hidden="true" tabindex="-1"></a><span class="fu">### Segmentation</span></span>
<span id="cb2-150"><a href="#cb2-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-151"><a href="#cb2-151" aria-hidden="true" tabindex="-1"></a>In segmentation, individual pixels are assigned to specific objects (instance segmentation) or classes (semantic segmentation). @fig-mask-rcnn-object-segmentation-example shows an example of object segmentation (instance segmentation) from the paper by @he_mask_2018, where individual objects are detected and precisely localized (segmented) at the pixel level.</span>
<span id="cb2-152"><a href="#cb2-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-153"><a href="#cb2-153" aria-hidden="true" tabindex="-1"></a>::: {#fig-mask-rcnn-object-segmentation-example}</span>
<span id="cb2-154"><a href="#cb2-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-155"><a href="#cb2-155" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}mask_rcnn_object_segmentation_example.png)</span>{width=600}</span>
<span id="cb2-156"><a href="#cb2-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-157"><a href="#cb2-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-158"><a href="#cb2-158" aria-hidden="true" tabindex="-1"></a>Object Segmentation example (from @he_mask_2018).</span>
<span id="cb2-159"><a href="#cb2-159" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-160"><a href="#cb2-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-161"><a href="#cb2-161" aria-hidden="true" tabindex="-1"></a>The following video shows an example of semantic segmentation:</span>
<span id="cb2-162"><a href="#cb2-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-163"><a href="#cb2-163" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">iframe</span><span class="ot"> width</span><span class="op">=</span><span class="st">"560"</span><span class="ot"> height</span><span class="op">=</span><span class="st">"315"</span><span class="ot"> src</span><span class="op">=</span><span class="st">"https://www.youtube-nocookie.com/embed/wfObVKKKJkE"</span><span class="ot"> title</span><span class="op">=</span><span class="st">"YouTube video player"</span><span class="ot"> frameborder</span><span class="op">=</span><span class="st">"0"</span><span class="ot"> allow</span><span class="op">=</span><span class="st">"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"</span><span class="ot"> allowfullscreen</span><span class="dt">&gt;&lt;/</span><span class="kw">iframe</span><span class="dt">&gt;</span></span>
<span id="cb2-164"><a href="#cb2-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-165"><a href="#cb2-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-166"><a href="#cb2-166" aria-hidden="true" tabindex="-1"></a><span class="fu">### Keypoint Detection</span></span>
<span id="cb2-167"><a href="#cb2-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-168"><a href="#cb2-168" aria-hidden="true" tabindex="-1"></a>In keypoint detection, keypoints of people are localized. People must be detected and their keypoints (joints) localized. @fig-mask-rcnn-keypoint-detection-example shows an example of keypoint detection from the paper by @he_mask_2018, where</span>
<span id="cb2-169"><a href="#cb2-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-170"><a href="#cb2-170" aria-hidden="true" tabindex="-1"></a> individual people are detected and their joints localized. This can be used to transfer movements from a person to an avatar or to recognize activities of people (action recognition).</span>
<span id="cb2-171"><a href="#cb2-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-172"><a href="#cb2-172" aria-hidden="true" tabindex="-1"></a>::: {#fig-mask-rcnn-keypoint-detection-example}</span>
<span id="cb2-173"><a href="#cb2-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-174"><a href="#cb2-174" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}mask_rcnn_keypoint_detection_example.png)</span>{width=600}</span>
<span id="cb2-175"><a href="#cb2-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-176"><a href="#cb2-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-177"><a href="#cb2-177" aria-hidden="true" tabindex="-1"></a>Keypoint Detection example (from @he_mask_2018).</span>
<span id="cb2-178"><a href="#cb2-178" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-179"><a href="#cb2-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-180"><a href="#cb2-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-181"><a href="#cb2-181" aria-hidden="true" tabindex="-1"></a><span class="fu">### Image Generation</span></span>
<span id="cb2-182"><a href="#cb2-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-183"><a href="#cb2-183" aria-hidden="true" tabindex="-1"></a>There are various applications where models transform input images into specific output images (image-to-image or image translation) or generate completely new images (image generation). Below some example applications.</span>
<span id="cb2-184"><a href="#cb2-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-185"><a href="#cb2-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-186"><a href="#cb2-186" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Image Manipulation</span></span>
<span id="cb2-187"><a href="#cb2-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-188"><a href="#cb2-188" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-image-manipulation}</span>
<span id="cb2-189"><a href="#cb2-189" aria-hidden="true" tabindex="-1"></a>{{&lt; video https://vcai.mpi-inf.mpg.de/projects/DragGAN/data/DragGAN.mp4 width=800 &gt;}}</span>
<span id="cb2-190"><a href="#cb2-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-191"><a href="#cb2-191" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Source: Link</span><span class="co">](https://vcai.mpi-inf.mpg.de/projects/DragGAN/)</span>, DragGAN by @pan_drag_2023</span>
<span id="cb2-192"><a href="#cb2-192" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-193"><a href="#cb2-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-194"><a href="#cb2-194" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Image Translation</span></span>
<span id="cb2-195"><a href="#cb2-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-196"><a href="#cb2-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-197"><a href="#cb2-197" aria-hidden="true" tabindex="-1"></a>@fig-intro-pix2pix-example shows an example of image generation from the paper by @image_to_image_isola2018, where images are generated conditioned on image inputs (translated). This can be used, for example, to convert a sketch of an object into a photorealistic depiction of it.</span>
<span id="cb2-198"><a href="#cb2-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-199"><a href="#cb2-199" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-pix2pix-example width=600}</span>
<span id="cb2-200"><a href="#cb2-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-201"><a href="#cb2-201" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}pix2pix_examples.png)</span></span>
<span id="cb2-202"><a href="#cb2-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-203"><a href="#cb2-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-204"><a href="#cb2-204" aria-hidden="true" tabindex="-1"></a>Image Generation example (from @image_to_image_isola2018).</span>
<span id="cb2-205"><a href="#cb2-205" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-206"><a href="#cb2-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-207"><a href="#cb2-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-208"><a href="#cb2-208" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Image Super Resolution</span></span>
<span id="cb2-209"><a href="#cb2-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-210"><a href="#cb2-210" aria-hidden="true" tabindex="-1"></a>In the gaming industry, Deep Learning is used to generate high-resolution images, scaling low-resolution images efficiently (image super resolution), as shown in @fig-intro-dssl. This allows for higher frame rates.</span>
<span id="cb2-211"><a href="#cb2-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-212"><a href="#cb2-212" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-dssl  width=600}</span>
<span id="cb2-213"><a href="#cb2-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-214"><a href="#cb2-214" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}dssl.png)</span></span>
<span id="cb2-215"><a href="#cb2-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-216"><a href="#cb2-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-217"><a href="#cb2-217" aria-hidden="true" tabindex="-1"></a>Nvidia DLSS: <span class="co">[</span><span class="ot">Link</span><span class="co">](https://images.nvidia.com/aem-dam/Solutions/geforce/news/control-nvidia-dlss-2-0-update/deliver-us-the-moon-nvidia-dlss-2-0-performance-boost.png)</span></span>
<span id="cb2-218"><a href="#cb2-218" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-219"><a href="#cb2-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-220"><a href="#cb2-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-221"><a href="#cb2-221" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Image Colorization</span></span>
<span id="cb2-222"><a href="#cb2-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-223"><a href="#cb2-223" aria-hidden="true" tabindex="-1"></a>Image colorization can be learned with Deep Learning. This involves transforming a black-and-white image into an RGB image. @fig-intro-colorization-example shows an example.</span>
<span id="cb2-224"><a href="#cb2-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-225"><a href="#cb2-225" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-colorization-example width=600}</span>
<span id="cb2-226"><a href="#cb2-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-227"><a href="#cb2-227" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}colorization_example.png)</span></span>
<span id="cb2-228"><a href="#cb2-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-229"><a href="#cb2-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-230"><a href="#cb2-230" aria-hidden="true" tabindex="-1"></a>Norwegian Bride (est late 1890s) from DeOldify: <span class="co">[</span><span class="ot">Link</span><span class="co">](https://github.com/jantic/DeOldify)</span></span>
<span id="cb2-231"><a href="#cb2-231" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-232"><a href="#cb2-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-233"><a href="#cb2-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-234"><a href="#cb2-234" aria-hidden="true" tabindex="-1"></a><span class="fu">#### View Synthesis</span></span>
<span id="cb2-235"><a href="#cb2-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-236"><a href="#cb2-236" aria-hidden="true" tabindex="-1"></a>In view synthesis, views of certain scenes are generated from models. Neural Radiance Fields (NeRFs) are simple models that can generate new views from known viewpoints and their images. @fig-intro-nerf shows the data on which such a model is trained and what can be generated with it.</span>
<span id="cb2-237"><a href="#cb2-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-238"><a href="#cb2-238" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-nerf width=600}</span>
<span id="cb2-239"><a href="#cb2-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-240"><a href="#cb2-240" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}nerf.png)</span></span>
<span id="cb2-241"><a href="#cb2-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-242"><a href="#cb2-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-243"><a href="#cb2-243" aria-hidden="true" tabindex="-1"></a>Neural Radiance Fields - example (from @mildenhall_nerf_2020).</span>
<span id="cb2-244"><a href="#cb2-244" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-245"><a href="#cb2-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-246"><a href="#cb2-246" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Unconditional Generation</span></span>
<span id="cb2-247"><a href="#cb2-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-248"><a href="#cb2-248" aria-hidden="true" tabindex="-1"></a>In unconditional image generation, data (images) are generated that resemble those in the training data. Here, you have no direct control over the model's output. However, you can often make changes to generated images or interpolate between data points. @fig-intro-stylegan3 shows generated images from a model trained on portrait images of people.</span>
<span id="cb2-249"><a href="#cb2-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-250"><a href="#cb2-250" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-stylegan3 width=600}</span>
<span id="cb2-251"><a href="#cb2-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-252"><a href="#cb2-252" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}stylegan3_examples.png)</span></span>
<span id="cb2-253"><a href="#cb2-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-254"><a href="#cb2-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-255"><a href="#cb2-255" aria-hidden="true" tabindex="-1"></a>StyleGan3 (from @karras_alias-free_2021).</span>
<span id="cb2-256"><a href="#cb2-256" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-257"><a href="#cb2-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-258"><a href="#cb2-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-259"><a href="#cb2-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-260"><a href="#cb2-260" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Text to Image</span></span>
<span id="cb2-261"><a href="#cb2-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-262"><a href="#cb2-262" aria-hidden="true" tabindex="-1"></a>@fig-stable-diffusion-example shows an example of image generation from the paper by @rombach_high-resolution_2022, where images are generated conditioned on text inputs.</span>
<span id="cb2-263"><a href="#cb2-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-264"><a href="#cb2-264" aria-hidden="true" tabindex="-1"></a>::: {#fig-stable-diffusion-example width=600}</span>
<span id="cb2-265"><a href="#cb2-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-266"><a href="#cb2-266" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}stable_diffusion_example.png)</span>{width=600}</span>
<span id="cb2-267"><a href="#cb2-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-268"><a href="#cb2-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-269"><a href="#cb2-269" aria-hidden="true" tabindex="-1"></a>Image Generation example (from @rombach_high-resolution_2022.)</span>
<span id="cb2-270"><a href="#cb2-270" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-271"><a href="#cb2-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-272"><a href="#cb2-272" aria-hidden="true" tabindex="-1"></a>On <span class="co">[</span><span class="ot">civitai</span><span class="co">](https://civitai.com/)</span>, there are numerous examples and models to admire or download.</span>
<span id="cb2-273"><a href="#cb2-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-274"><a href="#cb2-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-275"><a href="#cb2-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-276"><a href="#cb2-276" aria-hidden="true" tabindex="-1"></a><span class="fu">### Further tasks</span></span>
<span id="cb2-277"><a href="#cb2-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-278"><a href="#cb2-278" aria-hidden="true" tabindex="-1"></a>There are many more computer vision tasks. The following list is not exhaustive:</span>
<span id="cb2-279"><a href="#cb2-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-280"><a href="#cb2-280" aria-hidden="true" tabindex="-1"></a>:::: {.columns}</span>
<span id="cb2-281"><a href="#cb2-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-282"><a href="#cb2-282" aria-hidden="true" tabindex="-1"></a>::: {.column width="50%"}</span>
<span id="cb2-283"><a href="#cb2-283" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Image Classification</span>
<span id="cb2-284"><a href="#cb2-284" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Object Detection (and Tracking)</span>
<span id="cb2-285"><a href="#cb2-285" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Image Segmentation</span>
<span id="cb2-286"><a href="#cb2-286" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Semantic Segmentation</span>
<span id="cb2-287"><a href="#cb2-287" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Instance Segmentation</span>
<span id="cb2-288"><a href="#cb2-288" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Optical Character Recognition (OCR)</span>
<span id="cb2-289"><a href="#cb2-289" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Pose Estimation</span>
<span id="cb2-290"><a href="#cb2-290" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Facial Recognition</span>
<span id="cb2-291"><a href="#cb2-291" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Action Recognition</span>
<span id="cb2-292"><a href="#cb2-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-293"><a href="#cb2-293" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-294"><a href="#cb2-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-295"><a href="#cb2-295" aria-hidden="true" tabindex="-1"></a>::: {.column width="50%"}</span>
<span id="cb2-296"><a href="#cb2-296" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Image Generation</span>
<span id="cb2-297"><a href="#cb2-297" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Style Transfer</span>
<span id="cb2-298"><a href="#cb2-298" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Image Inpainting</span>
<span id="cb2-299"><a href="#cb2-299" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Super-Resolution</span>
<span id="cb2-300"><a href="#cb2-300" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Text-to-Image (and more)</span>
<span id="cb2-301"><a href="#cb2-301" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Image Captioning</span>
<span id="cb2-302"><a href="#cb2-302" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>3D Reconstruction</span>
<span id="cb2-303"><a href="#cb2-303" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Image Retrieval</span>
<span id="cb2-304"><a href="#cb2-304" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-305"><a href="#cb2-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-306"><a href="#cb2-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-307"><a href="#cb2-307" aria-hidden="true" tabindex="-1"></a>::::</span>
<span id="cb2-308"><a href="#cb2-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-309"><a href="#cb2-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-310"><a href="#cb2-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-311"><a href="#cb2-311" aria-hidden="true" tabindex="-1"></a><span class="fu">## Challenges</span></span>
<span id="cb2-312"><a href="#cb2-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-313"><a href="#cb2-313" aria-hidden="true" tabindex="-1"></a>We will now explore some challenges that must be overcome when analyzing images with machine learning models.</span>
<span id="cb2-314"><a href="#cb2-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-315"><a href="#cb2-315" aria-hidden="true" tabindex="-1"></a><span class="fu">### Semantic Gap</span></span>
<span id="cb2-316"><a href="#cb2-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-317"><a href="#cb2-317" aria-hidden="true" tabindex="-1"></a>The semantic gap refers to the discrepancy between low-level information that can be extracted from an image and the interpretation of an image by a viewer. Simply put: an image often consists of millions of pixels whose information must be condensed to ultimately derive semantically meaningful information. This is an extremely complex task.</span>
<span id="cb2-318"><a href="#cb2-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-319"><a href="#cb2-319" aria-hidden="true" tabindex="-1"></a>::: {#fig-cat-number-grid}</span>
<span id="cb2-320"><a href="#cb2-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-321"><a href="#cb2-321" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}semantic_gap.jpg)</span>{width=600}</span>
<span id="cb2-322"><a href="#cb2-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-323"><a href="#cb2-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-324"><a href="#cb2-324" aria-hidden="true" tabindex="-1"></a>Illustration of the semantic gap.</span>
<span id="cb2-325"><a href="#cb2-325" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-326"><a href="#cb2-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-327"><a href="#cb2-327" aria-hidden="true" tabindex="-1"></a><span class="fu">### Viewpoint</span></span>
<span id="cb2-328"><a href="#cb2-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-329"><a href="#cb2-329" aria-hidden="true" tabindex="-1"></a>The meaning of the image does not change with the viewpoint, but the pixels do.</span>
<span id="cb2-330"><a href="#cb2-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-331"><a href="#cb2-331" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-challenge-viewpoint}</span>
<span id="cb2-332"><a href="#cb2-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-333"><a href="#cb2-333" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}viewpoint.png)</span>{width=600}</span>
<span id="cb2-334"><a href="#cb2-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-335"><a href="#cb2-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-336"><a href="#cb2-336" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Source</span><span class="co">](http://cs231n.stanford.edu/)</span></span>
<span id="cb2-337"><a href="#cb2-337" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-338"><a href="#cb2-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-339"><a href="#cb2-339" aria-hidden="true" tabindex="-1"></a><span class="fu">### Deformation</span></span>
<span id="cb2-340"><a href="#cb2-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-341"><a href="#cb2-341" aria-hidden="true" tabindex="-1"></a>Objects are often flexible and appear in different shapes and poses.</span>
<span id="cb2-342"><a href="#cb2-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-343"><a href="#cb2-343" aria-hidden="true" tabindex="-1"></a>::: {#fig-challenge-deformation}</span>
<span id="cb2-344"><a href="#cb2-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-345"><a href="#cb2-345" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}challenge_deformation.png)</span>{width=600}</span>
<span id="cb2-346"><a href="#cb2-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-347"><a href="#cb2-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-348"><a href="#cb2-348" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Source</span><span class="co">](http://cs231n.stanford.edu/)</span></span>
<span id="cb2-349"><a href="#cb2-349" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-350"><a href="#cb2-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-351"><a href="#cb2-351" aria-hidden="true" tabindex="-1"></a><span class="fu">### Illumination</span></span>
<span id="cb2-352"><a href="#cb2-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-353"><a href="#cb2-353" aria-hidden="true" tabindex="-1"></a>Changes in illumination affect pixel values and the visibility of objects.</span>
<span id="cb2-354"><a href="#cb2-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-355"><a href="#cb2-355" aria-hidden="true" tabindex="-1"></a>::: {#fig-challenge-illumination}</span>
<span id="cb2-356"><a href="#cb2-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-357"><a href="#cb2-357" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}challenge_illumination.png)</span>{width=600}</span>
<span id="cb2-358"><a href="#cb2-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-359"><a href="#cb2-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-360"><a href="#cb2-360" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Source</span><span class="co">](http://cs231n.stanford.edu/)</span></span>
<span id="cb2-361"><a href="#cb2-361" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-362"><a href="#cb2-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-363"><a href="#cb2-363" aria-hidden="true" tabindex="-1"></a><span class="fu">### Background</span></span>
<span id="cb2-364"><a href="#cb2-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-365"><a href="#cb2-365" aria-hidden="true" tabindex="-1"></a>Background pixels can resemble objects and make their exact delineation or visibility more difficult.</span>
<span id="cb2-366"><a href="#cb2-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-367"><a href="#cb2-367" aria-hidden="true" tabindex="-1"></a>::: {#fig-challenge-background}</span>
<span id="cb2-368"><a href="#cb2-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-369"><a href="#cb2-369" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}challenge_background.png)</span>{width=600}</span>
<span id="cb2-370"><a href="#cb2-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-371"><a href="#cb2-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-372"><a href="#cb2-372" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Source</span><span class="co">](http://cs231n.stanford.edu/)</span></span>
<span id="cb2-373"><a href="#cb2-373" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-374"><a href="#cb2-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-375"><a href="#cb2-375" aria-hidden="true" tabindex="-1"></a><span class="fu">### Occlusion</span></span>
<span id="cb2-376"><a href="#cb2-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-377"><a href="#cb2-377" aria-hidden="true" tabindex="-1"></a>Objects are not always fully visible, which can make their detection more difficult.</span>
<span id="cb2-378"><a href="#cb2-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-379"><a href="#cb2-379" aria-hidden="true" tabindex="-1"></a>::: {#fig-challenge-occlusion}</span>
<span id="cb2-380"><a href="#cb2-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-381"><a href="#cb2-381" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}challenge_occlusion.png)</span>{width=600}</span>
<span id="cb2-382"><a href="#cb2-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-383"><a href="#cb2-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-384"><a href="#cb2-384" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Source</span><span class="co">](http://cs231n.stanford.edu/)</span></span>
<span id="cb2-385"><a href="#cb2-385" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-386"><a href="#cb2-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-387"><a href="#cb2-387" aria-hidden="true" tabindex="-1"></a><span class="fu">### Intra-Class Variation</span></span>
<span id="cb2-388"><a href="#cb2-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-389"><a href="#cb2-389" aria-hidden="true" tabindex="-1"></a>Objects of the same class can exhibit large intra-class variability.</span>
<span id="cb2-390"><a href="#cb2-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-391"><a href="#cb2-391" aria-hidden="true" tabindex="-1"></a>::: {#fig-challenge-intra-class}</span>
<span id="cb2-392"><a href="#cb2-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-393"><a href="#cb2-393" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}challenge_intra_class_variation.jpg)</span>{width=600}</span>
<span id="cb2-394"><a href="#cb2-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-395"><a href="#cb2-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-396"><a href="#cb2-396" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Source</span><span class="co">](https://www.maxpixel.net/Cat-Kittens-Free-Float-Kitten-Rush-Cat-Puppy-555822)</span></span>
<span id="cb2-397"><a href="#cb2-397" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-398"><a href="#cb2-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-399"><a href="#cb2-399" aria-hidden="true" tabindex="-1"></a><span class="fu">### Context Dependence</span></span>
<span id="cb2-400"><a href="#cb2-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-401"><a href="#cb2-401" aria-hidden="true" tabindex="-1"></a>@fig-tiger-context shows that context information can be important to correctly classify an object.</span>
<span id="cb2-402"><a href="#cb2-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-403"><a href="#cb2-403" aria-hidden="true" tabindex="-1"></a>::: {#fig-tiger-context}</span>
<span id="cb2-404"><a href="#cb2-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-405"><a href="#cb2-405" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}tiger_context.jpg)</span>{width=600}</span>
<span id="cb2-406"><a href="#cb2-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-407"><a href="#cb2-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-408"><a href="#cb2-408" aria-hidden="true" tabindex="-1"></a>Context <span class="co">[</span><span class="ot">Source</span><span class="co">](https://www.linkedin.com/posts/ralph-aboujaoude-diaz-40838313_technology-artificialintelligence-computervision-activity-6912446088364875776-h-Iq?utm_source=linkedin_share&amp;utm_medium=member_desktop_web)</span></span>
<span id="cb2-409"><a href="#cb2-409" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-410"><a href="#cb2-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-411"><a href="#cb2-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-412"><a href="#cb2-412" aria-hidden="true" tabindex="-1"></a><span class="fu">## Software</span></span>
<span id="cb2-413"><a href="#cb2-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-414"><a href="#cb2-414" aria-hidden="true" tabindex="-1"></a><span class="fu">### Pipelines</span></span>
<span id="cb2-415"><a href="#cb2-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-416"><a href="#cb2-416" aria-hidden="true" tabindex="-1"></a>Various models can also be combined into pipelines. One example is <span class="co">[</span><span class="ot">Grounded-Segment-Anything</span><span class="co">](https://github.com/IDEA-Research/Grounded-Segment-Anything)</span>, shown in @fig-intro-grounded-segment-anything. Object detection models that process text queries are used to detect objects. These detections are used by a segmentation model to segment the target object. This segmentation is then used in a text-to-image model to make the desired change at the correct location.</span>
<span id="cb2-417"><a href="#cb2-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-418"><a href="#cb2-418" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-grounded-segment-anything}</span>
<span id="cb2-419"><a href="#cb2-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-420"><a href="#cb2-420" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}grounded_segment_anything.png)</span>{width=600}</span>
<span id="cb2-421"><a href="#cb2-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-422"><a href="#cb2-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-423"><a href="#cb2-423" aria-hidden="true" tabindex="-1"></a>Example from Grounded-Segment-Anything <span class="co">[</span><span class="ot">Link</span><span class="co">](https://github.com/IDEA-Research/Grounded-Segment-Anything)</span></span>
<span id="cb2-424"><a href="#cb2-424" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-425"><a href="#cb2-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-426"><a href="#cb2-426" aria-hidden="true" tabindex="-1"></a><span class="fu">### Image Analysis Libraries</span></span>
<span id="cb2-427"><a href="#cb2-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-428"><a href="#cb2-428" aria-hidden="true" tabindex="-1"></a>There are numerous open-source libraries that provide pre-trained models to handle the tasks mentioned above. Therefore, you do not always have to train a model yourself. @fig-detectron-example shows the capabilities of Detectron 2, an object detection library from Facebook, which can also be used for other tasks like segmentation.</span>
<span id="cb2-429"><a href="#cb2-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-430"><a href="#cb2-430" aria-hidden="true" tabindex="-1"></a>::: {#fig-detectron-example}</span>
<span id="cb2-431"><a href="#cb2-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-432"><a href="#cb2-432" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}detectron2_animation.png)</span>{width=600}</span>
<span id="cb2-433"><a href="#cb2-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-434"><a href="#cb2-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-435"><a href="#cb2-435" aria-hidden="true" tabindex="-1"></a>Example from Facebook's Detectron Library <span class="co">[</span><span class="ot">Link</span><span class="co">](https://github.com/facebookresearch/detectron2)</span></span>
<span id="cb2-436"><a href="#cb2-436" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-437"><a href="#cb2-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-438"><a href="#cb2-438" aria-hidden="true" tabindex="-1"></a>Hugging Face is also well-known. It offers numerous models and datasets for various computer vision questions like object detection, segmentation, and classification.</span>
<span id="cb2-439"><a href="#cb2-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-440"><a href="#cb2-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-441"><a href="#cb2-441" aria-hidden="true" tabindex="-1"></a><span class="fu">## Machine Learning</span></span>
<span id="cb2-442"><a href="#cb2-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-443"><a href="#cb2-443" aria-hidden="true" tabindex="-1"></a>We follow a data-driven approach in machine learning to solve various tasks. Typically, the process involves:</span>
<span id="cb2-444"><a href="#cb2-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-445"><a href="#cb2-445" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Collecting a dataset of images and their labels.</span>
<span id="cb2-446"><a href="#cb2-446" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Using a machine learning algorithm to train a model that learns to associate images with labels.</span>
<span id="cb2-447"><a href="#cb2-447" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Evaluating/applying the model on new data.</span>
<span id="cb2-448"><a href="#cb2-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-449"><a href="#cb2-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-452"><a href="#cb2-452" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb2-453"><a href="#cb2-453" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb2-454"><a href="#cb2-454" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb2-455"><a href="#cb2-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-456"><a href="#cb2-456" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(images, labels):</span>
<span id="cb2-457"><a href="#cb2-457" aria-hidden="true" tabindex="-1"></a> <span class="co">""" Train a Model """</span></span>
<span id="cb2-458"><a href="#cb2-458" aria-hidden="true" tabindex="-1"></a> <span class="co"># Fit Model here</span></span>
<span id="cb2-459"><a href="#cb2-459" aria-hidden="true" tabindex="-1"></a> <span class="cf">return</span> model</span>
<span id="cb2-460"><a href="#cb2-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-461"><a href="#cb2-461" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict(test_images, model):</span>
<span id="cb2-462"><a href="#cb2-462" aria-hidden="true" tabindex="-1"></a> <span class="co">""" Predict """</span></span>
<span id="cb2-463"><a href="#cb2-463" aria-hidden="true" tabindex="-1"></a> predictions <span class="op">=</span> model(test_images)</span>
<span id="cb2-464"><a href="#cb2-464" aria-hidden="true" tabindex="-1"></a> <span class="cf">return</span> predictions</span>
<span id="cb2-465"><a href="#cb2-465" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-466"><a href="#cb2-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-467"><a href="#cb2-467" aria-hidden="true" tabindex="-1"></a>::: {.callout-note appearance="simple"}</span>
<span id="cb2-468"><a href="#cb2-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-469"><a href="#cb2-469" aria-hidden="true" tabindex="-1"></a>**Question**</span>
<span id="cb2-470"><a href="#cb2-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-471"><a href="#cb2-471" aria-hidden="true" tabindex="-1"></a>How would you train a model for super resolution? The task of the model would be to scale low-resolution images to high-resolution images with the best possible quality.</span>
<span id="cb2-472"><a href="#cb2-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-473"><a href="#cb2-473" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-474"><a href="#cb2-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-475"><a href="#cb2-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-476"><a href="#cb2-476" aria-hidden="true" tabindex="-1"></a><span class="fu">### Machine Learning Process</span></span>
<span id="cb2-477"><a href="#cb2-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-478"><a href="#cb2-478" aria-hidden="true" tabindex="-1"></a>When modeling data, one often follows certain process steps: acquiring data, preparing it, training multiple models, selecting the most suitable model, estimating its future performance, and finally deploying it in production. @fig-intro-ml-pipeline illustrates this process graphically.</span>
<span id="cb2-479"><a href="#cb2-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-480"><a href="#cb2-480" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-ml-pipeline}</span>
<span id="cb2-481"><a href="#cb2-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-482"><a href="#cb2-482" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}python_ml.png)</span>{width=600}</span>
<span id="cb2-483"><a href="#cb2-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-484"><a href="#cb2-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-485"><a href="#cb2-485" aria-hidden="true" tabindex="-1"></a>Machine Learning Pipeline (Source: @raschka_python_2020)</span>
<span id="cb2-486"><a href="#cb2-486" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-487"><a href="#cb2-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-488"><a href="#cb2-488" aria-hidden="true" tabindex="-1"></a>At the core of a machine learning application is typically a mathematical model, which is fitted to a dataset so that it can then be used for prediction (in supervised learning). We often refer to 'models', meaning the mathematical description of the dataset.</span>
<span id="cb2-489"><a href="#cb2-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-490"><a href="#cb2-490" aria-hidden="true" tabindex="-1"></a><span class="fu">### Models</span></span>
<span id="cb2-491"><a href="#cb2-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-492"><a href="#cb2-492" aria-hidden="true" tabindex="-1"></a>A model is typically described as a function of a data point, generating an output $\hat{y}$:</span>
<span id="cb2-493"><a href="#cb2-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-494"><a href="#cb2-494" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb2-495"><a href="#cb2-495" aria-hidden="true" tabindex="-1"></a>f(\mathbf{x}^{(i)}) = \hat{y}^{(i)}</span>
<span id="cb2-496"><a href="#cb2-496" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb2-497"><a href="#cb2-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-498"><a href="#cb2-498" aria-hidden="true" tabindex="-1"></a>Most models have parameters or coefficients that describe the model. The entirety of all parameters is denoted by $\theta$.</span>
<span id="cb2-499"><a href="#cb2-499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-500"><a href="#cb2-500" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb2-501"><a href="#cb2-501" aria-hidden="true" tabindex="-1"></a>f_{\theta}(\mathbf{x}^{(i)}) \text{ or } f(\theta, \mathbf{x}^{(i)})</span>
<span id="cb2-502"><a href="#cb2-502" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb2-503"><a href="#cb2-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-504"><a href="#cb2-504" aria-hidden="true" tabindex="-1"></a>For simplicity, we often omit $\theta$: $f(\mathbf{x}^{(i)})$</span>
<span id="cb2-505"><a href="#cb2-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-506"><a href="#cb2-506" aria-hidden="true" tabindex="-1"></a><span class="fu">### Optimization</span></span>
<span id="cb2-507"><a href="#cb2-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-508"><a href="#cb2-508" aria-hidden="true" tabindex="-1"></a>The coefficients are fitted to a training dataset through an optimization procedure.</span>
<span id="cb2-509"><a href="#cb2-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-510"><a href="#cb2-510" aria-hidden="true" tabindex="-1"></a>The optimization procedure can often be influenced by additional factors, called hyperparameters ($\alpha, \lambda, \dots$). These cannot be directly optimized.</span>
<span id="cb2-511"><a href="#cb2-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-512"><a href="#cb2-512" aria-hidden="true" tabindex="-1"></a>The function/quantity to be optimized is usually called the cost function, i.e., cost function (other terms include objective function, loss function, etc.). We use $J(\cdot)$ to denote the cost function. Often, the cost function is also referred to as the loss function $L(\cdot)$. We use $l(\cdot)$ for the per-sample loss, i.e., the computation of the cost function on a single sample.</span>
<span id="cb2-513"><a href="#cb2-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-514"><a href="#cb2-514" aria-hidden="true" tabindex="-1"></a>Our goal is to find a model (and its parameters) that minimizes the cost function:</span>
<span id="cb2-515"><a href="#cb2-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-516"><a href="#cb2-516" aria-hidden="true" tabindex="-1"></a>\begin{equation*}</span>
<span id="cb2-517"><a href="#cb2-517" aria-hidden="true" tabindex="-1"></a>\mathsf{argmin}_{\theta, \lambda} J\Big(f_{\theta, \lambda}(\mathbf{X}), \mathbf{y}\Big)</span>
<span id="cb2-518"><a href="#cb2-518" aria-hidden="true" tabindex="-1"></a>\end{equation*}</span>
<span id="cb2-519"><a href="#cb2-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-520"><a href="#cb2-520" aria-hidden="true" tabindex="-1"></a>Usually, preprocessing of variables precedes the learning of</span>
<span id="cb2-521"><a href="#cb2-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-522"><a href="#cb2-522" aria-hidden="true" tabindex="-1"></a> the coefficients. Forms of preprocessing include standardizing, normalizing, feature encoding, dimensionality reduction, and more. This preprocessing also affects the optimization procedure and can be considered hyperparameters.</span>
<span id="cb2-523"><a href="#cb2-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-524"><a href="#cb2-524" aria-hidden="true" tabindex="-1"></a><span class="fu">### Model Selection</span></span>
<span id="cb2-525"><a href="#cb2-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-526"><a href="#cb2-526" aria-hidden="true" tabindex="-1"></a>Model selection is one of the most important and complex components of the machine learning process. This step involves comparing multiple models and selecting the "best" model for the task to be modeled. Which model is the "best" must be defined based on a metric that measures the model's performance.</span>
<span id="cb2-527"><a href="#cb2-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-528"><a href="#cb2-528" aria-hidden="true" tabindex="-1"></a>If we calculate the value of the metric on the training dataset, our model is usually too optimistic about its general performance. This is because the data points in the training dataset were directly used to optimize the cost function, and the model coefficients are thus optimally adjusted to them. New data points, for which predictions are to be made, could not have been used for optimization. Therefore, a dataset is usually divided into a training set and a test set. The model is trained with the training set and its performance is measured on the test set. When comparing many models, it is advisable to compare them on a separate validation set (see @fig-intro-train-test-split) and evaluate only the best model on the test set. This makes the estimate on the test set more accurate.</span>
<span id="cb2-529"><a href="#cb2-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-530"><a href="#cb2-530" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-train-test-split}</span>
<span id="cb2-531"><a href="#cb2-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-532"><a href="#cb2-532" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}train_test_split.png)</span>{width=600}</span>
<span id="cb2-533"><a href="#cb2-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-534"><a href="#cb2-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-535"><a href="#cb2-535" aria-hidden="true" tabindex="-1"></a>Train-Test Split to select and evaluate models.</span>
<span id="cb2-536"><a href="#cb2-536" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-537"><a href="#cb2-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-538"><a href="#cb2-538" aria-hidden="true" tabindex="-1"></a><span class="fu">### ML with Classical Computer Vision</span></span>
<span id="cb2-539"><a href="#cb2-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-540"><a href="#cb2-540" aria-hidden="true" tabindex="-1"></a>Images typically have very high dimensionality. For example, an RGB image with a resolution of $800 \times 600$ has a dimensionality of $800 \times 600 \times 3 = 1,440,000$. Classical machine learning algorithms often struggle with such high dimensionalities:</span>
<span id="cb2-541"><a href="#cb2-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-542"><a href="#cb2-542" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>They are very slow or require a lot of memory.</span>
<span id="cb2-543"><a href="#cb2-543" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>They cannot exploit the 2-D structure of images.</span>
<span id="cb2-544"><a href="#cb2-544" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>They are very sensitive to slight changes in images (e.g., rotations).</span>
<span id="cb2-545"><a href="#cb2-545" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>They can easily overfit, as the number of features is close to the number of observations (training set).</span>
<span id="cb2-546"><a href="#cb2-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-547"><a href="#cb2-547" aria-hidden="true" tabindex="-1"></a>When modeling images with (classical) machine learning algorithms, methods from (classical) computer vision are often used. With the help of such methods, features can be extracted from images, and the algorithms can learn on these features. This avoids modeling high-dimensional raw data. @fig-intro-color-histograms, @fig-intro-hog-features, and @fig-intro-bow-features show various feature extraction methods.</span>
<span id="cb2-548"><a href="#cb2-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-549"><a href="#cb2-549" aria-hidden="true" tabindex="-1"></a>@fig-intro-color-histograms shows that, for example, the distribution over the color spectrum can be extracted from an image. This could be an important feature.</span>
<span id="cb2-550"><a href="#cb2-550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-551"><a href="#cb2-551" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-color-histograms}</span>
<span id="cb2-552"><a href="#cb2-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-553"><a href="#cb2-553" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}color_histogram.png)</span>{width=600}</span>
<span id="cb2-554"><a href="#cb2-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-555"><a href="#cb2-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-556"><a href="#cb2-556" aria-hidden="true" tabindex="-1"></a>Color Histograms as Features (Source: @johnson_eecs_2022)</span>
<span id="cb2-557"><a href="#cb2-557" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-558"><a href="#cb2-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-559"><a href="#cb2-559" aria-hidden="true" tabindex="-1"></a>@fig-intro-hog-features shows that techniques like Histogram of Oriented Gradients (HOG) @dalal_histograms_2005 can be used to extract structures from images. Such features were successfully used for pedestrian detection @dalal_histograms_2005.</span>
<span id="cb2-560"><a href="#cb2-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-561"><a href="#cb2-561" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-hog-features}</span>
<span id="cb2-562"><a href="#cb2-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-563"><a href="#cb2-563" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}hog_features.png)</span>{width=600}</span>
<span id="cb2-564"><a href="#cb2-564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-565"><a href="#cb2-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-566"><a href="#cb2-566" aria-hidden="true" tabindex="-1"></a>HOG as Features (Source: @johnson_eecs_2022)</span>
<span id="cb2-567"><a href="#cb2-567" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-568"><a href="#cb2-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-569"><a href="#cb2-569" aria-hidden="true" tabindex="-1"></a>@fig-intro-bow-features shows another feature variant. Visual patches can be extracted from a dataset, clustered, and then used as descriptors.</span>
<span id="cb2-570"><a href="#cb2-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-571"><a href="#cb2-571" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-bow-features}</span>
<span id="cb2-572"><a href="#cb2-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-573"><a href="#cb2-573" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}bag_of_words_features.png)</span>{width=600}</span>
<span id="cb2-574"><a href="#cb2-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-575"><a href="#cb2-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-576"><a href="#cb2-576" aria-hidden="true" tabindex="-1"></a>Bag of (visual) words Features (Source: @johnson_eecs_2022)</span>
<span id="cb2-577"><a href="#cb2-577" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-578"><a href="#cb2-578" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-579"><a href="#cb2-579" aria-hidden="true" tabindex="-1"></a>Finally, all features can be combined, often more is better, as shown in @fig-intro-concat-features.</span>
<span id="cb2-580"><a href="#cb2-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-581"><a href="#cb2-581" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-concat-features}</span>
<span id="cb2-582"><a href="#cb2-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-583"><a href="#cb2-583" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}features_concat.png)</span>{width=600}</span>
<span id="cb2-584"><a href="#cb2-584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-585"><a href="#cb2-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-586"><a href="#cb2-586" aria-hidden="true" tabindex="-1"></a>Image Features (Source: @johnson_eecs_2022)</span>
<span id="cb2-587"><a href="#cb2-587" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-588"><a href="#cb2-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-589"><a href="#cb2-589" aria-hidden="true" tabindex="-1"></a>Depending on the parameterization, HOG descriptors, for example, can be very large for images. The resulting feature vector from @fig-intro-concat-features can still be reduced in dimensionality, e.g., with Principal Component Analysis.</span>
<span id="cb2-590"><a href="#cb2-590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-591"><a href="#cb2-591" aria-hidden="true" tabindex="-1"></a>@fig-intro-cifar10 shows the CIFAR10 dataset. A well-known dataset for testing models, consisting of 10 classes with 6,000 images each.</span>
<span id="cb2-592"><a href="#cb2-592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-593"><a href="#cb2-593" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-cifar10}</span>
<span id="cb2-594"><a href="#cb2-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-595"><a href="#cb2-595" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}cifar10.jpg)</span>{width=600}</span>
<span id="cb2-596"><a href="#cb2-596" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-597"><a href="#cb2-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-598"><a href="#cb2-598" aria-hidden="true" tabindex="-1"></a>CIFAR10 Dataset <span class="co">[</span><span class="ot">Source</span><span class="co">](https://www.cs.toronto.edu/~kriz/cifar.html)</span></span>
<span id="cb2-599"><a href="#cb2-599" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-600"><a href="#cb2-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-601"><a href="#cb2-601" aria-hidden="true" tabindex="-1"></a><span class="fu">## Deep Learning</span></span>
<span id="cb2-602"><a href="#cb2-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-603"><a href="#cb2-603" aria-hidden="true" tabindex="-1"></a>To accelerate research in computer vision and make progress more measurable, the PASCAL VOC Challenges were introduced @everingham_pascal_2007. These involved various tasks, such as detecting objects in photographs (@fig-intro-pascal).</span>
<span id="cb2-604"><a href="#cb2-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-605"><a href="#cb2-605" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-pascal}</span>
<span id="cb2-606"><a href="#cb2-606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-607"><a href="#cb2-607" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}pascal_voc_2007.png)</span>{width=600}</span>
<span id="cb2-608"><a href="#cb2-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-609"><a href="#cb2-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-610"><a href="#cb2-610" aria-hidden="true" tabindex="-1"></a>Images/illustrations from <span class="co">[</span><span class="ot">Link</span><span class="co">](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/)</span> and @johnson_eecs_2022. On the left, you see object annotations in images, and on the right, you see the development of Mean Average Precision over the years.</span>
<span id="cb2-611"><a href="#cb2-611" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-612"><a href="#cb2-612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-613"><a href="#cb2-613" aria-hidden="true" tabindex="-1"></a>The easy availability of images on the internet has made it possible to collect increasingly larger datasets. <span class="co">[</span><span class="ot">ImageNet</span><span class="co">](https://www.image-net.org/)</span> is such a very large, hierarchically annotated image dataset @deng_imagenet_2009 with over 1.4 million images, categorized into 1,000 object classes. @fig-intro-imagenet illustrates the dataset.</span>
<span id="cb2-614"><a href="#cb2-614" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-615"><a href="#cb2-615" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-imagenet}</span>
<span id="cb2-616"><a href="#cb2-616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-617"><a href="#cb2-617" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}imagenet.jpg)</span>{width=600}</span>
<span id="cb2-618"><a href="#cb2-618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-619"><a href="#cb2-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-620"><a href="#cb2-620" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">ImageNet</span><span class="co">](https://www.image-net.org/)</span>, <span class="co">[</span><span class="ot">Image Source</span><span class="co">](https://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_full_1k.jpg)</span>, details in @deng_imagenet_2009</span>
<span id="cb2-621"><a href="#cb2-621" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-622"><a href="#cb2-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-623"><a href="#cb2-623" aria-hidden="true" tabindex="-1"></a>Since 2010, challenges have been regularly conducted on the ImageNet dataset @russakovsky_imagenet_2015, such as image classification and object detection. @fig-intro-imagenet-perf shows the development of the error rate over time.</span>
<span id="cb2-624"><a href="#cb2-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-625"><a href="#cb2-625" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-imagenet-perf}</span>
<span id="cb2-626"><a href="#cb2-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-627"><a href="#cb2-627" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}image_net_perf.png)</span>{width=600}</span>
<span id="cb2-628"><a href="#cb2-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-629"><a href="#cb2-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-630"><a href="#cb2-630" aria-hidden="true" tabindex="-1"></a>Source: @johnson_eecs_2022</span>
<span id="cb2-631"><a href="#cb2-631" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-632"><a href="#cb2-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-633"><a href="#cb2-633" aria-hidden="true" tabindex="-1"></a>In 2011, a team won @perronnin_large-scale_2010 by combining various (classical) feature extraction methods with machine learning. They used, among other things, SIFT features to train SVMs.</span>
<span id="cb2-634"><a href="#cb2-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-635"><a href="#cb2-635" aria-hidden="true" tabindex="-1"></a>In 2012, a drastic reduction in the error rate was achieved in the ImageNet competition. This development marked the end of classical computer vision methods in many areas. Krizhevsky et al. @krizhevsky_imagenet_2012 impressively demonstrated the potential of neural networks in 2012. They implemented a convolutional neural network (CNN) with multiple layers, the so-called AlexNet architecture, as shown in @fig-intro-alexnet.</span>
<span id="cb2-636"><a href="#cb2-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-637"><a href="#cb2-637" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-alexnet}</span>
<span id="cb2-638"><a href="#cb2-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-639"><a href="#cb2-639" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}alexnet.png)</span>{width=600}</span>
<span id="cb2-640"><a href="#cb2-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-641"><a href="#cb2-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-642"><a href="#cb2-642" aria-hidden="true" tabindex="-1"></a>Alexnet @krizhevsky_imagenet_2012.</span>
<span id="cb2-643"><a href="#cb2-643" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-644"><a href="#cb2-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-645"><a href="#cb2-645" aria-hidden="true" tabindex="-1"></a>While classical computer vision trains a machine learning model on features extracted with hand-crafted algorithms @fig-intro-classical-ml, the development is increasingly moving towards end-to-end learning. In this approach, one avoids as much as possible static/hand-designed components and learns everything, including feature extraction, with machine learning @fig-intro-end-to-end.</span>
<span id="cb2-646"><a href="#cb2-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-647"><a href="#cb2-647" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-classical-ml}</span>
<span id="cb2-648"><a href="#cb2-648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-649"><a href="#cb2-649" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}classical_ml.png)</span>{width=600}</span>
<span id="cb2-650"><a href="#cb2-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-651"><a href="#cb2-651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-652"><a href="#cb2-652" aria-hidden="true" tabindex="-1"></a>Illustration from @johnson_eecs_2022</span>
<span id="cb2-653"><a href="#cb2-653" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-654"><a href="#cb2-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-655"><a href="#cb2-655" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-end-to-end}</span>
<span id="cb2-656"><a href="#cb2-656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-657"><a href="#cb2-657" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}end-to-end-ml.png)</span>{width=600}</span>
<span id="cb2-658"><a href="#cb2-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-659"><a href="#cb2-659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-660"><a href="#cb2-660" aria-hidden="true" tabindex="-1"></a>Illustration from @johnson_eecs_2022</span>
<span id="cb2-661"><a href="#cb2-661" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-662"><a href="#cb2-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-663"><a href="#cb2-663" aria-hidden="true" tabindex="-1"></a>Deep learning-based approaches have several advantages over classical machine learning methods:</span>
<span id="cb2-664"><a href="#cb2-664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-665"><a href="#cb2-665" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Automatic feature extraction: no manual feature extraction procedures are needed.</span>
<span id="cb2-666"><a href="#cb2-666" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Hierarchical features: these are particularly valuable for processing and understanding visual data.</span>
<span id="cb2-667"><a href="#cb2-667" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Generalization: with more training data, deep learning methods generalize better.</span>
<span id="cb2-668"><a href="#cb2-668" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>End-to-end learning: this approach allows many problems to be modeled similarly.</span>
<span id="cb2-669"><a href="#cb2-669" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Robustness to variability: certain models are naturally invariant to</span>
<span id="cb2-670"><a href="#cb2-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-671"><a href="#cb2-671" aria-hidden="true" tabindex="-1"></a> transformations like translations, scalings, etc.</span>
<span id="cb2-672"><a href="#cb2-672" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Adaptability and transferability: deep learning models can often be easily adapted (transfer learning) and can create good models even with little data.</span>
<span id="cb2-673"><a href="#cb2-673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-674"><a href="#cb2-674" aria-hidden="true" tabindex="-1"></a><span class="fu">### Deep Learning History</span></span>
<span id="cb2-675"><a href="#cb2-675" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-676"><a href="#cb2-676" aria-hidden="true" tabindex="-1"></a>We will now explore the most important milestones in deep learning for image analysis.</span>
<span id="cb2-677"><a href="#cb2-677" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-678"><a href="#cb2-678" aria-hidden="true" tabindex="-1"></a><span class="fu">### Hubel and Wiesel</span></span>
<span id="cb2-679"><a href="#cb2-679" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-680"><a href="#cb2-680" aria-hidden="true" tabindex="-1"></a>@hubel_receptive_1959 showed in experiments on cats that there are complex cells/neurons in the visual cortex that only respond to certain patterns. In addition to these complex cells, there are also simple cells that only respond to lines at a certain angle.</span>
<span id="cb2-681"><a href="#cb2-681" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-682"><a href="#cb2-682" aria-hidden="true" tabindex="-1"></a>::: {#fig-huber-wiesel}</span>
<span id="cb2-683"><a href="#cb2-683" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-684"><a href="#cb2-684" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}huber_wiesel_cat_experiment.jpg)</span>{width=600}</span>
<span id="cb2-685"><a href="#cb2-685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-686"><a href="#cb2-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-687"><a href="#cb2-687" aria-hidden="true" tabindex="-1"></a>Illustration <span class="co">[</span><span class="ot">Source</span><span class="co">](https://link.springer.com/chapter/10.1007/978-3-030-28954-6_4/figures/1)</span></span>
<span id="cb2-688"><a href="#cb2-688" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-689"><a href="#cb2-689" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-690"><a href="#cb2-690" aria-hidden="true" tabindex="-1"></a><span class="fu">### Neocognitron</span></span>
<span id="cb2-691"><a href="#cb2-691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-692"><a href="#cb2-692" aria-hidden="true" tabindex="-1"></a>@fukushima_neocognitron_1980 defined a model of a neural network that can recognize visual patterns. It already has the hierarchical structure of a modern convolutional neural network and is inspired by biological neural networks, particularly from the insights of @hubel_receptive_1959.</span>
<span id="cb2-693"><a href="#cb2-693" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-694"><a href="#cb2-694" aria-hidden="true" tabindex="-1"></a>::: {#fig-neocognitron}</span>
<span id="cb2-695"><a href="#cb2-695" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-696"><a href="#cb2-696" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}neocogitron.png)</span>{width=600}</span>
<span id="cb2-697"><a href="#cb2-697" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-698"><a href="#cb2-698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-699"><a href="#cb2-699" aria-hidden="true" tabindex="-1"></a>The Neocognitron @fukushima_neocognitron_1980.</span>
<span id="cb2-700"><a href="#cb2-700" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-701"><a href="#cb2-701" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-702"><a href="#cb2-702" aria-hidden="true" tabindex="-1"></a><span class="fu">### Backpropagation</span></span>
<span id="cb2-703"><a href="#cb2-703" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-704"><a href="#cb2-704" aria-hidden="true" tabindex="-1"></a>@rumelhart_learning_1986 introduced the backpropagation algorithm in the context of neural networks. This algorithm is used to train modern neural networks: it finds the parameters of an artificial neural network (ANN) to solve a specific task. Backpropagation is based on the chain rule from calculus and is also important for other machine learning models.</span>
<span id="cb2-705"><a href="#cb2-705" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-706"><a href="#cb2-706" aria-hidden="true" tabindex="-1"></a>::: {#fig-rumalhart-backprop}</span>
<span id="cb2-707"><a href="#cb2-707" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-708"><a href="#cb2-708" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}rumelhart_backprop.png)</span>{width=600}</span>
<span id="cb2-709"><a href="#cb2-709" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-710"><a href="#cb2-710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-711"><a href="#cb2-711" aria-hidden="true" tabindex="-1"></a>Backpropagation in neural networks @rumelhart_learning_1986.</span>
<span id="cb2-712"><a href="#cb2-712" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-713"><a href="#cb2-713" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-714"><a href="#cb2-714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-715"><a href="#cb2-715" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb2-716"><a href="#cb2-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-717"><a href="#cb2-717" aria-hidden="true" tabindex="-1"></a>A good video on backpropagation: <span class="co">[</span><span class="ot">3Blue1Brown Backpropagation Calculus</span><span class="co">](https://www.youtube.com/watch?v=tIeHLnjs5U8)</span></span>
<span id="cb2-718"><a href="#cb2-718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-719"><a href="#cb2-719" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-720"><a href="#cb2-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-721"><a href="#cb2-721" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-722"><a href="#cb2-722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-723"><a href="#cb2-723" aria-hidden="true" tabindex="-1"></a><span class="fu">### Convolutional Neural Networks (CNNs)</span></span>
<span id="cb2-724"><a href="#cb2-724" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-725"><a href="#cb2-725" aria-hidden="true" tabindex="-1"></a>@lecun_gradient-based_1998 implemented convolutional neural networks (CNNs) to recognize handwritten digits. It is specialized for the 2-D structure of the input data. They trained a model very similar to modern CNNs, as shown in @fig-lecun-cnn.</span>
<span id="cb2-726"><a href="#cb2-726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-727"><a href="#cb2-727" aria-hidden="true" tabindex="-1"></a>::: {#fig-lecun-cnn}</span>
<span id="cb2-728"><a href="#cb2-728" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-729"><a href="#cb2-729" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}lecun_cnn.png)</span>{width=600}</span>
<span id="cb2-730"><a href="#cb2-730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-731"><a href="#cb2-731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-732"><a href="#cb2-732" aria-hidden="true" tabindex="-1"></a>Modern CNN @lecun_gradient-based_1998.</span>
<span id="cb2-733"><a href="#cb2-733" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-734"><a href="#cb2-734" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-735"><a href="#cb2-735" aria-hidden="true" tabindex="-1"></a>CNNs became extremely popular after winning the <span class="co">[</span><span class="ot">ImageNet</span><span class="co">](https://www.image-net.org/)</span> competition. @krizhevsky_imagenet_2012 implemented a CNN with multiple layers, the so-called AlexNet architecture, as shown in @fig-alexnet.</span>
<span id="cb2-736"><a href="#cb2-736" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-737"><a href="#cb2-737" aria-hidden="true" tabindex="-1"></a>::: {#fig-alexnet}</span>
<span id="cb2-738"><a href="#cb2-738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-739"><a href="#cb2-739" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}alexnet.png)</span>{width=600}</span>
<span id="cb2-740"><a href="#cb2-740" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-741"><a href="#cb2-741" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-742"><a href="#cb2-742" aria-hidden="true" tabindex="-1"></a>Alexnet @krizhevsky_imagenet_2012.</span>
<span id="cb2-743"><a href="#cb2-743" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-744"><a href="#cb2-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-745"><a href="#cb2-745" aria-hidden="true" tabindex="-1"></a>Since the breakthrough in 2012, CNNs have been used for increasingly complex tasks and further developed. Well-known are, for example, the <span class="co">[</span><span class="ot">COCO Challenges</span><span class="co">](https://cocodataset.org/workshop/coco-lvis-eccv-2020.html)</span>, with various tasks.</span>
<span id="cb2-746"><a href="#cb2-746" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-747"><a href="#cb2-747" aria-hidden="true" tabindex="-1"></a><span class="fu">### Other Architectures in Computer Vision</span></span>
<span id="cb2-748"><a href="#cb2-748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-749"><a href="#cb2-749" aria-hidden="true" tabindex="-1"></a>CNNs are still in use today (2024). Meanwhile, there are alternative architectures, such as transformer-based models (@dosovitskiy_image_2020), which are extremely successful in language modeling, or multilayer perceptron-based architectures (@liu_pay_2021). However, it has been shown that CNNs are still competitive and sometimes superior to alternative architectures (see @woo_convnext_2023). The question of which architecture type will prevail is open. Currently, it seems that CNNs and transformer-based models perform similarily (@smith_convnets_2023).</span>
<span id="cb2-750"><a href="#cb2-750" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-751"><a href="#cb2-751" aria-hidden="true" tabindex="-1"></a><span class="fu">## References</span></span>
<span id="cb2-752"><a href="#cb2-752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-753"><a href="#cb2-753" aria-hidden="true" tabindex="-1"></a>::: {#refs}</span>
<span id="cb2-754"><a href="#cb2-754" aria-hidden="true" tabindex="-1"></a>:::</span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© Copyright 2024, Marco Willi</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2025/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This page is built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>
<script>videojs(video_shortcode_videojs_video1);</script>
<script>videojs(video_shortcode_videojs_video2);</script>
<script>videojs(video_shortcode_videojs_video3);</script>




</body></html>