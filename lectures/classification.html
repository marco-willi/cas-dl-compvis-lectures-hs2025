<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.24">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Marco Willi">

<title>5 - Image Classification – CAS Deep Learning - Computer Vision</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2" crossorigin="anonymous"></script><script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../lectures/practical.html" rel="next">
<link href="../lectures/cnns.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-a6e161b2431e1f94a14e0f5d32135a3c.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-7365f4c1a6be05280eba792c58017f0b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-SCHRGR3LNM"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-SCHRGR3LNM', { 'anonymize_ip': true});
</script>
<script src="https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js" integrity="sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script type="text/javascript">
window.PlotlyConfig = {MathJaxConfig: 'local'};
if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}
</script>
<script type="module">import "https://cdn.plot.ly/plotly-3.1.0.min"</script>


  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../lectures/intro.html">Lectures</a></li><li class="breadcrumb-item"><a href="../lectures/classification.html">5 - Image Classification</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-center sidebar-header">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../assets/logo.webp" alt="" class="sidebar-logo light-content py-0 d-lg-inline d-none">
      <img src="../assets/logo.webp" alt="" class="sidebar-logo dark-content py-0 d-lg-inline d-none">
      </a>
      <div class="sidebar-tools-main">
    <a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2025" title="GitHub organization" class="quarto-navigation-tool px-1" aria-label="GitHub organization"><i class="bi bi-github"></i></a>
    <a href="" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-code-square"></i></a>
</div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Slides</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../slides/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Einführung Computer Vision mit Deep Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../slides/cnns.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Convolutional Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../slides/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../slides/practical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Practical Considerations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../slides/recent_advances.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Recent Advances</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/quiz.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Quiz &amp; Exam Prep</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Lectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1 - Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2 - Software &amp; Hardware for Deep Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/neural_networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3 - Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/cnns.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4 - Convolutional Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/classification.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">5 - Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/practical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6 - Practical</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/recent_advances.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">7 - Recent Advances</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Resources</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/literature.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Books</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/links.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Helpful Links &amp; Resources</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Mathematical Notation</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#parametric-approach" id="toc-parametric-approach" class="nav-link" data-scroll-target="#parametric-approach">Parametric Approach</a>
  <ul class="collapse">
  <li><a href="#softmax-classifier" id="toc-softmax-classifier" class="nav-link" data-scroll-target="#softmax-classifier">Softmax Classifier</a></li>
  </ul></li>
  <li><a href="#loss-function" id="toc-loss-function" class="nav-link" data-scroll-target="#loss-function">Loss Function</a>
  <ul class="collapse">
  <li><a href="#likelihood" id="toc-likelihood" class="nav-link" data-scroll-target="#likelihood">Likelihood</a></li>
  <li><a href="#maximum-likelihood" id="toc-maximum-likelihood" class="nav-link" data-scroll-target="#maximum-likelihood">Maximum Likelihood</a></li>
  <li><a href="#negative-log-likelihood" id="toc-negative-log-likelihood" class="nav-link" data-scroll-target="#negative-log-likelihood">Negative Log-Likelihood</a></li>
  <li><a href="#cross-entropy" id="toc-cross-entropy" class="nav-link" data-scroll-target="#cross-entropy">Cross-Entropy</a></li>
  </ul></li>
  <li><a href="#architectures" id="toc-architectures" class="nav-link" data-scroll-target="#architectures">Architectures</a>
  <ul class="collapse">
  <li><a href="#alexnet" id="toc-alexnet" class="nav-link" data-scroll-target="#alexnet">AlexNet</a></li>
  <li><a href="#vgg" id="toc-vgg" class="nav-link" data-scroll-target="#vgg">VGG</a></li>
  <li><a href="#resnet" id="toc-resnet" class="nav-link" data-scroll-target="#resnet">ResNet</a></li>
  <li><a href="#convnext" id="toc-convnext" class="nav-link" data-scroll-target="#convnext">ConvNext</a></li>
  <li><a href="#imagenet-performance" id="toc-imagenet-performance" class="nav-link" data-scroll-target="#imagenet-performance">ImageNet Performance</a></li>
  <li><a href="#which-architecture" id="toc-which-architecture" class="nav-link" data-scroll-target="#which-architecture">Which Architecture?</a></li>
  </ul></li>
  <li><a href="#further-variations" id="toc-further-variations" class="nav-link" data-scroll-target="#further-variations">Further Variations</a>
  <ul class="collapse">
  <li><a href="#squeezeexcite-networks" id="toc-squeezeexcite-networks" class="nav-link" data-scroll-target="#squeezeexcite-networks">Squeeze/Excite Networks</a></li>
  <li><a href="#normalization-layers" id="toc-normalization-layers" class="nav-link" data-scroll-target="#normalization-layers">Normalization Layers</a></li>
  </ul></li>
  <li><a href="#pre-processing" id="toc-pre-processing" class="nav-link" data-scroll-target="#pre-processing">Pre-Processing</a></li>
  <li><a href="#transfer-learning" id="toc-transfer-learning" class="nav-link" data-scroll-target="#transfer-learning">Transfer Learning</a></li>
  <li><a href="#pytorch-example" id="toc-pytorch-example" class="nav-link" data-scroll-target="#pytorch-example">PyTorch Example</a>
  <ul class="collapse">
  <li><a href="#architecture" id="toc-architecture" class="nav-link" data-scroll-target="#architecture">Architecture</a></li>
  <li><a href="#loss-function-1" id="toc-loss-function-1" class="nav-link" data-scroll-target="#loss-function-1">Loss Function</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2025/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../lectures/intro.html">Lectures</a></li><li class="breadcrumb-item"><a href="../lectures/classification.html">5 - Image Classification</a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">5 - Image Classification</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Marco Willi </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Institute for Data Science I4DS, FHNW
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  


</header>


<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Image classification is a core task of Computer Vision. In image classification, an image is assigned to a predefined set of classes. In multi-class classification, there are ≥ 2 classes; in binary classification, there are 2 classes; and in multi-label classification, there are several sets of classes. <a href="#fig-classification-classification-illustration" class="quarto-xref">Figure&nbsp;1</a> illustrates the problem in multi-class classification.</p>
<div id="fig-classification-classification-illustration" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-classification-classification-illustration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/classification/classification_illustration.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classification-classification-illustration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Image Classification example.
</figcaption>
</figure>
</div>
<p><a href="#fig-image-classifier-example" class="quarto-xref">Figure&nbsp;2</a> shows an example from the paper by <span class="citation" data-cites="krizhevsky_imagenet_2012">Krizhevsky, Sutskever, and Hinton (<a href="#ref-krizhevsky_imagenet_2012" role="doc-biblioref">2012</a>)</span>, which achieved the best results in the 2012 ImageNet competition, demonstrating how well CNNs work.</p>
<div id="fig-image-classifier-example" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-image-classifier-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/classification/image_classification_example.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-image-classifier-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Image Classification example (from <span class="citation" data-cites="krizhevsky_imagenet_2012">Krizhevsky, Sutskever, and Hinton (<a href="#ref-krizhevsky_imagenet_2012" role="doc-biblioref">2012</a>)</span>).
</figcaption>
</figure>
</div>
<p><a href="#fig-classification-camera-traps" class="quarto-xref">Figure&nbsp;3</a> illustrates the challenge with images taken by camera traps, which need to be classified along animal species.</p>
<div id="fig-classification-camera-traps" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-classification-camera-traps-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/classification/camera_traps.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classification-camera-traps-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Example images from camera traps.
</figcaption>
</figure>
</div>
</section>
<section id="parametric-approach" class="level2">
<h2 class="anchored" data-anchor-id="parametric-approach">Parametric Approach</h2>
<p>In a parametric approach, we seek a model of the following form:</p>
<p><span class="math display">\[
\hat{y}^{(i)} = f(\theta, \mathbf{x}^{(i)})
\]</span></p>
<p>We want to find model parameters <span class="math inline">\(\theta\)</span> that output a score/prediction <span class="math inline">\(\hat{y}^{(i)}\)</span> for any data points <span class="math inline">\(\mathbf{x}^{(i)}\)</span> for each class <span class="math inline">\(k \in K\)</span>. We then want to assess how good this score is with the help of a loss function.</p>
<section id="softmax-classifier" class="level3">
<h3 class="anchored" data-anchor-id="softmax-classifier">Softmax Classifier</h3>
<p>With a Softmax Classifier, we interpret model predictions/scores as probabilities of class memberships: <span class="math inline">\(P(Y=\mathbf{y}^{(i)}| X = \mathbf{x}^{(i)})\)</span>. We interpret the output as a <a href="https://en.wikipedia.org/wiki/Categorical_distribution">Categorical Distribution</a> over all possible classes.</p>
<p>To obtain a valid probability distribution, the untransformed outputs <span class="math inline">\(\mathbf{z}\)</span>, also called logits, of a model are transformed with the softmax function <span class="math inline">\(\sigma(\mathbf{z})\)</span>:</p>
<p><span class="math display">\[
P(Y = k| X = \mathbf{x}^{(i)}) = \sigma(\mathbf{z})_k = \frac{e^{z_k}}{\sum_i^K e^{z_i}}
\]</span></p>
<p><a href="#fig-classification-logits-to-probs" class="quarto-xref">Figure&nbsp;4</a> shows an example of the effect of the softmax transformation.</p>
<div id="cell-fig-classification-logits-to-probs" class="cell" data-fig-width="600" data-execution_count="1">
<div class="cell-output cell-output-display">
<div id="fig-classification-logits-to-probs" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-classification-logits-to-probs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="classification_files/figure-html/fig-classification-logits-to-probs-output-1.png" width="854" height="278" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classification-logits-to-probs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Logits (left) to probabilities with the Softmax function (right).
</figcaption>
</figure>
</div>
</div>
</div>
<!-- {{< embed /demos/logits_to_probabilities.ipynb#fig-demo-logits-to-probs >}} -->
<div id="f4b1b74e" class="cell" data-fig-width="800" data-execution_count="2">
<div class="cell-output cell-output-display">
<div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-3.1.0.min.js" integrity="sha256-Ei4740bWZhaUTQuD6q9yQlgVCMPBz6CZWhevDYPv93A=" crossorigin="anonymous"></script>                <div id="dcde16c5-3bb0-4b27-bd9f-21c49b7b335e" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById("dcde16c5-3bb0-4b27-bd9f-21c49b7b335e")) {                    Plotly.newPlot(                        "dcde16c5-3bb0-4b27-bd9f-21c49b7b335e",                        [{"name":"Logits","x":["Logit 1","Logit 2","Logit 3"],"y":[2.0,1.0,0.1],"type":"bar","xaxis":"x","yaxis":"y"},{"name":"Softmax Probabilities","x":["Class 1","Class 2","Class 3"],"y":{"dtype":"f8","bdata":"81dxjokW5T9ADlMoCwjPP+gjzzudO7k\u002f"},"type":"bar","xaxis":"x2","yaxis":"y2"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermap":[{"type":"scattermap","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"},"margin":{"b":0,"l":0,"r":0,"t":30}}},"xaxis":{"anchor":"y","domain":[0.0,0.45]},"yaxis":{"anchor":"x","domain":[0.0,1.0]},"xaxis2":{"anchor":"y2","domain":[0.55,1.0]},"yaxis2":{"anchor":"x2","domain":[0.0,1.0],"range":[0,1]},"annotations":[{"font":{"size":16},"showarrow":false,"text":"Logits","x":0.225,"xanchor":"center","xref":"paper","y":1.0,"yanchor":"bottom","yref":"paper"},{"font":{"size":16},"showarrow":false,"text":"Softmax Probabilities","x":0.775,"xanchor":"center","xref":"paper","y":1.0,"yanchor":"bottom","yref":"paper"}],"title":{"text":"Interactive Softmax Calculation"},"showlegend":false,"updatemenus":[{"buttons":[{"args":[{"y":[[2.0,1.0,0.1],[0.6590011388859679,0.24243297070471392,0.09856589040931818]]}],"label":"Set 1","method":"update"},{"args":[{"y":[[1.0,2.0,3.0],[0.09003057317038046,0.24472847105479764,0.6652409557748218]]}],"label":"Set 2","method":"update"},{"args":[{"y":[[1.0,1.0,1.0],[0.3333333333333333,0.3333333333333333,0.3333333333333333]]}],"label":"Set 3","method":"update"},{"args":[{"y":[[3.0,1.0,0.5],[0.8214090194651259,0.11116562230242114,0.06742535823245292]]}],"label":"Set 4","method":"update"},{"args":[{"y":[[0.5,0.2,0.1],[0.41474187266806956,0.3072483361521629,0.27800979117976765]]}],"label":"Set 5","method":"update"}],"direction":"down","showactive":true,"x":0.5,"xanchor":"center","y":1.15,"yanchor":"top"}]},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('dcde16c5-3bb0-4b27-bd9f-21c49b7b335e');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };            </script>        </div>
</div>
</div>
</section>
</section>
<section id="loss-function" class="level2">
<h2 class="anchored" data-anchor-id="loss-function">Loss Function</h2>
<p>Now we look at how to define the loss function <span class="math inline">\(L(\mathbf{X}, \mathbf{y}, \theta)\)</span> for a multi-class classification problem.</p>
<section id="likelihood" class="level3">
<h3 class="anchored" data-anchor-id="likelihood">Likelihood</h3>
<p>The likelihood of a data point <span class="math inline">\((\mathbf{x}^{(i)}, y^{(i)})\)</span> is the probability of observing/realizing a data point, given a model with certain parameters:</p>
<p><span class="math display">\[
P(Y=y^{(i)}| X = \mathbf{x}^{(i)}) = f(\theta, \mathbf{x}^{(i)})
\]</span></p>
<p>This means we formulate a model with a probabilistic interpretation of predictions: <span class="math inline">\(f(\theta, \mathbf{x}^{(i)}): \mathbb{R}^{n} \mapsto [0, 1]\)</span></p>
<p>For a multi-class classification, the label vector is one-hot encoded <span class="math inline">\(\mathbf{y}^{(i)} \in \{0, 1\}^K\)</span>, where the true class is coded with 1 and the rest with 0. The likelihood of a data point is defined as:</p>
<p><span class="math display">\[
\prod_{j=1}^K  P(Y = j| X = \mathbf{x}^{(i)})^{y^{(i)}_j}
\]</span></p>
<p>Since only one entry in <span class="math inline">\(\mathbf{y}^{(i)}\)</span> is 1, the likelihood is simply the prediction for the true class <span class="math inline">\(P(Y = y^{(i)}| X = \mathbf{x}^{(i)})\)</span>.</p>
<p><a href="http://www.awebb.info/probability/2017/05/18/cross-entropy-and-log-likelihood.html">More Info</a></p>
<p>Given <span class="math inline">\(\mathbf{y} = [0, 1, 0, 1, 1]\)</span> and the following <span class="math inline">\(\hat{\mathbf{y}}\)</span>, calculate the likelihood.</p>
<p>For <span class="math inline">\(\hat{\mathbf{y}} = [0.1, 0.8, 0.2, 0.7, 0.9]\)</span>:</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Click for result
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="1713a14d" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> np.array([<span class="fl">0.1</span>, <span class="fl">0.8</span>, <span class="fl">0.2</span>, <span class="fl">0.7</span>, <span class="fl">0.9</span>])</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>display_likelihood(y_true, y_pred)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Total Likelihood: 0.362880</code></pre>
</div>
</div>
</div>
</div>
</div>
<p>Does it get larger for <span class="math inline">\(\hat{\mathbf{y}} = [0.1, 0.9, 0.2, 0.7, 0.9]\)</span>?</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Click for result
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="48efeeac" class="cell" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> np.array([<span class="fl">0.1</span>, <span class="fl">0.9</span>, <span class="fl">0.2</span>, <span class="fl">0.7</span>, <span class="fl">0.9</span>])</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>display_likelihood(y_true, y_pred)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Total Likelihood: 0.408240</code></pre>
</div>
</div>
</div>
</div>
</div>
<p>What happens if we increase the dataset by copying the vector 10 times? <span class="math inline">\(\hat{\mathbf{y}} = [0.1, 0.8, 0.2, 0.7, 0.9, 0.1, 0.8, ...]\)</span>?</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Click for result
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div id="97e7acf3" class="cell" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> np.array([<span class="fl">0.1</span>, <span class="fl">0.8</span>, <span class="fl">0.2</span>, <span class="fl">0.7</span>, <span class="fl">0.9</span>])</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>y_true <span class="op">=</span> np.repeat(y_true, <span class="dv">10</span>, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> np.repeat(y_pred, <span class="dv">10</span>, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>display_likelihood(y_true, y_pred)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Total Likelihood: 0.000040</code></pre>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="maximum-likelihood" class="level3">
<h3 class="anchored" data-anchor-id="maximum-likelihood">Maximum Likelihood</h3>
<p>The likelihood <span class="math inline">\(P(\mathbf{y} | \theta, \mathbf{X})\)</span> of observing our entire dataset <span class="math inline">\((\mathbf{X}, \mathbf{y})\)</span>, given the parameters <span class="math inline">\(\theta\)</span> and assuming that the data points <span class="math inline">\((\mathbf{x}^{(i)}, y^{(i)})\)</span> are independent and identically distributed, can be calculated as:</p>
<p><span class="math display">\[\begin{equation}
\prod_{i=1}^N \prod_{j=1}^K  P(Y = j| X = \mathbf{x}^{(i)})^{y^{(i)}_j}
\end{equation}\]</span></p>
<p>Under the Maximum Likelihood approach, we seek the parameters <span class="math inline">\(\theta\)</span> that maximize the likelihood of observing the dataset.</p>
</section>
<section id="negative-log-likelihood" class="level3">
<h3 class="anchored" data-anchor-id="negative-log-likelihood">Negative Log-Likelihood</h3>
<p>With Maximum Likelihood, we aim to choose the parameters <span class="math inline">\(\theta\)</span> such that <span class="math inline">\(p(\mathbf{y} | \theta, \mathbf{X})\)</span> is maximized. However, this function can be complex to handle, and we can use some mathematical tricks. We take the logarithm of the likelihood so that the product of probabilities becomes a sum. Since the logarithm is a monotonically increasing function, we can maximize its logarithm instead of the function <span class="math inline">\(p(\mathbf{y} | \theta, \mathbf{X})\)</span>. Finally, we take the negative of the function, allowing us to minimize it.</p>
<p><span class="math display">\[\begin{equation}
L(\mathbf{X}, \mathbf{y}, \theta) = - \log \prod_{i=1}^N \prod_{j=1}^K P(Y = j| X = \mathbf{x}^{(i)})^{y^{(i)}_j} \\
L(\mathbf{X}, \mathbf{y}, \theta) = -\sum_{i=1}^N \sum_{j=1}^K y^{(i)}_j \log  P(Y = j| X = \mathbf{x}^{(i)})
\end{equation}\]</span></p>
</section>
<section id="cross-entropy" class="level3">
<h3 class="anchored" data-anchor-id="cross-entropy">Cross-Entropy</h3>
<p>The loss function, derived with Maximum Likelihood, can also be viewed through the lens of cross-entropy between two discrete probability functions. Specifically, we can calculate and minimize the cross-entropy between the true distribution <span class="math inline">\(\mathbf{y}^{(i)}\)</span> and the predicted <span class="math inline">\(\mathbf{\hat{y}}^{(i)}\)</span>. Cross-entropy comes from information theory and measures how many bits/nats on average are needed to describe an event of a probability distribution <span class="math inline">\(p(x)\)</span> when using the approximation <span class="math inline">\(q(x)\)</span>.</p>
<p><span class="math display">\[\begin{equation}
CE = - \sum_{x \in X} p(x) \log q(x) \\
CE = - \sum_{i=1}^N \sum_{j=1}^K y_j^{(i)} \log \hat{y}_j^{(i)}
\end{equation}\]</span></p>
<p>It is evident that cross-entropy is identical to the negative log-likelihood.</p>
<div id="cell-fig-classification-cross-entropy" class="cell" data-fig-width="600" data-execution_count="7">
<div class="cell-output cell-output-display">
<div id="fig-classification-cross-entropy" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-classification-cross-entropy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="classification_files/figure-html/fig-classification-cross-entropy-output-1.png" width="854" height="278" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classification-cross-entropy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: True Distribution (left) and Predicted Distribution (right).
</figcaption>
</figure>
</div>
</div>
</div>
<p><a href="#fig-classification-cross-entropy" class="quarto-xref">Figure&nbsp;5</a> shows an example with a cross-entropy value of: 0.266.</p>
</section>
</section>
<section id="architectures" class="level2">
<h2 class="anchored" data-anchor-id="architectures">Architectures</h2>
<p>Architecture refers to the complete definition of an untrained (deep-learning) model, i.e., the sequence and configuration of layers. The following are some important architectures that have been successfully used for image classification.</p>
<section id="alexnet" class="level3">
<h3 class="anchored" data-anchor-id="alexnet">AlexNet</h3>
<p>CNNs became extremely popular after winning the <a href="https://www.image-net.org/">ImageNet</a> Competition. <span class="citation" data-cites="krizhevsky_imagenet_2012">Krizhevsky, Sutskever, and Hinton (<a href="#ref-krizhevsky_imagenet_2012" role="doc-biblioref">2012</a>)</span> implemented a CNN with multiple layers, known as the AlexNet architecture, as shown in <a href="#fig-classification-alexnet" class="quarto-xref">Figure&nbsp;6</a>. ImageNet is a large, hierarchical image dataset <span class="citation" data-cites="deng_imagenet_2009">Deng et al. (<a href="#ref-deng_imagenet_2009" role="doc-biblioref">2009</a>)</span>, which enabled efficient training of CNNs for the first time.</p>
<p>AlexNet consists of 5 convolutional layers and 3 fully-connected layers. The last layer is a 1000-way softmax output to model the classes in ImageNet.</p>
<p>The model was trained with two GPUs (GTX 580) with 3GB memory each. Since 3GB was insufficient to train the model, the architecture was split across the GPUs. Some layers were split between the GPUs, allowing a larger network to be trained.</p>
<p><a href="#fig-classification-alexnet" class="quarto-xref">Figure&nbsp;6</a> shows the detailed architecture, including kernel sizes, the number of filters per layer, activation map dimensions, and pooling layers. ReLU was used as the activation function. This representation is from the original paper and shows how the model was split across two GPUs.</p>
<div id="fig-classification-alexnet" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-classification-alexnet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/classification/alexnet.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classification-alexnet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: AlexNet <span class="citation" data-cites="krizhevsky_imagenet_2012">Krizhevsky, Sutskever, and Hinton (<a href="#ref-krizhevsky_imagenet_2012" role="doc-biblioref">2012</a>)</span>.
</figcaption>
</figure>
</div>
<p>Since the split across two GPUs is no longer necessary, the architecture is somewhat simplified. Modern implementations are shown in <a href="#fig-classification-alexnet2" class="quarto-xref">Figure&nbsp;7</a>.</p>
<div id="fig-classification-alexnet2" class="quarto-float quarto-figure quarto-figure-center anchored" width="400">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-classification-alexnet2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/classification/alexnet_illustration.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classification-alexnet2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: AlexNet <span class="citation" data-cites="prince_understanding_2023">Prince (<a href="#ref-prince_understanding_2023" role="doc-biblioref">2023</a>)</span>
</figcaption>
</figure>
</div>
<p><a href="#fig-classification-alexnet-table" class="quarto-xref">Figure&nbsp;8</a> presents the operations in AlexNet in tabular form.</p>
<div id="fig-classification-alexnet-table" class="quarto-float quarto-figure quarto-figure-center anchored" width="600">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-classification-alexnet-table-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/classification/alexnet_table.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classification-alexnet-table-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Source: <span class="citation" data-cites="johnson_eecs_2019">Johnson (<a href="#ref-johnson_eecs_2019" role="doc-biblioref">2019</a>)</span>.
</figcaption>
</figure>
</div>
<p>We can also easily load AlexNet via torchvision.</p>
<div id="a71feab4" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.models <span class="im">as</span> models</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchinfo</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>alexnet <span class="op">=</span> models.alexnet()</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.zeros(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">224</span>, <span class="dv">224</span>, dtype<span class="op">=</span>torch.<span class="bu">float</span>, requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>yhat <span class="op">=</span> alexnet(x)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(torchinfo.summary(alexnet, input_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">224</span>, <span class="dv">224</span>)))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
AlexNet                                  [1, 1000]                 --
├─Sequential: 1-1                        [1, 256, 6, 6]            --
│    └─Conv2d: 2-1                       [1, 64, 55, 55]           23,296
│    └─ReLU: 2-2                         [1, 64, 55, 55]           --
│    └─MaxPool2d: 2-3                    [1, 64, 27, 27]           --
│    └─Conv2d: 2-4                       [1, 192, 27, 27]          307,392
│    └─ReLU: 2-5                         [1, 192, 27, 27]          --
│    └─MaxPool2d: 2-6                    [1, 192, 13, 13]          --
│    └─Conv2d: 2-7                       [1, 384, 13, 13]          663,936
│    └─ReLU: 2-8                         [1, 384, 13, 13]          --
│    └─Conv2d: 2-9                       [1, 256, 13, 13]          884,992
│    └─ReLU: 2-10                        [1, 256, 13, 13]          --
│    └─Conv2d: 2-11                      [1, 256, 13, 13]          590,080
│    └─ReLU: 2-12                        [1, 256, 13, 13]          --
│    └─MaxPool2d: 2-13                   [1, 256, 6, 6]            --
├─AdaptiveAvgPool2d: 1-2                 [1, 256, 6, 6]            --
├─Sequential: 1-3                        [1, 1000]                 --
│    └─Dropout: 2-14                     [1, 9216]                 --
│    └─Linear: 2-15                      [1, 4096]                 37,752,832
│    └─ReLU: 2-16                        [1, 4096]                 --
│    └─Dropout: 2-17                     [1, 4096]                 --
│    └─Linear: 2-18                      [1, 4096]                 16,781,312
│    └─ReLU: 2-19                        [1, 4096]                 --
│    └─Linear: 2-20                      [1, 1000]                 4,097,000
==========================================================================================
Total params: 61,100,840
Trainable params: 61,100,840
Non-trainable params: 0
Total mult-adds (Units.MEGABYTES): 714.68
==========================================================================================
Input size (MB): 0.60
Forward/backward pass size (MB): 3.95
Params size (MB): 244.40
Estimated Total Size (MB): 248.96
==========================================================================================</code></pre>
</div>
</div>
</section>
<section id="vgg" class="level3">
<h3 class="anchored" data-anchor-id="vgg">VGG</h3>
<p><span class="citation" data-cites="simonyan_very_2015">Simonyan and Zisserman (<a href="#ref-simonyan_very_2015" role="doc-biblioref">2015</a>)</span> won the ImageNet Challenge in 2014 with their VGG architecture. They showed that smaller 3x3 kernels work significantly better and that deeper networks with 16-19 layers can be trained. <a href="#fig-classification-vgg" class="quarto-xref">Figure&nbsp;9</a> shows the architecture as presented in the original paper. <a href="#fig-classification-vgg-arch" class="quarto-xref">Figure&nbsp;10</a> visualizes the architecture.</p>
<div id="fig-classification-vgg" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-classification-vgg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/classification/vgg.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classification-vgg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: VGG <span class="citation" data-cites="simonyan_very_2015">Simonyan and Zisserman (<a href="#ref-simonyan_very_2015" role="doc-biblioref">2015</a>)</span>.
</figcaption>
</figure>
</div>
<div id="fig-classification-vgg-arch" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-classification-vgg-arch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/classification/vgg_illustration.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classification-vgg-arch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: VGG <span class="citation" data-cites="prince_understanding_2023">Prince (<a href="#ref-prince_understanding_2023" role="doc-biblioref">2023</a>)</span>
</figcaption>
</figure>
</div>
<p>VGG introduced a popular design element: A layer has the same number of filters as the previous layer unless the activation map dimensions are halved, in which case the number of filters is doubled (see also <a href="#fig-classification-vgg-arch" class="quarto-xref">Figure&nbsp;10</a>). This was done to maintain the time complexity of the layers. VGG does not use normalization layers.</p>
<p><a href="#fig-classification-vgg-design" class="quarto-xref">Figure&nbsp;11</a> compares VGG with AlexNet.</p>
<div id="fig-classification-vgg-design" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-classification-vgg-design-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/classification/vgg_design.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classification-vgg-design-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: Source: <span class="citation" data-cites="johnson_eecs_2019">Johnson (<a href="#ref-johnson_eecs_2019" role="doc-biblioref">2019</a>)</span>.
</figcaption>
</figure>
</div>
</section>
<section id="resnet" class="level3">
<h3 class="anchored" data-anchor-id="resnet">ResNet</h3>
<p><span class="citation" data-cites="He2016">He et al. (<a href="#ref-He2016" role="doc-biblioref">2016</a>)</span> wondered whether CNNs could be improved simply by making them deeper, i.e., adding more layers. Their experiments showed that adding layers eventually stops being beneficial, and performance saturates and then rapidly degrades (see <a href="#fig-classification-resnet-motivation1" class="quarto-xref">Figure&nbsp;12</a>). They noticed that the performance did not degrade due to overfitting (see <a href="#fig-classification-resnet-motivation2" class="quarto-xref">Figure&nbsp;13</a>, which shows that training error is also poor), but because back-propagation becomes less effective and the weights do not optimize well. <span class="citation" data-cites="He2016">He et al. (<a href="#ref-He2016" role="doc-biblioref">2016</a>)</span> hypothesized that deeper networks could not perform worse than shallower ones because the additional layers could simply pass on activations unchanged.</p>
<div id="fig-classification-resnet-motivation1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-classification-resnet-motivation1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/classification/resnet_test_error.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classification-resnet-motivation1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: Source: <span class="citation" data-cites="He2016">He et al. (<a href="#ref-He2016" role="doc-biblioref">2016</a>)</span>
</figcaption>
</figure>
</div>
<div id="fig-classification-resnet-motivation2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-classification-resnet-motivation2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/classification/resnet_train_error.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classification-resnet-motivation2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13: Source: <span class="citation" data-cites="He2016">He et al. (<a href="#ref-He2016" role="doc-biblioref">2016</a>)</span>
</figcaption>
</figure>
</div>
<p>They then hypothesized that passing on activations unchanged as a default behavior could avoid this problem. They introduced identity mappings. <a href="#fig-classification-residual-connection" class="quarto-xref">Figure&nbsp;14</a> shows such a connection. These connections are also called residual connections because the network only needs to learn the change in activations from layer <span class="math inline">\(i\)</span> to layer <span class="math inline">\(i+1\)</span>, the residue.</p>
<div id="fig-classification-residual-connection" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-classification-residual-connection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/classification/residual_connection.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classification-residual-connection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14: ResNet <span class="citation" data-cites="He2016">He et al. (<a href="#ref-He2016" role="doc-biblioref">2016</a>)</span> (Graphic from <span class="citation" data-cites="johnson_eecs_2019">Johnson (<a href="#ref-johnson_eecs_2019" role="doc-biblioref">2019</a>)</span>.)
</figcaption>
</figure>
</div>
<p>ResNet was otherwise inspired by VGG16. The convolutional layers consist of 3x3 kernels (except the first one), and the number of filters is doubled when the activation map dimensions are halved. ResNet uses convolutions with stride 2 for down-sampling and no max pooling. At the end, ResNet uses a global average pooling layer followed by a fully-connected layer with the number of classes. Variants of ResNet have 18, 34, 50, and over 100 layers. ResNet also uses batch normalization. <a href="#fig-classification-resnet-struk" class="quarto-xref">Figure&nbsp;15</a> shows the architecture.</p>
<div id="fig-classification-resnet-struk" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-classification-resnet-struk-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/classification/resnet_arch_horiz.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classification-resnet-struk-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15: Source: <span class="citation" data-cites="He2016">He et al. (<a href="#ref-He2016" role="doc-biblioref">2016</a>)</span>.
</figcaption>
</figure>
</div>
<p>ResNet is very popular and is still widely used today (there are now more modern variants). Recent studies of identity mappings (or more generally skip connections) have shown that the gradient surface of the loss function becomes smoother, allowing the network to be optimized better. This is impressively illustrated in <a href="#fig-classification-skip-loss-landscape" class="quarto-xref">Figure&nbsp;16</a>.</p>
<div id="fig-classification-skip-loss-landscape" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-classification-skip-loss-landscape-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/classification/loss_landscape_skip.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classification-skip-loss-landscape-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16: Source: <span class="citation" data-cites="li_visualizing_2018">Li et al. (<a href="#ref-li_visualizing_2018" role="doc-biblioref">2018</a>)</span>.
</figcaption>
</figure>
</div>
</section>
<section id="convnext" class="level3">
<h3 class="anchored" data-anchor-id="convnext">ConvNext</h3>
<p>One of the most modern CNN architectures was described in <span class="citation" data-cites="liu_convnet_2022">Liu et al. (<a href="#ref-liu_convnet_2022" role="doc-biblioref">2022</a>)</span>. This architecture uses tricks and implementation ideas accumulated over decades from various architectures. <a href="#fig-classification-convnext" class="quarto-xref">Figure&nbsp;17</a> shows, starting from a modern version of ResNet, what has been adjusted to define this state-of-the-art architecture. Examples include: larger kernels, different activation functions, layer normalization instead of batch normalization, and depthwise separable convolutions.</p>
<div id="fig-classification-convnext" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-classification-convnext-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/classification/convnext.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classification-convnext-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17: Convnext <span class="citation" data-cites="liu_convnet_2022">Liu et al. (<a href="#ref-liu_convnet_2022" role="doc-biblioref">2022</a>)</span>.
</figcaption>
</figure>
</div>
<p>There is already a new version of this architecture <span class="citation" data-cites="woo_convnext_2023">Woo et al. (<a href="#ref-woo_convnext_2023" role="doc-biblioref">2023</a>)</span>.</p>
</section>
<section id="imagenet-performance" class="level3">
<h3 class="anchored" data-anchor-id="imagenet-performance">ImageNet Performance</h3>
<p><a href="#fig-classification-image-net-performance" class="quarto-xref">Figure&nbsp;18</a> shows the development of ImageNet performance. As of 2023, we are much closer to 100%, see <a href="https://paperswithcode.com/sota/image-classification-on-imagenet?metric=Top%205%20Accuracy">Link</a>.</p>
<div id="fig-classification-image-net-performance" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-classification-image-net-performance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/classification/image_net_performance.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classification-image-net-performance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;18: Graphic from <span class="citation" data-cites="johnson_eecs_2019">Johnson (<a href="#ref-johnson_eecs_2019" role="doc-biblioref">2019</a>)</span>
</figcaption>
</figure>
</div>
</section>
<section id="which-architecture" class="level3">
<h3 class="anchored" data-anchor-id="which-architecture">Which Architecture?</h3>
<p>Which architecture should be chosen for a specific problem? A common tip is: <em>Don’t be a hero.</em></p>
<p>One should rely on off-the-shelf architectures and not implement their own without a good reason.</p>
<p>Typically, ResNet-50 or ResNet-101 are good choices. However, there are also models that require significantly fewer parameters, such as Efficient Nets.</p>
<p>Important considerations are also the requirements regarding accuracy, performance (FLOPs), and model size (memory).</p>
</section>
</section>
<section id="further-variations" class="level2">
<h2 class="anchored" data-anchor-id="further-variations">Further Variations</h2>
<p>There are countless other architectures and especially specific layers or layer blocks that can be used in various architectures. The following are Squeeze/Excite Networks and the important class of normalization layers, which are often used and can significantly improve models or accelerate the optimization process.</p>
<section id="squeezeexcite-networks" class="level3">
<h3 class="anchored" data-anchor-id="squeezeexcite-networks">Squeeze/Excite Networks</h3>
<p>Squeeze-and-Excite Networks (SE-Networks) were introduced in 2019 <span class="citation" data-cites="hu_squeeze-and-excitation_2019">Hu et al. (<a href="#ref-hu_squeeze-and-excitation_2019" role="doc-biblioref">2019</a>)</span>. These include so-called Squeeze and Excite blocks (SE blocks), which allow the scaling of activation maps of a layer. This scaling is learnable through a few additional parameters. In practice, significant performance gains have been observed. <a href="#fig-classification-se-networks" class="quarto-xref">Figure&nbsp;19</a> shows an illustration.</p>
<div id="fig-classification-se-networks" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-classification-se-networks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/classification/se_networks.png" class="img-fluid figure-img" width="800">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classification-se-networks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;19: Source: <span class="citation" data-cites="hu_squeeze-and-excitation_2019">Hu et al. (<a href="#ref-hu_squeeze-and-excitation_2019" role="doc-biblioref">2019</a>)</span>.
</figcaption>
</figure>
</div>
<p>These SE blocks can be easily applied to arbitrary activation maps. <a href="#fig-classification-se-networks" class="quarto-xref">Figure&nbsp;19</a> shows an input feature map <span class="math inline">\(\mathbf{\mathsf{X}} \in \mathbb{R}^{H' \times W' \times C'}\)</span> that is transformed with <span class="math inline">\(F_{tr}\)</span> (e.g., with a convolutional layer). This results in the activation maps <span class="math inline">\(\mathbf{\mathsf{U}} \in \mathbb{R}^{H \times W \times C}\)</span>.</p>
<p>The transformation <span class="math inline">\(F_{sq}(\cdot)\)</span> applies the squeeze operation, a global average pooling, to generate a description of each channel by aggregating spatial information of the activation map <span class="math inline">\(\mathbf{\mathsf{U}}\)</span>. This results in a vector <span class="math inline">\(z \in \mathbb{R}^{1 \times 1 \times C}\)</span>.</p>
<p>The excitation operation <span class="math inline">\(F_{ex}(\cdot, W)\)</span> uses a gating mechanism with parameters <span class="math inline">\(W\)</span>, implemented with two fully-connected layers and activation functions. The result is <span class="math inline">\(s \in \mathbb{R}^{1 \times 1 \times C}\)</span>, the channel weights. It is called gating because the weights range from <span class="math inline">\([0, 1]\)</span> and thus control how much information of a channel flows through (gatekeeping).</p>
<p>Finally, <span class="math inline">\(F_{scale}(\cdot)\)</span> scales the activation maps <span class="math inline">\(\mathbf{\mathsf{U}}\)</span> with the channel weights.</p>
<p>The operation can be described as follows:</p>
<p><span class="math display">\[\begin{equation}
U = F_{tr}(X) \\
z = F_{sq}(U) = \text{GlobalAvgPool}(U) \\
s = F_{ex}(z, W) = \sigma(\mathbf{W}_2 g(\mathbf{W}_1 z)) \\
\hat{X} = F_{scale}(U, s) = U \odot s
\end{equation}\]</span></p>
<p>where <span class="math inline">\(g()\)</span> represents the ReLU function, and <span class="math inline">\(\sigma\)</span> represents the sigmoid function.</p>
</section>
<section id="normalization-layers" class="level3">
<h3 class="anchored" data-anchor-id="normalization-layers">Normalization Layers</h3>
<p>Normalization layers normalize activation maps to improve parameter learning. There are many variations, as shown in <a href="#fig-classification-normalization" class="quarto-xref">Figure&nbsp;20</a>. In modern architectures, normalization layers are typically used. Popular is, for example, layer normalization (see <span class="citation" data-cites="ba_layer_2016">Ba, Kiros, and Hinton (<a href="#ref-ba_layer_2016" role="doc-biblioref">2016</a>)</span>). The general form of normalization is given in equation <a href="#eq-classification-normalization" class="quarto-xref">Equation&nbsp;1</a>. The parameters <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\beta\)</span> are learned, while the means <span class="math inline">\(E[x]\)</span> and variances <span class="math inline">\(\sigma^2[x]\)</span> are estimated from the activations. Why normalization layers work and which ones to prefer are still subjects of research and are often empirically tested in practice (considered as hyper-parameters). It is believed that the cost function becomes smoother overall, allowing the network to train faster and better <span class="citation" data-cites="santurkar_how_2019">Santurkar et al. (<a href="#ref-santurkar_how_2019" role="doc-biblioref">2019</a>)</span>.</p>
<p><span id="eq-classification-normalization"><span class="math display">\[
y = \frac{x - E[x]}{\sqrt{\sigma^2[x] + \epsilon}} * \gamma + \beta
\tag{1}\]</span></span></p>
<div id="fig-classification-normalization" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-classification-normalization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/classification/normalization.png" class="img-fluid figure-img" width="300">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classification-normalization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;20: Source: <span class="citation" data-cites="qiao_micro-batch_2020">Qiao et al. (<a href="#ref-qiao_micro-batch_2020" role="doc-biblioref">2020</a>)</span>.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="pre-processing" class="level2">
<h2 class="anchored" data-anchor-id="pre-processing">Pre-Processing</h2>
<p>Typically, images are pre-processed before being processed by the CNN. This is done to make the optimization process converge faster. Common steps include:</p>
<ul>
<li>Resizing/cropping to a fixed size, e.g., 224x224x3. This is necessary for the images to be processed in a batch (they must have the same dimensions).</li>
<li>Scaling: Images are scaled from the interval [0, 255] to the interval [0, 1].</li>
<li>Normalization: Normalization is often done along the color channels so that the mean value of the pixel values is 0.</li>
</ul>
<p>In <code>torchvision</code>, the following pre-processing steps are used: <a href="https://github.com/pytorch/vision/blob/main/torchvision/transforms/_presets.py">Link</a></p>
</section>
<section id="transfer-learning" class="level2">
<h2 class="anchored" data-anchor-id="transfer-learning">Transfer Learning</h2>
<p>Transfer Learning refers to the process of adapting a trained model for Task A to Task B. Adapting pre-trained models often leads to better results and also reduces the number of training iterations. Deep learning frameworks often provide easy access to such models (also known as Model Zoo). For example, from PyTorch: https://pytorch.org/vision/stable/models.html.</p>
<div id="ba2ff1d0" class="cell" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.models <span class="im">import</span> resnet50, ResNet50_Weights</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>resnet50(weights<span class="op">=</span>ResNet50_Weights.IMAGENET1K_V2)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="pytorch-example" class="level2">
<h2 class="anchored" data-anchor-id="pytorch-example">PyTorch Example</h2>
<section id="architecture" class="level3">
<h3 class="anchored" data-anchor-id="architecture">Architecture</h3>
<p>Example of a CNN architecture.</p>
<div id="e8a699b2" class="cell" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Net(nn.Module):</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(<span class="dv">3</span>, <span class="dv">6</span>, <span class="dv">5</span>)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pool <span class="op">=</span> nn.MaxPool2d(<span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(<span class="dv">6</span>, <span class="dv">16</span>, <span class="dv">5</span>)</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">16</span> <span class="op">*</span> <span class="dv">5</span> <span class="op">*</span> <span class="dv">5</span>, <span class="dv">120</span>)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">120</span>, <span class="dv">84</span>)</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc3 <span class="op">=</span> nn.Linear(<span class="dv">84</span>, <span class="dv">10</span>)</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(F.relu(<span class="va">self</span>.conv1(x)))</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(F.relu(<span class="va">self</span>.conv2(x)))</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.flatten(x, <span class="dv">1</span>)</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc2(x))</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc3(x)</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>net <span class="op">=</span> Net()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="loss-function-1" class="level3">
<h3 class="anchored" data-anchor-id="loss-function-1">Loss Function</h3>
<div id="57c9fb24" class="cell" data-execution_count="11">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(net.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>, momentum<span class="op">=</span><span class="fl">0.9</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-ba_layer_2016" class="csl-entry" role="listitem">
Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. <span>“Layer <span>Normalization</span>.”</span> arXiv. <a href="http://arxiv.org/abs/1607.06450">http://arxiv.org/abs/1607.06450</a>.
</div>
<div id="ref-deng_imagenet_2009" class="csl-entry" role="listitem">
Deng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. <span>“<span>ImageNet</span>: <span>A</span> Large-Scale Hierarchical Image Database.”</span> In <em>2009 <span>IEEE</span> <span>Conference</span> on <span>Computer</span> <span>Vision</span> and <span>Pattern</span> <span>Recognition</span></em>, 248–55. Miami, FL: IEEE. <a href="https://doi.org/10.1109/CVPR.2009.5206848">https://doi.org/10.1109/CVPR.2009.5206848</a>.
</div>
<div id="ref-He2016" class="csl-entry" role="listitem">
He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. <span>“Deep <span>Residual</span> <span>Learning</span> for <span>Image</span> <span>Recognition</span>.”</span> In <em>2016 <span>IEEE</span> <span>Conference</span> on <span>Computer</span> <span>Vision</span> and <span>Pattern</span> <span>Recognition</span> (<span>CVPR</span>)</em>, 770–78. IEEE. <a href="https://doi.org/10.1109/CVPR.2016.90">https://doi.org/10.1109/CVPR.2016.90</a>.
</div>
<div id="ref-hu_squeeze-and-excitation_2019" class="csl-entry" role="listitem">
Hu, Jie, Li Shen, Samuel Albanie, Gang Sun, and Enhua Wu. 2019. <span>“Squeeze-and-<span>Excitation</span> <span>Networks</span>.”</span> arXiv. <a href="http://arxiv.org/abs/1709.01507">http://arxiv.org/abs/1709.01507</a>.
</div>
<div id="ref-johnson_eecs_2019" class="csl-entry" role="listitem">
Johnson, Justin. 2019. <span>“<span>EECS</span> 498-007 / 598-005: <span>Deep</span> <span>Learning</span> for <span>Computer</span> <span>Vision</span>.”</span> Lecture {Notes} / {Slides}. <a href="https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2019/">https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2019/</a>.
</div>
<div id="ref-krizhevsky_imagenet_2012" class="csl-entry" role="listitem">
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. <span>“<span>ImageNet</span> <span>Classification</span> with <span>Deep</span> <span>Convolutional</span> <span>Neural</span> <span>Networks</span>.”</span> In <em>Advances in <span>Neural</span> <span>Information</span> <span>Processing</span> <span>Systems</span></em>, edited by F. Pereira, C. J. Burges, L. Bottou, and K. Q. Weinberger. Vol. 25. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf</a>.
</div>
<div id="ref-li_visualizing_2018" class="csl-entry" role="listitem">
Li, Hao, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. 2018. <span>“Visualizing the <span>Loss</span> <span>Landscape</span> of <span>Neural</span> <span>Nets</span>.”</span> arXiv. <a href="http://arxiv.org/abs/1712.09913">http://arxiv.org/abs/1712.09913</a>.
</div>
<div id="ref-liu_convnet_2022" class="csl-entry" role="listitem">
Liu, Zhuang, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. 2022. <span>“A <span>ConvNet</span> for the 2020s.”</span> arXiv. <a href="http://arxiv.org/abs/2201.03545">http://arxiv.org/abs/2201.03545</a>.
</div>
<div id="ref-prince_understanding_2023" class="csl-entry" role="listitem">
Prince, Simon J. D. 2023. <em>Understanding <span>Deep</span> <span>Learning</span></em>. MIT Press. <a href="https://udlbook.github.io/udlbook/">https://udlbook.github.io/udlbook/</a>.
</div>
<div id="ref-qiao_micro-batch_2020" class="csl-entry" role="listitem">
Qiao, Siyuan, Huiyu Wang, Chenxi Liu, Wei Shen, and Alan Yuille. 2020. <span>“Micro-<span>Batch</span> <span>Training</span> with <span>Batch</span>-<span>Channel</span> <span>Normalization</span> and <span>Weight</span> <span>Standardization</span>.”</span> arXiv. <a href="http://arxiv.org/abs/1903.10520">http://arxiv.org/abs/1903.10520</a>.
</div>
<div id="ref-santurkar_how_2019" class="csl-entry" role="listitem">
Santurkar, Shibani, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. 2019. <span>“How <span>Does</span> <span>Batch</span> <span>Normalization</span> <span>Help</span> <span>Optimization</span>?”</span> arXiv. <a href="http://arxiv.org/abs/1805.11604">http://arxiv.org/abs/1805.11604</a>.
</div>
<div id="ref-simonyan_very_2015" class="csl-entry" role="listitem">
Simonyan, Karen, and Andrew Zisserman. 2015. <span>“Very <span>Deep</span> <span>Convolutional</span> <span>Networks</span> for <span>Large</span>-<span>Scale</span> <span>Image</span> <span>Recognition</span>.”</span> arXiv. <a href="http://arxiv.org/abs/1409.1556">http://arxiv.org/abs/1409.1556</a>.
</div>
<div id="ref-woo_convnext_2023" class="csl-entry" role="listitem">
Woo, Sanghyun, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, and Saining Xie. 2023. <span>“<span>ConvNeXt</span> <span>V2</span>: <span>Co</span>-Designing and <span>Scaling</span> <span>ConvNets</span> with <span>Masked</span> <span>Autoencoders</span>.”</span> arXiv. <a href="http://arxiv.org/abs/2301.00808">http://arxiv.org/abs/2301.00808</a>.
</div>
</div>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/marco-willi\.github\.io\/cas-dl-compvis-lectures-hs2025\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../lectures/cnns.html" class="pagination-link" aria-label="4 - Convolutional Neural Networks">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">4 - Convolutional Neural Networks</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../lectures/practical.html" class="pagination-link" aria-label="6 - Practical">
        <span class="nav-page-text">6 - Practical</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb12" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "5 - Image Classification"</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="an">params:</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co">   images_path: "/assets/images/classification/"</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="an">execute:</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="co">  cache: false</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="co">  freeze: false</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>::: {.content-hidden}</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>{{&lt; include /assets/_macros.tex &gt;}}</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction</span></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>Image classification is a core task of Computer Vision. In image classification, an image is assigned to a predefined set of classes. In multi-class classification, there are ≥ 2 classes; in binary classification, there are 2 classes; and in multi-label classification, there are several sets of classes. @fig-classification-classification-illustration illustrates the problem in multi-class classification.</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>::: {#fig-classification-classification-illustration}</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}classification_illustration.jpg)</span>{width=600}</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>Image Classification example.</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>@fig-image-classifier-example shows an example from the paper by @krizhevsky_imagenet_2012, which achieved the best results in the 2012 ImageNet competition, demonstrating how well CNNs work.</span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>::: {#fig-image-classifier-example}</span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}image_classification_example.png)</span>{width=600}</span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a>Image Classification example (from @krizhevsky_imagenet_2012).</span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a>@fig-classification-camera-traps illustrates the challenge with images taken by camera traps, which need to be classified along animal species.</span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a>::: {#fig-classification-camera-traps}</span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}camera_traps.png)</span>{width=600}</span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a>Example images from camera traps.</span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a><span class="fu">## Parametric Approach</span></span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a>In a parametric approach, we seek a model of the following form:</span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a>\hat{y}^{(i)} = f(\theta, \vect{x}^{(i)})</span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-51"><a href="#cb12-51" aria-hidden="true" tabindex="-1"></a>We want to find model parameters $\theta$ that output a score/prediction $\hat{y}^{(i)}$ for any data points $\vect{x}^{(i)}$ for each class $k \in K$. We then want to assess how good this score is with the help of a loss function.</span>
<span id="cb12-52"><a href="#cb12-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-53"><a href="#cb12-53" aria-hidden="true" tabindex="-1"></a><span class="fu">### Softmax Classifier</span></span>
<span id="cb12-54"><a href="#cb12-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-55"><a href="#cb12-55" aria-hidden="true" tabindex="-1"></a>With a Softmax Classifier, we interpret model predictions/scores as probabilities of class memberships: $P(Y=\vect{y}^{(i)}| X = \vect{x}^{(i)})$. We interpret the output as a <span class="co">[</span><span class="ot">Categorical Distribution</span><span class="co">](https://en.wikipedia.org/wiki/Categorical_distribution)</span> over all possible classes.</span>
<span id="cb12-56"><a href="#cb12-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-57"><a href="#cb12-57" aria-hidden="true" tabindex="-1"></a>To obtain a valid probability distribution, the untransformed outputs $\vect{z}$, also called logits, of a model are transformed with the softmax function $\sigma(\vect{z})$:</span>
<span id="cb12-58"><a href="#cb12-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-59"><a href="#cb12-59" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-60"><a href="#cb12-60" aria-hidden="true" tabindex="-1"></a>P(Y = k| X = \vect{x}^{(i)}) = \sigma(\vect{z})_k = \frac{e^{z_k}}{\sum_i^K e^{z_i}}</span>
<span id="cb12-61"><a href="#cb12-61" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-62"><a href="#cb12-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-63"><a href="#cb12-63" aria-hidden="true" tabindex="-1"></a>@fig-classification-logits-to-probs shows an example of the effect of the softmax transformation.</span>
<span id="cb12-64"><a href="#cb12-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-67"><a href="#cb12-67" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-68"><a href="#cb12-68" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb12-69"><a href="#cb12-69" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb12-70"><a href="#cb12-70" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 600</span></span>
<span id="cb12-71"><a href="#cb12-71" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-classification-logits-to-probs</span></span>
<span id="cb12-72"><a href="#cb12-72" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap:</span></span>
<span id="cb12-73"><a href="#cb12-73" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - "Logits (left) to probabilities with the Softmax function (right)."</span></span>
<span id="cb12-74"><a href="#cb12-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-75"><a href="#cb12-75" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb12-76"><a href="#cb12-76" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-77"><a href="#cb12-77" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb12-78"><a href="#cb12-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-79"><a href="#cb12-79" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">123</span>)</span>
<span id="cb12-80"><a href="#cb12-80" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> np.random.normal(size<span class="op">=</span>(<span class="dv">10</span>, )) <span class="op">*</span> <span class="dv">2</span></span>
<span id="cb12-81"><a href="#cb12-81" aria-hidden="true" tabindex="-1"></a>softmax_output <span class="op">=</span> np.exp(logits) <span class="op">/</span> np.exp(logits).<span class="bu">sum</span>()</span>
<span id="cb12-82"><a href="#cb12-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-83"><a href="#cb12-83" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(ncols<span class="op">=</span><span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">9</span>, <span class="dv">3</span>))</span>
<span id="cb12-84"><a href="#cb12-84" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> sns.barplot(x<span class="op">=</span>[i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">10</span>)], y<span class="op">=</span>logits, ax<span class="op">=</span>ax[<span class="dv">0</span>]).<span class="bu">set</span>(</span>
<span id="cb12-85"><a href="#cb12-85" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">"Logits"</span>)</span>
<span id="cb12-86"><a href="#cb12-86" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> sns.barplot(x<span class="op">=</span>[i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">10</span>)], y<span class="op">=</span>softmax_output, ax<span class="op">=</span>ax[<span class="dv">1</span>]).<span class="bu">set</span>(</span>
<span id="cb12-87"><a href="#cb12-87" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">"Softmax"</span>, ylim<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb12-88"><a href="#cb12-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-89"><a href="#cb12-89" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb12-90"><a href="#cb12-90" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb12-91"><a href="#cb12-91" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-92"><a href="#cb12-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-93"><a href="#cb12-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-94"><a href="#cb12-94" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- {{&lt; embed /demos/logits_to_probabilities.ipynb#fig-demo-logits-to-probs &gt;}} --&gt;</span></span>
<span id="cb12-95"><a href="#cb12-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-96"><a href="#cb12-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-97"><a href="#cb12-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-100"><a href="#cb12-100" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-101"><a href="#cb12-101" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb12-102"><a href="#cb12-102" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb12-103"><a href="#cb12-103" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 800</span></span>
<span id="cb12-104"><a href="#cb12-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-105"><a href="#cb12-105" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-106"><a href="#cb12-106" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.graph_objects <span class="im">as</span> go</span>
<span id="cb12-107"><a href="#cb12-107" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> plotly.subplots <span class="im">import</span> make_subplots</span>
<span id="cb12-108"><a href="#cb12-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-109"><a href="#cb12-109" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to calculate softmax</span></span>
<span id="cb12-110"><a href="#cb12-110" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> softmax(logits):</span>
<span id="cb12-111"><a href="#cb12-111" aria-hidden="true" tabindex="-1"></a>    e_logits <span class="op">=</span> np.exp(logits <span class="op">-</span> np.<span class="bu">max</span>(logits))</span>
<span id="cb12-112"><a href="#cb12-112" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> e_logits <span class="op">/</span> e_logits.<span class="bu">sum</span>()</span>
<span id="cb12-113"><a href="#cb12-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-114"><a href="#cb12-114" aria-hidden="true" tabindex="-1"></a><span class="co"># Predefined set of logits</span></span>
<span id="cb12-115"><a href="#cb12-115" aria-hidden="true" tabindex="-1"></a>logits_set <span class="op">=</span> {</span>
<span id="cb12-116"><a href="#cb12-116" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Set 1"</span>: [<span class="fl">2.0</span>, <span class="fl">1.0</span>, <span class="fl">0.1</span>],</span>
<span id="cb12-117"><a href="#cb12-117" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Set 2"</span>: [<span class="fl">1.0</span>, <span class="fl">2.0</span>, <span class="fl">3.0</span>],</span>
<span id="cb12-118"><a href="#cb12-118" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Set 3"</span>: [<span class="fl">1.0</span>, <span class="fl">1.0</span>, <span class="fl">1.0</span>],</span>
<span id="cb12-119"><a href="#cb12-119" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Set 4"</span>: [<span class="fl">3.0</span>, <span class="fl">1.0</span>, <span class="fl">0.5</span>],</span>
<span id="cb12-120"><a href="#cb12-120" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Set 5"</span>: [<span class="fl">0.5</span>, <span class="fl">0.2</span>, <span class="fl">0.1</span>]</span>
<span id="cb12-121"><a href="#cb12-121" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb12-122"><a href="#cb12-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-123"><a href="#cb12-123" aria-hidden="true" tabindex="-1"></a><span class="co"># Create initial plot</span></span>
<span id="cb12-124"><a href="#cb12-124" aria-hidden="true" tabindex="-1"></a>initial_logits <span class="op">=</span> logits_set[<span class="st">"Set 1"</span>]</span>
<span id="cb12-125"><a href="#cb12-125" aria-hidden="true" tabindex="-1"></a>initial_probs <span class="op">=</span> softmax(initial_logits)</span>
<span id="cb12-126"><a href="#cb12-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-127"><a href="#cb12-127" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> make_subplots(rows<span class="op">=</span><span class="dv">1</span>, cols<span class="op">=</span><span class="dv">2</span>, subplot_titles<span class="op">=</span>(<span class="st">"Logits"</span>, <span class="st">"Softmax Probabilities"</span>))</span>
<span id="cb12-128"><a href="#cb12-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-129"><a href="#cb12-129" aria-hidden="true" tabindex="-1"></a><span class="co"># Add initial logits bar chart</span></span>
<span id="cb12-130"><a href="#cb12-130" aria-hidden="true" tabindex="-1"></a>logits_bar <span class="op">=</span> go.Bar(x<span class="op">=</span>[<span class="ss">f'Logit </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(initial_logits))],</span>
<span id="cb12-131"><a href="#cb12-131" aria-hidden="true" tabindex="-1"></a>                    y<span class="op">=</span>initial_logits, name<span class="op">=</span><span class="st">"Logits"</span>)</span>
<span id="cb12-132"><a href="#cb12-132" aria-hidden="true" tabindex="-1"></a>fig.add_trace(logits_bar, row<span class="op">=</span><span class="dv">1</span>, col<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb12-133"><a href="#cb12-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-134"><a href="#cb12-134" aria-hidden="true" tabindex="-1"></a><span class="co"># Add initial softmax probabilities bar chart</span></span>
<span id="cb12-135"><a href="#cb12-135" aria-hidden="true" tabindex="-1"></a>softmax_bar <span class="op">=</span> go.Bar(x<span class="op">=</span>[<span class="ss">f'Class </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(initial_probs))],</span>
<span id="cb12-136"><a href="#cb12-136" aria-hidden="true" tabindex="-1"></a>                     y<span class="op">=</span>initial_probs, name<span class="op">=</span><span class="st">"Softmax Probabilities"</span>)</span>
<span id="cb12-137"><a href="#cb12-137" aria-hidden="true" tabindex="-1"></a>fig.add_trace(softmax_bar, row<span class="op">=</span><span class="dv">1</span>, col<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb12-138"><a href="#cb12-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-139"><a href="#cb12-139" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to create update args for a given set of logits</span></span>
<span id="cb12-140"><a href="#cb12-140" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_update_args(logits):</span>
<span id="cb12-141"><a href="#cb12-141" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [</span>
<span id="cb12-142"><a href="#cb12-142" aria-hidden="true" tabindex="-1"></a>        {<span class="st">"y"</span>: [logits, softmax(logits)]}</span>
<span id="cb12-143"><a href="#cb12-143" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb12-144"><a href="#cb12-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-145"><a href="#cb12-145" aria-hidden="true" tabindex="-1"></a><span class="co"># Update layout</span></span>
<span id="cb12-146"><a href="#cb12-146" aria-hidden="true" tabindex="-1"></a>fig.update_layout(</span>
<span id="cb12-147"><a href="#cb12-147" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">"Interactive Softmax Calculation"</span>,</span>
<span id="cb12-148"><a href="#cb12-148" aria-hidden="true" tabindex="-1"></a>    showlegend<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb12-149"><a href="#cb12-149" aria-hidden="true" tabindex="-1"></a>    yaxis2<span class="op">=</span><span class="bu">dict</span>(<span class="bu">range</span><span class="op">=</span>[<span class="dv">0</span>, <span class="dv">1</span>]),  <span class="co"># Set y-axis limit for softmax plot</span></span>
<span id="cb12-150"><a href="#cb12-150" aria-hidden="true" tabindex="-1"></a>    updatemenus<span class="op">=</span>[</span>
<span id="cb12-151"><a href="#cb12-151" aria-hidden="true" tabindex="-1"></a>        {</span>
<span id="cb12-152"><a href="#cb12-152" aria-hidden="true" tabindex="-1"></a>            <span class="st">"buttons"</span>: [</span>
<span id="cb12-153"><a href="#cb12-153" aria-hidden="true" tabindex="-1"></a>                {</span>
<span id="cb12-154"><a href="#cb12-154" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"args"</span>: create_update_args(logits),</span>
<span id="cb12-155"><a href="#cb12-155" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"label"</span>: <span class="ss">f"Set </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">"</span>,</span>
<span id="cb12-156"><a href="#cb12-156" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"method"</span>: <span class="st">"update"</span></span>
<span id="cb12-157"><a href="#cb12-157" aria-hidden="true" tabindex="-1"></a>                } <span class="cf">for</span> i, logits <span class="kw">in</span> <span class="bu">enumerate</span>(logits_set.values())</span>
<span id="cb12-158"><a href="#cb12-158" aria-hidden="true" tabindex="-1"></a>            ],</span>
<span id="cb12-159"><a href="#cb12-159" aria-hidden="true" tabindex="-1"></a>            <span class="st">"direction"</span>: <span class="st">"down"</span>,</span>
<span id="cb12-160"><a href="#cb12-160" aria-hidden="true" tabindex="-1"></a>            <span class="st">"showactive"</span>: <span class="va">True</span>,</span>
<span id="cb12-161"><a href="#cb12-161" aria-hidden="true" tabindex="-1"></a>            <span class="st">"x"</span>: <span class="fl">0.5</span>,</span>
<span id="cb12-162"><a href="#cb12-162" aria-hidden="true" tabindex="-1"></a>            <span class="st">"xanchor"</span>: <span class="st">"center"</span>,</span>
<span id="cb12-163"><a href="#cb12-163" aria-hidden="true" tabindex="-1"></a>            <span class="st">"y"</span>: <span class="fl">1.15</span>,</span>
<span id="cb12-164"><a href="#cb12-164" aria-hidden="true" tabindex="-1"></a>            <span class="st">"yanchor"</span>: <span class="st">"top"</span></span>
<span id="cb12-165"><a href="#cb12-165" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb12-166"><a href="#cb12-166" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb12-167"><a href="#cb12-167" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-168"><a href="#cb12-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-169"><a href="#cb12-169" aria-hidden="true" tabindex="-1"></a>fig.show()</span>
<span id="cb12-170"><a href="#cb12-170" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-171"><a href="#cb12-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-172"><a href="#cb12-172" aria-hidden="true" tabindex="-1"></a><span class="fu">## Loss Function</span></span>
<span id="cb12-173"><a href="#cb12-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-174"><a href="#cb12-174" aria-hidden="true" tabindex="-1"></a>Now we look at how to define the loss function $L(\m{X}, \vect{y}, \theta)$ for a multi-class classification problem.</span>
<span id="cb12-175"><a href="#cb12-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-176"><a href="#cb12-176" aria-hidden="true" tabindex="-1"></a><span class="fu">### Likelihood</span></span>
<span id="cb12-177"><a href="#cb12-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-178"><a href="#cb12-178" aria-hidden="true" tabindex="-1"></a>The likelihood of a data point $(\vect{x}^{(i)}, y^{(i)})$ is the probability of observing/realizing a data point, given a model with certain parameters:</span>
<span id="cb12-179"><a href="#cb12-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-180"><a href="#cb12-180" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-181"><a href="#cb12-181" aria-hidden="true" tabindex="-1"></a>P(Y=y^{(i)}| X = \vect{x}^{(i)}) = f(\theta, \vect{x}^{(i)})</span>
<span id="cb12-182"><a href="#cb12-182" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-183"><a href="#cb12-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-184"><a href="#cb12-184" aria-hidden="true" tabindex="-1"></a>This means we formulate a model with a probabilistic interpretation of predictions: $f(\theta, \vect{x}^{(i)}): \mathbb{R}^{n} \mapsto <span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>$</span>
<span id="cb12-185"><a href="#cb12-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-186"><a href="#cb12-186" aria-hidden="true" tabindex="-1"></a>For a multi-class classification, the label vector is one-hot encoded $\vect{y}^{(i)} \in <span class="sc">\{</span>0, 1<span class="sc">\}</span>^K$, where the true class is coded with 1 and the rest with 0. The likelihood of a data point is defined as:</span>
<span id="cb12-187"><a href="#cb12-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-188"><a href="#cb12-188" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-189"><a href="#cb12-189" aria-hidden="true" tabindex="-1"></a>\prod_{j=1}^K  P(Y = j| X = \vect{x}^{(i)})^{y^{(i)}_j}</span>
<span id="cb12-190"><a href="#cb12-190" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-191"><a href="#cb12-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-192"><a href="#cb12-192" aria-hidden="true" tabindex="-1"></a>Since only one entry in $\vect{y}^{(i)}$ is 1, the likelihood is simply the prediction for the true class $P(Y = y^{(i)}| X = \vect{x}^{(i)})$.</span>
<span id="cb12-193"><a href="#cb12-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-194"><a href="#cb12-194" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">More Info</span><span class="co">](http://www.awebb.info/probability/2017/05/18/cross-entropy-and-log-likelihood.html)</span></span>
<span id="cb12-195"><a href="#cb12-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-198"><a href="#cb12-198" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-199"><a href="#cb12-199" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb12-200"><a href="#cb12-200" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb12-201"><a href="#cb12-201" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-202"><a href="#cb12-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-203"><a href="#cb12-203" aria-hidden="true" tabindex="-1"></a><span class="co"># Example For Likelihood Calculation</span></span>
<span id="cb12-204"><a href="#cb12-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-205"><a href="#cb12-205" aria-hidden="true" tabindex="-1"></a>y_true <span class="op">=</span> np.array(</span>
<span id="cb12-206"><a href="#cb12-206" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>]</span>
<span id="cb12-207"><a href="#cb12-207" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-208"><a href="#cb12-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-209"><a href="#cb12-209" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> np.array(</span>
<span id="cb12-210"><a href="#cb12-210" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.1</span>, <span class="fl">0.8</span>, <span class="fl">0.2</span>, <span class="fl">0.7</span>, <span class="fl">0.9</span>]</span>
<span id="cb12-211"><a href="#cb12-211" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-212"><a href="#cb12-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-213"><a href="#cb12-213" aria-hidden="true" tabindex="-1"></a><span class="co"># Perfect Prediction</span></span>
<span id="cb12-214"><a href="#cb12-214" aria-hidden="true" tabindex="-1"></a><span class="co">#y_pred = np.array([0, 1, 0, 1, 1])</span></span>
<span id="cb12-215"><a href="#cb12-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-216"><a href="#cb12-216" aria-hidden="true" tabindex="-1"></a><span class="co"># Larger Dataset</span></span>
<span id="cb12-217"><a href="#cb12-217" aria-hidden="true" tabindex="-1"></a><span class="co">#y_true = np.repeat(y_true, 10, axis=0)</span></span>
<span id="cb12-218"><a href="#cb12-218" aria-hidden="true" tabindex="-1"></a><span class="co">#y_pred = np.repeat(y_pred, 10, axis=0)</span></span>
<span id="cb12-219"><a href="#cb12-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-220"><a href="#cb12-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-221"><a href="#cb12-221" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_likelihood(y_true: <span class="bu">float</span>, y_pred: <span class="bu">float</span>) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb12-222"><a href="#cb12-222" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (y_pred <span class="op">**</span> y_true) <span class="op">*</span> ((<span class="dv">1</span> <span class="op">-</span> y_pred) <span class="op">**</span> (<span class="dv">1</span> <span class="op">-</span> y_true))</span>
<span id="cb12-223"><a href="#cb12-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-224"><a href="#cb12-224" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> display_likelihood(y_true, y_pred):</span>
<span id="cb12-225"><a href="#cb12-225" aria-hidden="true" tabindex="-1"></a>    sample_likelihoods <span class="op">=</span> [calculate_likelihood(<span class="bu">float</span>(yt), <span class="bu">float</span>(yp)) <span class="cf">for</span> yt, yp <span class="kw">in</span> <span class="bu">zip</span>(y_true, y_pred)]</span>
<span id="cb12-226"><a href="#cb12-226" aria-hidden="true" tabindex="-1"></a>    likelihood <span class="op">=</span> np.prod(sample_likelihoods)</span>
<span id="cb12-227"><a href="#cb12-227" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Total Likelihood: </span><span class="sc">{</span>likelihood<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb12-228"><a href="#cb12-228" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-229"><a href="#cb12-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-230"><a href="#cb12-230" aria-hidden="true" tabindex="-1"></a>Given $\vect{y} = <span class="co">[</span><span class="ot">0, 1, 0, 1, 1</span><span class="co">]</span>$ and the following $\hat{\vect{y}}$, calculate the likelihood.</span>
<span id="cb12-231"><a href="#cb12-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-232"><a href="#cb12-232" aria-hidden="true" tabindex="-1"></a>For $\hat{\vect{y}} = <span class="co">[</span><span class="ot">0.1, 0.8, 0.2, 0.7, 0.9</span><span class="co">]</span>$:</span>
<span id="cb12-233"><a href="#cb12-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-234"><a href="#cb12-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-235"><a href="#cb12-235" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb12-236"><a href="#cb12-236" aria-hidden="true" tabindex="-1"></a><span class="fu">## Click for result</span></span>
<span id="cb12-239"><a href="#cb12-239" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-240"><a href="#cb12-240" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb12-241"><a href="#cb12-241" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb12-242"><a href="#cb12-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-243"><a href="#cb12-243" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> np.array([<span class="fl">0.1</span>, <span class="fl">0.8</span>, <span class="fl">0.2</span>, <span class="fl">0.7</span>, <span class="fl">0.9</span>])</span>
<span id="cb12-244"><a href="#cb12-244" aria-hidden="true" tabindex="-1"></a>display_likelihood(y_true, y_pred)</span>
<span id="cb12-245"><a href="#cb12-245" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-246"><a href="#cb12-246" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-247"><a href="#cb12-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-248"><a href="#cb12-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-249"><a href="#cb12-249" aria-hidden="true" tabindex="-1"></a>Does it get larger for $\hat{\vect{y}} = <span class="co">[</span><span class="ot">0.1, 0.9, 0.2, 0.7, 0.9</span><span class="co">]</span>$?</span>
<span id="cb12-250"><a href="#cb12-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-251"><a href="#cb12-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-252"><a href="#cb12-252" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb12-253"><a href="#cb12-253" aria-hidden="true" tabindex="-1"></a><span class="fu">## Click for result</span></span>
<span id="cb12-256"><a href="#cb12-256" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-257"><a href="#cb12-257" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb12-258"><a href="#cb12-258" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb12-259"><a href="#cb12-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-260"><a href="#cb12-260" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> np.array([<span class="fl">0.1</span>, <span class="fl">0.9</span>, <span class="fl">0.2</span>, <span class="fl">0.7</span>, <span class="fl">0.9</span>])</span>
<span id="cb12-261"><a href="#cb12-261" aria-hidden="true" tabindex="-1"></a>display_likelihood(y_true, y_pred)</span>
<span id="cb12-262"><a href="#cb12-262" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-263"><a href="#cb12-263" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-264"><a href="#cb12-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-265"><a href="#cb12-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-266"><a href="#cb12-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-267"><a href="#cb12-267" aria-hidden="true" tabindex="-1"></a>What happens if we increase the dataset by copying the vector 10 times? $\hat{\vect{y}} = <span class="co">[</span><span class="ot">0.1, 0.8, 0.2, 0.7, 0.9, 0.1, 0.8, ...</span><span class="co">]</span>$?</span>
<span id="cb12-268"><a href="#cb12-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-269"><a href="#cb12-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-270"><a href="#cb12-270" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb12-271"><a href="#cb12-271" aria-hidden="true" tabindex="-1"></a><span class="fu">## Click for result</span></span>
<span id="cb12-274"><a href="#cb12-274" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-275"><a href="#cb12-275" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb12-276"><a href="#cb12-276" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb12-277"><a href="#cb12-277" aria-hidden="true" tabindex="-1"></a><span class="co">#|</span></span>
<span id="cb12-278"><a href="#cb12-278" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> np.array([<span class="fl">0.1</span>, <span class="fl">0.8</span>, <span class="fl">0.2</span>, <span class="fl">0.7</span>, <span class="fl">0.9</span>])</span>
<span id="cb12-279"><a href="#cb12-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-280"><a href="#cb12-280" aria-hidden="true" tabindex="-1"></a>y_true <span class="op">=</span> np.repeat(y_true, <span class="dv">10</span>, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb12-281"><a href="#cb12-281" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> np.repeat(y_pred, <span class="dv">10</span>, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb12-282"><a href="#cb12-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-283"><a href="#cb12-283" aria-hidden="true" tabindex="-1"></a>display_likelihood(y_true, y_pred)</span>
<span id="cb12-284"><a href="#cb12-284" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-285"><a href="#cb12-285" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-286"><a href="#cb12-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-287"><a href="#cb12-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-288"><a href="#cb12-288" aria-hidden="true" tabindex="-1"></a><span class="fu">### Maximum Likelihood</span></span>
<span id="cb12-289"><a href="#cb12-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-290"><a href="#cb12-290" aria-hidden="true" tabindex="-1"></a>The likelihood $P(\vect{y} | \theta, \m{X})$ of observing our entire dataset $(\m{X}, \vect{y})$, given the parameters $\theta$ and assuming that the data points $(\vect{x}^{(i)}, y^{(i)})$ are independent and identically distributed, can be calculated as:</span>
<span id="cb12-291"><a href="#cb12-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-292"><a href="#cb12-292" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb12-293"><a href="#cb12-293" aria-hidden="true" tabindex="-1"></a>\prod_{i=1}^N \prod_{j=1}^K  P(Y = j| X = \vect{x}^{(i)})^{y^{(i)}_j}</span>
<span id="cb12-294"><a href="#cb12-294" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb12-295"><a href="#cb12-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-296"><a href="#cb12-296" aria-hidden="true" tabindex="-1"></a>Under the Maximum Likelihood approach, we seek the parameters $\theta$ that maximize the likelihood of observing the dataset.</span>
<span id="cb12-297"><a href="#cb12-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-298"><a href="#cb12-298" aria-hidden="true" tabindex="-1"></a><span class="fu">### Negative Log-Likelihood</span></span>
<span id="cb12-299"><a href="#cb12-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-300"><a href="#cb12-300" aria-hidden="true" tabindex="-1"></a>With Maximum Likelihood, we aim to choose the parameters $\theta$ such that $p(\vect{y} | \theta, \m{X})$ is maximized. However, this function can be complex to handle, and we can use some mathematical tricks. We take the logarithm of the likelihood so that the product of probabilities becomes a sum. Since the logarithm is a monotonically increasing function, we can maximize its logarithm instead of the function $p(\vect{y} | \theta, \m{X})$. Finally, we take the negative of the function, allowing us to minimize it.</span>
<span id="cb12-301"><a href="#cb12-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-302"><a href="#cb12-302" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb12-303"><a href="#cb12-303" aria-hidden="true" tabindex="-1"></a>L(\m{X}, \vect{y}, \theta) = - \log \prod_{i=1}^N \prod_{j=1}^K P(Y = j| X = \vect{x}^{(i)})^{y^{(i)}_j} <span class="sc">\\</span></span>
<span id="cb12-304"><a href="#cb12-304" aria-hidden="true" tabindex="-1"></a>L(\m{X}, \vect{y}, \theta) = -\sum_{i=1}^N \sum_{j=1}^K y^{(i)}_j \log  P(Y = j| X = \vect{x}^{(i)})</span>
<span id="cb12-305"><a href="#cb12-305" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb12-306"><a href="#cb12-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-307"><a href="#cb12-307" aria-hidden="true" tabindex="-1"></a><span class="fu">### Cross-Entropy</span></span>
<span id="cb12-308"><a href="#cb12-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-309"><a href="#cb12-309" aria-hidden="true" tabindex="-1"></a>The loss function, derived with Maximum Likelihood, can also be viewed through the lens of cross-entropy between two discrete probability functions. Specifically, we can calculate and minimize the cross-entropy between the true distribution $\vect{y}^{(i)}$ and the predicted $\vect{\hat{y}}^{(i)}$. Cross-entropy comes from information theory and measures how many bits/nats on average are needed to describe an event of a probability distribution $p(x)$ when using the approximation $q(x)$.</span>
<span id="cb12-310"><a href="#cb12-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-311"><a href="#cb12-311" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb12-312"><a href="#cb12-312" aria-hidden="true" tabindex="-1"></a>CE = - \sum_{x \in X} p(x) \log q(x) <span class="sc">\\</span></span>
<span id="cb12-313"><a href="#cb12-313" aria-hidden="true" tabindex="-1"></a>CE = - \sum_{i=1}^N \sum_{j=1}^K y_j^{(i)} \log \hat{y}_j^{(i)}</span>
<span id="cb12-314"><a href="#cb12-314" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb12-315"><a href="#cb12-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-316"><a href="#cb12-316" aria-hidden="true" tabindex="-1"></a>It is evident that cross-entropy is identical to the negative log-likelihood.</span>
<span id="cb12-317"><a href="#cb12-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-320"><a href="#cb12-320" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-321"><a href="#cb12-321" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb12-322"><a href="#cb12-322" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb12-323"><a href="#cb12-323" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 600</span></span>
<span id="cb12-324"><a href="#cb12-324" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-classification-cross-entropy</span></span>
<span id="cb12-325"><a href="#cb12-325" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap:</span></span>
<span id="cb12-326"><a href="#cb12-326" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - "True Distribution (left) and Predicted Distribution (right)."</span></span>
<span id="cb12-327"><a href="#cb12-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-328"><a href="#cb12-328" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb12-329"><a href="#cb12-329" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-330"><a href="#cb12-330" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb12-331"><a href="#cb12-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-332"><a href="#cb12-332" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">123</span>)</span>
<span id="cb12-333"><a href="#cb12-333" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb12-334"><a href="#cb12-334" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> np.random.normal(size<span class="op">=</span>(<span class="dv">5</span>, )) <span class="op">*</span> <span class="dv">2</span></span>
<span id="cb12-335"><a href="#cb12-335" aria-hidden="true" tabindex="-1"></a>y_hat <span class="op">=</span> np.exp(logits) <span class="op">/</span> np.exp(logits).<span class="bu">sum</span>()</span>
<span id="cb12-336"><a href="#cb12-336" aria-hidden="true" tabindex="-1"></a>y_true <span class="op">=</span> np.zeros_like(logits)</span>
<span id="cb12-337"><a href="#cb12-337" aria-hidden="true" tabindex="-1"></a>y_true[<span class="dv">1</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb12-338"><a href="#cb12-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-339"><a href="#cb12-339" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(ncols<span class="op">=</span><span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">9</span>, <span class="dv">3</span>))</span>
<span id="cb12-340"><a href="#cb12-340" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> sns.barplot(x<span class="op">=</span>[i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, k)], y<span class="op">=</span>y_true, ax<span class="op">=</span>ax[<span class="dv">0</span>]).<span class="bu">set</span>(</span>
<span id="cb12-341"><a href="#cb12-341" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">"True Distribution"</span>)</span>
<span id="cb12-342"><a href="#cb12-342" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> sns.barplot(x<span class="op">=</span>[i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, k)], y<span class="op">=</span>y_hat, ax<span class="op">=</span>ax[<span class="dv">1</span>]).<span class="bu">set</span>(</span>
<span id="cb12-343"><a href="#cb12-343" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">"Predicted Distribution"</span>, ylim<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb12-344"><a href="#cb12-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-345"><a href="#cb12-345" aria-hidden="true" tabindex="-1"></a>cross_entropy <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>(<span class="op">-</span> y_true <span class="op">*</span> np.log(y_hat))<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">:.3f}</span><span class="ss">"</span></span>
<span id="cb12-346"><a href="#cb12-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-347"><a href="#cb12-347" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb12-348"><a href="#cb12-348" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb12-349"><a href="#cb12-349" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-350"><a href="#cb12-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-351"><a href="#cb12-351" aria-hidden="true" tabindex="-1"></a>@fig-classification-cross-entropy shows an example with a cross-entropy value of: <span class="in">`{python} cross_entropy`</span>.</span>
<span id="cb12-352"><a href="#cb12-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-353"><a href="#cb12-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-354"><a href="#cb12-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-355"><a href="#cb12-355" aria-hidden="true" tabindex="-1"></a><span class="fu">## Architectures</span></span>
<span id="cb12-356"><a href="#cb12-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-357"><a href="#cb12-357" aria-hidden="true" tabindex="-1"></a>Architecture refers to the complete definition of an untrained (deep-learning) model, i.e., the sequence and configuration of layers. The following are some important architectures that have been successfully used for image classification.</span>
<span id="cb12-358"><a href="#cb12-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-359"><a href="#cb12-359" aria-hidden="true" tabindex="-1"></a><span class="fu">### AlexNet</span></span>
<span id="cb12-360"><a href="#cb12-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-361"><a href="#cb12-361" aria-hidden="true" tabindex="-1"></a>CNNs became extremely popular after winning the <span class="co">[</span><span class="ot">ImageNet</span><span class="co">](https://www.image-net.org/)</span> Competition. @krizhevsky_imagenet_2012 implemented a CNN with multiple layers, known as the AlexNet architecture, as shown in @fig-classification-alexnet. ImageNet is a large, hierarchical image dataset @deng_imagenet_2009, which enabled efficient training of CNNs for the first time.</span>
<span id="cb12-362"><a href="#cb12-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-363"><a href="#cb12-363" aria-hidden="true" tabindex="-1"></a>AlexNet consists of 5 convolutional layers and 3 fully-connected layers. The last layer is a 1000-way softmax output to model the classes in ImageNet.</span>
<span id="cb12-364"><a href="#cb12-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-365"><a href="#cb12-365" aria-hidden="true" tabindex="-1"></a>The model was trained with two GPUs (GTX 580) with 3GB memory each. Since 3GB was insufficient to train the model, the architecture was split across the GPUs. Some layers were split between the GPUs, allowing a larger network to be trained.</span>
<span id="cb12-366"><a href="#cb12-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-367"><a href="#cb12-367" aria-hidden="true" tabindex="-1"></a>@fig-classification-alexnet shows the detailed architecture, including kernel sizes, the number of filters per layer, activation map dimensions, and pooling layers. ReLU was used as the activation function. This representation is from the original paper and shows how the model was split across two GPUs.</span>
<span id="cb12-368"><a href="#cb12-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-369"><a href="#cb12-369" aria-hidden="true" tabindex="-1"></a>::: {#fig-classification-alexnet}</span>
<span id="cb12-370"><a href="#cb12-370" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}alexnet.png)</span>{width=600}</span>
<span id="cb12-371"><a href="#cb12-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-372"><a href="#cb12-372" aria-hidden="true" tabindex="-1"></a>AlexNet @krizhevsky_imagenet_2012.</span>
<span id="cb12-373"><a href="#cb12-373" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-374"><a href="#cb12-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-375"><a href="#cb12-375" aria-hidden="true" tabindex="-1"></a>Since the split across two GPUs is no longer necessary, the architecture is somewhat simplified. Modern implementations are shown in @fig-classification-alexnet2.</span>
<span id="cb12-376"><a href="#cb12-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-377"><a href="#cb12-377" aria-hidden="true" tabindex="-1"></a>::: {#fig-classification-alexnet2 width=400}</span>
<span id="cb12-378"><a href="#cb12-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-379"><a href="#cb12-379" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}alexnet_illustration.jpg)</span></span>
<span id="cb12-380"><a href="#cb12-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-381"><a href="#cb12-381" aria-hidden="true" tabindex="-1"></a>AlexNet @prince_understanding_2023</span>
<span id="cb12-382"><a href="#cb12-382" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-383"><a href="#cb12-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-384"><a href="#cb12-384" aria-hidden="true" tabindex="-1"></a>@fig-classification-alexnet-table presents the operations in AlexNet in tabular form.</span>
<span id="cb12-385"><a href="#cb12-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-386"><a href="#cb12-386" aria-hidden="true" tabindex="-1"></a>::: {#fig-classification-alexnet-table width=600}</span>
<span id="cb12-387"><a href="#cb12-387" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}alexnet_table.jpg)</span></span>
<span id="cb12-388"><a href="#cb12-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-389"><a href="#cb12-389" aria-hidden="true" tabindex="-1"></a>Source: @johnson_eecs_2019.</span>
<span id="cb12-390"><a href="#cb12-390" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-391"><a href="#cb12-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-392"><a href="#cb12-392" aria-hidden="true" tabindex="-1"></a>We can also easily load AlexNet via torchvision.</span>
<span id="cb12-393"><a href="#cb12-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-396"><a href="#cb12-396" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-397"><a href="#cb12-397" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb12-398"><a href="#cb12-398" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb12-399"><a href="#cb12-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-400"><a href="#cb12-400" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb12-401"><a href="#cb12-401" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.models <span class="im">as</span> models</span>
<span id="cb12-402"><a href="#cb12-402" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchinfo</span>
<span id="cb12-403"><a href="#cb12-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-404"><a href="#cb12-404" aria-hidden="true" tabindex="-1"></a>alexnet <span class="op">=</span> models.alexnet()</span>
<span id="cb12-405"><a href="#cb12-405" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.zeros(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">224</span>, <span class="dv">224</span>, dtype<span class="op">=</span>torch.<span class="bu">float</span>, requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb12-406"><a href="#cb12-406" aria-hidden="true" tabindex="-1"></a>yhat <span class="op">=</span> alexnet(x)</span>
<span id="cb12-407"><a href="#cb12-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-408"><a href="#cb12-408" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(torchinfo.summary(alexnet, input_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">224</span>, <span class="dv">224</span>)))</span>
<span id="cb12-409"><a href="#cb12-409" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-410"><a href="#cb12-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-411"><a href="#cb12-411" aria-hidden="true" tabindex="-1"></a><span class="fu">### VGG</span></span>
<span id="cb12-412"><a href="#cb12-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-413"><a href="#cb12-413" aria-hidden="true" tabindex="-1"></a>@simonyan_very_2015 won the ImageNet Challenge in 2014 with their VGG architecture. They showed that smaller 3x3 kernels work significantly better and that deeper networks with 16-19 layers can be trained. @fig-classification-vgg shows the architecture as presented in the original paper. @fig-classification-vgg-arch visualizes the architecture.</span>
<span id="cb12-414"><a href="#cb12-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-415"><a href="#cb12-415" aria-hidden="true" tabindex="-1"></a>::: {#fig-classification-vgg}</span>
<span id="cb12-416"><a href="#cb12-416" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}vgg.png)</span>{width=600}</span>
<span id="cb12-417"><a href="#cb12-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-418"><a href="#cb12-418" aria-hidden="true" tabindex="-1"></a>VGG @simonyan_very_2015.</span>
<span id="cb12-419"><a href="#cb12-419" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-420"><a href="#cb12-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-421"><a href="#cb12-421" aria-hidden="true" tabindex="-1"></a>::: {#fig-classification-vgg-arch}</span>
<span id="cb12-422"><a href="#cb12-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-423"><a href="#cb12-423" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}vgg_illustration.jpg)</span></span>
<span id="cb12-424"><a href="#cb12-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-425"><a href="#cb12-425" aria-hidden="true" tabindex="-1"></a>VGG @prince_understanding_2023</span>
<span id="cb12-426"><a href="#cb12-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-427"><a href="#cb12-427" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-428"><a href="#cb12-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-429"><a href="#cb12-429" aria-hidden="true" tabindex="-1"></a>VGG introduced a popular design element: A layer has the same number of filters as the previous layer unless the activation map dimensions are halved, in which case the number of filters is doubled (see also @fig-classification-vgg-arch). This was done to maintain the time complexity of the layers. VGG does not use normalization layers.</span>
<span id="cb12-430"><a href="#cb12-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-431"><a href="#cb12-431" aria-hidden="true" tabindex="-1"></a>@fig-classification-vgg-design compares VGG with AlexNet.</span>
<span id="cb12-432"><a href="#cb12-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-433"><a href="#cb12-433" aria-hidden="true" tabindex="-1"></a>::: {#fig-classification-vgg-design}</span>
<span id="cb12-434"><a href="#cb12-434" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}vgg_design.jpg)</span>{width=600}</span>
<span id="cb12-435"><a href="#cb12-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-436"><a href="#cb12-436" aria-hidden="true" tabindex="-1"></a>Source: @johnson_eecs_2019.</span>
<span id="cb12-437"><a href="#cb12-437" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-438"><a href="#cb12-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-439"><a href="#cb12-439" aria-hidden="true" tabindex="-1"></a><span class="fu">### ResNet</span></span>
<span id="cb12-440"><a href="#cb12-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-441"><a href="#cb12-441" aria-hidden="true" tabindex="-1"></a>@He2016 wondered whether CNNs could be improved simply by making them deeper, i.e., adding more layers. Their experiments showed that adding layers eventually stops being beneficial, and performance saturates and then rapidly degrades (see @fig-classification-resnet-motivation1). They noticed that the performance did not degrade due to overfitting (see @fig-classification-resnet-motivation2, which shows that training error is also poor), but because back-propagation becomes less effective and the weights do not optimize well. @He2016 hypothesized that deeper networks could not perform worse than shallower ones because the additional layers could simply pass on activations unchanged.</span>
<span id="cb12-442"><a href="#cb12-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-443"><a href="#cb12-443" aria-hidden="true" tabindex="-1"></a>::: {#fig-classification-resnet-motivation1}</span>
<span id="cb12-444"><a href="#cb12-444" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}resnet_test_error.jpg)</span>{width=600}</span>
<span id="cb12-445"><a href="#cb12-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-446"><a href="#cb12-446" aria-hidden="true" tabindex="-1"></a>Source: @He2016</span>
<span id="cb12-447"><a href="#cb12-447" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-448"><a href="#cb12-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-449"><a href="#cb12-449" aria-hidden="true" tabindex="-1"></a>::: {#fig-classification-resnet-motivation2}</span>
<span id="cb12-450"><a href="#cb12-450" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}resnet_train_error.jpg)</span>{width=600}</span>
<span id="cb12-451"><a href="#cb12-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-452"><a href="#cb12-452" aria-hidden="true" tabindex="-1"></a>Source: @He2016</span>
<span id="cb12-453"><a href="#cb12-453" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-454"><a href="#cb12-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-455"><a href="#cb12-455" aria-hidden="true" tabindex="-1"></a>They then hypothesized that passing on activations unchanged as a default behavior could avoid this problem. They introduced identity mappings. @fig-classification-residual-connection shows such a connection. These connections are also called residual connections because the network only needs to learn the change in activations from layer $i$ to layer $i+1$, the residue.</span>
<span id="cb12-456"><a href="#cb12-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-457"><a href="#cb12-457" aria-hidden="true" tabindex="-1"></a>::: {#fig-classification-residual-connection}</span>
<span id="cb12-458"><a href="#cb12-458" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}residual_connection.jpg)</span>{width=600}</span>
<span id="cb12-459"><a href="#cb12-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-460"><a href="#cb12-460" aria-hidden="true" tabindex="-1"></a>ResNet @He2016 (Graphic from @johnson_eecs_2019.)</span>
<span id="cb12-461"><a href="#cb12-461" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-462"><a href="#cb12-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-463"><a href="#cb12-463" aria-hidden="true" tabindex="-1"></a>ResNet was otherwise inspired by VGG16. The convolutional layers consist of 3x3 kernels (except the first one), and the number of filters is doubled when the activation map dimensions are halved. ResNet uses convolutions with stride 2 for down-sampling and no max pooling. At the end, ResNet uses a global average pooling layer followed by a fully-connected layer with the number of classes. Variants of ResNet have 18, 34, 50, and over 100 layers. ResNet also uses batch normalization. @fig-classification-resnet-struk shows the architecture.</span>
<span id="cb12-464"><a href="#cb12-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-465"><a href="#cb12-465" aria-hidden="true" tabindex="-1"></a>::: {#fig-classification-resnet-struk}</span>
<span id="cb12-466"><a href="#cb12-466" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}resnet_arch_horiz.png)</span>{width=600}</span>
<span id="cb12-467"><a href="#cb12-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-468"><a href="#cb12-468" aria-hidden="true" tabindex="-1"></a>Source: @He2016.</span>
<span id="cb12-469"><a href="#cb12-469" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-470"><a href="#cb12-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-471"><a href="#cb12-471" aria-hidden="true" tabindex="-1"></a>ResNet is very popular and is still widely used today (there are now more modern variants). Recent studies of identity mappings (or more generally skip connections) have shown that the gradient surface of the loss function becomes smoother, allowing the network to be optimized better. This is impressively illustrated in @fig-classification-skip-loss-landscape.</span>
<span id="cb12-472"><a href="#cb12-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-473"><a href="#cb12-473" aria-hidden="true" tabindex="-1"></a>::: {#fig-classification-skip-loss-landscape}</span>
<span id="cb12-474"><a href="#cb12-474" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}loss_landscape_skip.png)</span>{width=600}</span>
<span id="cb12-475"><a href="#cb12-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-476"><a href="#cb12-476" aria-hidden="true" tabindex="-1"></a>Source: @li_visualizing_2018.</span>
<span id="cb12-477"><a href="#cb12-477" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-478"><a href="#cb12-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-479"><a href="#cb12-479" aria-hidden="true" tabindex="-1"></a><span class="fu">### ConvNext</span></span>
<span id="cb12-480"><a href="#cb12-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-481"><a href="#cb12-481" aria-hidden="true" tabindex="-1"></a>One of the most modern CNN architectures was described in @liu_convnet_2022. This architecture uses tricks and implementation ideas accumulated over decades from various architectures. @fig-classification-convnext shows, starting from a modern version of ResNet, what has been adjusted to define this state-of-the-art architecture. Examples include: larger kernels, different activation functions, layer normalization instead of batch normalization, and depthwise separable convolutions.</span>
<span id="cb12-482"><a href="#cb12-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-483"><a href="#cb12-483" aria-hidden="true" tabindex="-1"></a>::: {#fig-classification-convnext}</span>
<span id="cb12-484"><a href="#cb12-484" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}convnext.png)</span>{width=600}</span>
<span id="cb12-485"><a href="#cb12-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-486"><a href="#cb12-486" aria-hidden="true" tabindex="-1"></a>Convnext @liu_convnet_2022.</span>
<span id="cb12-487"><a href="#cb12-487" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-488"><a href="#cb12-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-489"><a href="#cb12-489" aria-hidden="true" tabindex="-1"></a>There is already a new version of this architecture @woo_convnext_2023.</span>
<span id="cb12-490"><a href="#cb12-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-491"><a href="#cb12-491" aria-hidden="true" tabindex="-1"></a><span class="fu">### ImageNet Performance</span></span>
<span id="cb12-492"><a href="#cb12-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-493"><a href="#cb12-493" aria-hidden="true" tabindex="-1"></a>@fig-classification-image-net-performance shows the development of ImageNet performance. As of 2023, we are much closer to 100%, see <span class="co">[</span><span class="ot">Link</span><span class="co">](https://paperswithcode.com/sota/image-classification-on-imagenet?metric=Top%205%20Accuracy)</span>.</span>
<span id="cb12-494"><a href="#cb12-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-495"><a href="#cb12-495" aria-hidden="true" tabindex="-1"></a>::: {#fig-classification-image-net-performance}</span>
<span id="cb12-496"><a href="#cb12-496" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}image_net_performance.jpg)</span>{width=600}</span>
<span id="cb12-497"><a href="#cb12-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-498"><a href="#cb12-498" aria-hidden="true" tabindex="-1"></a>Graphic from @johnson_eecs_2019</span>
<span id="cb12-499"><a href="#cb12-499" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-500"><a href="#cb12-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-501"><a href="#cb12-501" aria-hidden="true" tabindex="-1"></a><span class="fu">### Which Architecture?</span></span>
<span id="cb12-502"><a href="#cb12-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-503"><a href="#cb12-503" aria-hidden="true" tabindex="-1"></a>Which architecture should be chosen for a specific problem? A common tip is: *Don't be a hero.*</span>
<span id="cb12-504"><a href="#cb12-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-505"><a href="#cb12-505" aria-hidden="true" tabindex="-1"></a>One should rely on off-the-shelf architectures and not implement their own without a good reason.</span>
<span id="cb12-506"><a href="#cb12-506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-507"><a href="#cb12-507" aria-hidden="true" tabindex="-1"></a>Typically, ResNet-50 or ResNet-101 are good choices. However, there are also models that require significantly fewer parameters, such as Efficient Nets.</span>
<span id="cb12-508"><a href="#cb12-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-509"><a href="#cb12-509" aria-hidden="true" tabindex="-1"></a>Important considerations are also the requirements regarding accuracy, performance (FLOPs), and model size (memory).</span>
<span id="cb12-510"><a href="#cb12-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-511"><a href="#cb12-511" aria-hidden="true" tabindex="-1"></a><span class="fu">## Further Variations</span></span>
<span id="cb12-512"><a href="#cb12-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-513"><a href="#cb12-513" aria-hidden="true" tabindex="-1"></a>There are countless other architectures and especially specific layers or layer blocks that can be used in various architectures. The following are Squeeze/Excite Networks and the important class of normalization layers, which are often used and can significantly improve models or accelerate the optimization process.</span>
<span id="cb12-514"><a href="#cb12-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-515"><a href="#cb12-515" aria-hidden="true" tabindex="-1"></a><span class="fu">### Squeeze/Excite Networks</span></span>
<span id="cb12-516"><a href="#cb12-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-517"><a href="#cb12-517" aria-hidden="true" tabindex="-1"></a>Squeeze-and-Excite Networks (SE-Networks) were introduced in 2019 @hu_squeeze-and-excitation_2019. These include so-called Squeeze and Excite blocks (SE blocks), which allow the scaling of activation maps of a layer. This scaling is learnable through a few additional parameters. In practice, significant performance gains have been observed. @fig-classification-se-networks shows an illustration.</span>
<span id="cb12-518"><a href="#cb12-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-519"><a href="#cb12-519" aria-hidden="true" tabindex="-1"></a>::: {#fig-classification-se-networks}</span>
<span id="cb12-520"><a href="#cb12-520" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}se_networks.png)</span>{width=800}</span>
<span id="cb12-521"><a href="#cb12-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-522"><a href="#cb12-522" aria-hidden="true" tabindex="-1"></a>Source: @hu_squeeze-and-excitation_2019.</span>
<span id="cb12-523"><a href="#cb12-523" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-524"><a href="#cb12-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-525"><a href="#cb12-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-526"><a href="#cb12-526" aria-hidden="true" tabindex="-1"></a>These SE blocks can be easily applied to arbitrary activation maps. @fig-classification-se-networks shows an input feature map $\tensor{X} \in \mathbb{R}^{H' \times W' \times C'}$ that is transformed with $F_{tr}$ (e.g., with a convolutional layer). This results in the activation maps $\tensor{U} \in \mathbb{R}^{H \times W \times C}$.</span>
<span id="cb12-527"><a href="#cb12-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-528"><a href="#cb12-528" aria-hidden="true" tabindex="-1"></a>The transformation $F_{sq}(\cdot)$ applies the squeeze operation, a global average pooling, to generate a description of each channel by aggregating spatial information of the activation map $\tensor{U}$. This results in a vector $z \in \mathbb{R}^{1 \times 1 \times C}$.</span>
<span id="cb12-529"><a href="#cb12-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-530"><a href="#cb12-530" aria-hidden="true" tabindex="-1"></a>The excitation operation $F_{ex}(\cdot, W)$ uses a gating mechanism with parameters $W$, implemented with two fully-connected layers and activation functions. The result is $s \in \mathbb{R}^{1 \times 1 \times C}$, the channel weights. It is called gating because the weights range from $<span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>$ and thus control how much information of a channel flows through (gatekeeping).</span>
<span id="cb12-531"><a href="#cb12-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-532"><a href="#cb12-532" aria-hidden="true" tabindex="-1"></a>Finally, $F_{scale}(\cdot)$ scales the activation maps $\tensor{U}$ with the channel weights.</span>
<span id="cb12-533"><a href="#cb12-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-534"><a href="#cb12-534" aria-hidden="true" tabindex="-1"></a>The operation can be described as follows:</span>
<span id="cb12-535"><a href="#cb12-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-536"><a href="#cb12-536" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb12-537"><a href="#cb12-537" aria-hidden="true" tabindex="-1"></a>U = F_{tr}(X) <span class="sc">\\</span></span>
<span id="cb12-538"><a href="#cb12-538" aria-hidden="true" tabindex="-1"></a>z = F_{sq}(U) = \text{GlobalAvgPool}(U) <span class="sc">\\</span></span>
<span id="cb12-539"><a href="#cb12-539" aria-hidden="true" tabindex="-1"></a>s = F_{ex}(z, W) = \sigma(\mathbf{W}_2 g(\mathbf{W}_1 z)) <span class="sc">\\</span></span>
<span id="cb12-540"><a href="#cb12-540" aria-hidden="true" tabindex="-1"></a>\hat{X} = F_{scale}(U, s) = U \odot s</span>
<span id="cb12-541"><a href="#cb12-541" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb12-542"><a href="#cb12-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-543"><a href="#cb12-543" aria-hidden="true" tabindex="-1"></a>where $g()$ represents the ReLU function, and $\sigma$ represents the sigmoid function.</span>
<span id="cb12-544"><a href="#cb12-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-545"><a href="#cb12-545" aria-hidden="true" tabindex="-1"></a><span class="fu">### Normalization Layers</span></span>
<span id="cb12-546"><a href="#cb12-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-547"><a href="#cb12-547" aria-hidden="true" tabindex="-1"></a>Normalization layers normalize activation maps to improve parameter learning. There are many variations, as shown in @fig-classification-normalization. In modern architectures, normalization layers are typically used. Popular is, for example, layer normalization (see @ba_layer_2016). The general form of normalization is given in equation @eq-classification-normalization. The parameters $\gamma$ and $\beta$ are learned, while the means $E<span class="co">[</span><span class="ot">x</span><span class="co">]</span>$ and variances $\sigma^2<span class="co">[</span><span class="ot">x</span><span class="co">]</span>$ are estimated from the activations. Why normalization layers work and which ones to prefer are still subjects of research and are often empirically tested in practice (considered as hyper-parameters). It is believed that the cost function becomes smoother overall, allowing the network to train faster and better @santurkar_how_2019.</span>
<span id="cb12-548"><a href="#cb12-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-549"><a href="#cb12-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-550"><a href="#cb12-550" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb12-551"><a href="#cb12-551" aria-hidden="true" tabindex="-1"></a>y = \frac{x - E<span class="co">[</span><span class="ot">x</span><span class="co">]</span>}{\sqrt{\sigma^2<span class="co">[</span><span class="ot">x</span><span class="co">]</span> + \epsilon}} * \gamma + \beta</span>
<span id="cb12-552"><a href="#cb12-552" aria-hidden="true" tabindex="-1"></a>$$ {#eq-classification-normalization}</span>
<span id="cb12-553"><a href="#cb12-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-554"><a href="#cb12-554" aria-hidden="true" tabindex="-1"></a>::: {#fig-classification-normalization}</span>
<span id="cb12-555"><a href="#cb12-555" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}normalization.png)</span>{width=300}</span>
<span id="cb12-556"><a href="#cb12-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-557"><a href="#cb12-557" aria-hidden="true" tabindex="-1"></a>Source: @qiao_micro-batch_2020.</span>
<span id="cb12-558"><a href="#cb12-558" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb12-559"><a href="#cb12-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-560"><a href="#cb12-560" aria-hidden="true" tabindex="-1"></a><span class="fu">## Pre-Processing</span></span>
<span id="cb12-561"><a href="#cb12-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-562"><a href="#cb12-562" aria-hidden="true" tabindex="-1"></a>Typically, images are pre-processed before being processed by the CNN. This is done to make the optimization process converge faster. Common steps include:</span>
<span id="cb12-563"><a href="#cb12-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-564"><a href="#cb12-564" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Resizing/cropping to a fixed size, e.g., 224x224x3. This is necessary for the images to be processed in a batch (they must have the same dimensions).</span>
<span id="cb12-565"><a href="#cb12-565" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Scaling: Images are scaled from the interval <span class="co">[</span><span class="ot">0, 255</span><span class="co">]</span> to the interval <span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>.</span>
<span id="cb12-566"><a href="#cb12-566" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Normalization: Normalization is often done along the color channels so that the mean value of the pixel values is 0.</span>
<span id="cb12-567"><a href="#cb12-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-568"><a href="#cb12-568" aria-hidden="true" tabindex="-1"></a>In <span class="in">`torchvision`</span>, the following pre-processing steps are used: <span class="co">[</span><span class="ot">Link</span><span class="co">](https://github.com/pytorch/vision/blob/main/torchvision/transforms/_presets.py)</span></span>
<span id="cb12-569"><a href="#cb12-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-570"><a href="#cb12-570" aria-hidden="true" tabindex="-1"></a><span class="fu">## Transfer Learning</span></span>
<span id="cb12-571"><a href="#cb12-571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-572"><a href="#cb12-572" aria-hidden="true" tabindex="-1"></a>Transfer Learning refers to the process of adapting a trained model for Task A to Task B. Adapting pre-trained models often leads to better results and also reduces the number of training iterations. Deep learning frameworks often provide easy access to such models (also known as Model Zoo). For example, from PyTorch: https://pytorch.org/vision/stable/models.html.</span>
<span id="cb12-573"><a href="#cb12-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-576"><a href="#cb12-576" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-577"><a href="#cb12-577" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb12-578"><a href="#cb12-578" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb12-579"><a href="#cb12-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-580"><a href="#cb12-580" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.models <span class="im">import</span> resnet50, ResNet50_Weights</span>
<span id="cb12-581"><a href="#cb12-581" aria-hidden="true" tabindex="-1"></a>resnet50(weights<span class="op">=</span>ResNet50_Weights.IMAGENET1K_V2)</span>
<span id="cb12-582"><a href="#cb12-582" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-583"><a href="#cb12-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-584"><a href="#cb12-584" aria-hidden="true" tabindex="-1"></a><span class="fu">## PyTorch Example</span></span>
<span id="cb12-585"><a href="#cb12-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-586"><a href="#cb12-586" aria-hidden="true" tabindex="-1"></a><span class="fu">### Architecture</span></span>
<span id="cb12-587"><a href="#cb12-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-588"><a href="#cb12-588" aria-hidden="true" tabindex="-1"></a>Example of a CNN architecture.</span>
<span id="cb12-589"><a href="#cb12-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-592"><a href="#cb12-592" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-593"><a href="#cb12-593" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb12-594"><a href="#cb12-594" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb12-595"><a href="#cb12-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-596"><a href="#cb12-596" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb12-597"><a href="#cb12-597" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb12-598"><a href="#cb12-598" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb12-599"><a href="#cb12-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-600"><a href="#cb12-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-601"><a href="#cb12-601" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Net(nn.Module):</span>
<span id="cb12-602"><a href="#cb12-602" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb12-603"><a href="#cb12-603" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb12-604"><a href="#cb12-604" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(<span class="dv">3</span>, <span class="dv">6</span>, <span class="dv">5</span>)</span>
<span id="cb12-605"><a href="#cb12-605" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pool <span class="op">=</span> nn.MaxPool2d(<span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb12-606"><a href="#cb12-606" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(<span class="dv">6</span>, <span class="dv">16</span>, <span class="dv">5</span>)</span>
<span id="cb12-607"><a href="#cb12-607" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">16</span> <span class="op">*</span> <span class="dv">5</span> <span class="op">*</span> <span class="dv">5</span>, <span class="dv">120</span>)</span>
<span id="cb12-608"><a href="#cb12-608" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">120</span>, <span class="dv">84</span>)</span>
<span id="cb12-609"><a href="#cb12-609" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc3 <span class="op">=</span> nn.Linear(<span class="dv">84</span>, <span class="dv">10</span>)</span>
<span id="cb12-610"><a href="#cb12-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-611"><a href="#cb12-611" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb12-612"><a href="#cb12-612" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(F.relu(<span class="va">self</span>.conv1(x)))</span>
<span id="cb12-613"><a href="#cb12-613" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(F.relu(<span class="va">self</span>.conv2(x)))</span>
<span id="cb12-614"><a href="#cb12-614" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.flatten(x, <span class="dv">1</span>)</span>
<span id="cb12-615"><a href="#cb12-615" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb12-616"><a href="#cb12-616" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc2(x))</span>
<span id="cb12-617"><a href="#cb12-617" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc3(x)</span>
<span id="cb12-618"><a href="#cb12-618" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb12-619"><a href="#cb12-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-620"><a href="#cb12-620" aria-hidden="true" tabindex="-1"></a>net <span class="op">=</span> Net()</span>
<span id="cb12-621"><a href="#cb12-621" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-622"><a href="#cb12-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-623"><a href="#cb12-623" aria-hidden="true" tabindex="-1"></a><span class="fu">### Loss Function</span></span>
<span id="cb12-624"><a href="#cb12-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-627"><a href="#cb12-627" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb12-628"><a href="#cb12-628" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb12-629"><a href="#cb12-629" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb12-630"><a href="#cb12-630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-631"><a href="#cb12-631" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb12-632"><a href="#cb12-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-633"><a href="#cb12-633" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb12-634"><a href="#cb12-634" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(net.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>, momentum<span class="op">=</span><span class="fl">0.9</span>)</span>
<span id="cb12-635"><a href="#cb12-635" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb12-636"><a href="#cb12-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-637"><a href="#cb12-637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-638"><a href="#cb12-638" aria-hidden="true" tabindex="-1"></a><span class="fu">## References</span></span>
<span id="cb12-639"><a href="#cb12-639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-640"><a href="#cb12-640" aria-hidden="true" tabindex="-1"></a>::: {#refs}</span>
<span id="cb12-641"><a href="#cb12-641" aria-hidden="true" tabindex="-1"></a>:::</span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© Copyright 2024, Marco Willi</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2025/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This page is built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>