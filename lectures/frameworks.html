<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.24">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Marco Willi">

<title>2 - Software &amp; Hardware for Deep Learning – CAS Deep Learning - Computer Vision</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../lectures/neural_networks.html" rel="next">
<link href="../lectures/intro.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-a6e161b2431e1f94a14e0f5d32135a3c.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-e0449c57dc659bcd97f89e2c92d66f0b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-SCHRGR3LNM"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-SCHRGR3LNM', { 'anonymize_ip': true});
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../lectures/intro.html">Lectures</a></li><li class="breadcrumb-item"><a href="../lectures/frameworks.html">2 - Software &amp; Hardware for Deep Learning</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-center sidebar-header">
      <a href="../index.html" class="sidebar-logo-link">
      <img src="../assets/logo.webp" alt="" class="sidebar-logo light-content py-0 d-lg-inline d-none">
      <img src="../assets/logo.webp" alt="" class="sidebar-logo dark-content py-0 d-lg-inline d-none">
      </a>
      <div class="sidebar-tools-main">
    <a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2025" title="GitHub organization" class="quarto-navigation-tool px-1" aria-label="GitHub organization"><i class="bi bi-github"></i></a>
    <a href="" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-code-square"></i></a>
</div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Slides</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../slides/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Einführung Computer Vision mit Deep Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../slides/cnns.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Convolutional Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../slides/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../slides/practical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Practical Considerations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../slides/recent_advances.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Recent Advances</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/quiz.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Quiz &amp; Exam Prep</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Lectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1 - Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/frameworks.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">2 - Software &amp; Hardware for Deep Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/neural_networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3 - Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/cnns.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4 - Convolutional Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5 - Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/practical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6 - Practical</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../lectures/recent_advances.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">7 - Recent Advances</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Resources</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/literature.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Books</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/links.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Helpful Links &amp; Resources</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pages/notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Mathematical Notation</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#deep-learning-software" id="toc-deep-learning-software" class="nav-link active" data-scroll-target="#deep-learning-software">Deep Learning Software</a>
  <ul class="collapse">
  <li><a href="#computational-graph-autograd" id="toc-computational-graph-autograd" class="nav-link" data-scroll-target="#computational-graph-autograd">Computational Graph &amp; Autograd</a></li>
  </ul></li>
  <li><a href="#pytorch" id="toc-pytorch" class="nav-link" data-scroll-target="#pytorch">PyTorch</a>
  <ul class="collapse">
  <li><a href="#fundamental-concepts" id="toc-fundamental-concepts" class="nav-link" data-scroll-target="#fundamental-concepts">Fundamental Concepts</a></li>
  <li><a href="#tensors" id="toc-tensors" class="nav-link" data-scroll-target="#tensors">Tensors</a></li>
  <li><a href="#autograd" id="toc-autograd" class="nav-link" data-scroll-target="#autograd">Autograd</a></li>
  <li><a href="#torch.nn" id="toc-torch.nn" class="nav-link" data-scroll-target="#torch.nn">torch.nn</a></li>
  <li><a href="#torch.optim" id="toc-torch.optim" class="nav-link" data-scroll-target="#torch.optim">torch.optim</a></li>
  <li><a href="#training-loops" id="toc-training-loops" class="nav-link" data-scroll-target="#training-loops">Training Loops</a></li>
  <li><a href="#pre-trained-models" id="toc-pre-trained-models" class="nav-link" data-scroll-target="#pre-trained-models">Pre-trained models</a></li>
  </ul></li>
  <li><a href="#other-frameworks" id="toc-other-frameworks" class="nav-link" data-scroll-target="#other-frameworks">Other Frameworks</a>
  <ul class="collapse">
  <li><a href="#tensorflow" id="toc-tensorflow" class="nav-link" data-scroll-target="#tensorflow">TensorFlow</a></li>
  <li><a href="#jax" id="toc-jax" class="nav-link" data-scroll-target="#jax">Jax</a></li>
  <li><a href="#scikit-learn" id="toc-scikit-learn" class="nav-link" data-scroll-target="#scikit-learn">Scikit-Learn</a></li>
  <li><a href="#onnx" id="toc-onnx" class="nav-link" data-scroll-target="#onnx">ONNX</a></li>
  <li><a href="#monitoring" id="toc-monitoring" class="nav-link" data-scroll-target="#monitoring">Monitoring</a></li>
  </ul></li>
  <li><a href="#hardware" id="toc-hardware" class="nav-link" data-scroll-target="#hardware">Hardware</a>
  <ul class="collapse">
  <li><a href="#tensor-operations" id="toc-tensor-operations" class="nav-link" data-scroll-target="#tensor-operations">Tensor Operations</a></li>
  <li><a href="#graphics-processing-units-gpus" id="toc-graphics-processing-units-gpus" class="nav-link" data-scroll-target="#graphics-processing-units-gpus">Graphics Processing Units (GPUs)</a></li>
  <li><a href="#cuda-cudnn" id="toc-cuda-cudnn" class="nav-link" data-scroll-target="#cuda-cudnn">CUDA &amp; cuDNN</a></li>
  <li><a href="#data-loading" id="toc-data-loading" class="nav-link" data-scroll-target="#data-loading">Data Loading</a></li>
  <li><a href="#gpu-parallelism" id="toc-gpu-parallelism" class="nav-link" data-scroll-target="#gpu-parallelism">GPU Parallelism</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2025/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../lectures/intro.html">Lectures</a></li><li class="breadcrumb-item"><a href="../lectures/frameworks.html">2 - Software &amp; Hardware for Deep Learning</a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">2 - Software &amp; Hardware for Deep Learning</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Marco Willi </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Institute for Data Science I4DS, FHNW
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  


</header>


<section id="deep-learning-software" class="level1">
<h1>Deep Learning Software</h1>
<p>There are a variety of Deep Learning frameworks. These frameworks allow for easy configuration, training, and deploying of neural networks. They are often developed via Python API. <a href="#fig-infrastructure-frameworks" class="quarto-xref">Figure&nbsp;1</a> shows some frameworks.</p>
<div id="fig-infrastructure-frameworks" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-infrastructure-frameworks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/frameworks/frameworks.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-infrastructure-frameworks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Frameworks (from <span class="citation" data-cites="li_cs231n_2022">Li (<a href="#ref-li_cs231n_2022" role="doc-biblioref">2022</a>)</span>).
</figcaption>
</figure>
</div>
<p>Key features of such frameworks are:</p>
<ul>
<li>Fast development and testing of neural networks</li>
<li>Automatic differentiation of operations</li>
<li>Efficient execution on diverse hardware</li>
</ul>
<section id="computational-graph-autograd" class="level2">
<h2 class="anchored" data-anchor-id="computational-graph-autograd">Computational Graph &amp; Autograd</h2>
<p>At the core of neural networks is the <em>Computational Graph</em>. It automatically embeds dependent operations in a <em>directed acyclic graph (DAG)</em>. Gradients are tracked as needed, allowing variables to be efficiently updated/trained.</p>
<p>The following shows an example in Numpy where we define computations and manually calculate derivatives. The graph is shown in <a href="#fig-infrastructure-comp-graph2" class="quarto-xref">Figure&nbsp;2</a>.</p>
<p><span class="math display">\[\begin{equation}
    f(\mathbf{A}, \mathbf{B}, \mathbf{C}) =  \sum_{ij} \big((\mathbf{A} \odot \mathbf{B}) + \mathbf{C}\big)_{ij}
\end{equation}\]</span></p>
<div id="fig-infrastructure-comp-graph2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-infrastructure-comp-graph2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/frameworks/comp-graph2.jpg" class="img-fluid figure-img" width="200">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-infrastructure-comp-graph2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Computational Graph.
</figcaption>
</figure>
</div>
<div id="257c2fa8" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">123</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>H, W <span class="op">=</span> <span class="dv">2</span>, <span class="dv">3</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> np.random.random(size<span class="op">=</span>(H, W))</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> np.random.random(size<span class="op">=</span>(H, W))</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> np.random.random(size<span class="op">=</span>(H, W))</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> a <span class="op">*</span> b</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>e <span class="op">=</span> d <span class="op">+</span> c</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> e.<span class="bu">sum</span>()</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>df_de <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>de_dd <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>de_dc <span class="op">=</span> c</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>dd_da <span class="op">=</span> b</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>df_da <span class="op">=</span> df_de <span class="op">*</span> de_dd <span class="op">*</span> dd_da</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df_da)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[0.9807642  0.68482974 0.4809319 ]
 [0.39211752 0.34317802 0.72904971]]</code></pre>
</div>
</div>
<p>Here’s the same example in PyTorch. Using <code>x.backward()</code>, gradients with respect to <code>x</code> are computed for variables connected to <code>x</code>.</p>
<div id="df3c3c86" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">123</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>H, W <span class="op">=</span> <span class="dv">2</span>, <span class="dv">3</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.tensor(a, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.tensor(b, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> torch.tensor(c, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> a <span class="op">*</span> b</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>e <span class="op">=</span> d <span class="op">+</span> c</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> e.<span class="bu">sum</span>()</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>f.backward()</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(a.grad)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[0.9808, 0.6848, 0.4809],
        [0.3921, 0.3432, 0.7290]], dtype=torch.float64)</code></pre>
</div>
</div>
<p>Here are the nodes of the computational graph.</p>
<div id="bdd5c07e" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchviz <span class="im">import</span> make_dot</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>make_dot(f, params<span class="op">=</span>{<span class="st">'a'</span>: a, <span class="st">'b'</span>: b, <span class="st">'c'</span>: c, <span class="st">'f'</span>:f , <span class="st">'d'</span>: d, <span class="st">'e'</span>:e })</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<div>
<figure class="figure">
<p><img src="frameworks_files/figure-html/cell-4-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>To perform the computation on a GPU, a simple instruction is enough:</p>
<div id="6cef798a" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Using </span><span class="sc">{</span>device<span class="sc">}</span><span class="ss"> device"</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> a.to(device<span class="op">=</span>device)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> b.to(device<span class="op">=</span>device)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> c.to(device<span class="op">=</span>device)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Using cpu device</code></pre>
</div>
</div>
</section>
</section>
<section id="pytorch" class="level1">
<h1>PyTorch</h1>
<p>In this class, we use PyTorch. PyTorch has gained enormous popularity in recent years and stands out for its high flexibility, a clean API, and many open-source resources.</p>
<section id="fundamental-concepts" class="level2">
<h2 class="anchored" data-anchor-id="fundamental-concepts">Fundamental Concepts</h2>
<ul>
<li>Tensor: N-dimensional array, similar to <a href="https://numpy.org/doc/stable/reference/generated/numpy.array.html"><code>numpy.array</code></a></li>
<li>Autograd: Functionality to create computational graphs and compute gradients.</li>
<li>Module: Class to define components of neural networks</li>
</ul>
</section>
<section id="tensors" class="level2">
<h2 class="anchored" data-anchor-id="tensors">Tensors</h2>
<p><a href="https://pytorch.org/docs/stable/tensors.html"><code>torch.Tensor</code></a> is the central data structure in PyTorch. Essentially very similar to <code>numpy.array</code>, it can be easily loaded onto GPUs.</p>
<p>Tensors can be created in various ways. For example, from lists:</p>
<div id="01194edc" class="cell" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> [[<span class="dv">1</span>, <span class="dv">2</span>],[<span class="dv">3</span>, <span class="dv">4</span>]]</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>x_data <span class="op">=</span> torch.tensor(data)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x_data)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[1, 2],
        [3, 4]])</code></pre>
</div>
</div>
<p>Or from numpy.ndarray:</p>
<div id="76a99e9e" class="cell" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>np_array <span class="op">=</span> np.array(data)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>x_np <span class="op">=</span> torch.from_numpy(np_array)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x_np)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[1, 2],
        [3, 4]])</code></pre>
</div>
</div>
<p>Or from other tensors:</p>
<div id="a0d02e7c" class="cell" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>x_ones <span class="op">=</span> torch.ones_like(x_data) <span class="co"># retains the properties of x_data</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Ones Tensor: </span><span class="ch">\n</span><span class="ss"> </span><span class="sc">{</span>x_ones<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>x_rand <span class="op">=</span> torch.rand_like(x_data, dtype<span class="op">=</span>torch.<span class="bu">float</span>) <span class="co"># overrides the datatype of x_data</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Random Tensor: </span><span class="ch">\n</span><span class="ss"> </span><span class="sc">{</span>x_rand<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Ones Tensor: 
 tensor([[1, 1],
        [1, 1]]) 

Random Tensor: 
 tensor([[0.3688, 0.6850],
        [0.7611, 0.8019]]) 
</code></pre>
</div>
</div>
<p>Or with randomly generated numbers or constants:</p>
<div id="b613f40c" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>shape <span class="op">=</span> (<span class="dv">2</span>,<span class="dv">3</span>,)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>rand_tensor <span class="op">=</span> torch.rand(shape)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>ones_tensor <span class="op">=</span> torch.ones(shape)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>zeros_tensor <span class="op">=</span> torch.zeros(shape)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Random Tensor: </span><span class="ch">\n</span><span class="ss"> </span><span class="sc">{</span>rand_tensor<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Ones Tensor: </span><span class="ch">\n</span><span class="ss"> </span><span class="sc">{</span>ones_tensor<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Zeros Tensor: </span><span class="ch">\n</span><span class="ss"> </span><span class="sc">{</span>zeros_tensor<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Random Tensor: 
 tensor([[0.7244, 0.8384, 0.5232],
        [0.8262, 0.3556, 0.2649]]) 

Ones Tensor: 
 tensor([[1., 1., 1.],
        [1., 1., 1.]]) 

Zeros Tensor: 
 tensor([[0., 0., 0.],
        [0., 0., 0.]])</code></pre>
</div>
</div>
<p>Tensor attributes:</p>
<div id="d5309c87" class="cell" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>tensor <span class="op">=</span> torch.rand(<span class="dv">3</span>,<span class="dv">4</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Shape of tensor: </span><span class="sc">{</span>tensor<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Datatype of tensor: </span><span class="sc">{</span>tensor<span class="sc">.</span>dtype<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Device tensor is stored on: </span><span class="sc">{</span>tensor<span class="sc">.</span>device<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Shape of tensor: torch.Size([3, 4])
Datatype of tensor: torch.float32
Device tensor is stored on: cpu</code></pre>
</div>
</div>
<p>There are over 100 operations that can be performed on a tensor. The full list is available <a href="https://pytorch.org/docs/stable/torch.html">here</a>.</p>
<p>Indexing and Slicing:</p>
<div id="9c711d65" class="cell" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>tensor <span class="op">=</span> torch.ones(<span class="dv">4</span>, <span class="dv">4</span>)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"First row: </span><span class="sc">{</span>tensor[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"First column: </span><span class="sc">{</span>tensor[:, <span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Last column: </span><span class="sc">{</span>tensor[:, <span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>tensor[:,<span class="dv">1</span>] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tensor)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>First row: tensor([1., 1., 1., 1.])
First column: tensor([1., 1., 1., 1.])
Last column: tensor([1., 1., 1., 1.])
tensor([[1., 0., 1., 1.],
        [1., 0., 1., 1.],
        [1., 0., 1., 1.],
        [1., 0., 1., 1.]])</code></pre>
</div>
</div>
<p>Joining tensors:</p>
<div id="f4bd39e4" class="cell" data-execution_count="11">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>t1 <span class="op">=</span> torch.cat([tensor, tensor, tensor], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(t1)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],
        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],
        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],
        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])</code></pre>
</div>
</div>
<p>Arithmetic operations:</p>
<div id="b92f9c71" class="cell" data-execution_count="12">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>y1 <span class="op">=</span> tensor <span class="op">@</span> tensor.T</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>y2 <span class="op">=</span> tensor.matmul(tensor.T)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>y3 <span class="op">=</span> torch.rand_like(y1)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>torch.matmul(tensor, tensor.T, out<span class="op">=</span>y3)</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="co"># This computes the element-wise product. z1, z2, z3 will have the same value</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>z1 <span class="op">=</span> tensor <span class="op">*</span> tensor</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>z2 <span class="op">=</span> tensor.mul(tensor)</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>z3 <span class="op">=</span> torch.rand_like(tensor)</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>torch.mul(tensor, tensor, out<span class="op">=</span>z3)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>tensor([[1., 0., 1., 1.],
        [1., 0., 1., 1.],
        [1., 0., 1., 1.],
        [1., 0., 1., 1.]])</code></pre>
</div>
</div>
</section>
<section id="autograd" class="level2">
<h2 class="anchored" data-anchor-id="autograd">Autograd</h2>
<p>To train neural networks, backpropagation is typically used. This calculates the gradient of the loss function with respect to the model parameters. To compute these gradients, PyTorch provides an <em>auto-diff</em> functionality: <a href="https://pytorch.org/docs/stable/autograd.html"><code>torch.autograd</code></a>. This can automatically compute gradients for a <em>computational graph</em>.</p>
<p>The following is an example using a 1-layer neural network (see <a href="#fig-infrastructure-comp-graph" class="quarto-xref">Figure&nbsp;3</a> ):</p>
<div id="fig-infrastructure-comp-graph" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-infrastructure-comp-graph-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/frameworks/comp-graph.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-infrastructure-comp-graph-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Source: <a href="https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html">PyTorch</a>
</figcaption>
</figure>
</div>
<p>Here is the definition of the network in PyTorch:</p>
<div id="2f26b9e7" class="cell" data-execution_count="13">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.ones(<span class="dv">5</span>)  <span class="co"># input tensor</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.zeros(<span class="dv">3</span>)  <span class="co"># expected output</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> torch.randn(<span class="dv">5</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>, <span class="dv">3</span>, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.randn(<span class="dv">3</span>, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> torch.matmul(x, w)<span class="op">+</span>b</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> torch.nn.functional.binary_cross_entropy_with_logits(z, y)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>We can now use Autograd to compute the gradient:</p>
<div id="f0006078" class="cell" data-execution_count="14">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>loss.backward()</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(w.grad)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(b.grad)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[0.2619, 0.0016, 0.0136],
        [0.2619, 0.0016, 0.0136],
        [0.2619, 0.0016, 0.0136],
        [0.2619, 0.0016, 0.0136],
        [0.2619, 0.0016, 0.0136]])
tensor([0.2619, 0.0016, 0.0136])</code></pre>
</div>
</div>
</section>
<section id="torch.nn" class="level2">
<h2 class="anchored" data-anchor-id="torch.nn">torch.nn</h2>
<p>PyTorch provides various building blocks for creating neural networks. These are available in <a href="https://pytorch.org/docs/stable/nn.html"><code>torch.nn</code></a>. Additionally, you can define any compositions of such building blocks that inherit from <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module"><code>torch.nn.Module</code></a>. A neural network is typically a <code>torch.nn.Module</code>. Each module implements the <code>forward()</code> method to define how data is processed.</p>
<p>Here is an example:</p>
<div id="719c70a5" class="cell" data-execution_count="15">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NeuralNetwork(nn.Module):</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(NeuralNetwork, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.flatten <span class="op">=</span> nn.Flatten()</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear_relu_stack <span class="op">=</span> nn.Sequential(</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">28</span><span class="op">*</span><span class="dv">28</span>, <span class="dv">512</span>),</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">512</span>, <span class="dv">512</span>),</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">512</span>, <span class="dv">10</span>),</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.flatten(x)</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> <span class="va">self</span>.linear_relu_stack(x)</span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> logits</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>You can also visualize the model:</p>
<div id="af8261f7" class="cell" data-execution_count="16">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> NeuralNetwork()</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>NeuralNetwork(
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (linear_relu_stack): Sequential(
    (0): Linear(in_features=784, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=512, bias=True)
    (3): ReLU()
    (4): Linear(in_features=512, out_features=10, bias=True)
  )
)</code></pre>
</div>
</div>
<p>To use the model, you can pass input data. This will execute the <code>forward()</code> method, along with background operations.</p>
<div id="e62ea611" class="cell" data-execution_count="17">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> torch.rand(<span class="dv">1</span>, <span class="dv">28</span>, <span class="dv">28</span>)</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> model(X)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>pred_probab <span class="op">=</span> nn.Softmax(dim<span class="op">=</span><span class="dv">1</span>)(logits)</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> pred_probab.argmax(<span class="dv">1</span>)</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Predicted class: </span><span class="sc">{</span>y_pred<span class="sc">}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Predicted class: tensor([9])</code></pre>
</div>
</div>
<p>The executed operations will look like this:</p>
<div id="195f36f1" class="cell" data-execution_count="18">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchviz <span class="im">import</span> make_dot</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>make_dot(logits)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<div>
<figure class="figure">
<p><img src="frameworks_files/figure-html/cell-19-output-1.svg" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="torch.optim" class="level2">
<h2 class="anchored" data-anchor-id="torch.optim">torch.optim</h2>
<p>To optimize the parameters of a model, you need an optimization algorithm. <a href="https://pytorch.org/docs/stable/optim.html"><code>torch.optim</code></a> implements various algorithms, such as <em>Stochastic Gradient Descent</em> or the often used <em>Adam Optimizer</em>.</p>
<div id="0eda591c" class="cell" data-execution_count="19">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> optim</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>, momentum<span class="op">=</span><span class="fl">0.9</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>You can then use the optimizer to adjust the parameters, you just need to define a loss function:</p>
<div id="b23d6a22" class="cell" data-execution_count="20">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> torch.nn.CrossEntropyLoss()</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">3</span>):</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">input</span>, target <span class="op">=</span> torch.rand(<span class="dv">1</span>, <span class="dv">28</span>, <span class="dv">28</span>), torch.randint(low<span class="op">=</span><span class="dv">0</span>, high<span class="op">=</span><span class="dv">10</span>, size<span class="op">=</span>(<span class="dv">1</span>, ))</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> model(<span class="bu">input</span>)</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> loss_fn(output, target)</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note <code>optimizer.zero_grad()</code> which resets the accumulated gradients of the variables to 0.</p>
</div>
</div>
</section>
<section id="training-loops" class="level2">
<h2 class="anchored" data-anchor-id="training-loops">Training Loops</h2>
<p>Typically, you put together a training loop to train a model. A training loop iterates over batches of data and optimizes the model parameters with each iteration.</p>
<div id="6d3422c1" class="cell" data-execution_count="21">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_loop(dataloader, model, loss_fn, optimizer):</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>    size <span class="op">=</span> <span class="bu">len</span>(dataloader.dataset)</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch, (X, y) <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute prediction and loss</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> model(X)</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_fn(pred, y)</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Backpropagation</span></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> batch <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>            loss, current <span class="op">=</span> loss.item(), batch <span class="op">*</span> <span class="bu">len</span>(X)</span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"loss: </span><span class="sc">{</span>loss<span class="sc">:&gt;7f}</span><span class="ss">  [</span><span class="sc">{</span>current<span class="sc">:&gt;5d}</span><span class="ss">/</span><span class="sc">{</span>size<span class="sc">:&gt;5d}</span><span class="ss">]"</span>)</span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_loop(dataloader, model, loss_fn):</span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a>    size <span class="op">=</span> <span class="bu">len</span>(dataloader.dataset)</span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a>    num_batches <span class="op">=</span> <span class="bu">len</span>(dataloader)</span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a>    test_loss, correct <span class="op">=</span> <span class="dv">0</span>, <span class="dv">0</span></span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> X, y <span class="kw">in</span> dataloader:</span>
<span id="cb35-25"><a href="#cb35-25" aria-hidden="true" tabindex="-1"></a>            pred <span class="op">=</span> model(X)</span>
<span id="cb35-26"><a href="#cb35-26" aria-hidden="true" tabindex="-1"></a>            test_loss <span class="op">+=</span> loss_fn(pred, y).item()</span>
<span id="cb35-27"><a href="#cb35-27" aria-hidden="true" tabindex="-1"></a>            correct <span class="op">+=</span> (pred.argmax(<span class="dv">1</span>) <span class="op">==</span> y).<span class="bu">type</span>(torch.<span class="bu">float</span>).<span class="bu">sum</span>().item()</span>
<span id="cb35-28"><a href="#cb35-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-29"><a href="#cb35-29" aria-hidden="true" tabindex="-1"></a>    test_loss <span class="op">/=</span> num_batches</span>
<span id="cb35-30"><a href="#cb35-30" aria-hidden="true" tabindex="-1"></a>    correct <span class="op">/=</span> size</span>
<span id="cb35-31"><a href="#cb35-31" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Test Error: </span><span class="ch">\n</span><span class="ss"> Accuracy: </span><span class="sc">{</span>(<span class="dv">100</span><span class="op">*</span>correct)<span class="sc">:&gt;0.1f}</span><span class="ss">%, Avg loss: </span><span class="sc">{</span>test_loss<span class="sc">:&gt;8f}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="5091f3a2" class="cell" data-execution_count="22">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(model.parameters(), lr<span class="op">=</span>learning_rate)</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>t<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ch">\n</span><span class="ss">-------------------------------"</span>)</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>    train_loop(train_dataloader, model, loss_fn, optimizer)</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>    test_loop(test_dataloader, model, loss_fn)</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Done!"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>High-level APIs such as <a href="https://lightning.ai/">Lightning</a> and <a href="https://keras.io/">Keras</a> provide many functionalities to simplify managing training loops. It is highly recommended to use such libraries to reduce boiler-plate code. However, it depends on the individual complexitiy of a project to what degree such libraries are useful.</p>
</div>
</div>
</section>
<section id="pre-trained-models" class="level2">
<h2 class="anchored" data-anchor-id="pre-trained-models">Pre-trained models</h2>
<p>Since training models can be time-consuming and expensive, pre-trained models are often used. They allow models to be adapted to a specific task more quickly and cost-effectively. In many areas, particularly NLP and computer vision, using pre-trained models is standard. PyTorch provides <a href="https://pytorch.org/vision/stable/index.html"><code>torchvision</code></a> for computer vision applications. <code>torchvision</code> provides functionalities useful for modeling image data. Pre-trained models can also be easily integrated, as shown in the following example:</p>
<div id="7fb45608" class="cell" data-execution_count="23">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.models <span class="im">import</span> resnet50, ResNet50_Weights</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> ResNet50_Weights.IMAGENET1K_V2</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> resnet50(weights<span class="op">=</span>weights)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
</section>
<section id="other-frameworks" class="level1">
<h1>Other Frameworks</h1>
<p>Other important frameworks are listed below (not exhaustive).</p>
<section id="tensorflow" class="level2">
<h2 class="anchored" data-anchor-id="tensorflow">TensorFlow</h2>
<p>For a long time, PyTorch and TensorFlow have been the biggest deep learning frameworks. TensorFlow stands out with a clean high-level API with <a href="https://keras.io/"><code>Keras</code></a>, which allows for easy implementation of complex models. Traditionally, TensorFlow is well established in the industry, while PyTorch is widely used in academia.</p>
</section>
<section id="jax" class="level2">
<h2 class="anchored" data-anchor-id="jax">Jax</h2>
<p><a href="https://jax.readthedocs.io/en/latest/">Jax</a> has gained much popularity in recent years. It has been developed by people working for Google and is being used primarily in research settings. It is aking to <code>numpy</code>, however, with automatic-differentiation.</p>
</section>
<section id="scikit-learn" class="level2">
<h2 class="anchored" data-anchor-id="scikit-learn">Scikit-Learn</h2>
<p><a href="https://scikit-learn.org/stable/"><code>Scikit-Learn</code></a> is THE machine learning framework in Python. However, Scikit-Learn never covered the area of neural networks and lacks auto-diff functionality. Therefore, Scikit-Learn is irrelevant when training neural networks. However, Scikit-Learn functionalities are often used to carry out the machine learning process, such as splitting datasets into train, validation, and test sets. Also, visualizations, such as the confusion matrix or calculating metrics, can be done via Scikit-Learn.</p>
</section>
<section id="onnx" class="level2">
<h2 class="anchored" data-anchor-id="onnx">ONNX</h2>
<p><a href="https://onnx.ai/">ONNX</a> is an open format to represent machine learning models. It allows models trained in one framework to be transferred to another. Trained models can also be deployed on various platforms.</p>
</section>
<section id="monitoring" class="level2">
<h2 class="anchored" data-anchor-id="monitoring">Monitoring</h2>
<p>When training models, monitoring the training process, debugging, and logging hyperparameters, metrics, etc., is very important. Various tools enable these functionalities. Well-known examples are <a href="https://www.tensorflow.org/tensorboard"><code>TensorBoard</code></a> and <a href="https://wandb.ai/site"><code>Weights &amp; Biases</code></a>.</p>
</section>
</section>
<section id="hardware" class="level1">
<h1>Hardware</h1>
<section id="tensor-operations" class="level2">
<h2 class="anchored" data-anchor-id="tensor-operations">Tensor Operations</h2>
<p>In neural networks, there are many tensor operations. Tensors are essentially multi-dimensional arrays, such as a scalar <span class="math inline">\(x\)</span>, a vector <span class="math inline">\(\mathbf{x}\)</span>, or a matrix <span class="math inline">\(\mathbf{X}\)</span>.</p>
<p><a href="#fig-infrastructure-matrix-mult" class="quarto-xref">Figure&nbsp;4</a> illustrates a matrix multiplication, a typical representative of a tensor operation. As you can see, the calculations (entries of the matrix <span class="math inline">\(\mathbf{A}\mathbf{C}\)</span>) are independent of each other and can be fully parallelized.</p>
<div id="fig-infrastructure-matrix-mult" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-infrastructure-matrix-mult-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/frameworks/matrix_mult.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-infrastructure-matrix-mult-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Matrix Multiplication (from <span class="citation" data-cites="li_cs231n_2022">Li (<a href="#ref-li_cs231n_2022" role="doc-biblioref">2022</a>)</span>).
</figcaption>
</figure>
</div>
</section>
<section id="graphics-processing-units-gpus" class="level2">
<h2 class="anchored" data-anchor-id="graphics-processing-units-gpus">Graphics Processing Units (GPUs)</h2>
<p>GPUs have made deep learning possible in the first place. With their parallel structure, they can efficiently compute parallelizable tasks such as tensor operations.</p>
<p>CPUs have far fewer cores than GPUs, but they are faster and can handle more complex tasks. CPUs are therefore ideal for sequential tasks. GPUs have many more cores, which are less complex and slower. Therefore, GPUs are excellent for parallel tasks. <a href="#fig-infrastructure-cpu-vs-gpu" class="quarto-xref">Figure&nbsp;5</a> illustrates the differences.</p>
<div id="fig-infrastructure-cpu-vs-gpu" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-infrastructure-cpu-vs-gpu-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/frameworks/cpu_vs_gpu.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-infrastructure-cpu-vs-gpu-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: CPU vs GPU example (from <span class="citation" data-cites="li_cs231n_2022">Li (<a href="#ref-li_cs231n_2022" role="doc-biblioref">2022</a>)</span>).
</figcaption>
</figure>
</div>
</section>
<section id="cuda-cudnn" class="level2">
<h2 class="anchored" data-anchor-id="cuda-cudnn">CUDA &amp; cuDNN</h2>
<p><a href="https://developer.nvidia.com/cuda-toolkit">CUDA</a> is an API by Nvidia to perform computations on the GPU. It allows parallelizable tasks to be implemented efficiently. <a href="https://developer.nvidia.com/cudnn">cuDNN</a> is a library that efficiently executes certain operations, such as convolutions, in neural networks on the GPU. cuDNN is based on CUDA and significantly accelerates the training of neural networks. <a href="#fig-infrastructure-speed-gpu-cpu" class="quarto-xref">Figure&nbsp;6</a> illustrates speed differences when training various neural networks with CPU, GPU, and optimized cuDNN.</p>
<div id="fig-infrastructure-speed-gpu-cpu" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-infrastructure-speed-gpu-cpu-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/frameworks/speed_gpu_cpu.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-infrastructure-speed-gpu-cpu-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Speed comparison (from <span class="citation" data-cites="li_cs231n_2022">Li (<a href="#ref-li_cs231n_2022" role="doc-biblioref">2022</a>)</span>, data from <a href="https://github.com/jcjohnson/cnn-benchmarks">Link</a>)
</figcaption>
</figure>
</div>
</section>
<section id="data-loading" class="level2">
<h2 class="anchored" data-anchor-id="data-loading">Data Loading</h2>
<p>A crucial bottleneck in practice is the transfer of data (such as images) from the disk to the GPU. If this transfer is not fast enough, it is referred to as <em>GPU starvation</em>. There are several approaches to solve this problem:</p>
<ul>
<li>Read the data into RAM (not feasible for larger datasets)</li>
<li>Use fast disks, such as SSDs</li>
<li>Utilize multiple CPU threads to read data in parallel and keep it in RAM (<em>pre-fetching</em>)</li>
</ul>
<p><a href="#fig-infrastructure-computer" class="quarto-xref">Figure&nbsp;7</a> shows the various components.</p>
<div id="fig-infrastructure-computer" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-infrastructure-computer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/frameworks/computer.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-infrastructure-computer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Source: <span class="citation" data-cites="li_cs231n_2022">Li (<a href="#ref-li_cs231n_2022" role="doc-biblioref">2022</a>)</span>
</figcaption>
</figure>
</div>
<p>Deep learning frameworks like PyTorch implement special classes that allow data to be prepared in multiple threads. Sometimes a certain number of CPU cores is needed to supply a GPU with enough data. <a href="#fig-infrastructure-gpu-starvation" class="quarto-xref">Figure&nbsp;8</a> shows a starved GPU: You can clearly see that the utilization repeatedly drops to 0 because the GPU has to wait for data.</p>
<div id="fig-infrastructure-gpu-starvation" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-infrastructure-gpu-starvation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/frameworks/gpu_starvation.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-infrastructure-gpu-starvation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: The Y-axis shows the GPU utilization in percentage, while the X-axis represents time. <a href="https://stackoverflow.com/questions/44598246/tensorflow-data-starved-gpu">Source</a>
</figcaption>
</figure>
</div>
</section>
<section id="gpu-parallelism" class="level2">
<h2 class="anchored" data-anchor-id="gpu-parallelism">GPU Parallelism</h2>
<p>Models can also be trained on multiple GPUs. There are two main paradigms: <em>data parallelism</em> and <em>model parallelism</em> (see <a href="#fig-infrastructure-parallelism" class="quarto-xref">Figure&nbsp;9</a> ). With <em>data parallelism</em>, each GPU has a copy of the model, and each GPU is trained on different data batches. With <em>model parallelism</em>, the model is split across multiple GPUs. Models can be trained on a server with multiple GPUs or even over the network (<em>distributed</em>). ML frameworks provide functionalities to handle these.</p>
<div id="fig-infrastructure-parallelism" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-infrastructure-parallelism-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../assets/images/frameworks/parallelism.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-infrastructure-parallelism-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Data and Model Parallelism (from <span class="citation" data-cites="li_cs231n_2022">Li (<a href="#ref-li_cs231n_2022" role="doc-biblioref">2022</a>)</span>).
</figcaption>
</figure>
</div>
</section>
</section>
<section id="references" class="level1">
<h1>References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-li_cs231n_2022" class="csl-entry" role="listitem">
Li, Fei-Fei. 2022. <span>“<span>CS231n</span> <span>Convolutional</span> <span>Neural</span> <span>Networks</span> for <span>Visual</span> <span>Recognition</span>.”</span> Lecture {Notes}. <a href="https://cs231n.github.io">https://cs231n.github.io</a>.
</div>
</div>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/marco-willi\.github\.io\/cas-dl-compvis-lectures-hs2025\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../lectures/intro.html" class="pagination-link" aria-label="1 - Introduction">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">1 - Introduction</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../lectures/neural_networks.html" class="pagination-link" aria-label="3 - Neural Networks">
        <span class="nav-page-text">3 - Neural Networks</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb38" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "2 - Software &amp; Hardware for Deep Learning"</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="an">params:</span></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a><span class="co">   images_path: "/assets/images/frameworks/"</span></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>::: {.content-hidden}</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>{{&lt; include /assets/_macros.tex &gt;}}</span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a><span class="fu"># Deep Learning Software</span></span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a>There are a variety of Deep Learning frameworks. These frameworks allow for easy configuration, training, and deploying of neural networks. They are often developed via Python API. @fig-infrastructure-frameworks shows some frameworks.</span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a>::: {#fig-infrastructure-frameworks}</span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}frameworks.png)</span>{width=600}</span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a>Frameworks (from @li_cs231n_2022).</span>
<span id="cb38-21"><a href="#cb38-21" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb38-22"><a href="#cb38-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-23"><a href="#cb38-23" aria-hidden="true" tabindex="-1"></a>Key features of such frameworks are:</span>
<span id="cb38-24"><a href="#cb38-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-25"><a href="#cb38-25" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Fast development and testing of neural networks</span>
<span id="cb38-26"><a href="#cb38-26" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Automatic differentiation of operations</span>
<span id="cb38-27"><a href="#cb38-27" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Efficient execution on diverse hardware</span>
<span id="cb38-28"><a href="#cb38-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-29"><a href="#cb38-29" aria-hidden="true" tabindex="-1"></a><span class="fu">## Computational Graph &amp; Autograd</span></span>
<span id="cb38-30"><a href="#cb38-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-31"><a href="#cb38-31" aria-hidden="true" tabindex="-1"></a>At the core of neural networks is the _Computational Graph_. It automatically embeds dependent operations in a _directed acyclic graph (DAG)_. Gradients are tracked as needed, allowing variables to be efficiently updated/trained.</span>
<span id="cb38-32"><a href="#cb38-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-33"><a href="#cb38-33" aria-hidden="true" tabindex="-1"></a>The following shows an example in Numpy where we define computations and manually calculate derivatives. The graph is shown in @fig-infrastructure-comp-graph2.</span>
<span id="cb38-34"><a href="#cb38-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-35"><a href="#cb38-35" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb38-36"><a href="#cb38-36" aria-hidden="true" tabindex="-1"></a>    f(\mathbf{A}, \mathbf{B}, \mathbf{C}) =  \sum_{ij} \big((\mathbf{A} \odot \mathbf{B}) + \mathbf{C}\big)_{ij}</span>
<span id="cb38-37"><a href="#cb38-37" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb38-38"><a href="#cb38-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-39"><a href="#cb38-39" aria-hidden="true" tabindex="-1"></a>::: {#fig-infrastructure-comp-graph2}</span>
<span id="cb38-40"><a href="#cb38-40" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}comp-graph2.jpg)</span>{width=200}</span>
<span id="cb38-41"><a href="#cb38-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-42"><a href="#cb38-42" aria-hidden="true" tabindex="-1"></a>Computational Graph.</span>
<span id="cb38-43"><a href="#cb38-43" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb38-44"><a href="#cb38-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-47"><a href="#cb38-47" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb38-48"><a href="#cb38-48" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb38-49"><a href="#cb38-49" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb38-50"><a href="#cb38-50" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb38-51"><a href="#cb38-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-52"><a href="#cb38-52" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">123</span>)</span>
<span id="cb38-53"><a href="#cb38-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-54"><a href="#cb38-54" aria-hidden="true" tabindex="-1"></a>H, W <span class="op">=</span> <span class="dv">2</span>, <span class="dv">3</span></span>
<span id="cb38-55"><a href="#cb38-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-56"><a href="#cb38-56" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> np.random.random(size<span class="op">=</span>(H, W))</span>
<span id="cb38-57"><a href="#cb38-57" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> np.random.random(size<span class="op">=</span>(H, W))</span>
<span id="cb38-58"><a href="#cb38-58" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> np.random.random(size<span class="op">=</span>(H, W))</span>
<span id="cb38-59"><a href="#cb38-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-60"><a href="#cb38-60" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> a <span class="op">*</span> b</span>
<span id="cb38-61"><a href="#cb38-61" aria-hidden="true" tabindex="-1"></a>e <span class="op">=</span> d <span class="op">+</span> c</span>
<span id="cb38-62"><a href="#cb38-62" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> e.<span class="bu">sum</span>()</span>
<span id="cb38-63"><a href="#cb38-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-64"><a href="#cb38-64" aria-hidden="true" tabindex="-1"></a>df_de <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb38-65"><a href="#cb38-65" aria-hidden="true" tabindex="-1"></a>de_dd <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb38-66"><a href="#cb38-66" aria-hidden="true" tabindex="-1"></a>de_dc <span class="op">=</span> c</span>
<span id="cb38-67"><a href="#cb38-67" aria-hidden="true" tabindex="-1"></a>dd_da <span class="op">=</span> b</span>
<span id="cb38-68"><a href="#cb38-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-69"><a href="#cb38-69" aria-hidden="true" tabindex="-1"></a>df_da <span class="op">=</span> df_de <span class="op">*</span> de_dd <span class="op">*</span> dd_da</span>
<span id="cb38-70"><a href="#cb38-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-71"><a href="#cb38-71" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df_da)</span>
<span id="cb38-72"><a href="#cb38-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-73"><a href="#cb38-73" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-74"><a href="#cb38-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-75"><a href="#cb38-75" aria-hidden="true" tabindex="-1"></a>Here's the same example in PyTorch. Using <span class="in">`x.backward()`</span>, gradients with respect to <span class="in">`x`</span> are computed for variables connected to <span class="in">`x`</span>.</span>
<span id="cb38-76"><a href="#cb38-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-79"><a href="#cb38-79" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb38-80"><a href="#cb38-80" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb38-81"><a href="#cb38-81" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb38-82"><a href="#cb38-82" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb38-83"><a href="#cb38-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-84"><a href="#cb38-84" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">123</span>)</span>
<span id="cb38-85"><a href="#cb38-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-86"><a href="#cb38-86" aria-hidden="true" tabindex="-1"></a>H, W <span class="op">=</span> <span class="dv">2</span>, <span class="dv">3</span></span>
<span id="cb38-87"><a href="#cb38-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-88"><a href="#cb38-88" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.tensor(a, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb38-89"><a href="#cb38-89" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.tensor(b, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb38-90"><a href="#cb38-90" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> torch.tensor(c, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb38-91"><a href="#cb38-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-92"><a href="#cb38-92" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> a <span class="op">*</span> b</span>
<span id="cb38-93"><a href="#cb38-93" aria-hidden="true" tabindex="-1"></a>e <span class="op">=</span> d <span class="op">+</span> c</span>
<span id="cb38-94"><a href="#cb38-94" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> e.<span class="bu">sum</span>()</span>
<span id="cb38-95"><a href="#cb38-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-96"><a href="#cb38-96" aria-hidden="true" tabindex="-1"></a>f.backward()</span>
<span id="cb38-97"><a href="#cb38-97" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(a.grad)</span>
<span id="cb38-98"><a href="#cb38-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-99"><a href="#cb38-99" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-100"><a href="#cb38-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-101"><a href="#cb38-101" aria-hidden="true" tabindex="-1"></a>Here are the nodes of the computational graph.</span>
<span id="cb38-102"><a href="#cb38-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-103"><a href="#cb38-103" aria-hidden="true" tabindex="-1"></a>::: {.content-hidden unless-format="html"}</span>
<span id="cb38-106"><a href="#cb38-106" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb38-107"><a href="#cb38-107" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb38-108"><a href="#cb38-108" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb38-109"><a href="#cb38-109" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchviz <span class="im">import</span> make_dot</span>
<span id="cb38-110"><a href="#cb38-110" aria-hidden="true" tabindex="-1"></a>make_dot(f, params<span class="op">=</span>{<span class="st">'a'</span>: a, <span class="st">'b'</span>: b, <span class="st">'c'</span>: c, <span class="st">'f'</span>:f , <span class="st">'d'</span>: d, <span class="st">'e'</span>:e })</span>
<span id="cb38-111"><a href="#cb38-111" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-112"><a href="#cb38-112" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb38-113"><a href="#cb38-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-114"><a href="#cb38-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-115"><a href="#cb38-115" aria-hidden="true" tabindex="-1"></a>To perform the computation on a GPU, a simple instruction is enough:</span>
<span id="cb38-116"><a href="#cb38-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-119"><a href="#cb38-119" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb38-120"><a href="#cb38-120" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb38-121"><a href="#cb38-121" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb38-122"><a href="#cb38-122" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb38-123"><a href="#cb38-123" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Using </span><span class="sc">{</span>device<span class="sc">}</span><span class="ss"> device"</span>)</span>
<span id="cb38-124"><a href="#cb38-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-125"><a href="#cb38-125" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> a.to(device<span class="op">=</span>device)</span>
<span id="cb38-126"><a href="#cb38-126" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> b.to(device<span class="op">=</span>device)</span>
<span id="cb38-127"><a href="#cb38-127" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> c.to(device<span class="op">=</span>device)</span>
<span id="cb38-128"><a href="#cb38-128" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-129"><a href="#cb38-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-130"><a href="#cb38-130" aria-hidden="true" tabindex="-1"></a><span class="fu"># PyTorch</span></span>
<span id="cb38-131"><a href="#cb38-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-132"><a href="#cb38-132" aria-hidden="true" tabindex="-1"></a>In this class, we use PyTorch. PyTorch has gained enormous popularity in recent years and stands out for its high flexibility, a clean API, and many open-source resources.</span>
<span id="cb38-133"><a href="#cb38-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-134"><a href="#cb38-134" aria-hidden="true" tabindex="-1"></a><span class="fu">## Fundamental Concepts</span></span>
<span id="cb38-135"><a href="#cb38-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-136"><a href="#cb38-136" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Tensor: N-dimensional array, similar to <span class="co">[</span><span class="ot">`numpy.array`</span><span class="co">](https://numpy.org/doc/stable/reference/generated/numpy.array.html)</span></span>
<span id="cb38-137"><a href="#cb38-137" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Autograd: Functionality to create computational graphs and compute gradients.</span>
<span id="cb38-138"><a href="#cb38-138" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Module: Class to define components of neural networks</span>
<span id="cb38-139"><a href="#cb38-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-140"><a href="#cb38-140" aria-hidden="true" tabindex="-1"></a><span class="fu">## Tensors</span></span>
<span id="cb38-141"><a href="#cb38-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-142"><a href="#cb38-142" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">`torch.Tensor`</span><span class="co">](https://pytorch.org/docs/stable/tensors.html)</span> is the central data structure in PyTorch. Essentially very similar to <span class="in">`numpy.array`</span>, it can be easily loaded onto GPUs.</span>
<span id="cb38-143"><a href="#cb38-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-144"><a href="#cb38-144" aria-hidden="true" tabindex="-1"></a>Tensors can be created in various ways. For example, from lists:</span>
<span id="cb38-145"><a href="#cb38-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-148"><a href="#cb38-148" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb38-149"><a href="#cb38-149" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb38-150"><a href="#cb38-150" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb38-151"><a href="#cb38-151" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> [[<span class="dv">1</span>, <span class="dv">2</span>],[<span class="dv">3</span>, <span class="dv">4</span>]]</span>
<span id="cb38-152"><a href="#cb38-152" aria-hidden="true" tabindex="-1"></a>x_data <span class="op">=</span> torch.tensor(data)</span>
<span id="cb38-153"><a href="#cb38-153" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x_data)</span>
<span id="cb38-154"><a href="#cb38-154" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-155"><a href="#cb38-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-156"><a href="#cb38-156" aria-hidden="true" tabindex="-1"></a>Or from numpy.ndarray:</span>
<span id="cb38-157"><a href="#cb38-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-160"><a href="#cb38-160" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb38-161"><a href="#cb38-161" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb38-162"><a href="#cb38-162" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb38-163"><a href="#cb38-163" aria-hidden="true" tabindex="-1"></a>np_array <span class="op">=</span> np.array(data)</span>
<span id="cb38-164"><a href="#cb38-164" aria-hidden="true" tabindex="-1"></a>x_np <span class="op">=</span> torch.from_numpy(np_array)</span>
<span id="cb38-165"><a href="#cb38-165" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x_np)</span>
<span id="cb38-166"><a href="#cb38-166" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-167"><a href="#cb38-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-168"><a href="#cb38-168" aria-hidden="true" tabindex="-1"></a>Or from other tensors:</span>
<span id="cb38-169"><a href="#cb38-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-172"><a href="#cb38-172" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb38-173"><a href="#cb38-173" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb38-174"><a href="#cb38-174" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb38-175"><a href="#cb38-175" aria-hidden="true" tabindex="-1"></a>x_ones <span class="op">=</span> torch.ones_like(x_data) <span class="co"># retains the properties of x_data</span></span>
<span id="cb38-176"><a href="#cb38-176" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Ones Tensor: </span><span class="ch">\n</span><span class="ss"> </span><span class="sc">{</span>x_ones<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb38-177"><a href="#cb38-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-178"><a href="#cb38-178" aria-hidden="true" tabindex="-1"></a>x_rand <span class="op">=</span> torch.rand_like(x_data, dtype<span class="op">=</span>torch.<span class="bu">float</span>) <span class="co"># overrides the datatype of x_data</span></span>
<span id="cb38-179"><a href="#cb38-179" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Random Tensor: </span><span class="ch">\n</span><span class="ss"> </span><span class="sc">{</span>x_rand<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb38-180"><a href="#cb38-180" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-181"><a href="#cb38-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-182"><a href="#cb38-182" aria-hidden="true" tabindex="-1"></a>Or with randomly generated numbers or constants:</span>
<span id="cb38-183"><a href="#cb38-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-186"><a href="#cb38-186" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb38-187"><a href="#cb38-187" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb38-188"><a href="#cb38-188" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb38-189"><a href="#cb38-189" aria-hidden="true" tabindex="-1"></a>shape <span class="op">=</span> (<span class="dv">2</span>,<span class="dv">3</span>,)</span>
<span id="cb38-190"><a href="#cb38-190" aria-hidden="true" tabindex="-1"></a>rand_tensor <span class="op">=</span> torch.rand(shape)</span>
<span id="cb38-191"><a href="#cb38-191" aria-hidden="true" tabindex="-1"></a>ones_tensor <span class="op">=</span> torch.ones(shape)</span>
<span id="cb38-192"><a href="#cb38-192" aria-hidden="true" tabindex="-1"></a>zeros_tensor <span class="op">=</span> torch.zeros(shape)</span>
<span id="cb38-193"><a href="#cb38-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-194"><a href="#cb38-194" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Random Tensor: </span><span class="ch">\n</span><span class="ss"> </span><span class="sc">{</span>rand_tensor<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb38-195"><a href="#cb38-195" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Ones Tensor: </span><span class="ch">\n</span><span class="ss"> </span><span class="sc">{</span>ones_tensor<span class="sc">}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb38-196"><a href="#cb38-196" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Zeros Tensor: </span><span class="ch">\n</span><span class="ss"> </span><span class="sc">{</span>zeros_tensor<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb38-197"><a href="#cb38-197" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-198"><a href="#cb38-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-199"><a href="#cb38-199" aria-hidden="true" tabindex="-1"></a>Tensor attributes:</span>
<span id="cb38-200"><a href="#cb38-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-203"><a href="#cb38-203" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb38-204"><a href="#cb38-204" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb38-205"><a href="#cb38-205" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb38-206"><a href="#cb38-206" aria-hidden="true" tabindex="-1"></a>tensor <span class="op">=</span> torch.rand(<span class="dv">3</span>,<span class="dv">4</span>)</span>
<span id="cb38-207"><a href="#cb38-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-208"><a href="#cb38-208" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Shape of tensor: </span><span class="sc">{</span>tensor<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb38-209"><a href="#cb38-209" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Datatype of tensor: </span><span class="sc">{</span>tensor<span class="sc">.</span>dtype<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb38-210"><a href="#cb38-210" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Device tensor is stored on: </span><span class="sc">{</span>tensor<span class="sc">.</span>device<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb38-211"><a href="#cb38-211" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-212"><a href="#cb38-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-213"><a href="#cb38-213" aria-hidden="true" tabindex="-1"></a>There are over 100 operations that can be performed on a tensor. The full list is available <span class="co">[</span><span class="ot">here</span><span class="co">](https://pytorch.org/docs/stable/torch.html)</span>.</span>
<span id="cb38-214"><a href="#cb38-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-215"><a href="#cb38-215" aria-hidden="true" tabindex="-1"></a>Indexing and Slicing:</span>
<span id="cb38-216"><a href="#cb38-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-219"><a href="#cb38-219" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb38-220"><a href="#cb38-220" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb38-221"><a href="#cb38-221" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb38-222"><a href="#cb38-222" aria-hidden="true" tabindex="-1"></a>tensor <span class="op">=</span> torch.ones(<span class="dv">4</span>, <span class="dv">4</span>)</span>
<span id="cb38-223"><a href="#cb38-223" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"First row: </span><span class="sc">{</span>tensor[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb38-224"><a href="#cb38-224" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"First column: </span><span class="sc">{</span>tensor[:, <span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb38-225"><a href="#cb38-225" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Last column: </span><span class="sc">{</span>tensor[:, <span class="op">-</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb38-226"><a href="#cb38-226" aria-hidden="true" tabindex="-1"></a>tensor[:,<span class="dv">1</span>] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb38-227"><a href="#cb38-227" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tensor)</span>
<span id="cb38-228"><a href="#cb38-228" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-229"><a href="#cb38-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-230"><a href="#cb38-230" aria-hidden="true" tabindex="-1"></a>Joining tensors:</span>
<span id="cb38-231"><a href="#cb38-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-234"><a href="#cb38-234" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb38-235"><a href="#cb38-235" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb38-236"><a href="#cb38-236" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb38-237"><a href="#cb38-237" aria-hidden="true" tabindex="-1"></a>t1 <span class="op">=</span> torch.cat([tensor, tensor, tensor], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb38-238"><a href="#cb38-238" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(t1)</span>
<span id="cb38-239"><a href="#cb38-239" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-240"><a href="#cb38-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-241"><a href="#cb38-241" aria-hidden="true" tabindex="-1"></a>Arithmetic operations:</span>
<span id="cb38-242"><a href="#cb38-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-245"><a href="#cb38-245" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb38-246"><a href="#cb38-246" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb38-247"><a href="#cb38-247" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb38-248"><a href="#cb38-248" aria-hidden="true" tabindex="-1"></a><span class="co"># This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value</span></span>
<span id="cb38-249"><a href="#cb38-249" aria-hidden="true" tabindex="-1"></a>y1 <span class="op">=</span> tensor <span class="op">@</span> tensor.T</span>
<span id="cb38-250"><a href="#cb38-250" aria-hidden="true" tabindex="-1"></a>y2 <span class="op">=</span> tensor.matmul(tensor.T)</span>
<span id="cb38-251"><a href="#cb38-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-252"><a href="#cb38-252" aria-hidden="true" tabindex="-1"></a>y3 <span class="op">=</span> torch.rand_like(y1)</span>
<span id="cb38-253"><a href="#cb38-253" aria-hidden="true" tabindex="-1"></a>torch.matmul(tensor, tensor.T, out<span class="op">=</span>y3)</span>
<span id="cb38-254"><a href="#cb38-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-255"><a href="#cb38-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-256"><a href="#cb38-256" aria-hidden="true" tabindex="-1"></a><span class="co"># This computes the element-wise product. z1, z2, z3 will have the same value</span></span>
<span id="cb38-257"><a href="#cb38-257" aria-hidden="true" tabindex="-1"></a>z1 <span class="op">=</span> tensor <span class="op">*</span> tensor</span>
<span id="cb38-258"><a href="#cb38-258" aria-hidden="true" tabindex="-1"></a>z2 <span class="op">=</span> tensor.mul(tensor)</span>
<span id="cb38-259"><a href="#cb38-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-260"><a href="#cb38-260" aria-hidden="true" tabindex="-1"></a>z3 <span class="op">=</span> torch.rand_like(tensor)</span>
<span id="cb38-261"><a href="#cb38-261" aria-hidden="true" tabindex="-1"></a>torch.mul(tensor, tensor, out<span class="op">=</span>z3)</span>
<span id="cb38-262"><a href="#cb38-262" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-263"><a href="#cb38-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-264"><a href="#cb38-264" aria-hidden="true" tabindex="-1"></a><span class="fu">## Autograd</span></span>
<span id="cb38-265"><a href="#cb38-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-266"><a href="#cb38-266" aria-hidden="true" tabindex="-1"></a>To train neural networks, backpropagation is typically used. This calculates the gradient of the loss function with respect to the model parameters. To compute these gradients, PyTorch provides an _auto-diff_ functionality:</span>
<span id="cb38-267"><a href="#cb38-267" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">`torch.autograd`</span><span class="co">](https://pytorch.org/docs/stable/autograd.html)</span>. This can automatically compute gradients for a _computational graph_.</span>
<span id="cb38-268"><a href="#cb38-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-269"><a href="#cb38-269" aria-hidden="true" tabindex="-1"></a>The following is an example using a 1-layer neural network (see @fig-infrastructure-comp-graph ):</span>
<span id="cb38-270"><a href="#cb38-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-271"><a href="#cb38-271" aria-hidden="true" tabindex="-1"></a>::: {#fig-infrastructure-comp-graph}</span>
<span id="cb38-272"><a href="#cb38-272" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}comp-graph.jpg)</span>{width=600}</span>
<span id="cb38-273"><a href="#cb38-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-274"><a href="#cb38-274" aria-hidden="true" tabindex="-1"></a>Source: <span class="co">[</span><span class="ot">PyTorch</span><span class="co">](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html)</span></span>
<span id="cb38-275"><a href="#cb38-275" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb38-276"><a href="#cb38-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-277"><a href="#cb38-277" aria-hidden="true" tabindex="-1"></a>Here is the definition of the network in PyTorch:</span>
<span id="cb38-278"><a href="#cb38-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-281"><a href="#cb38-281" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb38-282"><a href="#cb38-282" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb38-283"><a href="#cb38-283" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb38-284"><a href="#cb38-284" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb38-285"><a href="#cb38-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-286"><a href="#cb38-286" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.ones(<span class="dv">5</span>)  <span class="co"># input tensor</span></span>
<span id="cb38-287"><a href="#cb38-287" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.zeros(<span class="dv">3</span>)  <span class="co"># expected output</span></span>
<span id="cb38-288"><a href="#cb38-288" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> torch.randn(<span class="dv">5</span></span>
<span id="cb38-289"><a href="#cb38-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-290"><a href="#cb38-290" aria-hidden="true" tabindex="-1"></a>, <span class="dv">3</span>, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb38-291"><a href="#cb38-291" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.randn(<span class="dv">3</span>, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb38-292"><a href="#cb38-292" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> torch.matmul(x, w)<span class="op">+</span>b</span>
<span id="cb38-293"><a href="#cb38-293" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> torch.nn.functional.binary_cross_entropy_with_logits(z, y)</span>
<span id="cb38-294"><a href="#cb38-294" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-295"><a href="#cb38-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-296"><a href="#cb38-296" aria-hidden="true" tabindex="-1"></a>We can now use Autograd to compute the gradient:</span>
<span id="cb38-297"><a href="#cb38-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-300"><a href="#cb38-300" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb38-301"><a href="#cb38-301" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb38-302"><a href="#cb38-302" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb38-303"><a href="#cb38-303" aria-hidden="true" tabindex="-1"></a>loss.backward()</span>
<span id="cb38-304"><a href="#cb38-304" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(w.grad)</span>
<span id="cb38-305"><a href="#cb38-305" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(b.grad)</span>
<span id="cb38-306"><a href="#cb38-306" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-307"><a href="#cb38-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-308"><a href="#cb38-308" aria-hidden="true" tabindex="-1"></a><span class="fu">## torch.nn</span></span>
<span id="cb38-309"><a href="#cb38-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-310"><a href="#cb38-310" aria-hidden="true" tabindex="-1"></a>PyTorch provides various building blocks for creating neural networks. These are available in <span class="co">[</span><span class="ot">`torch.nn`</span><span class="co">](https://pytorch.org/docs/stable/nn.html)</span>. Additionally, you can define any compositions of such building blocks that inherit from <span class="co">[</span><span class="ot">`torch.nn.Module`</span><span class="co">](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module)</span>. A neural network is typically a <span class="in">`torch.nn.Module`</span>. Each module implements the <span class="in">`forward()`</span> method to define how data is processed.</span>
<span id="cb38-311"><a href="#cb38-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-312"><a href="#cb38-312" aria-hidden="true" tabindex="-1"></a>Here is an example:</span>
<span id="cb38-313"><a href="#cb38-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-316"><a href="#cb38-316" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb38-317"><a href="#cb38-317" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb38-318"><a href="#cb38-318" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb38-319"><a href="#cb38-319" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb38-320"><a href="#cb38-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-321"><a href="#cb38-321" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NeuralNetwork(nn.Module):</span>
<span id="cb38-322"><a href="#cb38-322" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb38-323"><a href="#cb38-323" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(NeuralNetwork, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb38-324"><a href="#cb38-324" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.flatten <span class="op">=</span> nn.Flatten()</span>
<span id="cb38-325"><a href="#cb38-325" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear_relu_stack <span class="op">=</span> nn.Sequential(</span>
<span id="cb38-326"><a href="#cb38-326" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">28</span><span class="op">*</span><span class="dv">28</span>, <span class="dv">512</span>),</span>
<span id="cb38-327"><a href="#cb38-327" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb38-328"><a href="#cb38-328" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">512</span>, <span class="dv">512</span>),</span>
<span id="cb38-329"><a href="#cb38-329" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb38-330"><a href="#cb38-330" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">512</span>, <span class="dv">10</span>),</span>
<span id="cb38-331"><a href="#cb38-331" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb38-332"><a href="#cb38-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-333"><a href="#cb38-333" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb38-334"><a href="#cb38-334" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.flatten(x)</span>
<span id="cb38-335"><a href="#cb38-335" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> <span class="va">self</span>.linear_relu_stack(x)</span>
<span id="cb38-336"><a href="#cb38-336" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> logits</span>
<span id="cb38-337"><a href="#cb38-337" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-338"><a href="#cb38-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-339"><a href="#cb38-339" aria-hidden="true" tabindex="-1"></a>You can also visualize the model:</span>
<span id="cb38-340"><a href="#cb38-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-343"><a href="#cb38-343" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb38-344"><a href="#cb38-344" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb38-345"><a href="#cb38-345" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb38-346"><a href="#cb38-346" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> NeuralNetwork()</span>
<span id="cb38-347"><a href="#cb38-347" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(model)</span>
<span id="cb38-348"><a href="#cb38-348" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-349"><a href="#cb38-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-350"><a href="#cb38-350" aria-hidden="true" tabindex="-1"></a>To use the model, you can pass input data. This will execute the <span class="in">`forward()`</span> method, along with background operations.</span>
<span id="cb38-351"><a href="#cb38-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-354"><a href="#cb38-354" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb38-355"><a href="#cb38-355" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb38-356"><a href="#cb38-356" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb38-357"><a href="#cb38-357" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> torch.rand(<span class="dv">1</span>, <span class="dv">28</span>, <span class="dv">28</span>)</span>
<span id="cb38-358"><a href="#cb38-358" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> model(X)</span>
<span id="cb38-359"><a href="#cb38-359" aria-hidden="true" tabindex="-1"></a>pred_probab <span class="op">=</span> nn.Softmax(dim<span class="op">=</span><span class="dv">1</span>)(logits)</span>
<span id="cb38-360"><a href="#cb38-360" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> pred_probab.argmax(<span class="dv">1</span>)</span>
<span id="cb38-361"><a href="#cb38-361" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Predicted class: </span><span class="sc">{</span>y_pred<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb38-362"><a href="#cb38-362" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-363"><a href="#cb38-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-364"><a href="#cb38-364" aria-hidden="true" tabindex="-1"></a>The executed operations will look like this:</span>
<span id="cb38-365"><a href="#cb38-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-366"><a href="#cb38-366" aria-hidden="true" tabindex="-1"></a>::: {.content-hidden unless-format="html"}</span>
<span id="cb38-369"><a href="#cb38-369" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb38-370"><a href="#cb38-370" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb38-371"><a href="#cb38-371" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb38-372"><a href="#cb38-372" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchviz <span class="im">import</span> make_dot</span>
<span id="cb38-373"><a href="#cb38-373" aria-hidden="true" tabindex="-1"></a>make_dot(logits)</span>
<span id="cb38-374"><a href="#cb38-374" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-375"><a href="#cb38-375" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb38-376"><a href="#cb38-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-377"><a href="#cb38-377" aria-hidden="true" tabindex="-1"></a><span class="fu">## torch.optim</span></span>
<span id="cb38-378"><a href="#cb38-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-379"><a href="#cb38-379" aria-hidden="true" tabindex="-1"></a>To optimize the parameters of a model, you need an optimization algorithm. <span class="co">[</span><span class="ot">`torch.optim`</span><span class="co">](https://pytorch.org/docs/stable/optim.html)</span> implements various algorithms, such as _Stochastic Gradient Descent_ or the often used _Adam Optimizer_.</span>
<span id="cb38-380"><a href="#cb38-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-383"><a href="#cb38-383" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb38-384"><a href="#cb38-384" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb38-385"><a href="#cb38-385" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb38-386"><a href="#cb38-386" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> optim</span>
<span id="cb38-387"><a href="#cb38-387" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>, momentum<span class="op">=</span><span class="fl">0.9</span>)</span>
<span id="cb38-388"><a href="#cb38-388" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-389"><a href="#cb38-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-390"><a href="#cb38-390" aria-hidden="true" tabindex="-1"></a>You can then use the optimizer to adjust the parameters, you just need to define a loss function:</span>
<span id="cb38-391"><a href="#cb38-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-394"><a href="#cb38-394" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb38-395"><a href="#cb38-395" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb38-396"><a href="#cb38-396" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb38-397"><a href="#cb38-397" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> torch.nn.CrossEntropyLoss()</span>
<span id="cb38-398"><a href="#cb38-398" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">3</span>):</span>
<span id="cb38-399"><a href="#cb38-399" aria-hidden="true" tabindex="-1"></a>    <span class="bu">input</span>, target <span class="op">=</span> torch.rand(<span class="dv">1</span>, <span class="dv">28</span>, <span class="dv">28</span>), torch.randint(low<span class="op">=</span><span class="dv">0</span>, high<span class="op">=</span><span class="dv">10</span>, size<span class="op">=</span>(<span class="dv">1</span>, ))</span>
<span id="cb38-400"><a href="#cb38-400" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb38-401"><a href="#cb38-401" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> model(<span class="bu">input</span>)</span>
<span id="cb38-402"><a href="#cb38-402" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> loss_fn(output, target)</span>
<span id="cb38-403"><a href="#cb38-403" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb38-404"><a href="#cb38-404" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb38-405"><a href="#cb38-405" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-406"><a href="#cb38-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-407"><a href="#cb38-407" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb38-408"><a href="#cb38-408" aria-hidden="true" tabindex="-1"></a>Note <span class="in">`optimizer.zero_grad()`</span> which resets the accumulated gradients of the variables to 0.</span>
<span id="cb38-409"><a href="#cb38-409" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb38-410"><a href="#cb38-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-411"><a href="#cb38-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-412"><a href="#cb38-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-413"><a href="#cb38-413" aria-hidden="true" tabindex="-1"></a><span class="fu">## Training Loops</span></span>
<span id="cb38-414"><a href="#cb38-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-415"><a href="#cb38-415" aria-hidden="true" tabindex="-1"></a>Typically, you put together a training loop to train a model. A training loop iterates over batches of data and optimizes the model parameters with each iteration.</span>
<span id="cb38-416"><a href="#cb38-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-419"><a href="#cb38-419" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb38-420"><a href="#cb38-420" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb38-421"><a href="#cb38-421" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb38-422"><a href="#cb38-422" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_loop(dataloader, model, loss_fn, optimizer):</span>
<span id="cb38-423"><a href="#cb38-423" aria-hidden="true" tabindex="-1"></a>    size <span class="op">=</span> <span class="bu">len</span>(dataloader.dataset)</span>
<span id="cb38-424"><a href="#cb38-424" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch, (X, y) <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="cb38-425"><a href="#cb38-425" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute prediction and loss</span></span>
<span id="cb38-426"><a href="#cb38-426" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> model(X)</span>
<span id="cb38-427"><a href="#cb38-427" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_fn(pred, y)</span>
<span id="cb38-428"><a href="#cb38-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-429"><a href="#cb38-429" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Backpropagation</span></span>
<span id="cb38-430"><a href="#cb38-430" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb38-431"><a href="#cb38-431" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb38-432"><a href="#cb38-432" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb38-433"><a href="#cb38-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-434"><a href="#cb38-434" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> batch <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb38-435"><a href="#cb38-435" aria-hidden="true" tabindex="-1"></a>            loss, current <span class="op">=</span> loss.item(), batch <span class="op">*</span> <span class="bu">len</span>(X)</span>
<span id="cb38-436"><a href="#cb38-436" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"loss: </span><span class="sc">{</span>loss<span class="sc">:&gt;7f}</span><span class="ss">  [</span><span class="sc">{</span>current<span class="sc">:&gt;5d}</span><span class="ss">/</span><span class="sc">{</span>size<span class="sc">:&gt;5d}</span><span class="ss">]"</span>)</span>
<span id="cb38-437"><a href="#cb38-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-438"><a href="#cb38-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-439"><a href="#cb38-439" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test_loop(dataloader, model, loss_fn):</span>
<span id="cb38-440"><a href="#cb38-440" aria-hidden="true" tabindex="-1"></a>    size <span class="op">=</span> <span class="bu">len</span>(dataloader.dataset)</span>
<span id="cb38-441"><a href="#cb38-441" aria-hidden="true" tabindex="-1"></a>    num_batches <span class="op">=</span> <span class="bu">len</span>(dataloader)</span>
<span id="cb38-442"><a href="#cb38-442" aria-hidden="true" tabindex="-1"></a>    test_loss, correct <span class="op">=</span> <span class="dv">0</span>, <span class="dv">0</span></span>
<span id="cb38-443"><a href="#cb38-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-444"><a href="#cb38-444" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb38-445"><a href="#cb38-445" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> X, y <span class="kw">in</span> dataloader:</span>
<span id="cb38-446"><a href="#cb38-446" aria-hidden="true" tabindex="-1"></a>            pred <span class="op">=</span> model(X)</span>
<span id="cb38-447"><a href="#cb38-447" aria-hidden="true" tabindex="-1"></a>            test_loss <span class="op">+=</span> loss_fn(pred, y).item()</span>
<span id="cb38-448"><a href="#cb38-448" aria-hidden="true" tabindex="-1"></a>            correct <span class="op">+=</span> (pred.argmax(<span class="dv">1</span>) <span class="op">==</span> y).<span class="bu">type</span>(torch.<span class="bu">float</span>).<span class="bu">sum</span>().item()</span>
<span id="cb38-449"><a href="#cb38-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-450"><a href="#cb38-450" aria-hidden="true" tabindex="-1"></a>    test_loss <span class="op">/=</span> num_batches</span>
<span id="cb38-451"><a href="#cb38-451" aria-hidden="true" tabindex="-1"></a>    correct <span class="op">/=</span> size</span>
<span id="cb38-452"><a href="#cb38-452" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Test Error: </span><span class="ch">\n</span><span class="ss"> Accuracy: </span><span class="sc">{</span>(<span class="dv">100</span><span class="op">*</span>correct)<span class="sc">:&gt;0.1f}</span><span class="ss">%, Avg loss: </span><span class="sc">{</span>test_loss<span class="sc">:&gt;8f}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb38-453"><a href="#cb38-453" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-454"><a href="#cb38-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-457"><a href="#cb38-457" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb38-458"><a href="#cb38-458" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb38-459"><a href="#cb38-459" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb38-460"><a href="#cb38-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-461"><a href="#cb38-461" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb38-462"><a href="#cb38-462" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(model.parameters(), lr<span class="op">=</span>learning_rate)</span>
<span id="cb38-463"><a href="#cb38-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-464"><a href="#cb38-464" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb38-465"><a href="#cb38-465" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb38-466"><a href="#cb38-466" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>t<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ch">\n</span><span class="ss">-------------------------------"</span>)</span>
<span id="cb38-467"><a href="#cb38-467" aria-hidden="true" tabindex="-1"></a>    train_loop(train_dataloader, model, loss_fn, optimizer)</span>
<span id="cb38-468"><a href="#cb38-468" aria-hidden="true" tabindex="-1"></a>    test_loop(test_dataloader, model, loss_fn)</span>
<span id="cb38-469"><a href="#cb38-469" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Done!"</span>)</span>
<span id="cb38-470"><a href="#cb38-470" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-471"><a href="#cb38-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-472"><a href="#cb38-472" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb38-473"><a href="#cb38-473" aria-hidden="true" tabindex="-1"></a>High-level APIs such as <span class="co">[</span><span class="ot">Lightning</span><span class="co">](https://lightning.ai/)</span> and <span class="co">[</span><span class="ot">Keras</span><span class="co">](https://keras.io/)</span> provide many functionalities to simplify managing training loops. It is highly recommended to use such libraries to reduce boiler-plate code. However, it depends on the individual complexitiy of a project to what degree such libraries are useful.</span>
<span id="cb38-474"><a href="#cb38-474" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb38-475"><a href="#cb38-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-476"><a href="#cb38-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-477"><a href="#cb38-477" aria-hidden="true" tabindex="-1"></a><span class="fu">## Pre-trained models</span></span>
<span id="cb38-478"><a href="#cb38-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-479"><a href="#cb38-479" aria-hidden="true" tabindex="-1"></a>Since training models can be time-consuming and expensive, pre-trained models are often used. They allow models to be adapted to a specific task more quickly and cost-effectively. In many areas, particularly NLP and computer vision, using pre-trained models is standard. PyTorch provides <span class="co">[</span><span class="ot">`torchvision`</span><span class="co">](https://pytorch.org/vision/stable/index.html)</span> for computer vision applications. <span class="in">`torchvision`</span> provides functionalities useful for modeling image data. Pre-trained models can also be easily integrated, as shown in the following example:</span>
<span id="cb38-480"><a href="#cb38-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-483"><a href="#cb38-483" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb38-484"><a href="#cb38-484" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: False</span></span>
<span id="cb38-485"><a href="#cb38-485" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb38-486"><a href="#cb38-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-487"><a href="#cb38-487" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.models <span class="im">import</span> resnet50, ResNet50_Weights</span>
<span id="cb38-488"><a href="#cb38-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-489"><a href="#cb38-489" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> ResNet50_Weights.IMAGENET1K_V2</span>
<span id="cb38-490"><a href="#cb38-490" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> resnet50(weights<span class="op">=</span>weights)</span>
<span id="cb38-491"><a href="#cb38-491" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb38-492"><a href="#cb38-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-493"><a href="#cb38-493" aria-hidden="true" tabindex="-1"></a><span class="fu"># Other Frameworks</span></span>
<span id="cb38-494"><a href="#cb38-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-495"><a href="#cb38-495" aria-hidden="true" tabindex="-1"></a>Other important frameworks are listed below (not exhaustive).</span>
<span id="cb38-496"><a href="#cb38-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-497"><a href="#cb38-497" aria-hidden="true" tabindex="-1"></a><span class="fu">## TensorFlow</span></span>
<span id="cb38-498"><a href="#cb38-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-499"><a href="#cb38-499" aria-hidden="true" tabindex="-1"></a>For a long time, PyTorch and TensorFlow have been the biggest deep learning frameworks. TensorFlow stands out with a clean high-level API with <span class="co">[</span><span class="ot">`Keras`</span><span class="co">](https://keras.io/)</span>, which allows for easy implementation of complex models. Traditionally, TensorFlow is well established in the industry, while PyTorch is widely used in academia.</span>
<span id="cb38-500"><a href="#cb38-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-501"><a href="#cb38-501" aria-hidden="true" tabindex="-1"></a><span class="fu">## Jax</span></span>
<span id="cb38-502"><a href="#cb38-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-503"><a href="#cb38-503" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Jax</span><span class="co">](https://jax.readthedocs.io/en/latest/)</span> has gained much popularity in recent years. It has been developed by people working for Google and is being used primarily in research settings. It is aking to <span class="in">`numpy`</span>, however, with automatic-differentiation.</span>
<span id="cb38-504"><a href="#cb38-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-505"><a href="#cb38-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-506"><a href="#cb38-506" aria-hidden="true" tabindex="-1"></a><span class="fu">## Scikit-Learn</span></span>
<span id="cb38-507"><a href="#cb38-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-508"><a href="#cb38-508" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">`Scikit-Learn`</span><span class="co">](https://scikit-learn.org/stable/)</span> is THE machine learning framework in Python. However, Scikit-Learn never covered the area of neural networks and lacks auto-diff functionality. Therefore, Scikit-Learn is irrelevant when training neural networks. However, Scikit-Learn functionalities are often used to carry out the machine learning process, such as splitting datasets into train, validation, and test sets. Also, visualizations, such as the confusion matrix or calculating metrics, can be done via Scikit-Learn.</span>
<span id="cb38-509"><a href="#cb38-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-510"><a href="#cb38-510" aria-hidden="true" tabindex="-1"></a><span class="fu">## ONNX</span></span>
<span id="cb38-511"><a href="#cb38-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-512"><a href="#cb38-512" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">ONNX</span><span class="co">](https://onnx.ai/)</span> is an open format to represent machine learning models. It allows models trained in one framework to be transferred to another. Trained models can also be deployed on various platforms.</span>
<span id="cb38-513"><a href="#cb38-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-514"><a href="#cb38-514" aria-hidden="true" tabindex="-1"></a><span class="fu">## Monitoring</span></span>
<span id="cb38-515"><a href="#cb38-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-516"><a href="#cb38-516" aria-hidden="true" tabindex="-1"></a>When training models, monitoring the training process, debugging, and logging hyperparameters, metrics, etc., is very important. Various tools enable these functionalities. Well-known examples are <span class="co">[</span><span class="ot">`TensorBoard`</span><span class="co">](https://www.tensorflow.org/tensorboard)</span> and <span class="co">[</span><span class="ot">`Weights &amp; Biases`</span><span class="co">](https://wandb.ai/site)</span>.</span>
<span id="cb38-517"><a href="#cb38-517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-518"><a href="#cb38-518" aria-hidden="true" tabindex="-1"></a><span class="fu"># Hardware</span></span>
<span id="cb38-519"><a href="#cb38-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-520"><a href="#cb38-520" aria-hidden="true" tabindex="-1"></a><span class="fu">## Tensor Operations</span></span>
<span id="cb38-521"><a href="#cb38-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-522"><a href="#cb38-522" aria-hidden="true" tabindex="-1"></a>In neural networks, there are many tensor operations. Tensors are essentially multi-dimensional arrays, such as a scalar $x$, a vector $\mathbf{x}$, or a matrix $\mathbf{X}$.</span>
<span id="cb38-523"><a href="#cb38-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-524"><a href="#cb38-524" aria-hidden="true" tabindex="-1"></a>@fig-infrastructure-matrix-mult illustrates a matrix multiplication, a typical representative of a tensor operation. As you can see, the calculations (entries of the matrix $\mathbf{A}\mathbf{C}$) are independent of each other and can be fully parallelized.</span>
<span id="cb38-525"><a href="#cb38-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-526"><a href="#cb38-526" aria-hidden="true" tabindex="-1"></a>::: {#fig-infrastructure-matrix-mult}</span>
<span id="cb38-527"><a href="#cb38-527" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}matrix_mult.png)</span>{width=600}</span>
<span id="cb38-528"><a href="#cb38-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-529"><a href="#cb38-529" aria-hidden="true" tabindex="-1"></a>Matrix Multiplication (from @li_cs231n_2022).</span>
<span id="cb38-530"><a href="#cb38-530" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb38-531"><a href="#cb38-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-532"><a href="#cb38-532" aria-hidden="true" tabindex="-1"></a><span class="fu">## Graphics Processing Units (GPUs)</span></span>
<span id="cb38-533"><a href="#cb38-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-534"><a href="#cb38-534" aria-hidden="true" tabindex="-1"></a>GPUs have made deep learning possible in the first place. With their parallel structure, they can efficiently compute parallelizable tasks such as tensor operations.</span>
<span id="cb38-535"><a href="#cb38-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-536"><a href="#cb38-536" aria-hidden="true" tabindex="-1"></a>CPUs have far fewer cores than GPUs, but they are faster and can handle more complex tasks. CPUs are therefore ideal for sequential tasks. GPUs have many more cores, which are less complex and slower. Therefore, GPUs are excellent for parallel tasks. @fig-infrastructure-cpu-vs-gpu illustrates the differences.</span>
<span id="cb38-537"><a href="#cb38-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-538"><a href="#cb38-538" aria-hidden="true" tabindex="-1"></a>::: {#fig-infrastructure-cpu-vs-gpu}</span>
<span id="cb38-539"><a href="#cb38-539" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}cpu_vs_gpu.png)</span>{width=600}</span>
<span id="cb38-540"><a href="#cb38-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-541"><a href="#cb38-541" aria-hidden="true" tabindex="-1"></a>CPU vs GPU example (from @li_cs231n_2022).</span>
<span id="cb38-542"><a href="#cb38-542" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb38-543"><a href="#cb38-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-544"><a href="#cb38-544" aria-hidden="true" tabindex="-1"></a><span class="fu">## CUDA &amp; cuDNN</span></span>
<span id="cb38-545"><a href="#cb38-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-546"><a href="#cb38-546" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">CUDA</span><span class="co">](https://developer.nvidia.com/cuda-toolkit)</span> is an API by Nvidia to perform computations on the GPU. It allows parallelizable tasks to be implemented efficiently. <span class="co">[</span><span class="ot">cuDNN</span><span class="co">](https://developer.nvidia.com/cudnn)</span> is a library that efficiently executes certain operations, such as convolutions, in neural networks on the GPU. cuDNN is based on CUDA and significantly accelerates the training of neural networks. @fig-infrastructure-speed-gpu-cpu illustrates speed differences when training various neural networks with CPU, GPU, and optimized cuDNN.</span>
<span id="cb38-547"><a href="#cb38-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-548"><a href="#cb38-548" aria-hidden="true" tabindex="-1"></a>::: {#fig-infrastructure-speed-gpu-cpu}</span>
<span id="cb38-549"><a href="#cb38-549" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}speed_gpu_cpu.png)</span>{width=600}</span>
<span id="cb38-550"><a href="#cb38-550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-551"><a href="#cb38-551" aria-hidden="true" tabindex="-1"></a>Speed comparison (from @li_cs231n_2022, data from <span class="co">[</span><span class="ot">Link</span><span class="co">](https://github.com/jcjohnson/cnn-benchmarks)</span>)</span>
<span id="cb38-552"><a href="#cb38-552" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb38-553"><a href="#cb38-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-554"><a href="#cb38-554" aria-hidden="true" tabindex="-1"></a><span class="fu">## Data Loading</span></span>
<span id="cb38-555"><a href="#cb38-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-556"><a href="#cb38-556" aria-hidden="true" tabindex="-1"></a>A crucial bottleneck in practice is the transfer of data (such as images) from the disk to the GPU. If this transfer is not fast enough, it is referred to as _GPU starvation_. There are several approaches to solve this problem:</span>
<span id="cb38-557"><a href="#cb38-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-558"><a href="#cb38-558" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Read the data into RAM (not feasible for larger datasets)</span>
<span id="cb38-559"><a href="#cb38-559" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Use fast disks, such as SSDs</span>
<span id="cb38-560"><a href="#cb38-560" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Utilize multiple CPU threads to read data in parallel and keep it in RAM (_pre-fetching_)</span>
<span id="cb38-561"><a href="#cb38-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-562"><a href="#cb38-562" aria-hidden="true" tabindex="-1"></a>@fig-infrastructure-computer shows the various components.</span>
<span id="cb38-563"><a href="#cb38-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-564"><a href="#cb38-564" aria-hidden="true" tabindex="-1"></a>::: {#fig-infrastructure-computer}</span>
<span id="cb38-565"><a href="#cb38-565" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}computer.jpg)</span>{width=600}</span>
<span id="cb38-566"><a href="#cb38-566" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-567"><a href="#cb38-567" aria-hidden="true" tabindex="-1"></a>Source: @li_cs231n_2022</span>
<span id="cb38-568"><a href="#cb38-568" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb38-569"><a href="#cb38-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-570"><a href="#cb38-570" aria-hidden="true" tabindex="-1"></a>Deep learning frameworks like PyTorch implement special classes that allow data to be prepared in multiple threads. Sometimes a certain number of CPU cores is needed to supply a GPU with enough data. @fig-infrastructure-gpu-starvation shows a starved GPU: You can clearly see that the utilization repeatedly drops to 0 because the GPU has to wait for data.</span>
<span id="cb38-571"><a href="#cb38-571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-572"><a href="#cb38-572" aria-hidden="true" tabindex="-1"></a>::: {#fig-infrastructure-gpu-starvation}</span>
<span id="cb38-573"><a href="#cb38-573" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}gpu_starvation.png)</span>{width=600}</span>
<span id="cb38-574"><a href="#cb38-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-575"><a href="#cb38-575" aria-hidden="true" tabindex="-1"></a>The Y-axis shows the GPU utilization in percentage, while the X-axis represents time. <span class="co">[</span><span class="ot">Source</span><span class="co">](https://stackoverflow.com/questions/44598246/tensorflow-data-starved-gpu)</span></span>
<span id="cb38-576"><a href="#cb38-576" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb38-577"><a href="#cb38-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-578"><a href="#cb38-578" aria-hidden="true" tabindex="-1"></a><span class="fu">## GPU Parallelism</span></span>
<span id="cb38-579"><a href="#cb38-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-580"><a href="#cb38-580" aria-hidden="true" tabindex="-1"></a>Models can also be trained on multiple GPUs. There are two main paradigms: _data parallelism_ and _model parallelism_ (see @fig-infrastructure-parallelism ). With _data parallelism_, each GPU has a copy of the model, and each GPU is trained on different data batches. With _model parallelism_, the model is split across multiple GPUs. Models can be trained on a server with multiple GPUs or even over the network (_distributed_). ML frameworks provide functionalities to handle these.</span>
<span id="cb38-581"><a href="#cb38-581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-582"><a href="#cb38-582" aria-hidden="true" tabindex="-1"></a>::: {#fig-infrastructure-parallelism}</span>
<span id="cb38-583"><a href="#cb38-583" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}parallelism.jpg)</span>{width=600}</span>
<span id="cb38-584"><a href="#cb38-584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-585"><a href="#cb38-585" aria-hidden="true" tabindex="-1"></a>Data and Model Parallelism (from @li_cs231n_2022).</span>
<span id="cb38-586"><a href="#cb38-586" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb38-587"><a href="#cb38-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-588"><a href="#cb38-588" aria-hidden="true" tabindex="-1"></a><span class="fu"># References</span></span>
<span id="cb38-589"><a href="#cb38-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-590"><a href="#cb38-590" aria-hidden="true" tabindex="-1"></a>::: {#refs}</span>
<span id="cb38-591"><a href="#cb38-591" aria-hidden="true" tabindex="-1"></a>:::</span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© Copyright 2024, Marco Willi</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2025/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This page is built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>