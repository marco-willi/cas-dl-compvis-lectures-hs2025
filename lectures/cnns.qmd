---
title: "4 - Convolutional Neural Networks"
params:
   images_path: "/assets/images/cnns/"
---

::: {.callout-tip title="Learning Objectives"}

After this lecture you should be able to:

- Explain why CNN inductive biases (local connectivity, weight sharing) improve efficiency on images.
- Compute output shapes and parameter counts for convolutional layers (stride, padding, dilation).
- Distinguish invariance vs equivariance and relate them to layer design (convs, pooling, global average pooling).
- Describe receptive field growth through stacking and dilation.
- Compare alternative convolution variants (depthwise separable, 1x1, transposed) and their use cases.
:::

::: {.content-hidden}
$$
{{< include /assets/_macros.tex >}}
$$
:::

## Introduction & Motivation

Images have unique properties that make them challenging for standard neural networks:

- **High-Dimensional**: An RGB image of size $224 \times 224$ has $224 \times 224 \times 3 = 150{,}528$ values
- **Locality**: Nearby pixels are statistically related (edges, textures, objects)
- **Translation Invariance**: A cat should be recognized regardless of its position in the image
- **Hierarchical Structure**: Images contain features at multiple scales (pixels → edges → textures → objects)

:::{#fig-cnn-example-rgb-image width=600}

![ ]({{< meta params.images_path >}}cows2_small.png)

[Image Source](https://unsplash.com/de/fotos/braune-schafe-auf-grunem-grasfeld-tagsuber-olonUwUrmQk)
:::

We will see that CNNs are much better suited to model images than MLPs. Unlike MLPs, CNNs explicitly assume that inputs (e.g., pixels) that are close together need to be considered together and that information is locally correlated. This allows certain properties to be embedded in the architecture of CNNs (inductive biases) to define models much more efficiently (with fewer parameters).

### MLPs and Images


The input to an MLP is a vector $\vect{x}^{(i)}$, which is transformed through multiple hidden layers to the output layer. Each hidden layer has a certain number of neurons, each connected to all neurons in the previous layer (fully-connected layers). The fully connected layers can only process 1-D vectors. Therefore, images $\in \mathbb{R}^{H \times W \times C}$ must be flattened into 1-D vectors $\in \mathbb{R}^p$. Here, $p= H \times W \times C$. This causes MLPs to become very large (having many learnable parameters) when applied to high-dimensional inputs such as images. See @fig-cnn-mlp for an illustration.


:::{#fig-cnn-mlp}
![ ]({{< meta params.images_path >}}mlp_images_example.png)

Illustration of an MLP applied to an image.
:::



 In the CIFAR-10 dataset, which consists of very small images of 32x32x3 (height, width, colors), a single neuron in the first hidden layer has 32 * 32 * 3 = 3,072 weights to learn (see @fig-cnn-mlp-images).


:::{#fig-cnn-mlp-images}

![]({{< meta params.images_path >}}mlp_images.jpg){width=600}

Source: @johnson_eecs_2019
:::

<!-- :::{#fig-cnn-spatial-structure-mlp}

![]({{< meta params.images_path >}}mlp-spatial-structure.png){width=600}

Source: @li_cs231n_2023
::: -->

For larger images, which are often encountered in practice, the number of weights is correspondingly much larger. Many neurons are also used, further increasing the number of parameters, leading to overfitting, and making learning the weights more difficult.


The dimensionality of weight matrix $\m{W}$ scales with input size.  $\m{W} \in \mathbb{R}^{d \times k}$, while $d$ the dimensionality of the inputs, and $k$ the number of neurons in the first hidden layer. The columns of the weight matrix $\m{W}$ (in the first hidden layer) can be visualized to learn what kind pattern a specific neuron has learned. See @fig-cnn-mlp-example-weights.

:::{#fig-cnn-mlp-example-weights}

![ ]({{< meta params.images_path >}}mlp_images_example_weights.png)

Illustration of weight matrix $\m{W}$ and how the weights of a single neuron can be reshaped into the input image dimensionality.
:::


Often the patterns we want to learn are not stationary. Ideally we want to recognize them at all positions, particularly under translations of the inputs, e.g. $g(\vect{x})$ where $g()$ is a spatial translation. Figure @fig-cnns-mlp-pattern-shift illustrated the problem.


::: {#fig-cnns-mlp-pattern-shift}

![]({{< meta params.images_path >}}mlp_images_example_pattern_shifts.png)

Illustration of different inputs with the same pattern but shifted spatially.

:::

::: {.callout-note appearance="simple"}

**Question**: Can an MLP learn to detect the same pattern at different spatial locations? If yes, how?

**Think about it:** If an MLP learns to detect a cat in the center of an image, what happens when the same cat appears in the corner?

<details>
<summary>Click for answer</summary>

MLPs must learn separate detectors for each possible position! This is extremely inefficient because:

1. **Parameter explosion**: Need different weights for each spatial location
2. **Data requirements**: Must see cats in every possible position during training
3. **Poor generalization**: Fails on positions not seen during training

This is why MLPs with the same number of parameters perform much worse on image tasks compared to CNNs.
</details>

:::

::: {.callout-tip title="Interactive Demo"}
Try this visualization to see the difference:

- [MLP vs CNN on 2D patterns](https://adamharley.com/nn_vis/mlp/2d.html)
- [CNN Filter Visualization](https://adamharley.com/nn_vis/cnn/2d.html)

Notice how CNNs reuse the same filter across all positions!
:::

:::: {.columns}

::: {.column width="70%"}

::: {#tbl-mlp-vs-cnn tbl-colwidths="[20,20,60]"}

| Aspect | MLP | CNN |
|--------|-----|-----|
| **Connectivity** | Every neuron connects to all inputs | Local connectivity only |
| **Parameters** | Scales with input size | Constant regardless of input size |
| **Translation** | Must learn patterns at each position | Automatically detects patterns everywhere |
| **Inductive Bias** | None (learns from scratch) | Locality + Translation equivariance |

Comparison of MLPs vs CNNs for image processing
:::

:::

:::: 



::: {#fig-cnns-mlp-pattern-shift}
![]({{< meta params.images_path >}}mlp_images_examples_permutation.png)

Inputs transformed with a permuation matrix.
:::



::: {.callout-note appearance="simple"}

**Question**

Can an MLP successfully learn patterns in images $\vect{x}$ that are permuted with a permutation matrix $\m{P}$, i.e., $f(P(\vect{x}))$?

**Hint:** Think carefully - what if EVERY image is permuted with the SAME permutation matrix?

<details>
<summary>Click for the surprising answer</summary>

**Yes, MLPs CAN learn on consistently permuted images!** This is a crucial insight:

**Why MLPs can handle consistent permutation:**

1. **Consistent mapping**: If every image uses the same permutation $P$, pixel position relationships are preserved
2. **Learning is still possible**: The MLP can learn that "pixel at position 247 (originally position 12) connects to pixel at position 891 (originally position 13)"
3. **No information is lost**: Permutation is just a reordering, not destruction of information

**The real insight:**

- ✅ **Same permutation for all images**: MLP can learn the task
- ❌ **Different permutations per image**: MLP cannot learn effectively  
- ❌ **No permutation but objects move around**: MLP struggles with translation

**Why this matters**: This thought experiment reveals that MLPs' problem with images isn't about permutation per se, but about **spatial relationships and translation invariance**:

1. **Natural images**: Objects appear at different positions → MLP must learn separate detectors for each position
2. **Consistently permuted images**: Spatial relationships are preserved (just reordered) → MLP can learn
3. **CNNs**: Built-in translation equivariance makes them efficient for natural images regardless of object position

**Key takeaway**: The issue isn't that spatial structure is "destroyed" by permutation, but that CNNs are specifically designed to handle the **translation variability** in natural images efficiently.
</details>

:::

::: {.callout-tip title="Deep Learning Insight"}
This reveals a fundamental principle: **The right inductive bias matters more than the raw capability to learn**.

- MLPs are universal function approximators and *could* learn vision tasks
- CNNs use the right inductive biases (locality + translation equivariance) to learn vision tasks *efficiently*
- Architecture choice is about **sample efficiency** and **generalization**, not just theoretical capability
:::


To summarize MLPs on images:

- High-Dimensional: The size of an MLP scales with the input dimensionality which might blow beyond memory and compute budgets.
- Locality: MLPs have no notion of locality and thus can't exploit this inherent bias in natural images.
- Stability under transformations: MLPs need to learn position-dependent patterns which is very inefficient.


## From Problems to Solutions: Enter CNNs

Now that we've seen the limitations of MLPs for images, let's explore how **Convolutional Neural Networks (CNNs)** address these challenges.

::: {.callout-tip title="The Big Idea"}
**Instead of connecting every neuron to every input**, CNNs use three key principles:

1. **Local Connectivity**: Each neuron only looks at a small spatial region
2. **Parameter Sharing**: The same filter/detector is used across all spatial locations  
3. **Translation Equivariance**: If the input shifts, the output shifts correspondingly

These aren't just technical tricks—they encode our **prior knowledge** about how images work!
:::

### The CNN Advantage

A single neuron in a CNN is only connected to a small portion (local connectivity) of the image (see @fig-cnn-cnn-spatial). As a result:

✅ **Fewer Parameters**: Neurons have far fewer connections than in MLPs  
✅ **Spatial Structure Preserved**: No need to flatten 2D images into 1D vectors  
✅ **Translation Equivariance**: Same detector works everywhere in the image  
✅ **Hierarchical Feature Learning**: Stack layers to learn increasingly complex patterns

:::{#fig-cnn-cnn-spatial}

![]({{< meta params.images_path >}}cnn_spatial.jpg){width=200}

Source: @johnson_eecs_2019
:::

::: {.callout-note}
CNNs are used not only for image data but also for data with spatial dependencies/local structures. This includes not only images but also time series, videos, audio, and text. The key is that signals that are spatially close together should be interpreted together.
:::

What properties do we want?



### Invariance and Equivariance

For many vision tasks, we need our models to handle transformations gracefully. There are two key concepts:

::: {.callout-important title="Key Definitions"}

A function $f(\vect{x})$ is **invariant** to transformation $g(\vect{x})$ if:
$$f(g(\vect{x})) = f(\vect{x})$$

A function $f(\vect{x})$ is **equivariant** to transformation $g(\vect{x})$ if:
$$f(g(\vect{x})) = g(f(\vect{x}))$$

**Intuition**: 

- **Invariant** = "I don't care about the transformation" 
- **Equivariant** = "I transform my output the same way the input was transformed"
:::

::: {.callout-note title="When do we want each?"}

| Task Type | Property Needed | Why? |
|-----------|-----------------|------|
| **Classification** | Translation Invariance | Cat is still a cat regardless of position |
| **Object Detection** | Translation Equivariance | If object moves right, bounding box moves right |
| **Semantic Segmentation** | Translation Equivariance | If input shifts, pixel-wise labels shift |
| **Style Transfer** | Some invariance to content position | Style should apply regardless of where objects are |

: {tbl-colwidths="[20,20,60]"}

Understanding this distinction is crucial for choosing the right architecture!
:::

**Example: Classification (Invariance Needed)**


::: {#fig-cnns-invariance-example}

![ ]({{< meta params.images_path >}}cows2_small_invariance_collage.png)

When objects in the input translate spatially, the output (in this case the classification of the images) does not change. The model  $f(\vect{x})$ is thus invariant to spatial translations.

:::


**Example: Object Detection (Equivariance Needed)**

::: {#fig-cnns-equivariance-example}

![]({{< meta params.images_path >}}cows2_small_detection_collage.png)

When objects in the input (first row) translate spatially, the detections (bounding boxes) change accordingly (bottom row). The model $f(\vect{x})$ that produces the bounding boxes is thus equivariant with respect to spatial translations.

:::

::: {.callout-tip title="Key Insight"}
**CNNs naturally provide translation equivariance** due to their convolution operation. However, they need additional components (like pooling or global average pooling) to achieve translation invariance for classification tasks.

This is why CNN architecture design matters so much!
:::



## Convolutional Layers

### Convolutional Neural Networks

CNNs consist of a sequence of different layers. Each layer transforms activations from the previous layer into new activations through a differentiable operation. Below we look at the main layer types: convolutional layers, pooling layers, activation layers, and fully connected layers. Arranged in a specific sequence, this is referred to as the architecture of the model.

@fig-cnn-convnet shows an example architecture. The activation maps of the various layers are shown, representing the corresponding outputs of the layers.

:::{#fig-cnn-convnet}

![]({{< meta params.images_path >}}convnet.jpeg){width=600}

The activations of a ConvNet architecture are shown. The input image is on the left and the predictions on the right. Source: @li_cs231n_2022.
:::

Sometimes different layers are combined and referred to as a block. For example, the combination of a convolutional layer followed by an activation layer and a pooling layer is often used. This would be a CONV-ACT-POOL block.

### Convolution Operation

Convolutional layers are the main layers in CNNs responsible for extracting visual features. The weights of a convolutional layer consist of a set of learnable filters. Each filter is typically small along the spatial dimensions (height, width) relative to the input but extends over the entire input depth. A typical filter in the first layer, for example, has the dimension $7 \times 7 \times 3$ (7 pixels along height/width and 3 along the color channels). During the forward pass, the filters are convolved along height/width over the input. At each position, the dot product (when considering the input and filter as 1-D vectors) between the filter and input is calculated. This produces a 2-D activation map representing the filter's expression at each position in the input. Intuitively, the CNN learns filters corresponding to typical visual patterns, such as edges and colors. A set of $K$ filters produces activation maps with a depth of $K$.


::: {.callout-note}
Filter and kernel are sometimes used synonymously. Here, we differentiate by considering a filter as 3-dimensional (CxHxW) and a kernel as 2-dimensional (HxW). A filter consists of C kernels.
:::


::: {.callout-note}
Convolution in deep learning is typically implemented as cross-correlation.

\begin{equation}
S(i, j) = (K * I)(i, j) =\sum_m\sum_n I(i + m, j + n)K(m, n)
\end{equation}


$I$ is the input (for example an image), $K$ is the kernel (typically smaller than $I$) and $b$ is a bias term which is being added to the weighted sum.

If $I$ is an RGB image (in the first layer of a CNN for example), the kernel $K$ would have dimensionality $3 \times K \times K$ (assuming a square kernel). More generally we learn kernels of the dimensionality $C_{in} \times K \times K$.

Multiple kernels, let's say $C_o$ kernels, can be grouped together: $C_o \times C_{in} \times K \times K$.

We often refer to such tensors as filters or filter banks.

:::


Convolution on RGB images is nicely illusrated in @fig-cnns-intro-example. We can see that the filter is multiplied element-wise with the corresponding values in the image, the result is summed top produce one single value in the output.

::: {#fig-cnns-intro-example}

![Source:  @prince_understanding_2023]({{< meta params.images_path >}}convolution_prince_rgb.png)

:::

The following figures show further step-by-step illustration of applying convolutions on input activations that produce multiple output activation maps.


:::{#fig-cnn-conv-one-number}

![]({{< meta params.images_path >}}cnn_conv_one_number.jpg){width=600}

Source: @johnson_eecs_2019
:::

:::{#fig-cnn-conv-activation-map}

![]({{< meta params.images_path >}}conv_activation_map.jpg){width=600}

Source: @johnson_eecs_2019
:::

:::{#fig-cnn-conv-activation-map2}

![]({{< meta params.images_path >}}conv_activation_map2.jpg){width=600}

Source: @johnson_eecs_2019
:::

:::{#fig-cnn-conv-activation-map3}

![]({{< meta params.images_path >}}conv_activation_map3.jpg){width=600}

Source: @johnson_eecs_2019
:::

The data is processed in mini-batches, i.e., multiple images at once, as shown in @fig-cnn-conv-activation-map4.

:::{#fig-cnn-conv-activation-map4}

![]({{< meta params.images_path >}}conv_activation_map4.jpg){width=600}

Source: @johnson_eecs_2019
:::

### Padding, Stride, Kernel Size, Dilation

To define a convolution, various hyperparameters need to be set. Some of the most important ones are:

- Padding: How much padding is added to the input?
- Stride: What is the step size (stride) of the kernel?
- Kernel-Size: What is the kernel size?
- Dilation: What is the dilation rate?


**Stride** determines how the filters are convolved over the input activations, essentially the step size. If the stride is 1, the filter moves one pixel at a time to compute the next activation (see @fig-cnn-stride1). If the stride is greater, e.g., 2, it moves two pixels at a time, making the activation maps smaller in width and height (see @fig-cnn-stride2).


:::{#fig-cnn-stride1}
![]({{< meta params.images_path >}}stride1_example.jpg)
Convolution with stride (1, 1)
:::


:::{#fig-cnn-stride2}
![]({{< meta params.images_path >}}stride2_example.jpg)
Convolution with stride (2, 2)
:::

**Padding** refers to adding (typically) zeros to the border of the input activations before performing the convolution. This can be useful to ensure, for example, that the spatial dimensions of the activation maps are identical to those of the input activations. This is essential for segmentation tasks. @fig-cnn-padding illustrates how padding helps maintain the spatial resolution when padding the input activations.


:::{#fig-cnn-padding}
![]({{< meta params.images_path >}}padding_example_result.jpg)

Left: Input (Yellow) with Zero-Padding (white border), Middle: Filter, Right: Output.
:::


**Kernel-size** determines the spatial extend from which a value in the output activation map migh aggregate values. It is the receptive field of an output value with respect to the input values. @fig-cnn-kernel-size3x3 and @fig-cnn-kernel-size5x5 show convolutions with different kernel sizes. Note that the amount of padding increases with kernel-size. Kernels usually have an odd number of pixels in height and width to make sure they have a center pixel. Also, the number of parameters increases with kernel-size $(C_{in} \times k \times k) + C_{out}$, which is often an important consideration.

:::{#fig-cnn-kernel-size3x3}

![ ]({{< meta params.images_path >}}kernel_size_example_3x3.jpg)

Convolution with zero-padding and $3 \times 3$ kernel that produces activations maps with unchanged spatial resolution.
:::

:::{#fig-cnn-kernel-size5x5}
![ ]({{< meta params.images_path >}}kernel_size_example_5x5.jpg)

Convolution with zero-padding and $5 \times 5$ kernel that produces activations maps with unchanged spatial resolution.
:::



::: {.callout-note appearance="simple"}

**Question**

What is the difference between using 3×3 vs 5×5 kernels? What is the same?

**Think first**: Consider computational cost, receptive field, and parameter count.

<details>
<summary>Click for detailed comparison</summary>

| Aspect | 3×3 Kernel | 5×5 Kernel |
|--------|------------|------------|
| **Parameters** | 9 × input_channels | 25 × input_channels |
| **Computation** | Less (fewer multiplications) | More (more multiplications) |
| **Receptive Field** | Smaller (grows slowly) | Larger (grows faster) |
| **Detail Capture** | Fine-grained features | Broader spatial patterns |

**What's the same**:
- Both preserve translation equivariance
- Both use parameter sharing
- Both can learn spatial features

**Key Insight**: You can achieve the same receptive field with **two 3×3 convolutions** as one 5×5, but with fewer parameters: $(3×3 + 3×3) = 18$ vs $5×5 = 25$ parameters per channel.

This is why modern architectures (like ResNet) prefer stacking smaller kernels!
</details>

:::

::: {.callout-tip title="Best Practice"}
**Modern CNN Design**: Use multiple 3×3 convolutions instead of larger kernels to get:

- ✅ Fewer parameters  
- ✅ More non-linearity (more activation functions)
- ✅ Better gradient flow
- ✅ Similar receptive field coverage
:::

**Dilation** specifies how much the kernel is spread out and thus directly influences the receptive field. @fig-cnn-dilation shows how it works.

:::{#fig-cnn-dilation}
![ ]({{< meta params.images_path >}}dilation_example.jpg)

Illustration of how a kernel with dilation=2 (middle) is applied to the input activations (left) to produce activations (right). Note the reduced spatial dimensionality of the output activations.
:::

::: {.callout-note appearance="simple"}

**Question**

Why would we use dilation $d > 1$?

**Consider**: What happens to the receptive field? What about computational cost?

<details>
<summary>Click for comprehensive answer</summary>

**Dilation increases receptive field without adding parameters or computation!**

**Benefits of dilation**:

1. **Larger receptive field**: See more context without more layers
2. **Same parameter count**: Filter size stays the same  
3. **Preserve resolution**: No spatial downsampling
4. **Multi-scale features**: Different dilation rates capture different scales

**Use cases**:

- **Semantic segmentation**: Need large receptive fields while preserving spatial resolution
- **Dense prediction tasks**: Want to see global context for local predictions  
- **Efficient architectures**: Alternative to deeper networks

**Trade-offs**:

- ✅ Larger receptive field with same parameters
- ✅ No resolution loss
- ❌ May miss fine-grained spatial relationships  
- ❌ Can create "grid artifacts" if not used carefully

**Example**: A 3×3 kernel with dilation=2 has the receptive field of a 5×5 kernel but only 9 parameters instead of 25!
</details>

:::

::: {.callout-tip title="Architectural Pattern"}
**Atrous/Dilated Convolutions** are especially popular in:

- **DeepLab** (semantic segmentation)
- **WaveNet** (audio generation)  
- **Dilated ResNets** (dense prediction)

The key insight: **expand the receptive field without losing resolution**!
:::

::: {.callout-note appearance="simple"}

@dumoulin_guide_2016 has created some animations for better understanding of convolutions and published them here: [https://github.com/vdumoulin/conv_arithmetic](https://github.com/vdumoulin/conv_arithmetic).


::: {.content-hidden unless-format="html"}

![Convolving a 3x3 kernel over a 5x5 input without padding and with stride 1x1. Source @dumoulin_guide_2016]({{< meta params.images_path >}}no_padding_no_strides.gif){#fig-cnn-stride-and-padding-gif3 width=200}

:::

:::



### Calculations: Spatial Dimensionality of Activation Maps

You can calculate the dimensionality of the activation maps with the following formulas:

- $i$: Side length of the input activations (assumption: square inputs)
- $k$: Kernel size (assumption: square kernel)
- $o$: Side length of the output activation maps
- $s$: Stride (assumption: same stride along spatial dimensions)
- $p$: Number of paddings on each side (assumption: same number of paddings along spatial dimensions)
- $d$: Dilation rate.


\begin{equation}
o = \left\lfloor \frac{i + 2p - d\cdot(k-1) - 1}{s} + 1 \right\rfloor
\end{equation}

::: {.callout-note title="📝 Quiz: Output Shape Calculation" collapse="true"}

**Problem**: You're designing a CNN layer with the following parameters:

- Input size: $i = 32$ (height and width)
- Kernel size: $k = 3$
- Padding: $p = 1$ 
- Stride: $s = 2$
- Dilation: $d = 1$

**Question**: What is the output size $o$?

**Step-by-step solution**:

1. Apply the formula: $o = \lfloor \frac{i + 2p - d(k-1) - 1}{s} + 1 \rfloor$
2. Substitute: $o = \lfloor \frac{32 + 2(1) - 1(3-1) - 1}{2} + 1 \rfloor$
3. Simplify: $o = \lfloor \frac{32 + 2 - 2 - 1}{2} + 1 \rfloor$
4. Calculate: $o = \lfloor \frac{31}{2} + 1 \rfloor = \lfloor 15.5 + 1 \rfloor = 16$

**Answer**: $o = 16$

**Intuition**: With stride 2, we're roughly halving the spatial dimensions, but padding helps maintain some size.
:::

::: {.callout-note title="🧮 Practice Quiz: Parameter Counting" collapse="true"}

**Scenario**: Design a convolutional layer with:

- Input: $3 \times 32 \times 32$ (RGB image)  
- 64 output channels
- Kernel size: $5 \times 5$
- Include bias terms

**Questions**:

1. How many weights in the filters?
2. How many bias parameters? 
3. Total parameters?
4. If we used a fully connected layer instead, how many parameters would that require?

**Solutions**:

1. **Filter weights**: $(5 \times 5) \times 3 \times 64 = 4{,}800$
2. **Bias parameters**: $64$ (one per output channel)
3. **Total parameters**: $4{,}800 + 64 = 4{,}864$ 
4. **Fully connected alternative**: $(32 \times 32 \times 3) \times 64 = 196{,}608$

**Insight**: CNNs use **40× fewer parameters** while being more effective for images!
:::

::: {.callout-note title="🎯 Challenge Quiz: Receptive Field" collapse="true"}

**Network Architecture**:

- Layer 1: Conv2d(kernel=3, stride=1)
- Layer 2: Conv2d(kernel=3, stride=2)  
- Layer 3: Conv2d(kernel=5, stride=1)

**Question**: What is the receptive field of the final layer with respect to the input?

**Solution using the formula**:
$$r_0 = \sum_{l=1}^3 \left( (k_l - 1) \prod_{i=1}^{l-1} s_i \right) + 1$$

- $r_0 = (3-1) \cdot 1 + (3-1) \cdot 1 + (5-1) \cdot (1 \cdot 2) + 1$
- $r_0 = 2 + 2 + 8 + 1 = 13$

**Answer**: $13 \times 13$ receptive field

**Practical insight**: Each output pixel "sees" a $13 \times 13$ region of the input image.
:::

Note: If dilation is 1 $(k-1)(d-1)$ equals 0.


### Calculations: Number of Weights in a Convolutional Layer

You can calculate the total number of weights in a convolutional layer with the following formula:

- $k$: Kernel size (assumption: square kernel, so kernel has dimensions $k \times k$)
- $C_{in}$: Number of input channels
- $C_{out}$: Number of output channels


\begin{equation}
\text{Total Weights} = (k \times k) \times C_{in} \times C_{out}
\end{equation}

Each filter has a size of $k \times k \times C_{in}$, and there are $C_{out}$ filters in total, resulting in $(k \times k \times C_{in}) \times C_{out}$ weights.


**Biases:**
If each output channel has a bias term, add an additional $C_{out}$ weights for the biases.


\begin{equation}
\text{Total Parameters} = (k \times k \times C_{in} \times C_{out}) + C_{out}
\end{equation}


::: {.callout-note appearance="simple"}

**Question**

**Real-World Scenario**: You're building a medical imaging system for chest X-ray analysis.

**Scenario:**

- Input X-rays: $1 \times 512 \times 512$ (grayscale)
- First conv layer: 32 filters, $k=7 \times 7$, $s=2$, $p=3$, $d=1$

**Your Tasks:**

1. What is the output activation map size $(C_{out}, H_{out}, W_{out})$?
2. How many parameters does this layer have?
3. Why might we choose $k=7$ for medical images vs $k=3$ for natural images?
4. What's the receptive field of each output pixel?

<details>
<summary>Click for detailed solutions</summary>

**Solutions:**

1. **Output size**:
   - $C_{out} = 32$ (number of filters)
   - $H_{out} = W_{out} = \lfloor \frac{512 + 2(3) - 1(7-1) - 1}{2} \rfloor + 1 = 256$
   - **Answer**: $(32, 256, 256)$

2. **Parameters**:
   - Filter weights: $(7 \times 7) \times 1 \times 32 = 1{,}568$
   - Biases: $32$
   - **Total**: $1{,}600$ parameters

3. **Why $k=7$ for medical images?**
   - Medical features often span larger spatial areas (organ boundaries, lesions)
   - Need to capture broader anatomical context early
   - Higher resolution medical images benefit from larger initial receptive fields
   - Natural images have more fine-grained local features

4. **Receptive field**: $7 \times 7$ (same as kernel size for first layer)

**Key Insight**: Domain knowledge should inform architectural choices!
</details>

:::

::: {.callout-tip title="Domain-Specific Design"}
Different domains need different CNN architectures:

| Domain | Typical Kernel Sizes | Why? |
|--------|---------------------|------|
| **Natural Images** | 3×3, 1×1 | Fine details, efficiency |
| **Medical Imaging** | 5×5, 7×7 | Larger anatomical features |
| **Satellite Imagery** | 7×7, 9×9 | Geographic features at scale |
| **Text Recognition** | Rectangular (1×3, 3×1) | Character and word shapes |

: {tbl-colwidths="[30,20,50]"}

Understanding your data is crucial for good architecture design!
:::


## Properties

### Local (Sparse) Connectivity & Parameter Sharing

Fully connected layers are, as discussed, impractical when working with high-dimensional inputs like images. If all neurons in a layer were connected to all previous neurons, the number of parameters to be estimated would increase massively, which is inefficient and leads to overfitting. Each neuron is therefore only connected to a local region of the input volume. The spatial extent of this region is a hyperparameter and is called the receptive field of a neuron (also kernel size) on the input volume. The connections along the depth (C) extend over the entire depth of the input volume. The connections are therefore local along the spatial dimensions (width and height) but complete along the depth.

Parameter sharing in convolutional layers is used to reduce the number of parameters. Since the filters are convolved over the inputs, the individual weights of the filters are identical over the spatial extent of the input volume. One of the main assumptions behind CNNs is the following: If it is useful to learn a specific (visual) feature at a certain position, then it is probably useful at other positions as well. In other words: If I learn filters that detect edges, corners, or cats, then it is a reasonable assumption that I want to do this throughout the image.

::: {.callout-note}
Sometimes parameter sharing does not make sense. This can be the case, for example, if we have centered structures in the images. Then you might want to learn position-dependent features. An example is images of faces that have been centered, where you might want to learn filters that detect the mouth only in the lower middle area (locally connected layers).
:::



The following output shows the number of parameters in an MLP and a CNN (each with two hidden layers) on the CIFAR10 dataset.

```{python}
#| eval: true
#| echo: true
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchinfo

class MLP(nn.Module):

    def __init__(self):
        super().__init__()
        self.flatten = nn.Flatten()
        self.hidden_layer1 = nn.Linear(3 * 32 * 32, 64)
        self.hidden_layer2 = nn.Linear(64, 32)
        self.output_layer = nn.Linear(32, 10)

    def forward(self, x):
        x = self.flatten(x)
        x = torch.relu(self.hidden_layer1(x))
        x = torch.relu(self.hidden_layer2(x))
        x = self.output_layer(x)
        return x

net = MLP()
print(torchinfo.summary(net, input_size=(1, 3, 32, 32)))
```

```{python}
#| eval: true
#| echo: true
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchinfo

class CNN(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 16, 7, stride=2, padding=3)
        self.conv2 = nn.Conv2d(16, 16, 3, stride=2, padding=1)
        self.flatten = nn.Flatten()
        self.output_layer = nn.Linear(16 * 8 * 8 , 10)

    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = torch.relu(self.conv2(x))
        x = self.flatten(x)
        x = self.output_layer(x)
        return x

cnn = CNN()
print(torchinfo.summary(cnn, input_size=(1, 3, 32, 32)))
```

:::{#fig-cnn-linear-transf-calc}

![]({{< meta params.images_path >}}linear_transf.png){width=800}

Input in 2-D (top left), the flattened version of it (bottom left), expected output (right), and unknown transformation (middle).
:::

::: {.callout-note appearance="simple"}

**Question**

How should the linear transformation be defined to obtain the desired result? How many parameters are needed? How could this be done with a convolution?

:::

### Translation Invariance / Equivariance

Translation invariant is a function that produces the same value under translations $g()$ of the input $x$:

\begin{equation}
f(g(x))=f(x)
\end{equation}

Translation equivariant is a function that produces the same value under translations $g()$ of the input $x$, provided that it is also shifted by $g()$:

\begin{equation}
f(g(x))=g(f(x))
\end{equation}

Convolutions are translation equivariant, as illustrated well in the following example:

{{< video https://www.youtube.com/embed/qoWAFBYOtoU start="50" >}}


<!-- <iframe width="560" height="315" src="https://www.youtube.com/embed/qoWAFBYOtoU?start=50" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
 -->


### Stacking & Receptive Field  {#sec-cnn-receptive-field}

Multiple convolutions can be executed in sequence (stacking). Each convolution is performed on the activation maps of another previous convolution. @fig-cnn-conv-stacking illustrates the result.

:::{#fig-cnn-conv-stacking}

![]({{< meta params.images_path >}}conv_stacking.jpg){width=600}

Source: @johnson_eecs_2019
:::

A convolution is therefore not only performed directly on the input (e.g., images) but is generally defined on inputs of dimensionality $H \times W \times C$! (There are also variants in higher dimensions.)

However, non-linear activation functions must be used between the convolutions. Otherwise, the stacked convolution can be expressed with a simple convolution (similar to an MLP, which can be expressed with a linear transformation without activation functions).

The receptive field defines which inputs influence the activations of a neuron. @fig-cnn-receptive-field-1 to @fig-cnn-receptive-field-4 illustrate the concept.


:::{#fig-cnn-receptive-field-1}
![]({{< meta params.images_path >}}receptive_field1.jpg)

Receptive field with a 5x5 kernel.
:::


:::{#fig-cnn-receptive-field-2}
![]({{< meta params.images_path >}}receptive_field2.jpg)

Receptive field with a 5x5 kernel and two layers.
:::


:::{#fig-cnn-receptive-field-3}
![ ]({{< meta params.images_path >}}receptive_field3.jpg)

Receptive field with a 5x5 kernel and two layers.
:::


:::{#fig-cnn-receptive-field-4}
![ ]({{< meta params.images_path >}}receptive_field4.jpg)

Receptive field with a 5x5 kernel and two layers. The output activation at the center (right) aggregates values over the union of all blue boxes in the left figure and has thus a receptive field of $9 \times 9$ with respect to the input activations.
:::


The receptive field $r_0$ of a convolutional neural network can be calculated as follows:

1. Define the recurrence relation:

\begin{equation}
r_{l-1} = s_l \cdot r_l + (k_l - s_l)
\end{equation}

- $r_l$: Receptive field at layer $l$.
- $s_l$: Stride of layer $l$.
- $k_l$: Kernel size of layer $l$.

2. Solving this recurrence relation, we get the closed-form solution for the receptive field at the input layer $r_0$:

\begin{equation}
r_0 = \sum_{l=1}^L \left( (k_l - 1) \prod_{i=1}^{l-1} s_i \right) + 1
\end{equation}

- $L$: Total number of layers.
- $k_l$: Kernel size at layer $l$.
- $s_i$: Stride at layer $i$, from layer 1 up to $l-1$.


@araujo_computing_2019 have written a nice article about receptive fields and how to calculate them.




## More Layers & Operations


### Downsampling

Spatial downsampling is often a desired property of convolutional layers. We can achieve that by choosing a stride $\gt 1$. However, there are other options, such as pooling layers.

:::{#fig-cnn-downsampling-pooling}
![]({{< meta params.images_path >}}downsampling_with_pooling.jpg)

a) Sub-Sampling, b) Max Pooling, c) Average Pooling . Source:  @prince_understanding_2023
:::


### Upsampling {#sec-cnn-upsampling}

Sometimes we need to increase the spatial dimensionality of our activation maps.


:::{#fig-cnn-upsampling}
![]({{< meta params.images_path >}}upsampling_options.jpg)

a) Duplicate, b) Max-Unpooling, c) Bilinear Interpolation . Source:  @prince_understanding_2023
:::

Upsampling can also be learned with Transposed Convolutions.

:::{#fig-cnn-transposed-convs}
![ ]({{< meta params.images_path >}}transposed_convolution.jpg)
:::


### Changing the number of channels

1x1 convolutions have a kernel size of 1x1 and thus no spatial extent. These layers are often used in CNNs to change the number ($C$) of activation maps with few parameters. For example, activation maps of dimensionality ($C \times H \times W$) can be changed to a volume of ($C2 \times H \times W$) with $C2 * (C + 1)$. This can be useful, for example, to save parameters before more complex layers or at the end of the CNN to adjust the size of the activation maps to the number of classes to be modeled (for classification problems) or to reduce to 3 color channels ($C2=3$) for image generation models. @fig-cnn-1x1-conv shows an example.

:::{#fig-cnn-1x1-conv}

![]({{< meta params.images_path >}}1x1_conv_example.jpg)

Source: @prince_understanding_2023
:::

### Global Average Pooling

Global Average Pooling is often an important component. It computes the average of the activations along the depth dimension, reducing activation maps from (C x H x W) to (C x 1 x 1). This is useful for directly modeling logits in a classification problem with C classes, enabling architectures that completely eliminate fully-connected layers.

::: {#fig-cnn-global-avg-pooling}

![]({{< meta params.images_path >}}global_average_pooling.jpg)

Global Average pooling, input (left) and output (right).
:::

### Other Layers

There are many other types of layers, also there are more flavours of convolutional layers. Here are some honorable mentions


#### Depthwise Separable Convolutions

Depthwise separable convolutions are a way to further reduce the number of parameters in convolutional layers. Instead of extending filters over the entire depth of the input activations, a separate filter (kernel) is used for each input channel, with the dimensionality ($1 \times K \times K$). @fig-cnn-depthwise shows an example. Subsequently, 1x1 convolutions are used to combine information across the input channels. See @fig-cnn-depthwise-separabel for a comparison of 'normal' convolutions and depthwise separable convolutions. Since 1x1 convolutions require fewer parameters, activation maps can be generated with fewer parameters.

:::{#fig-cnn-depthwise}

![]({{< meta params.images_path >}}depthwise.png){width=600}

Source: [https://paperswithcode.com/method/depthwise-convolution](https://paperswithcode.com/method/depthwise-convolution)
:::

:::{#fig-cnn-depthwise-separabel}

![]({{< meta params.images_path >}}depthwise_separabel.png){width=600}

Source: @yu_multi-scale_2016
:::


## PyTorch Examples

```{python}
#| eval: true
#| echo: true
import numpy as np
import torch
from torch.nn import functional as F
import torchshow as ts
from PIL import Image
from matplotlib import pyplot as plt
```

```{python}
#| eval: true
#| echo: true
#img = Image.open({{< meta params.images_path >}}'cat.jpg')
image_path = "../assets/images/cnns/cat.jpg"
img = Image.open(image_path)
img
```

```{python}
#| eval: true
#| echo: true
filter = torch.tensor(
    [   [[1, 0, -1], [1, 0, -1], [1, 0, -1]], # R
        [[1, 0, -1], [1, 0, -1], [1, 0, -1]], # G
        [[1, 0, -1], [1, 0, -1], [1, 0, -1]], # B
    ]).unsqueeze(0).float()
ts.show(filter, show_axis=False)
```

```{python}
#| eval: true
#| echo: true
input = torch.tensor(np.array(img)).unsqueeze(0).permute(0, 3, 1, 2).float() # (N, C, H, W)
input /= 255.0
input -= 1.0
result = F.conv2d(input, filter, stride=1, padding=0, dilation=1, groups=1)
```

```{python}
#| eval: true
#| echo: true
ts.show(result)
```

2D-Convolution:

```{python}
#| eval: true
#| echo: true
result = F.conv2d(input, filter, stride=6, padding=0, dilation=1, groups=1)
ts.show(result)
```

Transposed convolution:

```{python}
#| eval: true
#| echo: true
result = F.conv2d(input, filter, stride=6, padding=0, dilation=1, groups=1)
result = F.conv_transpose2d(result, weight=torch.ones_like(filter))
ts.show(result)
```

Max-Pooling:

```{python}
#| eval: true
#| echo: true
result = F.max_pool2d(input, kernel_size=8, stride=8)
ts.show(result)
```

::: {.callout-tip title="Next: Beyond Convolutions"}
CNNs bake in locality and translation equivariance as *inductive biases*. The next lecture **Foundation Models for Vision** shows how large-scale self-supervised and multimodal pretraining (e.g. DINOv3, CLIP) relaxes some of these architectural assumptions while achieving strong transfer with fewer task-specific labels.
:::

## References

::: {#refs}
:::
