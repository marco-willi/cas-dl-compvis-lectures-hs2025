<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Marco Willi">

<title>2 - Convolutional Neural Networks – CAS Deep Learning - Computer Vision</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../pages/lectures/classification.html" rel="next">
<link href="../../pages/lectures/intro.html" rel="prev">
<link href="../..//assets/cv_logo_small.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-065a5179aebd64318d7ea99d77b64a9e.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-159896a7fbbc1299bfd73fe3af9cd852.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-97BR3FTSN1"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-97BR3FTSN1', { 'anonymize_ip': true});
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../pages/lectures/intro.html">Lectures</a></li><li class="breadcrumb-item"><a href="../../pages/lectures/cnns.html">2 - Convolutional Neural Networks</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-center sidebar-header">
      <a href="../../index.html" class="sidebar-logo-link">
      <img src="../../assets/cv_logo_small.png" alt="" class="sidebar-logo light-content py-0 d-lg-inline d-none">
      <img src="../../assets/cv_logo_small.png" alt="" class="sidebar-logo dark-content py-0 d-lg-inline d-none">
      </a>
      <div class="sidebar-tools-main">
    <a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2025" title="GitHub organization" class="quarto-navigation-tool px-1" aria-label="GitHub organization"><i class="bi bi-github"></i></a>
    <a href="" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-code-square"></i></a>
</div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Background &amp; Preparation</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/background/machine_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1 - Machine Learning Basics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/background/neural_networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2 - Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/background/frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3 - Software &amp; Hardware for Deep Learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Lectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1 - Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/cnns.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">2 - Convolutional Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3 - Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/representation_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4 - Representation Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/practical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5 - Practical</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/recent_advances.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6 - Recent Advances</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/intro_genai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">7 - Intro to Generative AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/generative_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8 - Generative Models</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Slides</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/cnns.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Convolutional Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/representation_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Representation Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/practical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Practical Considerations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/recent_advances.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Recent Advances</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/intro_genai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Intro to Generative AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/generative_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Generative Models</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/quiz.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Quiz &amp; Exam Prep</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false">
 <span class="menu-text">Resources</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/literature.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Books</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/links.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Helpful Links &amp; Resources</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Mathematical Notation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/student_projects.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Project Ideas</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction-motivation" id="toc-introduction-motivation" class="nav-link active" data-scroll-target="#introduction-motivation"><span class="header-section-number">1</span> Introduction &amp; Motivation</a>
  <ul class="collapse">
  <li><a href="#mlps-and-images" id="toc-mlps-and-images" class="nav-link" data-scroll-target="#mlps-and-images"><span class="header-section-number">1.1</span> MLPs and Images</a></li>
  <li><a href="#sec-cnn-invariance" id="toc-sec-cnn-invariance" class="nav-link" data-scroll-target="#sec-cnn-invariance"><span class="header-section-number">1.2</span> Invariance and Equivariance</a></li>
  </ul></li>
  <li><a href="#convolutional-neural-networks" id="toc-convolutional-neural-networks" class="nav-link" data-scroll-target="#convolutional-neural-networks"><span class="header-section-number">2</span> Convolutional Neural Networks</a>
  <ul class="collapse">
  <li><a href="#the-cnn-advantage" id="toc-the-cnn-advantage" class="nav-link" data-scroll-target="#the-cnn-advantage"><span class="header-section-number">2.1</span> The CNN Advantage</a></li>
  <li><a href="#convolutional-neural-networks-1" id="toc-convolutional-neural-networks-1" class="nav-link" data-scroll-target="#convolutional-neural-networks-1"><span class="header-section-number">2.2</span> Convolutional Neural Networks</a></li>
  <li><a href="#convolution-operation" id="toc-convolution-operation" class="nav-link" data-scroll-target="#convolution-operation"><span class="header-section-number">2.3</span> Convolution Operation</a></li>
  <li><a href="#padding-stride-kernel-size-dilation" id="toc-padding-stride-kernel-size-dilation" class="nav-link" data-scroll-target="#padding-stride-kernel-size-dilation"><span class="header-section-number">2.4</span> Padding, Stride, Kernel Size, Dilation</a></li>
  <li><a href="#calculations-spatial-dimensionality-of-activation-maps" id="toc-calculations-spatial-dimensionality-of-activation-maps" class="nav-link" data-scroll-target="#calculations-spatial-dimensionality-of-activation-maps"><span class="header-section-number">2.5</span> Calculations: Spatial Dimensionality of Activation Maps</a></li>
  <li><a href="#calculations-number-of-weights-in-a-convolutional-layer" id="toc-calculations-number-of-weights-in-a-convolutional-layer" class="nav-link" data-scroll-target="#calculations-number-of-weights-in-a-convolutional-layer"><span class="header-section-number">2.6</span> Calculations: Number of Weights in a Convolutional Layer</a></li>
  </ul></li>
  <li><a href="#properties" id="toc-properties" class="nav-link" data-scroll-target="#properties"><span class="header-section-number">3</span> Properties</a>
  <ul class="collapse">
  <li><a href="#local-sparse-connectivity-parameter-sharing" id="toc-local-sparse-connectivity-parameter-sharing" class="nav-link" data-scroll-target="#local-sparse-connectivity-parameter-sharing"><span class="header-section-number">3.1</span> Local (Sparse) Connectivity &amp; Parameter Sharing</a></li>
  <li><a href="#translation-equivariance" id="toc-translation-equivariance" class="nav-link" data-scroll-target="#translation-equivariance"><span class="header-section-number">3.2</span> Translation Equivariance</a></li>
  <li><a href="#stacking-convolutions" id="toc-stacking-convolutions" class="nav-link" data-scroll-target="#stacking-convolutions"><span class="header-section-number">3.3</span> Stacking Convolutions</a></li>
  <li><a href="#sec-cnn-receptive-field" id="toc-sec-cnn-receptive-field" class="nav-link" data-scroll-target="#sec-cnn-receptive-field"><span class="header-section-number">3.4</span> Receptive Field</a></li>
  <li><a href="#comparing-mlps-and-cnns" id="toc-comparing-mlps-and-cnns" class="nav-link" data-scroll-target="#comparing-mlps-and-cnns"><span class="header-section-number">3.5</span> Comparing MLPs and CNNs</a></li>
  </ul></li>
  <li><a href="#more-layers-operations" id="toc-more-layers-operations" class="nav-link" data-scroll-target="#more-layers-operations"><span class="header-section-number">4</span> More Layers &amp; Operations</a>
  <ul class="collapse">
  <li><a href="#downsampling" id="toc-downsampling" class="nav-link" data-scroll-target="#downsampling"><span class="header-section-number">4.1</span> Downsampling</a></li>
  <li><a href="#sec-cnn-upsampling" id="toc-sec-cnn-upsampling" class="nav-link" data-scroll-target="#sec-cnn-upsampling"><span class="header-section-number">4.2</span> Upsampling</a></li>
  <li><a href="#changing-the-number-of-channels" id="toc-changing-the-number-of-channels" class="nav-link" data-scroll-target="#changing-the-number-of-channels"><span class="header-section-number">4.3</span> Changing the number of channels</a></li>
  <li><a href="#global-average-pooling" id="toc-global-average-pooling" class="nav-link" data-scroll-target="#global-average-pooling"><span class="header-section-number">4.4</span> Global Average Pooling</a></li>
  <li><a href="#other-layers" id="toc-other-layers" class="nav-link" data-scroll-target="#other-layers"><span class="header-section-number">4.5</span> Other Layers</a></li>
  </ul></li>
  <li><a href="#pytorch-examples" id="toc-pytorch-examples" class="nav-link" data-scroll-target="#pytorch-examples"><span class="header-section-number">5</span> PyTorch Examples</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">6</span> References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2025/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../pages/lectures/intro.html">Lectures</a></li><li class="breadcrumb-item"><a href="../../pages/lectures/cnns.html">2 - Convolutional Neural Networks</a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">2 - Convolutional Neural Networks</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Marco Willi </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Institute for Data Science I4DS, FHNW
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-tip callout-titled" title="Learning Objectives">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Learning Objectives
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li>Understand why MLPs are not ideal to model images.</li>
<li>Explain why CNN inductive biases improve efficiency on images over MLPs.</li>
<li>Understand how convolutions operate and how they can be parameterized.</li>
<li>Compute output shape and the number of parameters for convolutions.</li>
<li>Understand how different layers make up a CNN and what their role is.</li>
<li>Distinguish invariance vs equivariance and relate them to layer design and applications.</li>
<li>Describe receptive field growth through stacking.</li>
<li>On a high-level: know layers / variants such as 1x1 convolutions and global average pooling.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="TLDR Recap">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>TLDR Recap
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Why CNNs? The Problem with MLPs on Images</strong></p>
<ul>
<li>Images are high-dimensional (e.g., <span class="math inline">\(224 \times 224 \times 3 = 150{,}528\)</span> pixels)</li>
<li>MLPs flatten images, losing spatial structure</li>
<li>Fully connected layers require millions of parameters: <span class="math inline">\((150{,}528 \times 1{,}000 = 150M\)</span> parameters for just one layer!</li>
<li>MLPs don’t exploit natural image properties (locality, translation invariance)</li>
</ul>
<p><strong>The CNN Solution: Three Key Principles</strong></p>
<ol type="1">
<li><strong>Local Connectivity</strong>: Each neuron only connects to a small spatial region (receptive field)</li>
<li><strong>Parameter Sharing</strong>: Same filter/detector applied across all spatial locations</li>
<li><strong>Translation Equivariance</strong>: If input shifts, output shifts correspondingly</li>
</ol>
<p><strong>Core Operation: Convolution</strong></p>
<p><span class="math display">\[S(i, j) = (K * I)(i, j) = \sum_m\sum_n I(i + m, j + n)K(m, n)\]</span></p>
<ul>
<li><strong>Input</strong> <span class="math inline">\(I\)</span>: Activation maps <span class="math inline">\((C_{in} \times H \times W)\)</span></li>
<li><strong>Kernel</strong> <span class="math inline">\(K\)</span>: Learnable filter <span class="math inline">\((C_{out} \times C_{in} \times k \times k)\)</span></li>
<li><strong>Output</strong> <span class="math inline">\(S\)</span>: Feature maps <span class="math inline">\((C_{out} \times H' \times W')\)</span></li>
</ul>
<p><strong>Key Hyperparameters</strong></p>
<ul>
<li><strong>Kernel Size</strong> <span class="math inline">\(k\)</span>: Spatial extent (usually 3×3 or 5×5)</li>
<li><strong>Stride</strong> <span class="math inline">\(s\)</span>: Step size (controls downsampling)</li>
<li><strong>Padding</strong> <span class="math inline">\(p\)</span>: Zero-padding to preserve spatial size</li>
<li><strong>Dilation</strong> <span class="math inline">\(d\)</span>: Spacing between kernel elements (expands receptive field)</li>
</ul>
<p><strong>Output Size Formula</strong></p>
<p><span class="math display">\[o = \left\lfloor \frac{i + 2p - d\cdot(k-1) - 1}{s} + 1 \right\rfloor\]</span></p>
<p><strong>Parameter Count</strong></p>
<p><span class="math display">\[\text{Total Parameters} = (k \times k \times C_{in} \times C_{out}) + C_{out}\]</span></p>
<p><strong>Invariance vs Equivariance</strong></p>
<ul>
<li><strong>Invariant</strong>: <span class="math inline">\(f(g(x)) = f(x)\)</span> — output unchanged by transformation (needed for <em>classification</em>)</li>
<li><strong>Equivariant</strong>: <span class="math inline">\(f(g(x)) = g(f(x))\)</span> — output transforms with input (needed for <em>detection</em>, <em>segmentation</em>)</li>
<li><strong>CNNs are naturally translation equivariant</strong> due to convolution operation</li>
<li>Add pooling/global average pooling for translation <em>invariance</em></li>
</ul>
<p><strong>Receptive Field</strong></p>
<p>The input region that affects a single output activation:</p>
<p><span class="math display">\[r_0 = \sum_{l=1}^L \left( (k_l - 1) \prod_{i=1}^{l-1} s_i \right) + 1\]</span></p>
<ul>
<li>Grows with depth: stacking small kernels (3×3) more efficient than large kernels (5×5)</li>
<li>Example: Two 3×3 convs = 18 params/channel vs one 5×5 = 25 params/channel</li>
</ul>
<p><strong>Common Operations</strong></p>
<ul>
<li><strong>Pooling</strong>: Spatial downsampling (max-pooling, average-pooling)</li>
<li><strong>1×1 Convolution</strong>: Change number of channels without spatial effect</li>
<li><strong>Global Average Pooling</strong>: <span class="math inline">\((C \times H \times W) \rightarrow (C \times 1 \times 1)\)</span> for classification</li>
<li><strong>Upsampling</strong>: Increase spatial size (interpolation, transposed convolution)</li>
<li><strong>Depthwise Separable</strong>: Efficient convolutions with fewer parameters</li>
</ul>
<p><strong>Architectural Best Practices</strong></p>
<ul>
<li>✅ Use multiple small kernels (3×3) instead of large ones</li>
<li>✅ Add non-linearity (ReLU, etc.) between conv layers</li>
<li>✅ Use batch normalization for stable training</li>
<li>✅ Stack conv layers to build hierarchical features</li>
<li>✅ Use global average pooling instead of large fully-connected layers</li>
</ul>
<p><strong>Parameter Efficiency Example</strong></p>
<p>For CIFAR-10 images (32×32×3):</p>
<ul>
<li><strong>MLP</strong> (2 hidden layers): ~200K parameters</li>
<li><strong>CNN</strong> (2 conv layers): ~5K parameters (40× fewer!)</li>
</ul>
<p><strong>Key Insight</strong>: CNNs encode our prior knowledge about images as <em>inductive biases</em> — assumptions baked into the architecture that make learning more efficient and effective.</p>
</div>
</div>
</div>
<section id="introduction-motivation" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="introduction-motivation"><span class="header-section-number">1</span> Introduction &amp; Motivation</h2>
<p>Images have unique properties that make them challenging for standard neural networks:</p>
<ul>
<li><strong>High-Dimensional</strong>: An RGB image of size <span class="math inline">\(224 \times 224\)</span> has <span class="math inline">\(224 \times 224 \times 3 = 150{,}528\)</span> values</li>
<li><strong>Locality</strong>: Nearby pixels are statistically related (edges, textures, objects)</li>
<li><strong>Stability under transformations</strong>: Interpretation of an image does not change under many geometric transformations. A cat should be recognized regardless of its position in the image.</li>
<li><strong>Hierarchical Structure</strong>: Images contain features at multiple scales (pixels → edges → textures → objects)</li>
</ul>
<div id="fig-cnn-example-rgb-image" class="quarto-float quarto-figure quarto-figure-center anchored" width="600">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-example-rgb-image-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/cnns/cows2_small.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-example-rgb-image-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: A high-resolution photograph. <a href="https://unsplash.com/de/fotos/braune-schafe-auf-grunem-grasfeld-tagsuber-olonUwUrmQk">Image Source</a>
</figcaption>
</figure>
</div>
<div class="callout callout-style-simple callout-note callout-titled" title="Question: How are nearby pixels related?">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Question: How are nearby pixels related?
</div>
</div>
<div class="callout-body-container callout-body">
<p>Looking at <a href="#fig-cnn-example-rgb-image" class="quarto-xref">Figure&nbsp;1</a>, can you explain how nearby pixels are related?</p>
<p><strong>Think about it:</strong> What visual patterns or structures do you notice?</p>
<details>
<summary>
Click for answer
</summary>
<p><strong>Nearby pixels in images are strongly correlated due to:</strong></p>
<ol type="1">
<li><strong>Object Coherence</strong>: Objects occupy contiguous regions—pixels within an object (like a cow’s body) share similar colors and textures</li>
<li><strong>Smooth Transitions</strong>: Natural images have gradual changes in color and intensity, not abrupt random variations</li>
<li><strong>Texture Patterns</strong>: Repeating structures (like grass blades or fur) create local statistical patterns</li>
<li><strong>Edges and Boundaries</strong>: Sharp transitions occur at object boundaries, but even edges span multiple neighboring pixels</li>
</ol>
</details>
</div>
</div>
<p>Even though we will soon see that Convolutional Neural Networks (CNNs) are much better suited to model images as compared to Multi-Layer Pereptrons (MLPs), we start by investigating the properties and difficulties to model images with MLPs. This will help understand how properties embedded in the architecture of CNNs (inductive biases) make them much more suitable to model computer vision tasks.</p>
<section id="mlps-and-images" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="mlps-and-images"><span class="header-section-number">1.1</span> MLPs and Images</h3>
<p>The input to an MLP is a vector <span class="math inline">\(\mathbf{x}^{(i)}\)</span>, which is transformed through multiple hidden layers to the output layer. Each hidden layer has a certain number of neurons, each connected to all neurons in the previous layer (fully-connected layers). The fully connected layers can only process 1-D vectors. Therefore, images <span class="math inline">\(\in \mathbb{R}^{H \times W \times C}\)</span> must be flattened into 1-D vectors <span class="math inline">\(\in \mathbb{R}^p\)</span>. Here, <span class="math inline">\(p= H \times W \times C\)</span>. This causes MLPs to become very large (having many learnable parameters) when applied to high-dimensional inputs such as images. See <a href="#fig-cnn-mlp" class="quarto-xref">Figure&nbsp;2</a> for an illustration.</p>
<div id="fig-cnn-mlp" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-mlp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/cnns/mlp_images_example.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-mlp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Illustration of an MLP applied to an image.
</figcaption>
</figure>
</div>
<p>In the CIFAR-10 dataset, which consists of very small images of <span class="math inline">\(\in \mathbb{R}^{3 \times 32 \times 32}\)</span> (colors, height, width), a single neuron in the first hidden layer has 3 * 32 * 32 = 3,072 weights to learn (see <a href="#fig-cnn-mlp-images" class="quarto-xref">Figure&nbsp;3</a>).</p>
<div id="fig-cnn-mlp-images" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-mlp-images-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/cnns/mlp_images.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-mlp-images-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Source: <span class="citation" data-cites="johnson_eecs_2019">Johnson (<a href="#ref-johnson_eecs_2019" role="doc-biblioref">2019</a>)</span>
</figcaption>
</figure>
</div>
<!-- :::{#fig-cnn-spatial-structure-mlp}

![](/assets/images/cnns/mlp-spatial-structure.png){width=600}

Source: @li_cs231n_2023
::: -->
<p>For larger images, which are more common in practice, the number of weights is correspondingly much larger. Furthermore, to identify all significant patterns many neurons are required, further increasing the number of parameters. This may lead to overfitting and difficulty in learning the weights.</p>
<p>The dimensionality of weight matrix <span class="math inline">\(\mathbf{W}\)</span> scales linearly with input size: <span class="math inline">\(\mathbf{W} \in \mathbb{R}^{d \times k}\)</span> (<span class="math inline">\(d\)</span> is input dimensionality, <span class="math inline">\(k\)</span> the number of neurons), or quadratically with the side length of an image (assuming a square image).</p>
<p>Since each neuron in the first hidden layer is connected to each input value (single color channel of a pixel) and each such connection has a specific weight (matrix <span class="math inline">\(\mathbf{W}\)</span> ) the number of weights for each neuron is identical to the number of input values. Therefore, the weights of individual neurons (columns of <span class="math inline">\(\mathbf{W}\)</span> ) can be reshaped to the image dimensionality and thus be visualized. This reveals the patterns a specific neuron is reacting to. See <a href="#fig-cnn-mlp-example-weights" class="quarto-xref">Figure&nbsp;4</a> for an illustration.</p>
<div id="fig-cnn-mlp-example-weights" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-mlp-example-weights-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/cnns/mlp_images_example_weights.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-mlp-example-weights-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Illustration of weight matrix <span class="math inline">\(\mathbf{W}\)</span> and how the weights of a single neuron can be reshaped into the input image dimensionality.
</figcaption>
</figure>
</div>
<p>Often the patterns we want to learn are not stationary. Ideally we want to recognize them under translations of the inputs, e.g.&nbsp;<span class="math inline">\(g(\mathbf{x})\)</span> where <span class="math inline">\(g()\)</span> is a spatial translation. <a href="#fig-cnns-mlp-pattern-shift" class="quarto-xref">Figure&nbsp;6</a> illustrates the problem, where the same pattern appears at different spatial positions.</p>
<div id="fig-cnns-mlp-pattern-shift" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnns-mlp-pattern-shift-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/cnns/mlp_images_example_pattern_shifts.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnns-mlp-pattern-shift-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Illustration of different inputs with the same pattern but shifted spatially. Illustrated are the weights of the first neuron. Green cells represent high weight values and yellow cells represent low values.
</figcaption>
</figure>
</div>
<div class="callout callout-style-simple callout-note callout-titled" title="Question: Patterns at different locations">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Question: Patterns at different locations
</div>
</div>
<div class="callout-body-container callout-body">
<p>Can an MLP learn to detect the same pattern at different spatial locations? If yes, how? See <a href="#fig-cnns-mlp-pattern-shift" class="quarto-xref">Figure&nbsp;6</a>.</p>
<p><strong>Think about it:</strong> If an MLP learns to detect a cat in the center of an image, what happens when the same cat appears in the corner?</p>
<details>
<summary>
Click for answer
</summary>
<p>MLPs must learn separate detectors for each possible position! This is extremely inefficient because:</p>
<ol type="1">
<li><strong>Parameter explosion</strong>: Need different weights for each spatial location</li>
<li><strong>Data requirements</strong>: Must see target pattern in every possible position during training</li>
<li><strong>Poor generalization</strong>: Fails on positions not seen during training</li>
</ol>
This is why MLPs with the same number of parameters perform much worse on image tasks compared to CNNs.
</details>
</div>
</div>
<!-- ::: {.callout-tip title="Interactive MLP Demo"}
Try this visualization to see how an MLP reacts to shifted inputs:

- [MLP vs CNN on 2D patterns](https://adamharley.com/nn_vis/mlp/2d.html)
- [CNN Filter Visualization](https://adamharley.com/nn_vis/cnn/2d.html)

Notice how CNNs reuse the same filter across all positions!
::: -->
<div class="callout callout-style-simple callout-note callout-titled" title="Question: MLPs and permutations.">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Question: MLPs and permutations.
</div>
</div>
<div class="callout-body-container callout-body">
<p>Can an MLP successfully learn patterns in images <span class="math inline">\(\mathbf{x}\)</span> that are permuted with a permutation matrix <span class="math inline">\(\mathbf{P}\)</span>, i.e., <span class="math inline">\(f(P(\mathbf{x}))\)</span>? See <a href="#fig-cnns-mlp-pattern-shift" class="quarto-xref">Figure&nbsp;6</a>.</p>
<p><strong>Hint:</strong> Think carefully - what if EVERY image is permuted with the SAME permutation matrix?</p>
<details>
<summary>
Click for the surprising answer
</summary>
<p><strong>Yes, MLPs CAN learn on consistently permuted images!</strong> This is a crucial insight:</p>
<p><strong>Why MLPs can handle consistent permutation:</strong></p>
<ol type="1">
<li><strong>Consistent mapping</strong>: If every image uses the same permutation <span class="math inline">\(P\)</span>, pixel position relationships are preserved</li>
<li><strong>Learning is still possible</strong>: The MLP can learn that “pixel at position 247 (originally position 12) connects to pixel at position 891 (originally position 13)”</li>
<li><strong>No information is lost</strong>: Permutation is just a reordering, not destruction of information</li>
</ol>
<p><strong>The real insight:</strong></p>
<ul>
<li>✅ <strong>Same permutation for all images</strong>: MLP can learn the task</li>
<li>❌ <strong>Different permutations per image</strong>: MLP cannot learn effectively</li>
<li>❌ <strong>No permutation but objects move around</strong>: MLP struggles with translation</li>
</ul>
<p><strong>Why this matters</strong>: This thought experiment reveals that MLPs’ problem with images isn’t about permutation per se, but about <strong>spatial relationships and translation invariance</strong>:</p>
<ol type="1">
<li><strong>Natural images</strong>: Objects appear at different positions → MLP must learn separate detectors for each position</li>
<li><strong>Consistently permuted images</strong>: Spatial relationships are preserved (just reordered) → MLP can learn</li>
<li><strong>CNNs</strong>: Built-in translation equivariance makes them efficient for natural images regardless of object position</li>
</ol>
<strong>Key takeaway</strong>: The issue isn’t that spatial structure is “destroyed” by permutation, but that CNNs are specifically designed to handle the <strong>translation variability</strong> in natural images efficiently.
</details>
</div>
</div>
<div id="fig-cnns-mlp-pattern-shift" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnns-mlp-pattern-shift-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/cnns/mlp_images_examples_permutation.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnns-mlp-pattern-shift-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Inputs transformed with a permuation matrix.
</figcaption>
</figure>
</div>
<p>To summarize MLPs on images:</p>
<ul>
<li>High-Dimensional: The size of an MLP scales with the input dimensionality which might blow beyond memory and compute budgets.</li>
<li>Locality: MLPs have no notion of locality and thus can’t exploit this inherent bias in natural images.</li>
<li>Stability under transformations: MLPs need to learn position-dependent patterns which is very inefficient.</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled" title="Try MLPs on images in your browser.">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Try MLPs on images in your browser.
</div>
</div>
<div class="callout-body-container callout-body">
<p>Try to draw the same digit at different positions and observe what happens!</p>
<ul>
<li><a href="https://adamharley.com/nn_vis/mlp/2d.html">MLP Demo</a></li>
</ul>
</div>
</div>
</section>
<section id="sec-cnn-invariance" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="sec-cnn-invariance"><span class="header-section-number">1.2</span> Invariance and Equivariance</h3>
<p>For many tasks involving image data, small variations in the input should either <strong>not</strong> change the model output (<em>invariance</em>) or should change the output <strong>in tandem</strong> with the input changes (<em>equivariance</em>).</p>
<p>A function <span class="math inline">\(f(\mathbf{x})\)</span> is <strong>invariant</strong> to transformation <span class="math inline">\(g(\mathbf{x})\)</span> if: <span class="math display">\[f(g(\mathbf{x})) = f(\mathbf{x})\]</span></p>
<p>A function <span class="math inline">\(f(\mathbf{x})\)</span> is <strong>equivariant</strong> to transformation <span class="math inline">\(g(\mathbf{x})\)</span> if: <span class="math display">\[f(g(\mathbf{x})) = g(f(\mathbf{x}))\]</span></p>
<div class="callout callout-style-default callout-important callout-titled" title="Intuition">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Intuition
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Invariant</strong> = “I don’t care about the transformation”</li>
<li><strong>Equivariant</strong> = “I transform my output the same way the input was transformed”</li>
</ul>
</div>
</div>
<p><a href="#fig-cnns-invariance-example" class="quarto-xref">Figure&nbsp;7</a> shows an example of image classification. In this case, a model needs to be invariant to spatial translations of objects (if these do not change the image’s class).</p>
<div class="callout callout-style-simple callout-note callout-titled" title="Question: Inivariance for Image Classification">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Question: Inivariance for Image Classification
</div>
</div>
<div class="callout-body-container callout-body">
<p>What type of transformations <span class="math inline">\(g()\)</span> does an image classification model as in <a href="#fig-cnns-invariance-example" class="quarto-xref">Figure&nbsp;7</a> need to be invariant to?</p>
<p><strong>Think about it:</strong> Consider what changes when the cows move to different positions in the image.</p>
<details>
<summary>
Click for answer
</summary>
<p><strong>Translation invariance</strong> is crucial for image classification. The model should output the same class label (e.g., “cow”) regardless of where the object appears in the image.</p>
<p><strong>Key transformations for classification:</strong></p>
<ol type="1">
<li><strong>Translation</strong>: Object position in the frame</li>
<li><strong>Scale</strong>: Object size (small vs large)</li>
<li><strong>Rotation</strong>: Object orientation (within reasonable limits)</li>
<li><strong>Lighting</strong>: Brightness, contrast variations</li>
</ol>
<p>For classification tasks specifically, we need <strong>invariance</strong> because: - The class label doesn’t depend on spatial location - A cow is a cow whether it’s in the center or corner of the image - The model should “pool” spatial information to make position-independent predictions</p>
<p>This is typically achieved by combining CNNs’ translation <strong>equivariance</strong> with operations like: - <strong>Pooling layers</strong> (max-pooling, average-pooling) - <strong>Global average pooling</strong> at the end of the network - These convert equivariant features into invariant predictions</p>
</details>
</div>
</div>
<div id="fig-cnns-invariance-example" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnns-invariance-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/cnns/cows2_small_invariance_collage.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnns-invariance-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: When objects in the input translate spatially, the output (in this case the classification of the images) does not change. The model <span class="math inline">\(f(\mathbf{x})\)</span> is thus invariant to spatial translations.
</figcaption>
</figure>
</div>
<p><a href="#fig-cnns-equivariance-example" class="quarto-xref">Figure&nbsp;8</a> shows an application of object detection. In this case, the model needs to be equivariant with respect to spatial translations (i.e., the bounding boxes that indicate an object’s location need to translate with the input). The model should remain invariant with respect to small changes in color, such as brightness adjustments.</p>
<div id="fig-cnns-equivariance-example" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnns-equivariance-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/cnns/cows2_small_detection_collage.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnns-equivariance-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: When objects in the input (first row) translate spatially, the detections (bounding boxes) change accordingly (bottom row). The model <span class="math inline">\(f(\mathbf{x})\)</span> that produces the bounding boxes is thus equivariant with respect to spatial translations.
</figcaption>
</figure>
</div>
<p>We can summarise the required properties for different tasks:</p>
<div class="callout callout-style-default callout-note callout-titled" title="When do we want each?">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>When do we want each?
</div>
</div>
<div class="callout-body-container callout-body">
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 60%">
</colgroup>
<thead>
<tr class="header">
<th>Task Type</th>
<th>Property Needed</th>
<th>Why?</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Classification</strong></td>
<td>Translation Invariance</td>
<td>Cat is still a cat regardless of position</td>
</tr>
<tr class="even">
<td><strong>Object Detection</strong></td>
<td>Translation Equivariance</td>
<td>If object moves right, bounding box moves right</td>
</tr>
<tr class="odd">
<td><strong>Semantic Segmentation</strong></td>
<td>Translation Equivariance</td>
<td>If input shifts, pixel-wise labels shift</td>
</tr>
</tbody>
</table>
<p>Understanding this distinction is crucial for choosing the right architecture!</p>
</div>
</div>
<!-- TODO: move or remove this becauwse the convolution operation has not yet been discussed in detail -->
<!-- ::: {.callout-tip title="Key Insight"}
**CNNs naturally provide translation equivariance** due to their convolution operation. However, they need additional components (like pooling or global average pooling) to achieve translation invariance for classification tasks.

This is why CNN architecture design matters so much!
::: -->
</section>
</section>
<section id="convolutional-neural-networks" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="convolutional-neural-networks"><span class="header-section-number">2</span> Convolutional Neural Networks</h2>
<p>Now that we’ve seen the limitations of MLPs for images, let’s explore how <strong>Convolutional Neural Networks (CNNs)</strong> address these challenges.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="The Big Idea">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>The Big Idea
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Instead of connecting every neuron to every input</strong>, CNNs use three key principles:</p>
<ol type="1">
<li><strong>Local Connectivity</strong>: Each neuron only looks at a small spatial region</li>
<li><strong>Parameter Sharing</strong>: The same filter/detector is used across all spatial locations</li>
<li><strong>Translation Equivariance</strong>: If the input shifts, the output shifts correspondingly</li>
</ol>
<p>These aren’t just technical tricks—they encode our <strong>prior knowledge</strong> about how images work!</p>
</div>
</div>
<section id="the-cnn-advantage" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="the-cnn-advantage"><span class="header-section-number">2.1</span> The CNN Advantage</h3>
<p>A single neuron in a CNN is only connected to a small portion (local connectivity) of the image (see <a href="#fig-cnn-cnn-spatial" class="quarto-xref">Figure&nbsp;9</a>). As a result:</p>
<ul>
<li>✅ <strong>Fewer Parameters</strong>: Neurons have far fewer connections than in MLPs</li>
<li>✅ <strong>Spatial Structure Preserved</strong>: No need to flatten 2D images into 1D vectors</li>
<li>✅ <strong>Translation Equivariance</strong>: Same detector works everywhere in the image</li>
<li>✅ <strong>Hierarchical Feature Learning</strong>: Stack layers to learn increasingly complex patterns</li>
</ul>
<div id="fig-cnn-cnn-spatial" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-cnn-spatial-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/cnns/cnn_spatial.jpg" class="img-fluid figure-img" width="200">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-cnn-spatial-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Source: <span class="citation" data-cites="johnson_eecs_2019">Johnson (<a href="#ref-johnson_eecs_2019" role="doc-biblioref">2019</a>)</span>
</figcaption>
</figure>
</div>
<!-- TODO: consider moving this part to a later section -->
<!-- ::: {.callout-note}
CNNs are used not only for image data but also for data with spatial dependencies/local structures. This includes not only images but also time series, videos, audio, and text. The key is that signals that are spatially close together should be interpreted together.
::: -->
</section>
<section id="convolutional-neural-networks-1" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="convolutional-neural-networks-1"><span class="header-section-number">2.2</span> Convolutional Neural Networks</h3>
<p>CNNs consist of a sequence of different layers. Each layer transforms activations from the previous layer into new activations through a differentiable operation. Below we look at the main layer types: convolutional layers, pooling layers, activation layers, and fully connected layers. Arranged in a specific sequence, this is referred to as the architecture of the model.</p>
<p><a href="#fig-cnn-convnet" class="quarto-xref">Figure&nbsp;10</a> shows an example architecture. The activation maps of the various layers are shown, representing the corresponding outputs of the layers.</p>
<div id="fig-cnn-convnet" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-convnet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/cnns/convnet.jpeg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-convnet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: The activations of a ConvNet architecture are shown. The input image is on the left and the predictions on the right. Source: <span class="citation" data-cites="li_cs231n_2022">Li (<a href="#ref-li_cs231n_2022" role="doc-biblioref">2022</a>)</span>.
</figcaption>
</figure>
</div>
<p>Sometimes different layers are combined and referred to as a block. For example, the combination of a convolutional layer followed by an activation layer and a pooling layer is often used. This would be a CONV-ACT-POOL block. –&gt;</p>
</section>
<section id="convolution-operation" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="convolution-operation"><span class="header-section-number">2.3</span> Convolution Operation</h3>
<p>Convolutional layers are the primary layers in CNNs, responsible for extracting visual features. The weights of a convolutional layer consist of a set of learnable filters. Each filter is typically small along the spatial dimensions (height, width) relative to the input but extends across the entire input depth. For example, a typical filter in the first layer might have dimensions <span class="math inline">\(3 \times 7 \times 7\)</span> (3 pixels for the three color channels, 7 pixels along height/width). During the forward pass, the filters are convolved along the height and width over the input. At each position, the dot product between the filter and input is calculated (viewing both as 1-D vectors), producing a 2-D activation map that represents the filter’s response at each position in the input. Intuitively, the CNN learns filters corresponding to typical visual patterns, such as edges and colors, in the first layer. A set of <span class="math inline">\(C_{out}\)</span> filters produces activation maps with a depth of <span class="math inline">\(C_{out}\)</span>.</p>
<div class="callout callout-style-default callout-important callout-titled" title="Filters vs Kernels">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Filters vs Kernels
</div>
</div>
<div class="callout-body-container callout-body">
<p>Filter and kernel are sometimes used synonymously. Here, we differentiate by considering a filter as 3-dimensional (CxHxW) and a kernel as 2-dimensional (HxW). A filter consists of C kernels.</p>
</div>
</div>
<p><span id="eq-cnn-convolution"><span class="math display">\[
S(i, j) = (K * I)(i, j) =\sum_m\sum_n I(i + m, j + n)K(m, n)
\tag{1}\]</span></span></p>
<p>With the following components:</p>
<ul>
<li><span class="math inline">\(I\)</span> is the input (for example, an image),</li>
<li><span class="math inline">\(K\)</span> is the kernel (typically smaller than <span class="math inline">\(I\)</span>),</li>
<li><span class="math inline">\(b\)</span> is a bias term added to the weighted sum,</li>
<li><span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> index the spatial positions.</li>
</ul>
<p>If <span class="math inline">\(I\)</span> is an RGB image (e.g., in the first layer of a CNN), the filter <span class="math inline">\(K\)</span> would have dimensions <span class="math inline">\(3 \times k \times k\)</span> (assuming a square kernel). More generally, we learn filters with dimensions <span class="math inline">\(C_{in} \times k \times k\)</span>.</p>
<p>Multiple filters, say <span class="math inline">\(C_{out}\)</span> filters, can be grouped together: <span class="math inline">\(C_{out} \times C_{in} \times k \times k\)</span>.</p>
<p>We often refer to such tensors as filters or filter banks.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Convolution in deep learning is typically implemented as cross-correlation. <a href="#eq-cnn-convolution" class="quarto-xref">Equation&nbsp;1</a> actually defines cross-correlation.</p>
</div>
</div>
<p>Convolution on RGB images is nicely illusrated in <a href="#fig-cnns-intro-example" class="quarto-xref">Figure&nbsp;11</a>. We can see that the filter is multiplied element-wise with the corresponding values in the image, the result is summed top produce one single value in the output.</p>
<div id="fig-cnns-intro-example" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnns-intro-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../assets/images/cnns/convolution_prince_rgb.png" class="img-fluid figure-img"></p>
<figcaption>Source: <span class="citation" data-cites="prince_understanding_2023">Prince (<a href="#ref-prince_understanding_2023" role="doc-biblioref">2023</a>)</span></figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnns-intro-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: An RGB image is represented as a grid of pixels, each consisting of three color values <span class="math inline">\(C_{in}=3\)</span>. Illusrated is how a filter with dimensionality <span class="math inline">\(3 \times 3 \times 3 (C \times H \times W)\)</span> is first multiplied elementwise with a specific input volume <span class="math inline">\(3 \times 3 \times 3 (C \times H \times W)\)</span> and then summed up to produce one single value in the output activation map (bright gray). Source: <span class="citation" data-cites="prince_understanding_2023">Prince (<a href="#ref-prince_understanding_2023" role="doc-biblioref">2023</a>)</span>
</figcaption>
</figure>
</div>
<p>The following figures show further step-by-step illustration of applying convolutions on input activations that produce multiple output activation maps.</p>
<p><a href="#fig-cnn-conv-one-number" class="quarto-xref">Figure&nbsp;12</a> illustrates how a filter produces one value per position by taking the dot product at a specific location (and adding the bias term).</p>
<div id="fig-cnn-conv-one-number" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-conv-one-number-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/cnns/cnn_conv_one_number.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-conv-one-number-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: Source: <span class="citation" data-cites="johnson_eecs_2019">Johnson (<a href="#ref-johnson_eecs_2019" role="doc-biblioref">2019</a>)</span>
</figcaption>
</figure>
</div>
<p><a href="#fig-cnn-conv-activation-map" class="quarto-xref">Figure&nbsp;13</a> illustrates how a filter produces one activation map, which is an output with one channel and (almost) the same spatial dimensionality as the input activations.</p>
<div id="fig-cnn-conv-activation-map" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-conv-activation-map-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/cnns/conv_activation_map.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-conv-activation-map-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13: Source: <span class="citation" data-cites="johnson_eecs_2019">Johnson (<a href="#ref-johnson_eecs_2019" role="doc-biblioref">2019</a>)</span>
</figcaption>
</figure>
</div>
<p><a href="#fig-cnn-conv-activation-map2" class="quarto-xref">Figure&nbsp;14</a> illustrates how multiple filters produce multiple outputs. After all, we often need to extract / detect different features, each represented by a specific filter.</p>
<div id="fig-cnn-conv-activation-map2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-conv-activation-map2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/cnns/conv_activation_map2.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-conv-activation-map2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14: Source: <span class="citation" data-cites="johnson_eecs_2019">Johnson (<a href="#ref-johnson_eecs_2019" role="doc-biblioref">2019</a>)</span>
</figcaption>
</figure>
</div>
<p>Finally, <a href="#fig-cnn-conv-activation-map3" class="quarto-xref">Figure&nbsp;15</a> illustrates how a convolutional layer is defined: it consists of a set of filters and their corresponding biases. Note that the output is essentially a <span class="math inline">\(C_{out}\)</span>-dimensional image.</p>
<div id="fig-cnn-conv-activation-map3" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-conv-activation-map3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/cnns/conv_activation_map3.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-conv-activation-map3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15: Source: <span class="citation" data-cites="johnson_eecs_2019">Johnson (<a href="#ref-johnson_eecs_2019" role="doc-biblioref">2019</a>)</span>
</figcaption>
</figure>
</div>
<p>The data is processed in mini-batches, i.e., multiple images at once, as shown in <a href="#fig-cnn-conv-activation-map4" class="quarto-xref">Figure&nbsp;16</a>.</p>
<div id="fig-cnn-conv-activation-map4" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-conv-activation-map4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/cnns/conv_activation_map4.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-conv-activation-map4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16: Source: <span class="citation" data-cites="johnson_eecs_2019">Johnson (<a href="#ref-johnson_eecs_2019" role="doc-biblioref">2019</a>)</span>
</figcaption>
</figure>
</div>
</section>
<section id="padding-stride-kernel-size-dilation" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="padding-stride-kernel-size-dilation"><span class="header-section-number">2.4</span> Padding, Stride, Kernel Size, Dilation</h3>
<p>To define a convolution, various hyperparameters need to be set. Some of the most important ones are:</p>
<ul>
<li>Padding: How much padding is added to the input?</li>
<li>Stride: What is the step size (stride) of the kernel?</li>
<li>Kernel-Size: What is the kernel size?</li>
<li>Dilation: What is the dilation rate?</li>
</ul>
<p><strong>Stride</strong> determines how the filters are convolved over the input activations, essentially the step size. If the stride is 1, the filter moves one pixel at a time to compute the next activation (see <a href="#fig-cnn-stride1" class="quarto-xref">Figure&nbsp;17</a>). If the stride is greater, e.g., 2, it moves two pixels at a time, making the activation maps smaller in width and height (see <a href="#fig-cnn-stride2" class="quarto-xref">Figure&nbsp;18</a>).</p>
<div id="fig-cnn-stride1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-stride1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/cnns/stride1_example.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-stride1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17: Convolution with stride (1, 1). Left the input activations with (red) the individual positions of the center pixel of the kernel / filter.
</figcaption>
</figure>
</div>
<div id="fig-cnn-stride2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-stride2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/cnns/stride2_example.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-stride2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;18: Convolution with stride (2, 2). Left the input activations with (red) the individual positions of the center pixel of the kernel / filter.
</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="Stride > 1 often used!">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Stride &gt; 1 often used!
</div>
</div>
<div class="callout-body-container callout-body">
<p>Convolutions with stride provide a way to reduce the spatial dimensionality within a CNN and are commonly used. We will see later that other operations, such as pooling layers, also reduce spatial dimensionality. Using convolutions, however, is sometimes advantageous, since the downsampling is learned.</p>
</div>
</div>
<p><strong>Padding</strong> refers to adding (typically) zeros to the border of the input activations before performing the convolution. This can be useful to ensure, for example, that the spatial dimensions of the activation maps are identical to those of the input activations. This is essential for segmentation tasks. <a href="#fig-cnn-padding" class="quarto-xref">Figure&nbsp;19</a> illustrates how padding helps maintain the spatial resolution when padding the input activations.</p>
<div id="fig-cnn-padding" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-padding-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/cnns/padding_example_result.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-padding-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;19: Left: Input (Yellow) with Zero-Padding (white border), Middle: Filter, Right: Output.
</figcaption>
</figure>
</div>
<p><strong>Kernel-size</strong> determines the spatial extend from which a value in the output activation map migh aggregate values. It is the receptive field of an output value with respect to the input values. <a href="#fig-cnn-kernel-size3x3" class="quarto-xref">Figure&nbsp;20</a> and <a href="#fig-cnn-kernel-size5x5" class="quarto-xref">Figure&nbsp;21</a> show convolutions with different kernel sizes. Note that the amount of padding increases with kernel-size. Kernels usually have an odd number of pixels in height and width to make sure they have a center pixel. Also, the number of parameters increases with kernel-size <span class="math inline">\((C_{in} \times k \times k) + C_{out}\)</span>, which is often an important consideration.</p>
<div id="fig-cnn-kernel-size3x3" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-kernel-size3x3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/cnns/kernel_size_example_3x3.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-kernel-size3x3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;20: Convolution with zero-padding and <span class="math inline">\(3 \times 3\)</span> kernel that produces activations maps with unchanged spatial resolution.
</figcaption>
</figure>
</div>
<div id="fig-cnn-kernel-size5x5" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-kernel-size5x5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/cnns/kernel_size_example_5x5.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-kernel-size5x5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;21: Convolution with zero-padding and <span class="math inline">\(5 \times 5\)</span> kernel that produces activations maps with unchanged spatial resolution.
</figcaption>
</figure>
</div>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Question</strong></p>
<p>What is the difference between using 3×3 vs 5×5 kernels? What is the same?</p>
<p><strong>Think first</strong>: Consider computational cost, receptive field, and parameter count.</p>
<details>
<summary>
Click for detailed comparison
</summary>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 37%">
<col style="width: 37%">
</colgroup>
<thead>
<tr class="header">
<th>Aspect</th>
<th>3×3 Kernel</th>
<th>5×5 Kernel</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Parameters</strong></td>
<td>9 weights (plus bias) per input channel</td>
<td>25 weights (plus bias) per input channel</td>
</tr>
<tr class="even">
<td><strong>Computation</strong></td>
<td>Less (fewer multiplications)</td>
<td>More (more multiplications)</td>
</tr>
<tr class="odd">
<td><strong>Receptive Field</strong></td>
<td>Smaller (grows slowly)</td>
<td>Larger (grows faster)</td>
</tr>
<tr class="even">
<td><strong>Detail Capture</strong></td>
<td>Fine-grained features</td>
<td>Broader spatial patterns</td>
</tr>
</tbody>
</table>
<p><strong>What’s the same</strong>: - Both preserve translation equivariance - Both use parameter sharing - Both can learn spatial features</p>
</details>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="Best Practice">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Best Practice
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Modern CNN Design</strong>: Use multiple 3×3 convolutions instead of larger kernels to get:</p>
<ul>
<li>✅ Fewer parameters</li>
<li>✅ More non-linearity (more activation functions)</li>
<li>✅ Better gradient flow</li>
<li>✅ Similar receptive field coverage</li>
</ul>
</div>
</div>
<p><strong>Dilation</strong> specifies how much the kernel is spread out and thus directly influences the receptive field. <a href="#fig-cnn-dilation" class="quarto-xref">Figure&nbsp;22</a> shows how it works.</p>
<div id="fig-cnn-dilation" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-dilation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/cnns/dilation_example.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-dilation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;22: Illustration of how a kernel with dilation=2 (middle) is applied to the input activations (left) to produce activations (right). Note the reduced spatial dimensionality of the output activations.
</figcaption>
</figure>
</div>
<div class="callout callout-style-simple callout-note callout-titled" title="Questoin: Dilation?">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Questoin: Dilation?
</div>
</div>
<div class="callout-body-container callout-body">
<p>Why would we use dilation <span class="math inline">\(d &gt; 1\)</span>?</p>
<p><strong>Consider</strong>: What happens to the receptive field? What about computational cost?</p>
<details>
<summary>
Click for comprehensive answer
</summary>
<p><strong>Dilation increases receptive field without adding parameters or computation!</strong></p>
<p><strong>Benefits of dilation</strong>:</p>
<ol type="1">
<li><strong>Larger receptive field</strong>: See more context without more layers</li>
<li><strong>Same parameter count</strong>: Filter size stays the same</li>
<li><strong>Multi-scale features</strong>: Different dilation rates capture different scales</li>
</ol>
<p><strong>Use cases</strong>:</p>
<ul>
<li><strong>Semantic segmentation</strong>: Need large receptive fields while preserving spatial resolution</li>
<li><strong>Dense prediction tasks</strong>: Want to see global context for local predictions</li>
<li><strong>Efficient architectures</strong>: Alternative to deeper networks</li>
</ul>
<p><strong>Trade-offs</strong>:</p>
<ul>
<li>✅ Larger receptive field with same parameters</li>
<li>❌ May miss fine-grained spatial relationships</li>
<li>❌ Can create “grid artifacts” if not used carefully</li>
</ul>
<p><strong>Example</strong>: A 3×3 kernel with dilation=2 has the receptive field of a 5×5 kernel but only 9 parameters instead of 25!</p>
<p><strong>Atrous/Dilated Convolutions</strong> are especially popular in:</p>
<ul>
<li><strong>DeepLab</strong> (semantic segmentation)</li>
<li><strong>WaveNet</strong> (audio generation)</li>
<li><strong>Dilated ResNets</strong> (dense prediction)</li>
</ul>
<p>The key insight: <strong>effectively expand the receptive field</strong>!</p>
</details>
</div>
</div>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><span class="citation" data-cites="dumoulin_guide_2016">Dumoulin and Visin (<a href="#ref-dumoulin_guide_2016" role="doc-biblioref">2016</a>)</span> has created some animations for better understanding of convolutions and published them here: <a href="https://github.com/vdumoulin/conv_arithmetic">https://github.com/vdumoulin/conv_arithmetic</a>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../assets/images/cnns/no_padding_no_strides.gif" class="img-fluid figure-img" width="200"></p>
<figcaption>Convolving a 3x3 kernel over a 5x5 input without padding and with stride 1x1. Source <span class="citation" data-cites="dumoulin_guide_2016">Dumoulin and Visin (<a href="#ref-dumoulin_guide_2016" role="doc-biblioref">2016</a>)</span></figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="calculations-spatial-dimensionality-of-activation-maps" class="level3" data-number="2.5">
<h3 data-number="2.5" class="anchored" data-anchor-id="calculations-spatial-dimensionality-of-activation-maps"><span class="header-section-number">2.5</span> Calculations: Spatial Dimensionality of Activation Maps</h3>
<p>You can calculate the dimensionality of the activation maps with the following formulas:</p>
<ul>
<li><span class="math inline">\(i\)</span>: Side length of the input activations (assumption: square inputs)</li>
<li><span class="math inline">\(k\)</span>: Kernel size (assumption: square kernel)</li>
<li><span class="math inline">\(o\)</span>: Side length of the output activation maps</li>
<li><span class="math inline">\(s\)</span>: Stride (assumption: same stride along spatial dimensions)</li>
<li><span class="math inline">\(p\)</span>: Number of paddings on each side (assumption: same number of paddings along spatial dimensions)</li>
<li><span class="math inline">\(d\)</span>: Dilation rate.</li>
</ul>
<p><span class="math display">\[\begin{equation}
o = \left\lfloor \frac{i + 2p - d\cdot(k-1) - 1}{s} + 1 \right\rfloor
\end{equation}\]</span></p>
<!-- TODO: hide Answer part of the followig questions  -->
<div class="callout callout-style-default callout-note callout-titled" title="🧮 Quiz: Output Shape Calculation">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-16-contents" aria-controls="callout-16" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>🧮 Quiz: Output Shape Calculation
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-16" class="callout-16-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Problem</strong>: You’re designing a CNN layer with the following parameters:</p>
<ul>
<li>Input size: <span class="math inline">\(i = 32\)</span> (height and width)</li>
<li>Kernel size: <span class="math inline">\(k = 3\)</span></li>
<li>Padding: <span class="math inline">\(p = 1\)</span></li>
<li>Stride: <span class="math inline">\(s = 2\)</span></li>
<li>Dilation: <span class="math inline">\(d = 1\)</span></li>
</ul>
<p><strong>Question</strong>: What is the output size <span class="math inline">\(o\)</span>?</p>
<details>
<summary>
Click for step-by-step solution
</summary>
<p><strong>Step-by-step solution</strong>:</p>
<ol type="1">
<li>Apply the formula: <span class="math inline">\(o = \lfloor \frac{i + 2p - d(k-1) - 1}{s} + 1 \rfloor\)</span></li>
<li>Substitute: <span class="math inline">\(o = \lfloor \frac{32 + 2(1) - 1(3-1) - 1}{2} + 1 \rfloor\)</span></li>
<li>Simplify: <span class="math inline">\(o = \lfloor \frac{32 + 2 - 2 - 1}{2} + 1 \rfloor\)</span></li>
<li>Calculate: <span class="math inline">\(o = \lfloor \frac{31}{2} + 1 \rfloor = \lfloor 15.5 + 1 \rfloor = 16\)</span></li>
</ol>
<p><strong>Answer</strong>: <span class="math inline">\(o = 16\)</span></p>
<p><strong>Intuition</strong>: With stride 2, we’re roughly halving the spatial dimensions, but padding helps maintain some size.</p>
</details>
</div>
</div>
</div>
</section>
<section id="calculations-number-of-weights-in-a-convolutional-layer" class="level3" data-number="2.6">
<h3 data-number="2.6" class="anchored" data-anchor-id="calculations-number-of-weights-in-a-convolutional-layer"><span class="header-section-number">2.6</span> Calculations: Number of Weights in a Convolutional Layer</h3>
<p>You can calculate the total number of weights in a convolutional layer with the following formula:</p>
<ul>
<li><span class="math inline">\(k\)</span>: Kernel size (assumption: square kernel, so kernel has dimensions <span class="math inline">\(k \times k\)</span>)</li>
<li><span class="math inline">\(C_{in}\)</span>: Number of input channels</li>
<li><span class="math inline">\(C_{out}\)</span>: Number of output channels</li>
</ul>
<p><span class="math display">\[\begin{equation}
\text{Total Weights} = (k \times k) \times C_{in} \times C_{out}
\end{equation}\]</span></p>
<p>Each filter has a size of <span class="math inline">\(k \times k \times C_{in}\)</span>, and there are <span class="math inline">\(C_{out}\)</span> filters in total, resulting in <span class="math inline">\((k \times k \times C_{in}) \times C_{out}\)</span> weights.</p>
<p><strong>Biases:</strong> If each output channel has a bias term, add an additional <span class="math inline">\(C_{out}\)</span> weights for the biases.</p>
<p><span class="math display">\[\begin{equation}
\text{Total Parameters} = (k \times k \times C_{in} \times C_{out}) + C_{out}
\end{equation}\]</span></p>
<div class="callout callout-style-default callout-note callout-titled" title="🧮 Quiz: Parameter Counting">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-17-contents" aria-controls="callout-17" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>🧮 Quiz: Parameter Counting
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-17" class="callout-17-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Scenario</strong>: Design a convolutional layer with:</p>
<ul>
<li>Input: <span class="math inline">\(3 \times 32 \times 32\)</span> (RGB image)</li>
<li>64 output channels</li>
<li>Kernel size: <span class="math inline">\(5 \times 5\)</span></li>
<li>Include bias terms</li>
</ul>
<p><strong>Questions</strong>:</p>
<ol type="1">
<li>How many weights in the filters?</li>
<li>How many bias parameters?</li>
<li>Total parameters?</li>
<li>If we used a fully connected layer instead, how many parameters would that require?</li>
</ol>
<details>
<summary>
Click for solutions
</summary>
<p><strong>Solutions</strong>:</p>
<ol type="1">
<li><strong>Filter weights</strong>: <span class="math inline">\((5 \times 5) \times 3 \times 64 = 4{,}800\)</span></li>
<li><strong>Bias parameters</strong>: <span class="math inline">\(64\)</span> (one per output channel)</li>
<li><strong>Total parameters</strong>: <span class="math inline">\(4{,}800 + 64 = 4{,}864\)</span></li>
<li><strong>Fully connected alternative</strong>: <span class="math inline">\((32 \times 32 \times 3) \times 64 = 196{,}608\)</span></li>
</ol>
<p><strong>Insight</strong>: CNNs use <strong>40× fewer parameters</strong> while being more effective for images!</p>
</details>
</div>
</div>
</div>
<!-- TODO: verify whether k=7 and justification really makes sense -->
<!-- ::: {.callout-note appearance="simple"}

**Question**

**Real-World Scenario**: You're building a medical imaging system for chest X-ray analysis.

**Scenario:**

- Input X-rays: $1 \times 512 \times 512$ (grayscale)
- First conv layer: 32 filters, $k=7 \times 7$, $s=2$, $p=3$, $d=1$

**Your Tasks:**

1. What is the output activation map size $(C_{out}, H_{out}, W_{out})$?
2. How many parameters does this layer have?
3. Why might we choose $k=7$ for medical images vs $k=3$ for natural images?
4. What's the receptive field of each output pixel?

<details>
<summary>Click for detailed solutions</summary>

**Solutions:**

1. **Output size**:
   - $C_{out} = 32$ (number of filters)
   - $H_{out} = W_{out} = \lfloor \frac{512 + 2(3) - 1(7-1) - 1}{2} \rfloor + 1 = 256$
   - **Answer**: $(32, 256, 256)$

2. **Parameters**:
   - Filter weights: $(7 \times 7) \times 1 \times 32 = 1{,}568$
   - Biases: $32$
   - **Total**: $1{,}600$ parameters

3. **Why $k=7$ for medical images?**
   - Medical features often span larger spatial areas (organ boundaries, lesions)
   - Need to capture broader anatomical context early
   - Higher resolution medical images benefit from larger initial receptive fields
   - Natural images have more fine-grained local features

4. **Receptive field**: $7 \times 7$ (same as kernel size for first layer)

**Key Insight**: Domain knowledge should inform architectural choices!
</details>

::: -->
<!-- ::: {.callout-tip title="Domain-Specific Design"}
Different domains need different CNN architectures:

| Domain | Typical Kernel Sizes | Why? |
|--------|---------------------|------|
| **Natural Images** | 3×3, 1×1 | Fine details, efficiency |
| **Medical Imaging** | 5×5, 7×7 | Larger anatomical features |
| **Satellite Imagery** | 7×7, 9×9 | Geographic features at scale |
| **Text Recognition** | Rectangular (1×3, 3×1) | Character and word shapes |

: {tbl-colwidths="[30,20,50]"}

Understanding your data is crucial for good architecture design!
::: -->
<!-- TODO: figure out how to handel the Properties section here and some of this information that was already provided previously    -->
</section>
</section>
<section id="properties" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="properties"><span class="header-section-number">3</span> Properties</h2>
<section id="local-sparse-connectivity-parameter-sharing" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="local-sparse-connectivity-parameter-sharing"><span class="header-section-number">3.1</span> Local (Sparse) Connectivity &amp; Parameter Sharing</h3>
<p>Fully connected layers are impractical when working with high-dimensional inputs like images. If all neurons in a layer were connected to all previous neurons, the number of parameters to be estimated would increase massively, which is inefficient and leads to overfitting. Each neuron in a CNN is therefore only connected to a local region of the input volume (see <a href="#fig-cnn-conv-one-number" class="quarto-xref">Figure&nbsp;12</a>). The spatial extent of this region is a hyperparameter and is called the receptive field of a neuron (kernel size) on the input volume. The connections along the depth (C) extend over the entire depth of the input volume <span class="math inline">\(C_{in}\)</span>. The connections are therefore local along the spatial dimensions (width and height) but complete along the depth.</p>
<p>Parameter sharing in convolutional layers is used to reduce the number of parameters. Since the filters are convolved over the inputs, the individual weights of the filters are identical over the spatial extent of the input volume. One of the main assumptions behind CNNs is the following: If it is useful to learn a specific (visual) feature at a certain position, then it is probably useful at other positions as well. In other words: If I learn filters that detect edges, corners, or cats, then it is a reasonable assumption that I want to do this throughout the image.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Sometimes parameter sharing does not make sense. This can be the case, for example, if we have centered structures in the images. Then you might want to learn position-dependent features. An example is images of faces that have been centered, where you might want to learn filters that detect the mouth only in the lower middle area (locally connected layers).</p>
</div>
</div>
<p><a href="#lst-cnns-mlp" class="quarto-xref">Listing&nbsp;1</a> shows the number of parameters of an MLP, while <a href="#lst-cnns-cnn" class="quarto-xref">Listing&nbsp;2</a> shows the number of parameters of a CNN (each with two hidden layers) on the CIFAR10 dataset.</p>
<div id="7672799b" class="cell" data-execution_count="1">
<div id="lst-cnns-mlp" class="python cell-code listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-cnns-mlp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;1: PyTorch example of an MLP.
</figcaption>
<div aria-describedby="lst-cnns-mlp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchinfo</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MLP(nn.Module):</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.flatten <span class="op">=</span> nn.Flatten()</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layer1 <span class="op">=</span> nn.Linear(<span class="dv">3</span> <span class="op">*</span> <span class="dv">32</span> <span class="op">*</span> <span class="dv">32</span>, <span class="dv">64</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layer2 <span class="op">=</span> nn.Linear(<span class="dv">64</span>, <span class="dv">32</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_layer <span class="op">=</span> nn.Linear(<span class="dv">32</span>, <span class="dv">10</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.flatten(x)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.hidden_layer1(x))</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.hidden_layer2(x))</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.output_layer(x)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>net <span class="op">=</span> MLP()</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(torchinfo.summary(net, input_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">32</span>, <span class="dv">32</span>)))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</figure>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
MLP                                      [1, 10]                   --
├─Flatten: 1-1                           [1, 3072]                 --
├─Linear: 1-2                            [1, 64]                   196,672
├─Linear: 1-3                            [1, 32]                   2,080
├─Linear: 1-4                            [1, 10]                   330
==========================================================================================
Total params: 199,082
Trainable params: 199,082
Non-trainable params: 0
Total mult-adds (Units.MEGABYTES): 0.20
==========================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 0.00
Params size (MB): 0.80
Estimated Total Size (MB): 0.81
==========================================================================================</code></pre>
</div>
</div>
<div id="c7026d5d" class="cell" data-execution_count="2">
<div id="lst-cnns-cnn" class="python cell-code listing quarto-float quarto-figure quarto-figure-left anchored">
<figure class="quarto-float quarto-float-lst figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-lst" id="lst-cnns-cnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Listing&nbsp;2: PyTorch example of a CNN.
</figcaption>
<div aria-describedby="lst-cnns-cnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchinfo</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CNN(nn.Module):</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(<span class="dv">3</span>, <span class="dv">16</span>, <span class="dv">7</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(<span class="dv">16</span>, <span class="dv">16</span>, <span class="dv">3</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.flatten <span class="op">=</span> nn.Flatten()</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_layer <span class="op">=</span> nn.Linear(<span class="dv">16</span> <span class="op">*</span> <span class="dv">8</span> <span class="op">*</span> <span class="dv">8</span> , <span class="dv">10</span>)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.conv1(x))</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.conv2(x))</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.flatten(x)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.output_layer(x)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>cnn <span class="op">=</span> CNN()</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(torchinfo.summary(cnn, input_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">32</span>, <span class="dv">32</span>)))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</figure>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
CNN                                      [1, 10]                   --
├─Conv2d: 1-1                            [1, 16, 16, 16]           2,368
├─Conv2d: 1-2                            [1, 16, 8, 8]             2,320
├─Flatten: 1-3                           [1, 1024]                 --
├─Linear: 1-4                            [1, 10]                   10,250
==========================================================================================
Total params: 14,938
Trainable params: 14,938
Non-trainable params: 0
Total mult-adds (Units.MEGABYTES): 0.76
==========================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 0.04
Params size (MB): 0.06
Estimated Total Size (MB): 0.11
==========================================================================================</code></pre>
</div>
</div>
<p>We can see that the number of parameters of the CNN is much smaller. Feel free to compare performances.</p>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Question</strong></p>
<p>Why in <a href="#lst-cnns-cnn" class="quarto-xref">Listing&nbsp;2</a> is <code>pading=3</code> in <code>self.conv1 = nn.Conv2d(3, 16, 7, stride=2, padding=3)</code>? Would <code>padding=0</code>also be possible?</p>
<details>
<summary>
Click for answer
</summary>
<p><strong>Answer:</strong></p>
<p>The <code>padding=3</code> is chosen to maintain the spatial dimensions when using <code>stride=2</code>.</p>
<p>Using the output size formula: <span class="math display">\[o = \left\lfloor \frac{i + 2p - d\cdot(k-1) - 1}{s} + 1 \right\rfloor\]</span></p>
<p>With the given parameters: - Input: <span class="math inline">\(i = 32\)</span> - Kernel: <span class="math inline">\(k = 7\)</span> - Stride: <span class="math inline">\(s = 2\)</span> - Dilation: <span class="math inline">\(d = 1\)</span> - Padding: <span class="math inline">\(p = 3\)</span></p>
<p><span class="math display">\[o = \left\lfloor \frac{32 + 2(3) - 1(7-1) - 1}{2} + 1 \right\rfloor = \left\lfloor \frac{32 + 6 - 6 - 1}{2} + 1 \right\rfloor = \left\lfloor \frac{31}{2} + 1 \right\rfloor = 16\]</span></p>
<p><strong>Would <code>padding=0</code> work?</strong></p>
<p>Yes, but it would produce a different output size: <span class="math display">\[o = \left\lfloor \frac{32 + 0 - 6 - 1}{2} + 1 \right\rfloor = \left\lfloor \frac{25}{2} + 1 \right\rfloor = 13\]</span></p>
<p><strong>Why <code>padding=3</code> is preferred here:</strong></p>
<ul>
<li>Maintains a “nice” output dimension (<span class="math inline">\(16 \times 16\)</span>) that’s easy to work with in subsequent layers</li>
<li>Preserves more spatial information from the edges of the input</li>
<li>Common practice: use padding to keep output size as <span class="math inline">\(\lfloor \frac{i}{s} \rfloor\)</span> when possible</li>
</ul>
</details>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>A convolution can also be expressed as a single matrix multiplication and is thus a special case of a linear transformation (such as in an MLP). <a href="#fig-cnn-conv-as-matrix-illustration" class="quarto-xref">Figure&nbsp;23</a> shows how a linear layer in an MLP (top left) has different connections between each input <span class="math inline">\(x_1, \cdots, x_6\)</span> and neurons in a hidden layer <span class="math inline">\(h_1, \cdots, h_6\)</span> and the corresponding weight matrix (bottom left). On the right, the same is shown for a 1-D convolution with kernel size 3, however, note that the weight matrix is sparse (white values) and that the weights are shared (repeated patterns across the diagonal).</p>
<div id="fig-cnn-conv-as-matrix-illustration" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-conv-as-matrix-illustration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/cnns/1d_example_weight_matrix_mlp_vs_cnn.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-conv-as-matrix-illustration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;23: Illustration of linear layer in an MLP (left column), including graph (top) and weight matrix (bottom). Convolutional layer (right) represented as a linear transformation. Source <span class="citation" data-cites="prince_understanding_2023">Prince (<a href="#ref-prince_understanding_2023" role="doc-biblioref">2023</a>)</span>.
</figcaption>
</figure>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note callout-titled" title="Question: Linear Transformation">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-20-contents" aria-controls="callout-20" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Question: Linear Transformation
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-20" class="callout-20-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>How should the linear transformation be defined to obtain the desired result in <a href="#fig-cnn-linear-transf-calc" class="quarto-xref">Figure&nbsp;24</a>? How many parameters are needed? How could this be done with a convolution?</p>
<div id="fig-cnn-linear-transf-calc" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-linear-transf-calc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/cnns/linear_transf.png" class="img-fluid figure-img" width="800">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-linear-transf-calc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;24: Input in 2-D (top left), the flattened version of it (bottom left), expected output (right), and unknown transformation (middle).
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="translation-equivariance" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="translation-equivariance"><span class="header-section-number">3.2</span> Translation Equivariance</h3>
<p>Convolutions are translation equivariant, as illustrated well in the following example:</p>
<div id="fig-cnn-translation-equivariance" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-translation-equivariance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/qoWAFBYOtoU?start=50" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-translation-equivariance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;25: Illustration of translation equivariance in convolutional layers.
</figcaption>
</figure>
</div>
<p>See <a href="#sec-cnn-invariance" class="quarto-xref">Section&nbsp;1.2</a> for more information.</p>
</section>
<section id="stacking-convolutions" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="stacking-convolutions"><span class="header-section-number">3.3</span> Stacking Convolutions</h3>
<p>Multiple convolutions can be executed in sequence (stacking). Each convolution is performed on the activation maps of another, preceeding convolution. <a href="#fig-cnn-conv-stacking" class="quarto-xref">Figure&nbsp;26</a> illustrates the process.</p>
<div id="fig-cnn-conv-stacking" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-conv-stacking-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/cnns/conv_stacking.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-conv-stacking-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;26: Source: <span class="citation" data-cites="johnson_eecs_2019">Johnson (<a href="#ref-johnson_eecs_2019" role="doc-biblioref">2019</a>)</span>
</figcaption>
</figure>
</div>
<p>A convolution is therefore not only performed directly on the input (e.g., images) but is generally defined on inputs of dimensionality <span class="math inline">\(C \times H \times W\)</span>.</p>
<p><strong>The principle of stacking convolutional layers and therefore convolutions are not just applied on raw pixel values from an image is important to understand!</strong> While the filters in the first convolutional layer, which directly operate on the input image, typically learn basic features such as lines in different orientations or colors, filters in subsequent layers learn more complex features by combining the features from preceeding layers. This is similar to our visual cortex. <span class="citation" data-cites="Zeiler2014">Zeiler and Fergus (<a href="#ref-Zeiler2014" role="doc-biblioref">2014</a>)</span> used a special technique to visualize patterns that filters in deeper layers are reacting to (see <a href="#fig-cnn-conv-zeiler" class="quarto-xref">Figure&nbsp;27</a>).</p>
<p>However, non-linear activation functions must be used between the convolutions. Otherwise, the stacked convolution can be expressed with a simple convolution (similar to an MLP, which can be expressed with a linear transformation without activation functions).</p>
<div id="fig-cnn-conv-zeiler" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-conv-zeiler-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/cnns/zeiler_learned_filters.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-conv-zeiler-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;27: Shown are patterns the learned flters are reacting to for several layers (in gray) and the corresponding image patches that achieve maximum activation values for these patterns. Source: <span class="citation" data-cites="Zeiler2014">Zeiler and Fergus (<a href="#ref-Zeiler2014" role="doc-biblioref">2014</a>)</span>
</figcaption>
</figure>
</div>
</section>
<section id="sec-cnn-receptive-field" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="sec-cnn-receptive-field"><span class="header-section-number">3.4</span> Receptive Field</h3>
<p>The receptive field defines which inputs influence the activations of a neuron. The receptive field of a neuron with respect to the input activations (the image pixels) is quite important. If the receptive field is smaller than the objects of interest, the network is unable to “see” the whole object. This is particularly problematic for object detection. See <a href="#fig-cnn-receptive-field-1" class="quarto-xref">Figure&nbsp;28</a> and <a href="#fig-cnn-receptive-field-4" class="quarto-xref">Figure&nbsp;31</a> to illustrate the concept.</p>
<div id="fig-cnn-receptive-field-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-receptive-field-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/cnns/receptive_field1.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-receptive-field-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;28: Receptive field with a 5x5 kernel.
</figcaption>
</figure>
</div>
<div id="fig-cnn-receptive-field-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-receptive-field-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/cnns/receptive_field2.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-cnn-receptive-field-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29
</figcaption>
</figure>
</div>
<div id="fig-cnn-receptive-field-3" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-receptive-field-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/cnns/receptive_field3.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-receptive-field-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;30: Receptive field with a 5x5 kernel and two layers.
</figcaption>
</figure>
</div>
<div id="fig-cnn-receptive-field-4" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-receptive-field-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/cnns/receptive_field4.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-receptive-field-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;31: Receptive field with a 5x5 kernel and two layers. The output activation at the center (right) aggregates values over the union of all blue boxes in the left figure and has thus a receptive field of <span class="math inline">\(9 \times 9\)</span> with respect to the input activations.
</figcaption>
</figure>
</div>
<p>The receptive field <span class="math inline">\(r_0\)</span> of a convolutional neural network can be calculated as follows:</p>
<ol type="1">
<li>Define the recurrence relation:</li>
</ol>
<p><span class="math display">\[\begin{equation}
r_{l-1} = s_l \cdot r_l + (k_l - s_l)
\end{equation}\]</span></p>
<ul>
<li><span class="math inline">\(r_l\)</span>: Receptive field at layer <span class="math inline">\(l\)</span>.</li>
<li><span class="math inline">\(s_l\)</span>: Stride of layer <span class="math inline">\(l\)</span>.</li>
<li><span class="math inline">\(k_l\)</span>: Kernel size of layer <span class="math inline">\(l\)</span>.</li>
</ul>
<ol start="2" type="1">
<li>Solving this recurrence relation, we get the closed-form solution for the receptive field at the input layer <span class="math inline">\(r_0\)</span>:</li>
</ol>
<p><span class="math display">\[\begin{equation}
r_0 = \sum_{l=1}^L \left( (k_l - 1) \prod_{i=1}^{l-1} s_i \right) + 1
\end{equation}\]</span></p>
<ul>
<li><span class="math inline">\(L\)</span>: Total number of layers.</li>
<li><span class="math inline">\(k_l\)</span>: Kernel size at layer <span class="math inline">\(l\)</span>.</li>
<li><span class="math inline">\(s_i\)</span>: Stride at layer <span class="math inline">\(i\)</span>, from layer 1 up to <span class="math inline">\(l-1\)</span>.</li>
</ul>
<p><span class="citation" data-cites="araujo_computing_2019">Araujo, Norris, and Sim (<a href="#ref-araujo_computing_2019" role="doc-biblioref">2019</a>)</span> have written a nice article about receptive fields and how to calculate them (also refer to this article for more details on the notation and justification).</p>
<div class="callout callout-style-default callout-note callout-titled" title="🎯 Challenge Quiz: Receptive Field">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-21-contents" aria-controls="callout-21" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>🎯 Challenge Quiz: Receptive Field
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-21" class="callout-21-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Network Architecture</strong>:</p>
<ul>
<li>Layer 1: Conv2d(kernel=3, stride=1)</li>
<li>Layer 2: Conv2d(kernel=3, stride=2)</li>
<li>Layer 3: Conv2d(kernel=5, stride=1)</li>
</ul>
<p><strong>Question</strong>: What is the receptive field of the final layer with respect to the input?</p>
<details>
<summary>
Click for solution
</summary>
<p><strong>Solution using the formula</strong>: <span class="math display">\[r_0 = \sum_{l=1}^3 \left( (k_l - 1) \prod_{i=1}^{l-1} s_i \right) + 1\]</span></p>
<ul>
<li><span class="math inline">\(r_0 = (3-1) \cdot 1 + (3-1) \cdot 1 + (5-1) \cdot (1 \cdot 2) + 1\)</span></li>
<li><span class="math inline">\(r_0 = 2 + 2 + 8 + 1 = 13\)</span></li>
</ul>
<p><strong>Answer</strong>: <span class="math inline">\(13 \times 13\)</span> receptive field</p>
<p><strong>Practical insight</strong>: Each output pixel “sees” a <span class="math inline">\(13 \times 13\)</span> region of the input image.</p>
</details>
</div>
</div>
</div>
</section>
<section id="comparing-mlps-and-cnns" class="level3" data-number="3.5">
<h3 data-number="3.5" class="anchored" data-anchor-id="comparing-mlps-and-cnns"><span class="header-section-number">3.5</span> Comparing MLPs and CNNs</h3>
<p>Now that we understand how CNNs work, let’s compare them systematically to MLPs:</p>
<div class="callout callout-style-simple callout-note callout-titled" title="Question: MLP vs CNNs">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Question: MLP vs CNNs
</div>
</div>
<div class="callout-body-container callout-body">
<p>How do MLPs and CNNs differ in their approach to processing images?</p>
<p><strong>Think first</strong>: Consider parameter count, assumptions about data, and what each architecture “knows” about images.</p>
<details>
<summary>
Click for detailed comparison
</summary>
<table class="caption-top table">
<colgroup>
<col style="width: 20%">
<col style="width: 40%">
<col style="width: 40%">
</colgroup>
<thead>
<tr class="header">
<th>Aspect</th>
<th>MLP</th>
<th>CNN</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Input</strong></td>
<td>Flattened vector</td>
<td>Preserve spatial structure (H×W×C)</td>
</tr>
<tr class="even">
<td><strong>Connections</strong></td>
<td>Fully connected</td>
<td>Locally connected</td>
</tr>
<tr class="odd">
<td><strong>Parameter Sharing</strong></td>
<td>None (each weight unique)</td>
<td>Shared across spatial locations</td>
</tr>
<tr class="even">
<td><strong>Translation Equivariance</strong></td>
<td>❌ No</td>
<td>✅ Yes (by design)</td>
</tr>
<tr class="odd">
<td><strong>Assumptions</strong></td>
<td>None about spatial structure</td>
<td>Locality + stationarity of statistics</td>
</tr>
<tr class="even">
<td><strong>Parameter Count</strong></td>
<td>Very high</td>
<td>Much lower</td>
</tr>
<tr class="odd">
<td><strong>Learns Position</strong></td>
<td>Must learn same feature at each position</td>
<td>Learns once, applies everywhere</td>
</tr>
<tr class="even">
<td><strong>Best For</strong></td>
<td>Tabular data, general patterns</td>
<td>Images, spatial data</td>
</tr>
</tbody>
</table>
<p><strong>Key Takeaway</strong>: MLPs <em>can</em> theoretically learn to process images (universal approximation theorem), but CNNs bake in the right inductive biases, making learning vastly more efficient.</p>
</details>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="Deep Learning Insight: Inductive Bias">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Deep Learning Insight: Inductive Bias
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Inductive bias</strong> refers to the assumptions a learning algorithm makes to generalize from training data to unseen data.</p>
<p><strong>MLPs</strong>: Minimal inductive bias (very general, but requires lots of data) <strong>CNNs</strong>: Strong inductive bias (assumes locality, translation equivariance → more efficient learning)</p>
<p>The right inductive bias for your data is key to success! This is why:</p>
<ul>
<li>CNNs excel at images (spatial locality matters)</li>
<li>Transformers excel at sequences (long-range dependencies matter)</li>
<li>Graph Neural Networks excel at networks (graph structure matters)</li>
</ul>
<p>Choosing architecture = choosing which assumptions to bake in!</p>
</div>
</div>
</section>
</section>
<section id="more-layers-operations" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="more-layers-operations"><span class="header-section-number">4</span> More Layers &amp; Operations</h2>
<p>CNNs often do not consist solely of convolutional layers but have other layers with specific purposes.</p>
<section id="downsampling" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="downsampling"><span class="header-section-number">4.1</span> Downsampling</h3>
<p>Spatial downsampling is often a desired property of convolutional layers. We can achieve that by choosing a stride <span class="math inline">\(\gt 1\)</span>. However, there are other options, such as pooling layers. <a href="#fig-cnn-downsampling-pooling" class="quarto-xref">Figure&nbsp;32</a> illustrates some pooling variants.</p>
<div id="fig-cnn-downsampling-pooling" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-downsampling-pooling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/cnns/downsampling_with_pooling.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-downsampling-pooling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;32: Shown are a) Sub-Sampling, b) Max Pooling, c) Average Pooling. Source: <span class="citation" data-cites="prince_understanding_2023">Prince (<a href="#ref-prince_understanding_2023" role="doc-biblioref">2023</a>)</span>
</figcaption>
</figure>
</div>
</section>
<section id="sec-cnn-upsampling" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="sec-cnn-upsampling"><span class="header-section-number">4.2</span> Upsampling</h3>
<p>Sometimes we need to increase the spatial dimensionality of our activation maps.</p>
<div id="fig-cnn-upsampling" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-upsampling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/cnns/upsampling_options.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-upsampling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;33: Shown are a) Duplicate, b) Max-Unpooling, c) Bilinear Interpolation . Source: <span class="citation" data-cites="prince_understanding_2023">Prince (<a href="#ref-prince_understanding_2023" role="doc-biblioref">2023</a>)</span>
</figcaption>
</figure>
</div>
<p>Upsampling can also be learned with Transposed Convolutions. More on that topic in <em>Image Segmentation</em>.</p>
<p><!-- [image segmentation](segmentation.qmd#sec-segmentation-upsampling). --></p>
<!-- :::{#fig-cnn-transposed-convs}
![ ](/assets/images/cnns/transposed_convolution.jpg)
::: -->
</section>
<section id="changing-the-number-of-channels" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="changing-the-number-of-channels"><span class="header-section-number">4.3</span> Changing the number of channels</h3>
<p><span class="math inline">\(1 \times 1\)</span> convolutions have a kernel size of 1x1 and thus no spatial extent. These layers are often used in CNNs to change the number (<span class="math inline">\(C\)</span>) of activation maps with few parameters. For example, activation maps of dimensionality (<span class="math inline">\(C \times H \times W\)</span>) can be changed to a volume of (<span class="math inline">\(C2 \times H \times W\)</span>) using only <span class="math inline">\(C2 * (C + 1)\)</span> parameters. This can be useful, for example, to save parameters before more complex layers or at the end of the CNN to adjust the size of the activation maps to the number of classes to be modeled (for classification problems) or to reduce to 3 color channels (<span class="math inline">\(C2=3\)</span>) for image generation models. <a href="#fig-cnn-1x1-conv" class="quarto-xref">Figure&nbsp;34</a> shows an example that uses only 6 weights (excluding bias) to reduce the number of channels to one.</p>
<div id="fig-cnn-1x1-conv" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-1x1-conv-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/cnns/1x1_conv_example.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-1x1-conv-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;34: <span class="math inline">\(1 \times 1\)</span> convolution. Source: <span class="citation" data-cites="prince_understanding_2023">Prince (<a href="#ref-prince_understanding_2023" role="doc-biblioref">2023</a>)</span>
</figcaption>
</figure>
</div>
</section>
<section id="global-average-pooling" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="global-average-pooling"><span class="header-section-number">4.4</span> Global Average Pooling</h3>
<p>Global Average Pooling is often an important component. It computes the average of the activations along the depth dimension, reducing activation maps from (C x H x W) to (C x 1 x 1). This is useful for directly modeling logits in a classification problem with C classes, enabling architectures that completely eliminate fully-connected layers.</p>
<div id="fig-cnn-global-avg-pooling" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-global-avg-pooling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/cnns/global_average_pooling.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-global-avg-pooling-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;35: Global Average pooling, input (left) and output (right).
</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This can be achieved using PyTorchs <a href="https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html">https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html</a></p>
</div>
</div>
</section>
<section id="other-layers" class="level3" data-number="4.5">
<h3 data-number="4.5" class="anchored" data-anchor-id="other-layers"><span class="header-section-number">4.5</span> Other Layers</h3>
<p>There are many other types of layers, also there are more flavours of convolutional layers.</p>
<p>One layer often used to reduce the number of parameters are <strong>depthwise separable convolutions</strong>. Instead of extending filters over the entire depth of the input activations, a separate filter (kernel) is used for each input channel, with the dimensionality (<span class="math inline">\(1 \times k \times k\)</span>). <a href="#fig-cnn-depthwise" class="quarto-xref">Figure&nbsp;36</a> shows an example. Subsequently, 1x1 convolutions are used to combine information across the input channels. See <a href="#fig-cnn-depthwise-separabel" class="quarto-xref">Figure&nbsp;37</a> for a comparison of ‘normal’ convolutions and depthwise separable convolutions. Since 1x1 convolutions require fewer parameters, activation maps can be generated with fewer parameters.</p>
<div id="fig-cnn-depthwise" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-depthwise-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/cnns/depthwise.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-depthwise-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;36: Source: <a href="https://paperswithcode.com/method/depthwise-convolution">https://paperswithcode.com/method/depthwise-convolution</a>
</figcaption>
</figure>
</div>
<div id="fig-cnn-depthwise-separabel" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cnn-depthwise-separabel-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/cnns/depthwise_separabel.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cnn-depthwise-separabel-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;37: Source: <span class="citation" data-cites="yu_multi-scale_2016">Yu and Koltun (<a href="#ref-yu_multi-scale_2016" role="doc-biblioref">2016</a>)</span>
</figcaption>
</figure>
</div>
<p>Additionally, important layers include the diverse familiy of <strong>normalization layers</strong> which can improve convergence of the optimization procedure dramatically.</p>
</section>
</section>
<section id="pytorch-examples" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="pytorch-examples"><span class="header-section-number">5</span> PyTorch Examples</h2>
<div id="e67b5b48" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> functional <span class="im">as</span> F</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchshow <span class="im">as</span> ts</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="ee7b9bea" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co">#img = Image.open({{&lt; meta params.images_path &gt;}}'cat.jpg')</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>image_path <span class="op">=</span> <span class="st">"../../assets/images/cnns/cat.jpg"</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> Image.<span class="bu">open</span>(image_path)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>img</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<div>
<figure class="figure">
<p><img src="cnns_files/figure-html/cell-5-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="31a046e7" class="cell" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>filter_ <span class="op">=</span> torch.tensor(</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    [   [[<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>]], <span class="co"># R</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>        [[<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>]], <span class="co"># G</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>        [[<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>]], <span class="co"># B</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    ]).unsqueeze(<span class="dv">0</span>).<span class="bu">float</span>()</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>ts.show(filter_, show_axis<span class="op">=</span><span class="va">False</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="cnns_files/figure-html/cell-6-output-1.png" width="470" height="470" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="b90964c2" class="cell" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="bu">input</span> <span class="op">=</span> torch.tensor(np.array(img)).unsqueeze(<span class="dv">0</span>).permute(<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">2</span>).<span class="bu">float</span>() <span class="co"># (N, C, H, W)</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="bu">input</span> <span class="op">/=</span> <span class="fl">255.0</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="bu">input</span> <span class="op">-=</span> <span class="fl">1.0</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> F.conv2d(<span class="bu">input</span>, filter_, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">0</span>, dilation<span class="op">=</span><span class="dv">1</span>, groups<span class="op">=</span><span class="dv">1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="c15cc77e" class="cell" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>ts.show(result)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stderr">
<pre><code>/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torchshow/visualization.py:385: UserWarning:

Original input range is not 0-1 when using grayscale mode. Auto-rescaling it to 0-1 by default.
</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="cnns_files/figure-html/cell-8-output-2.png" width="621" height="470" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>2D-Convolution:</p>
<div id="7ac78e0b" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> F.conv2d(<span class="bu">input</span>, filter_, stride<span class="op">=</span><span class="dv">6</span>, padding<span class="op">=</span><span class="dv">0</span>, dilation<span class="op">=</span><span class="dv">1</span>, groups<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>ts.show(result)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="cnns_files/figure-html/cell-9-output-1.png" width="618" height="470" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Transposed convolution:</p>
<div id="13915796" class="cell" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> F.conv2d(<span class="bu">input</span>, filter_, stride<span class="op">=</span><span class="dv">6</span>, padding<span class="op">=</span><span class="dv">0</span>, dilation<span class="op">=</span><span class="dv">1</span>, groups<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> F.conv_transpose2d(result, weight<span class="op">=</span>torch.ones_like(filter_))</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>ts.show(result)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="cnns_files/figure-html/cell-10-output-1.png" width="616" height="470" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Max-Pooling:</p>
<div id="8d8f91c6" class="cell" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> F.max_pool2d(<span class="bu">input</span>, kernel_size<span class="op">=</span><span class="dv">8</span>, stride<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>ts.show(result)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="cnns_files/figure-html/cell-11-output-1.png" width="621" height="470" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="references" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="references"><span class="header-section-number">6</span> References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-araujo_computing_2019" class="csl-entry" role="listitem">
Araujo, Andr&amp;eacute;, Wade Norris, and Jack Sim. 2019. <span>“Computing <span>Receptive</span> <span>Fields</span> of <span>Convolutional</span> <span>Neural</span> <span>Networks</span>.”</span> <em>Distill</em> 4 (11): 10.23915/distill.00021. <a href="https://doi.org/10.23915/distill.00021">https://doi.org/10.23915/distill.00021</a>.
</div>
<div id="ref-dumoulin_guide_2016" class="csl-entry" role="listitem">
Dumoulin, Vincent, and Francesco Visin. 2016. <span>“A Guide to Convolution Arithmetic for Deep Learning.”</span> <em>ArXiv e-Prints</em>, March.
</div>
<div id="ref-johnson_eecs_2019" class="csl-entry" role="listitem">
Johnson, Justin. 2019. <span>“<span>EECS</span> 498-007 / 598-005: <span>Deep</span> <span>Learning</span> for <span>Computer</span> <span>Vision</span>.”</span> Lecture {Notes} / {Slides}. <a href="https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2019/">https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2019/</a>.
</div>
<div id="ref-li_cs231n_2022" class="csl-entry" role="listitem">
Li, Fei-Fei. 2022. <span>“<span>CS231n</span> <span>Convolutional</span> <span>Neural</span> <span>Networks</span> for <span>Visual</span> <span>Recognition</span>.”</span> Lecture {Notes}. <a href="https://cs231n.github.io">https://cs231n.github.io</a>.
</div>
<div id="ref-prince_understanding_2023" class="csl-entry" role="listitem">
Prince, Simon J. D. 2023. <em>Understanding <span>Deep</span> <span>Learning</span></em>. MIT Press. <a href="https://udlbook.github.io/udlbook/">https://udlbook.github.io/udlbook/</a>.
</div>
<div id="ref-yu_multi-scale_2016" class="csl-entry" role="listitem">
Yu, Fisher, and Vladlen Koltun. 2016. <span>“Multi-<span>Scale</span> <span>Context</span> <span>Aggregation</span> by <span>Dilated</span> <span>Convolutions</span>.”</span> arXiv. <a href="http://arxiv.org/abs/1511.07122">http://arxiv.org/abs/1511.07122</a>.
</div>
<div id="ref-Zeiler2014" class="csl-entry" role="listitem">
Zeiler, Matthew D., and Rob Fergus. 2014. <span>“Visualizing and Understanding Convolutional Networks.”</span> <em>Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)</em> 8689 LNCS (PART 1): 818–33. <a href="https://doi.org/10.1007/978-3-319-10590-1_53">https://doi.org/10.1007/978-3-319-10590-1_53</a>.
</div>
</div>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/marco-willi\.github\.io\/cas-dl-compvis-lectures-hs2025\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../pages/lectures/intro.html" class="pagination-link" aria-label="1 - Introduction">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">1 - Introduction</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../pages/lectures/classification.html" class="pagination-link" aria-label="3 - Image Classification">
        <span class="nav-page-text">3 - Image Classification</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb14" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "2 - Convolutional Neural Networks"</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="an">params:</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co">   images_path: "/assets/images/cnns/"</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title="Learning Objectives" collapse="true"}</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Understand why MLPs are not ideal to model images.</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Explain why CNN inductive biases improve efficiency on images over MLPs.</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Understand how convolutions operate and how they can be parameterized.</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Compute output shape and the number of parameters for convolutions.</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Understand how different layers make up a CNN and what their role is.</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Distinguish invariance vs equivariance and relate them to layer design and applications.</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Describe receptive field growth through stacking.</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>On a high-level: know layers / variants such as 1x1 convolutions and global average pooling.</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title="TLDR Recap" collapse="true"}</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>**Why CNNs? The Problem with MLPs on Images**</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Images are high-dimensional (e.g., $224 \times 224 \times 3 = 150{,}528$ pixels)</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>MLPs flatten images, losing spatial structure</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Fully connected layers require millions of parameters: $(150{,}528 \times 1{,}000 = 150M$ parameters for just one layer!</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>MLPs don't exploit natural image properties (locality, translation invariance)</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>**The CNN Solution: Three Key Principles**</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Local Connectivity**: Each neuron only connects to a small spatial region (receptive field)</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Parameter Sharing**: Same filter/detector applied across all spatial locations</span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Translation Equivariance**: If input shifts, output shifts correspondingly</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>**Core Operation: Convolution**</span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>$$S(i, j) = (K * I)(i, j) = \sum_m\sum_n I(i + m, j + n)K(m, n)$$</span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Input** $I$: Activation maps $(C_{in} \times H \times W)$</span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Kernel** $K$: Learnable filter $(C_{out} \times C_{in} \times k \times k)$</span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Output** $S$: Feature maps $(C_{out} \times H' \times W')$</span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a>**Key Hyperparameters**</span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Kernel Size** $k$: Spatial extent (usually 3×3 or 5×5)</span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Stride** $s$: Step size (controls downsampling)</span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Padding** $p$: Zero-padding to preserve spatial size</span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Dilation** $d$: Spacing between kernel elements (expands receptive field)</span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a>**Output Size Formula**</span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a>$$o = \left\lfloor \frac{i + 2p - d\cdot(k-1) - 1}{s} + 1 \right\rfloor$$</span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true" tabindex="-1"></a>**Parameter Count**</span>
<span id="cb14-54"><a href="#cb14-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-55"><a href="#cb14-55" aria-hidden="true" tabindex="-1"></a>$$\text{Total Parameters} = (k \times k \times C_{in} \times C_{out}) + C_{out}$$</span>
<span id="cb14-56"><a href="#cb14-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-57"><a href="#cb14-57" aria-hidden="true" tabindex="-1"></a>**Invariance vs Equivariance**</span>
<span id="cb14-58"><a href="#cb14-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-59"><a href="#cb14-59" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Invariant**: $f(g(x)) = f(x)$ — output unchanged by transformation (needed for *classification*)</span>
<span id="cb14-60"><a href="#cb14-60" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Equivariant**: $f(g(x)) = g(f(x))$ — output transforms with input (needed for *detection*, *segmentation*)</span>
<span id="cb14-61"><a href="#cb14-61" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**CNNs are naturally translation equivariant** due to convolution operation</span>
<span id="cb14-62"><a href="#cb14-62" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Add pooling/global average pooling for translation *invariance*</span>
<span id="cb14-63"><a href="#cb14-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-64"><a href="#cb14-64" aria-hidden="true" tabindex="-1"></a>**Receptive Field**</span>
<span id="cb14-65"><a href="#cb14-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-66"><a href="#cb14-66" aria-hidden="true" tabindex="-1"></a>The input region that affects a single output activation:</span>
<span id="cb14-67"><a href="#cb14-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-68"><a href="#cb14-68" aria-hidden="true" tabindex="-1"></a>$$r_0 = \sum_{l=1}^L \left( (k_l - 1) \prod_{i=1}^{l-1} s_i \right) + 1$$</span>
<span id="cb14-69"><a href="#cb14-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-70"><a href="#cb14-70" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Grows with depth: stacking small kernels (3×3) more efficient than large kernels (5×5)</span>
<span id="cb14-71"><a href="#cb14-71" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Example: Two 3×3 convs = 18 params/channel vs one 5×5 = 25 params/channel</span>
<span id="cb14-72"><a href="#cb14-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-73"><a href="#cb14-73" aria-hidden="true" tabindex="-1"></a>**Common Operations**</span>
<span id="cb14-74"><a href="#cb14-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-75"><a href="#cb14-75" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Pooling**: Spatial downsampling (max-pooling, average-pooling)</span>
<span id="cb14-76"><a href="#cb14-76" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**1×1 Convolution**: Change number of channels without spatial effect</span>
<span id="cb14-77"><a href="#cb14-77" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Global Average Pooling**: $(C \times H \times W) \rightarrow (C \times 1 \times 1)$ for classification</span>
<span id="cb14-78"><a href="#cb14-78" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Upsampling**: Increase spatial size (interpolation, transposed convolution)</span>
<span id="cb14-79"><a href="#cb14-79" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Depthwise Separable**: Efficient convolutions with fewer parameters</span>
<span id="cb14-80"><a href="#cb14-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-81"><a href="#cb14-81" aria-hidden="true" tabindex="-1"></a>**Architectural Best Practices**</span>
<span id="cb14-82"><a href="#cb14-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-83"><a href="#cb14-83" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✅ Use multiple small kernels (3×3) instead of large ones</span>
<span id="cb14-84"><a href="#cb14-84" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✅ Add non-linearity (ReLU, etc.) between conv layers</span>
<span id="cb14-85"><a href="#cb14-85" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✅ Use batch normalization for stable training</span>
<span id="cb14-86"><a href="#cb14-86" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✅ Stack conv layers to build hierarchical features</span>
<span id="cb14-87"><a href="#cb14-87" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✅ Use global average pooling instead of large fully-connected layers</span>
<span id="cb14-88"><a href="#cb14-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-89"><a href="#cb14-89" aria-hidden="true" tabindex="-1"></a>**Parameter Efficiency Example**</span>
<span id="cb14-90"><a href="#cb14-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-91"><a href="#cb14-91" aria-hidden="true" tabindex="-1"></a>For CIFAR-10 images (32×32×3):</span>
<span id="cb14-92"><a href="#cb14-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-93"><a href="#cb14-93" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**MLP** (2 hidden layers): ~200K parameters</span>
<span id="cb14-94"><a href="#cb14-94" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**CNN** (2 conv layers): ~5K parameters (40× fewer!)</span>
<span id="cb14-95"><a href="#cb14-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-96"><a href="#cb14-96" aria-hidden="true" tabindex="-1"></a>**Key Insight**: CNNs encode our prior knowledge about images as *inductive biases* — assumptions baked into the architecture that make learning more efficient and effective.</span>
<span id="cb14-97"><a href="#cb14-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-98"><a href="#cb14-98" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-99"><a href="#cb14-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-100"><a href="#cb14-100" aria-hidden="true" tabindex="-1"></a>::: {.content-hidden}</span>
<span id="cb14-101"><a href="#cb14-101" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-102"><a href="#cb14-102" aria-hidden="true" tabindex="-1"></a>{{&lt; include /assets/_macros.tex &gt;}}</span>
<span id="cb14-103"><a href="#cb14-103" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-104"><a href="#cb14-104" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-105"><a href="#cb14-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-106"><a href="#cb14-106" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction &amp; Motivation</span></span>
<span id="cb14-107"><a href="#cb14-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-108"><a href="#cb14-108" aria-hidden="true" tabindex="-1"></a>Images have unique properties that make them challenging for standard neural networks:</span>
<span id="cb14-109"><a href="#cb14-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-110"><a href="#cb14-110" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**High-Dimensional**: An RGB image of size $224 \times 224$ has $224 \times 224 \times 3 = 150{,}528$ values</span>
<span id="cb14-111"><a href="#cb14-111" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Locality**: Nearby pixels are statistically related (edges, textures, objects)</span>
<span id="cb14-112"><a href="#cb14-112" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Stability under transformations**: Interpretation of an image does not change under many geometric transformations. A cat should be recognized regardless of its position in the image.</span>
<span id="cb14-113"><a href="#cb14-113" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Hierarchical Structure**: Images contain features at multiple scales (pixels → edges → textures → objects)</span>
<span id="cb14-114"><a href="#cb14-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-115"><a href="#cb14-115" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-example-rgb-image width=600}</span>
<span id="cb14-116"><a href="#cb14-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-117"><a href="#cb14-117" aria-hidden="true" tabindex="-1"></a><span class="al">![ ]({{&lt; meta params.images_path &gt;}}cows2_small.png)</span>{width=600}</span>
<span id="cb14-118"><a href="#cb14-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-119"><a href="#cb14-119" aria-hidden="true" tabindex="-1"></a>A high-resolution photograph. <span class="co">[</span><span class="ot">Image Source</span><span class="co">](https://unsplash.com/de/fotos/braune-schafe-auf-grunem-grasfeld-tagsuber-olonUwUrmQk)</span></span>
<span id="cb14-120"><a href="#cb14-120" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-121"><a href="#cb14-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-122"><a href="#cb14-122" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Question: How are nearby pixels related?" appearance="simple"}</span>
<span id="cb14-123"><a href="#cb14-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-124"><a href="#cb14-124" aria-hidden="true" tabindex="-1"></a>Looking at @fig-cnn-example-rgb-image, can you explain how nearby pixels are related?</span>
<span id="cb14-125"><a href="#cb14-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-126"><a href="#cb14-126" aria-hidden="true" tabindex="-1"></a>**Think about it:** What visual patterns or structures do you notice?</span>
<span id="cb14-127"><a href="#cb14-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-128"><a href="#cb14-128" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-129"><a href="#cb14-129" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Click for answer<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb14-130"><a href="#cb14-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-131"><a href="#cb14-131" aria-hidden="true" tabindex="-1"></a>**Nearby pixels in images are strongly correlated due to:**</span>
<span id="cb14-132"><a href="#cb14-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-133"><a href="#cb14-133" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Object Coherence**: Objects occupy contiguous regions—pixels within an object (like a cow's body) share similar colors and textures</span>
<span id="cb14-134"><a href="#cb14-134" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Smooth Transitions**: Natural images have gradual changes in color and intensity, not abrupt random variations</span>
<span id="cb14-135"><a href="#cb14-135" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Texture Patterns**: Repeating structures (like grass blades or fur) create local statistical patterns</span>
<span id="cb14-136"><a href="#cb14-136" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Edges and Boundaries**: Sharp transitions occur at object boundaries, but even edges span multiple neighboring pixels</span>
<span id="cb14-137"><a href="#cb14-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-138"><a href="#cb14-138" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-139"><a href="#cb14-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-140"><a href="#cb14-140" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-141"><a href="#cb14-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-142"><a href="#cb14-142" aria-hidden="true" tabindex="-1"></a>Even though we will soon see that Convolutional Neural Networks (CNNs) are much better suited to model images as compared to Multi-Layer Pereptrons (MLPs), we start by investigating the properties and difficulties to model images with MLPs. This will help understand how properties embedded in the architecture of CNNs (inductive biases) make them much more suitable to model computer vision tasks.</span>
<span id="cb14-143"><a href="#cb14-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-144"><a href="#cb14-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-145"><a href="#cb14-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-146"><a href="#cb14-146" aria-hidden="true" tabindex="-1"></a><span class="fu">### MLPs and Images</span></span>
<span id="cb14-147"><a href="#cb14-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-148"><a href="#cb14-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-149"><a href="#cb14-149" aria-hidden="true" tabindex="-1"></a>The input to an MLP is a vector $\vect{x}^{(i)}$, which is transformed through multiple hidden layers to the output layer. Each hidden layer has a certain number of neurons, each connected to all neurons in the previous layer (fully-connected layers). The fully connected layers can only process 1-D vectors. Therefore, images $\in \mathbb{R}^{H \times W \times C}$ must be flattened into 1-D vectors $\in \mathbb{R}^p$. Here, $p= H \times W \times C$. This causes MLPs to become very large (having many learnable parameters) when applied to high-dimensional inputs such as images. See @fig-cnn-mlp for an illustration.</span>
<span id="cb14-150"><a href="#cb14-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-151"><a href="#cb14-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-152"><a href="#cb14-152" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-mlp}</span>
<span id="cb14-153"><a href="#cb14-153" aria-hidden="true" tabindex="-1"></a><span class="al">![ ]({{&lt; meta params.images_path &gt;}}mlp_images_example.png)</span>{width=600}</span>
<span id="cb14-154"><a href="#cb14-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-155"><a href="#cb14-155" aria-hidden="true" tabindex="-1"></a>Illustration of an MLP applied to an image.</span>
<span id="cb14-156"><a href="#cb14-156" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-157"><a href="#cb14-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-158"><a href="#cb14-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-159"><a href="#cb14-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-160"><a href="#cb14-160" aria-hidden="true" tabindex="-1"></a> In the CIFAR-10 dataset, which consists of very small images of $\in \mathbb{R}^{3 \times 32 \times 32}$ (colors, height, width), a single neuron in the first hidden layer has 3 * 32 * 32 = 3,072 weights to learn (see @fig-cnn-mlp-images).</span>
<span id="cb14-161"><a href="#cb14-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-162"><a href="#cb14-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-163"><a href="#cb14-163" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-mlp-images}</span>
<span id="cb14-164"><a href="#cb14-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-165"><a href="#cb14-165" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}mlp_images.jpg)</span>{width=600}</span>
<span id="cb14-166"><a href="#cb14-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-167"><a href="#cb14-167" aria-hidden="true" tabindex="-1"></a>Source: @johnson_eecs_2019</span>
<span id="cb14-168"><a href="#cb14-168" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-169"><a href="#cb14-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-170"><a href="#cb14-170" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- :::{#fig-cnn-spatial-structure-mlp}</span></span>
<span id="cb14-171"><a href="#cb14-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-172"><a href="#cb14-172" aria-hidden="true" tabindex="-1"></a><span class="co">![]({{&lt; meta params.images_path &gt;}}mlp-spatial-structure.png){width=600}</span></span>
<span id="cb14-173"><a href="#cb14-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-174"><a href="#cb14-174" aria-hidden="true" tabindex="-1"></a><span class="co">Source: @li_cs231n_2023</span></span>
<span id="cb14-175"><a href="#cb14-175" aria-hidden="true" tabindex="-1"></a><span class="co">::: --&gt;</span></span>
<span id="cb14-176"><a href="#cb14-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-177"><a href="#cb14-177" aria-hidden="true" tabindex="-1"></a>For larger images, which are more common in practice, the number of weights is correspondingly much larger. Furthermore, to identify all significant patterns many neurons are required, further increasing the number of parameters. This may lead to overfitting and difficulty in learning the weights.</span>
<span id="cb14-178"><a href="#cb14-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-179"><a href="#cb14-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-180"><a href="#cb14-180" aria-hidden="true" tabindex="-1"></a>The dimensionality of weight matrix $\m{W}$ scales linearly with input size:  $\m{W} \in \mathbb{R}^{d \times k}$ ($d$ is input dimensionality, $k$ the number of neurons), or quadratically with the side length of an image (assuming a square image).</span>
<span id="cb14-181"><a href="#cb14-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-182"><a href="#cb14-182" aria-hidden="true" tabindex="-1"></a>Since each neuron in the first hidden layer is connected to each input value (single color channel of a pixel) and each such connection has a specific weight (matrix $\m{W}$ ) the number of weights for each neuron is identical to the number of input values. Therefore, the weights of individual neurons (columns of $\m{W}$ ) can be reshaped to the image dimensionality and thus be visualized. This reveals the patterns a specific neuron is reacting to. See  @fig-cnn-mlp-example-weights for an illustration.</span>
<span id="cb14-183"><a href="#cb14-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-184"><a href="#cb14-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-185"><a href="#cb14-185" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-mlp-example-weights}</span>
<span id="cb14-186"><a href="#cb14-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-187"><a href="#cb14-187" aria-hidden="true" tabindex="-1"></a><span class="al">![ ]({{&lt; meta params.images_path &gt;}}mlp_images_example_weights.png)</span>{width=600}</span>
<span id="cb14-188"><a href="#cb14-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-189"><a href="#cb14-189" aria-hidden="true" tabindex="-1"></a>Illustration of weight matrix $\m{W}$ and how the weights of a single neuron can be reshaped into the input image dimensionality.</span>
<span id="cb14-190"><a href="#cb14-190" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-191"><a href="#cb14-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-192"><a href="#cb14-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-193"><a href="#cb14-193" aria-hidden="true" tabindex="-1"></a>Often the patterns we want to learn are not stationary. Ideally we want to recognize them under translations of the inputs, e.g. $g(\vect{x})$ where $g()$ is a spatial translation. @fig-cnns-mlp-pattern-shift illustrates the problem, where the same pattern appears at different spatial positions.</span>
<span id="cb14-194"><a href="#cb14-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-195"><a href="#cb14-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-196"><a href="#cb14-196" aria-hidden="true" tabindex="-1"></a>::: {#fig-cnns-mlp-pattern-shift}</span>
<span id="cb14-197"><a href="#cb14-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-198"><a href="#cb14-198" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}mlp_images_example_pattern_shifts.png)</span>{width=600}</span>
<span id="cb14-199"><a href="#cb14-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-200"><a href="#cb14-200" aria-hidden="true" tabindex="-1"></a>Illustration of different inputs with the same pattern but shifted spatially. Illustrated are the weights of the first neuron. Green cells represent high weight values and yellow cells represent low values.</span>
<span id="cb14-201"><a href="#cb14-201" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-202"><a href="#cb14-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-203"><a href="#cb14-203" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Question: Patterns at different locations" appearance="simple"}</span>
<span id="cb14-204"><a href="#cb14-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-205"><a href="#cb14-205" aria-hidden="true" tabindex="-1"></a>Can an MLP learn to detect the same pattern at different spatial locations? If yes, how? See @fig-cnns-mlp-pattern-shift.</span>
<span id="cb14-206"><a href="#cb14-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-207"><a href="#cb14-207" aria-hidden="true" tabindex="-1"></a>**Think about it:** If an MLP learns to detect a cat in the center of an image, what happens when the same cat appears in the corner?</span>
<span id="cb14-208"><a href="#cb14-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-209"><a href="#cb14-209" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-210"><a href="#cb14-210" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Click for answer<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb14-211"><a href="#cb14-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-212"><a href="#cb14-212" aria-hidden="true" tabindex="-1"></a>MLPs must learn separate detectors for each possible position! This is extremely inefficient because:</span>
<span id="cb14-213"><a href="#cb14-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-214"><a href="#cb14-214" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Parameter explosion**: Need different weights for each spatial location</span>
<span id="cb14-215"><a href="#cb14-215" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Data requirements**: Must see target pattern in every possible position during training</span>
<span id="cb14-216"><a href="#cb14-216" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Poor generalization**: Fails on positions not seen during training</span>
<span id="cb14-217"><a href="#cb14-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-218"><a href="#cb14-218" aria-hidden="true" tabindex="-1"></a>This is why MLPs with the same number of parameters perform much worse on image tasks compared to CNNs.</span>
<span id="cb14-219"><a href="#cb14-219" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-220"><a href="#cb14-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-221"><a href="#cb14-221" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-222"><a href="#cb14-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-223"><a href="#cb14-223" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ::: {.callout-tip title="Interactive MLP Demo"}</span></span>
<span id="cb14-224"><a href="#cb14-224" aria-hidden="true" tabindex="-1"></a><span class="co">Try this visualization to see how an MLP reacts to shifted inputs:</span></span>
<span id="cb14-225"><a href="#cb14-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-226"><a href="#cb14-226" aria-hidden="true" tabindex="-1"></a><span class="co">- [MLP vs CNN on 2D patterns](https://adamharley.com/nn_vis/mlp/2d.html)</span></span>
<span id="cb14-227"><a href="#cb14-227" aria-hidden="true" tabindex="-1"></a><span class="co">- [CNN Filter Visualization](https://adamharley.com/nn_vis/cnn/2d.html)</span></span>
<span id="cb14-228"><a href="#cb14-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-229"><a href="#cb14-229" aria-hidden="true" tabindex="-1"></a><span class="co">Notice how CNNs reuse the same filter across all positions!</span></span>
<span id="cb14-230"><a href="#cb14-230" aria-hidden="true" tabindex="-1"></a><span class="co">::: --&gt;</span></span>
<span id="cb14-231"><a href="#cb14-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-232"><a href="#cb14-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-233"><a href="#cb14-233" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Question: MLPs and permutations." appearance="simple"}</span>
<span id="cb14-234"><a href="#cb14-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-235"><a href="#cb14-235" aria-hidden="true" tabindex="-1"></a>Can an MLP successfully learn patterns in images $\vect{x}$ that are permuted with a permutation matrix $\m{P}$, i.e., $f(P(\vect{x}))$? See @fig-cnns-mlp-pattern-shift.</span>
<span id="cb14-236"><a href="#cb14-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-237"><a href="#cb14-237" aria-hidden="true" tabindex="-1"></a>**Hint:** Think carefully - what if EVERY image is permuted with the SAME permutation matrix?</span>
<span id="cb14-238"><a href="#cb14-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-239"><a href="#cb14-239" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-240"><a href="#cb14-240" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Click for the surprising answer<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb14-241"><a href="#cb14-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-242"><a href="#cb14-242" aria-hidden="true" tabindex="-1"></a>**Yes, MLPs CAN learn on consistently permuted images!** This is a crucial insight:</span>
<span id="cb14-243"><a href="#cb14-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-244"><a href="#cb14-244" aria-hidden="true" tabindex="-1"></a>**Why MLPs can handle consistent permutation:**</span>
<span id="cb14-245"><a href="#cb14-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-246"><a href="#cb14-246" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Consistent mapping**: If every image uses the same permutation $P$, pixel position relationships are preserved</span>
<span id="cb14-247"><a href="#cb14-247" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Learning is still possible**: The MLP can learn that "pixel at position 247 (originally position 12) connects to pixel at position 891 (originally position 13)"</span>
<span id="cb14-248"><a href="#cb14-248" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**No information is lost**: Permutation is just a reordering, not destruction of information</span>
<span id="cb14-249"><a href="#cb14-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-250"><a href="#cb14-250" aria-hidden="true" tabindex="-1"></a>**The real insight:**</span>
<span id="cb14-251"><a href="#cb14-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-252"><a href="#cb14-252" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✅ **Same permutation for all images**: MLP can learn the task</span>
<span id="cb14-253"><a href="#cb14-253" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>❌ **Different permutations per image**: MLP cannot learn effectively</span>
<span id="cb14-254"><a href="#cb14-254" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>❌ **No permutation but objects move around**: MLP struggles with translation</span>
<span id="cb14-255"><a href="#cb14-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-256"><a href="#cb14-256" aria-hidden="true" tabindex="-1"></a>**Why this matters**: This thought experiment reveals that MLPs' problem with images isn't about permutation per se, but about **spatial relationships and translation invariance**:</span>
<span id="cb14-257"><a href="#cb14-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-258"><a href="#cb14-258" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Natural images**: Objects appear at different positions → MLP must learn separate detectors for each position</span>
<span id="cb14-259"><a href="#cb14-259" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Consistently permuted images**: Spatial relationships are preserved (just reordered) → MLP can learn</span>
<span id="cb14-260"><a href="#cb14-260" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**CNNs**: Built-in translation equivariance makes them efficient for natural images regardless of object position</span>
<span id="cb14-261"><a href="#cb14-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-262"><a href="#cb14-262" aria-hidden="true" tabindex="-1"></a>**Key takeaway**: The issue isn't that spatial structure is "destroyed" by permutation, but that CNNs are specifically designed to handle the **translation variability** in natural images efficiently.</span>
<span id="cb14-263"><a href="#cb14-263" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-264"><a href="#cb14-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-265"><a href="#cb14-265" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-266"><a href="#cb14-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-267"><a href="#cb14-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-268"><a href="#cb14-268" aria-hidden="true" tabindex="-1"></a>::: {#fig-cnns-mlp-pattern-shift}</span>
<span id="cb14-269"><a href="#cb14-269" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}mlp_images_examples_permutation.png)</span>{width=600}</span>
<span id="cb14-270"><a href="#cb14-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-271"><a href="#cb14-271" aria-hidden="true" tabindex="-1"></a>Inputs transformed with a permuation matrix.</span>
<span id="cb14-272"><a href="#cb14-272" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-273"><a href="#cb14-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-274"><a href="#cb14-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-275"><a href="#cb14-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-276"><a href="#cb14-276" aria-hidden="true" tabindex="-1"></a>To summarize MLPs on images:</span>
<span id="cb14-277"><a href="#cb14-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-278"><a href="#cb14-278" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>High-Dimensional: The size of an MLP scales with the input dimensionality which might blow beyond memory and compute budgets.</span>
<span id="cb14-279"><a href="#cb14-279" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Locality: MLPs have no notion of locality and thus can't exploit this inherent bias in natural images.</span>
<span id="cb14-280"><a href="#cb14-280" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Stability under transformations: MLPs need to learn position-dependent patterns which is very inefficient.</span>
<span id="cb14-281"><a href="#cb14-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-282"><a href="#cb14-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-283"><a href="#cb14-283" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title="Try MLPs on images in your browser."}</span>
<span id="cb14-284"><a href="#cb14-284" aria-hidden="true" tabindex="-1"></a>Try to draw the same digit at different positions and observe what happens!</span>
<span id="cb14-285"><a href="#cb14-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-286"><a href="#cb14-286" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">MLP Demo</span><span class="co">](https://adamharley.com/nn_vis/mlp/2d.html)</span></span>
<span id="cb14-287"><a href="#cb14-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-288"><a href="#cb14-288" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-289"><a href="#cb14-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-290"><a href="#cb14-290" aria-hidden="true" tabindex="-1"></a><span class="fu">### Invariance and Equivariance {#sec-cnn-invariance}</span></span>
<span id="cb14-291"><a href="#cb14-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-292"><a href="#cb14-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-293"><a href="#cb14-293" aria-hidden="true" tabindex="-1"></a>For many tasks involving image data, small variations in the input should either **not** change the model output (_invariance_) or should change the output **in tandem** with the input changes (_equivariance_).</span>
<span id="cb14-294"><a href="#cb14-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-295"><a href="#cb14-295" aria-hidden="true" tabindex="-1"></a>A function $f(\vect{x})$ is **invariant** to transformation $g(\vect{x})$ if:</span>
<span id="cb14-296"><a href="#cb14-296" aria-hidden="true" tabindex="-1"></a>$$f(g(\vect{x})) = f(\vect{x})$$</span>
<span id="cb14-297"><a href="#cb14-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-298"><a href="#cb14-298" aria-hidden="true" tabindex="-1"></a>A function $f(\vect{x})$ is **equivariant** to transformation $g(\vect{x})$ if:</span>
<span id="cb14-299"><a href="#cb14-299" aria-hidden="true" tabindex="-1"></a>$$f(g(\vect{x})) = g(f(\vect{x}))$$</span>
<span id="cb14-300"><a href="#cb14-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-301"><a href="#cb14-301" aria-hidden="true" tabindex="-1"></a>::: {.callout-important title="Intuition"}</span>
<span id="cb14-302"><a href="#cb14-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-303"><a href="#cb14-303" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Invariant** = "I don't care about the transformation"</span>
<span id="cb14-304"><a href="#cb14-304" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Equivariant** = "I transform my output the same way the input was transformed"</span>
<span id="cb14-305"><a href="#cb14-305" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-306"><a href="#cb14-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-307"><a href="#cb14-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-308"><a href="#cb14-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-309"><a href="#cb14-309" aria-hidden="true" tabindex="-1"></a>@fig-cnns-invariance-example shows an example of image classification. In this case, a model needs to be invariant to spatial translations of objects (if these do not change the image's class).</span>
<span id="cb14-310"><a href="#cb14-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-311"><a href="#cb14-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-312"><a href="#cb14-312" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Question: Inivariance for Image Classification" appearance="simple"}</span>
<span id="cb14-313"><a href="#cb14-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-314"><a href="#cb14-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-315"><a href="#cb14-315" aria-hidden="true" tabindex="-1"></a>What type of transformations $g()$ does an image classification model as in @fig-cnns-invariance-example need to be invariant to?</span>
<span id="cb14-316"><a href="#cb14-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-317"><a href="#cb14-317" aria-hidden="true" tabindex="-1"></a>**Think about it:** Consider what changes when the cows move to different positions in the image.</span>
<span id="cb14-318"><a href="#cb14-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-319"><a href="#cb14-319" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-320"><a href="#cb14-320" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Click for answer<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb14-321"><a href="#cb14-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-322"><a href="#cb14-322" aria-hidden="true" tabindex="-1"></a>**Translation invariance** is crucial for image classification. The model should output the same class label (e.g., "cow") regardless of where the object appears in the image.</span>
<span id="cb14-323"><a href="#cb14-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-324"><a href="#cb14-324" aria-hidden="true" tabindex="-1"></a>**Key transformations for classification:**</span>
<span id="cb14-325"><a href="#cb14-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-326"><a href="#cb14-326" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Translation**: Object position in the frame</span>
<span id="cb14-327"><a href="#cb14-327" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Scale**: Object size (small vs large)</span>
<span id="cb14-328"><a href="#cb14-328" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Rotation**: Object orientation (within reasonable limits)</span>
<span id="cb14-329"><a href="#cb14-329" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Lighting**: Brightness, contrast variations</span>
<span id="cb14-330"><a href="#cb14-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-331"><a href="#cb14-331" aria-hidden="true" tabindex="-1"></a>For classification tasks specifically, we need **invariance** because:</span>
<span id="cb14-332"><a href="#cb14-332" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The class label doesn't depend on spatial location</span>
<span id="cb14-333"><a href="#cb14-333" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A cow is a cow whether it's in the center or corner of the image</span>
<span id="cb14-334"><a href="#cb14-334" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The model should "pool" spatial information to make position-independent predictions</span>
<span id="cb14-335"><a href="#cb14-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-336"><a href="#cb14-336" aria-hidden="true" tabindex="-1"></a>This is typically achieved by combining CNNs' translation **equivariance** with operations like:</span>
<span id="cb14-337"><a href="#cb14-337" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Pooling layers** (max-pooling, average-pooling)</span>
<span id="cb14-338"><a href="#cb14-338" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Global average pooling** at the end of the network</span>
<span id="cb14-339"><a href="#cb14-339" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>These convert equivariant features into invariant predictions</span>
<span id="cb14-340"><a href="#cb14-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-341"><a href="#cb14-341" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-342"><a href="#cb14-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-343"><a href="#cb14-343" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-344"><a href="#cb14-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-345"><a href="#cb14-345" aria-hidden="true" tabindex="-1"></a>::: {#fig-cnns-invariance-example}</span>
<span id="cb14-346"><a href="#cb14-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-347"><a href="#cb14-347" aria-hidden="true" tabindex="-1"></a><span class="al">![ ]({{&lt; meta params.images_path &gt;}}cows2_small_invariance_collage.png)</span></span>
<span id="cb14-348"><a href="#cb14-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-349"><a href="#cb14-349" aria-hidden="true" tabindex="-1"></a>When objects in the input translate spatially, the output (in this case the classification of the images) does not change. The model  $f(\vect{x})$ is thus invariant to spatial translations.</span>
<span id="cb14-350"><a href="#cb14-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-351"><a href="#cb14-351" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-352"><a href="#cb14-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-353"><a href="#cb14-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-354"><a href="#cb14-354" aria-hidden="true" tabindex="-1"></a>@fig-cnns-equivariance-example shows an application of object detection. In this case, the model needs to be equivariant with respect to spatial translations (i.e., the bounding boxes that indicate an object's location need to translate with the input). The model should remain invariant with respect to small changes in color, such as brightness adjustments.</span>
<span id="cb14-355"><a href="#cb14-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-356"><a href="#cb14-356" aria-hidden="true" tabindex="-1"></a>::: {#fig-cnns-equivariance-example}</span>
<span id="cb14-357"><a href="#cb14-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-358"><a href="#cb14-358" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}cows2_small_detection_collage.png)</span></span>
<span id="cb14-359"><a href="#cb14-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-360"><a href="#cb14-360" aria-hidden="true" tabindex="-1"></a>When objects in the input (first row) translate spatially, the detections (bounding boxes) change accordingly (bottom row). The model $f(\vect{x})$ that produces the bounding boxes is thus equivariant with respect to spatial translations.</span>
<span id="cb14-361"><a href="#cb14-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-362"><a href="#cb14-362" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-363"><a href="#cb14-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-364"><a href="#cb14-364" aria-hidden="true" tabindex="-1"></a>We can summarise the required properties for different tasks:</span>
<span id="cb14-365"><a href="#cb14-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-366"><a href="#cb14-366" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="When do we want each?"}</span>
<span id="cb14-367"><a href="#cb14-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-368"><a href="#cb14-368" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Task Type <span class="pp">|</span> Property Needed <span class="pp">|</span> Why? <span class="pp">|</span></span>
<span id="cb14-369"><a href="#cb14-369" aria-hidden="true" tabindex="-1"></a><span class="pp">|-----------|-----------------|------|</span></span>
<span id="cb14-370"><a href="#cb14-370" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Classification** <span class="pp">|</span> Translation Invariance <span class="pp">|</span> Cat is still a cat regardless of position <span class="pp">|</span></span>
<span id="cb14-371"><a href="#cb14-371" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Object Detection** <span class="pp">|</span> Translation Equivariance <span class="pp">|</span> If object moves right, bounding box moves right <span class="pp">|</span></span>
<span id="cb14-372"><a href="#cb14-372" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Semantic Segmentation** <span class="pp">|</span> Translation Equivariance <span class="pp">|</span> If input shifts, pixel-wise labels shift <span class="pp">|</span></span>
<span id="cb14-373"><a href="#cb14-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-374"><a href="#cb14-374" aria-hidden="true" tabindex="-1"></a>: {tbl-colwidths="<span class="co">[</span><span class="ot">20,20,60</span><span class="co">]</span>"}</span>
<span id="cb14-375"><a href="#cb14-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-376"><a href="#cb14-376" aria-hidden="true" tabindex="-1"></a>Understanding this distinction is crucial for choosing the right architecture!</span>
<span id="cb14-377"><a href="#cb14-377" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-378"><a href="#cb14-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-379"><a href="#cb14-379" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- </span><span class="al">TODO</span><span class="co">: move or remove this becauwse the convolution operation has not yet been discussed in detail --&gt;</span></span>
<span id="cb14-380"><a href="#cb14-380" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ::: {.callout-tip title="Key Insight"}</span></span>
<span id="cb14-381"><a href="#cb14-381" aria-hidden="true" tabindex="-1"></a><span class="co">**CNNs naturally provide translation equivariance** due to their convolution operation. However, they need additional components (like pooling or global average pooling) to achieve translation invariance for classification tasks.</span></span>
<span id="cb14-382"><a href="#cb14-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-383"><a href="#cb14-383" aria-hidden="true" tabindex="-1"></a><span class="co">This is why CNN architecture design matters so much!</span></span>
<span id="cb14-384"><a href="#cb14-384" aria-hidden="true" tabindex="-1"></a><span class="co">::: --&gt;</span></span>
<span id="cb14-385"><a href="#cb14-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-386"><a href="#cb14-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-387"><a href="#cb14-387" aria-hidden="true" tabindex="-1"></a><span class="fu">## Convolutional Neural Networks</span></span>
<span id="cb14-388"><a href="#cb14-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-389"><a href="#cb14-389" aria-hidden="true" tabindex="-1"></a>Now that we've seen the limitations of MLPs for images, let's explore how **Convolutional Neural Networks (CNNs)** address these challenges.</span>
<span id="cb14-390"><a href="#cb14-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-391"><a href="#cb14-391" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title="The Big Idea"}</span>
<span id="cb14-392"><a href="#cb14-392" aria-hidden="true" tabindex="-1"></a>**Instead of connecting every neuron to every input**, CNNs use three key principles:</span>
<span id="cb14-393"><a href="#cb14-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-394"><a href="#cb14-394" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Local Connectivity**: Each neuron only looks at a small spatial region</span>
<span id="cb14-395"><a href="#cb14-395" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Parameter Sharing**: The same filter/detector is used across all spatial locations</span>
<span id="cb14-396"><a href="#cb14-396" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Translation Equivariance**: If the input shifts, the output shifts correspondingly</span>
<span id="cb14-397"><a href="#cb14-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-398"><a href="#cb14-398" aria-hidden="true" tabindex="-1"></a>These aren't just technical tricks—they encode our **prior knowledge** about how images work!</span>
<span id="cb14-399"><a href="#cb14-399" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-400"><a href="#cb14-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-401"><a href="#cb14-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-402"><a href="#cb14-402" aria-hidden="true" tabindex="-1"></a><span class="fu">### The CNN Advantage</span></span>
<span id="cb14-403"><a href="#cb14-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-404"><a href="#cb14-404" aria-hidden="true" tabindex="-1"></a>A single neuron in a CNN is only connected to a small portion (local connectivity) of the image (see @fig-cnn-cnn-spatial). As a result:</span>
<span id="cb14-405"><a href="#cb14-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-406"><a href="#cb14-406" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✅ **Fewer Parameters**: Neurons have far fewer connections than in MLPs</span>
<span id="cb14-407"><a href="#cb14-407" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✅ **Spatial Structure Preserved**: No need to flatten 2D images into 1D vectors</span>
<span id="cb14-408"><a href="#cb14-408" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✅ **Translation Equivariance**: Same detector works everywhere in the image</span>
<span id="cb14-409"><a href="#cb14-409" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✅ **Hierarchical Feature Learning**: Stack layers to learn increasingly complex patterns</span>
<span id="cb14-410"><a href="#cb14-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-411"><a href="#cb14-411" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-cnn-spatial}</span>
<span id="cb14-412"><a href="#cb14-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-413"><a href="#cb14-413" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}cnn_spatial.jpg)</span>{width=200}</span>
<span id="cb14-414"><a href="#cb14-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-415"><a href="#cb14-415" aria-hidden="true" tabindex="-1"></a>Source: @johnson_eecs_2019</span>
<span id="cb14-416"><a href="#cb14-416" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-417"><a href="#cb14-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-418"><a href="#cb14-418" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- </span><span class="al">TODO</span><span class="co">: consider moving this part to a later section --&gt;</span></span>
<span id="cb14-419"><a href="#cb14-419" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ::: {.callout-note}</span></span>
<span id="cb14-420"><a href="#cb14-420" aria-hidden="true" tabindex="-1"></a><span class="co">CNNs are used not only for image data but also for data with spatial dependencies/local structures. This includes not only images but also time series, videos, audio, and text. The key is that signals that are spatially close together should be interpreted together.</span></span>
<span id="cb14-421"><a href="#cb14-421" aria-hidden="true" tabindex="-1"></a><span class="co">::: --&gt;</span></span>
<span id="cb14-422"><a href="#cb14-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-423"><a href="#cb14-423" aria-hidden="true" tabindex="-1"></a><span class="fu">### Convolutional Neural Networks</span></span>
<span id="cb14-424"><a href="#cb14-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-425"><a href="#cb14-425" aria-hidden="true" tabindex="-1"></a>CNNs consist of a sequence of different layers. Each layer transforms activations from the previous layer into new activations through a differentiable operation. Below we look at the main layer types: convolutional layers, pooling layers, activation layers, and fully connected layers. Arranged in a specific sequence, this is referred to as the architecture of the model.</span>
<span id="cb14-426"><a href="#cb14-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-427"><a href="#cb14-427" aria-hidden="true" tabindex="-1"></a>@fig-cnn-convnet shows an example architecture. The activation maps of the various layers are shown, representing the corresponding outputs of the layers.</span>
<span id="cb14-428"><a href="#cb14-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-429"><a href="#cb14-429" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-convnet}</span>
<span id="cb14-430"><a href="#cb14-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-431"><a href="#cb14-431" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}convnet.jpeg)</span>{width=600}</span>
<span id="cb14-432"><a href="#cb14-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-433"><a href="#cb14-433" aria-hidden="true" tabindex="-1"></a>The activations of a ConvNet architecture are shown. The input image is on the left and the predictions on the right. Source: @li_cs231n_2022.</span>
<span id="cb14-434"><a href="#cb14-434" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-435"><a href="#cb14-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-436"><a href="#cb14-436" aria-hidden="true" tabindex="-1"></a>Sometimes different layers are combined and referred to as a block. For example, the combination of a convolutional layer followed by an activation layer and a pooling layer is often used. This would be a CONV-ACT-POOL block. --&gt;</span>
<span id="cb14-437"><a href="#cb14-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-438"><a href="#cb14-438" aria-hidden="true" tabindex="-1"></a><span class="fu">### Convolution Operation</span></span>
<span id="cb14-439"><a href="#cb14-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-440"><a href="#cb14-440" aria-hidden="true" tabindex="-1"></a>Convolutional layers are the primary layers in CNNs, responsible for extracting visual features. The weights of a convolutional layer consist of a set of learnable filters. Each filter is typically small along the spatial dimensions (height, width) relative to the input but extends across the entire input depth. For example, a typical filter in the first layer might have dimensions $3 \times 7 \times 7$ (3 pixels for the three color channels, 7 pixels along height/width). During the forward pass, the filters are convolved along the height and width over the input. At each position, the dot product between the filter and input is calculated (viewing both as 1-D vectors), producing a 2-D activation map that represents the filter's response at each position in the input. Intuitively, the CNN learns filters corresponding to typical visual patterns, such as edges and colors, in the first layer. A set of $C_{out}$ filters produces activation maps with a depth of $C_{out}$.</span>
<span id="cb14-441"><a href="#cb14-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-442"><a href="#cb14-442" aria-hidden="true" tabindex="-1"></a>::: {.callout-important title="Filters vs Kernels"}</span>
<span id="cb14-443"><a href="#cb14-443" aria-hidden="true" tabindex="-1"></a>Filter and kernel are sometimes used synonymously. Here, we differentiate by considering a filter as 3-dimensional (CxHxW) and a kernel as 2-dimensional (HxW). A filter consists of C kernels.</span>
<span id="cb14-444"><a href="#cb14-444" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-445"><a href="#cb14-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-446"><a href="#cb14-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-447"><a href="#cb14-447" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-448"><a href="#cb14-448" aria-hidden="true" tabindex="-1"></a>S(i, j) = (K * I)(i, j) =\sum_m\sum_n I(i + m, j + n)K(m, n)</span>
<span id="cb14-449"><a href="#cb14-449" aria-hidden="true" tabindex="-1"></a>$$ {#eq-cnn-convolution}</span>
<span id="cb14-450"><a href="#cb14-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-451"><a href="#cb14-451" aria-hidden="true" tabindex="-1"></a>With the following components:</span>
<span id="cb14-452"><a href="#cb14-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-453"><a href="#cb14-453" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$I$ is the input (for example, an image),</span>
<span id="cb14-454"><a href="#cb14-454" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$K$ is the kernel (typically smaller than $I$),</span>
<span id="cb14-455"><a href="#cb14-455" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$b$ is a bias term added to the weighted sum,</span>
<span id="cb14-456"><a href="#cb14-456" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$i$ and $j$ index the spatial positions.</span>
<span id="cb14-457"><a href="#cb14-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-458"><a href="#cb14-458" aria-hidden="true" tabindex="-1"></a>If $I$ is an RGB image (e.g., in the first layer of a CNN), the filter $K$ would have dimensions $3 \times k \times k$ (assuming a square kernel). More generally, we learn filters with dimensions $C_{in} \times k \times k$.</span>
<span id="cb14-459"><a href="#cb14-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-460"><a href="#cb14-460" aria-hidden="true" tabindex="-1"></a>Multiple filters, say $C_{out}$ filters, can be grouped together: $C_{out} \times C_{in} \times k \times k$.</span>
<span id="cb14-461"><a href="#cb14-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-462"><a href="#cb14-462" aria-hidden="true" tabindex="-1"></a>We often refer to such tensors as filters or filter banks.</span>
<span id="cb14-463"><a href="#cb14-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-464"><a href="#cb14-464" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb14-465"><a href="#cb14-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-466"><a href="#cb14-466" aria-hidden="true" tabindex="-1"></a>Convolution in deep learning is typically implemented as cross-correlation. @eq-cnn-convolution actually defines cross-correlation.</span>
<span id="cb14-467"><a href="#cb14-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-468"><a href="#cb14-468" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-469"><a href="#cb14-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-470"><a href="#cb14-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-471"><a href="#cb14-471" aria-hidden="true" tabindex="-1"></a>Convolution on RGB images is nicely illusrated in @fig-cnns-intro-example. We can see that the filter is multiplied element-wise with the corresponding values in the image, the result is summed top produce one single value in the output.</span>
<span id="cb14-472"><a href="#cb14-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-473"><a href="#cb14-473" aria-hidden="true" tabindex="-1"></a>::: {#fig-cnns-intro-example}</span>
<span id="cb14-474"><a href="#cb14-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-475"><a href="#cb14-475" aria-hidden="true" tabindex="-1"></a><span class="al">![Source:  @prince_understanding_2023]({{&lt; meta params.images_path &gt;}}convolution_prince_rgb.png)</span></span>
<span id="cb14-476"><a href="#cb14-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-477"><a href="#cb14-477" aria-hidden="true" tabindex="-1"></a>An RGB image is represented as a grid of pixels, each consisting of three color values $C_{in}=3$. Illusrated is how a filter with dimensionality $3 \times 3 \times 3 (C \times H \times W)$ is first multiplied elementwise with a specific input volume $3 \times 3 \times 3 (C \times H \times W)$ and then summed up to produce one single value in the output activation map (bright gray). Source:  @prince_understanding_2023</span>
<span id="cb14-478"><a href="#cb14-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-479"><a href="#cb14-479" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-480"><a href="#cb14-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-481"><a href="#cb14-481" aria-hidden="true" tabindex="-1"></a>The following figures show further step-by-step illustration of applying convolutions on input activations that produce multiple output activation maps.</span>
<span id="cb14-482"><a href="#cb14-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-483"><a href="#cb14-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-484"><a href="#cb14-484" aria-hidden="true" tabindex="-1"></a>@fig-cnn-conv-one-number illustrates how a filter produces one value per position by taking the dot product at a specific location (and adding the bias term).</span>
<span id="cb14-485"><a href="#cb14-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-486"><a href="#cb14-486" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-conv-one-number}</span>
<span id="cb14-487"><a href="#cb14-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-488"><a href="#cb14-488" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}cnn_conv_one_number.jpg)</span>{width=600}</span>
<span id="cb14-489"><a href="#cb14-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-490"><a href="#cb14-490" aria-hidden="true" tabindex="-1"></a>Source: @johnson_eecs_2019</span>
<span id="cb14-491"><a href="#cb14-491" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-492"><a href="#cb14-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-493"><a href="#cb14-493" aria-hidden="true" tabindex="-1"></a>@fig-cnn-conv-activation-map illustrates how a filter produces one activation map, which is an output with one channel and (almost) the same spatial dimensionality as the input activations.</span>
<span id="cb14-494"><a href="#cb14-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-495"><a href="#cb14-495" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-conv-activation-map}</span>
<span id="cb14-496"><a href="#cb14-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-497"><a href="#cb14-497" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}conv_activation_map.jpg)</span>{width=600}</span>
<span id="cb14-498"><a href="#cb14-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-499"><a href="#cb14-499" aria-hidden="true" tabindex="-1"></a>Source: @johnson_eecs_2019</span>
<span id="cb14-500"><a href="#cb14-500" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-501"><a href="#cb14-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-502"><a href="#cb14-502" aria-hidden="true" tabindex="-1"></a>@fig-cnn-conv-activation-map2  illustrates how multiple filters produce multiple outputs. After all, we often need to extract / detect different features, each represented by a specific filter.</span>
<span id="cb14-503"><a href="#cb14-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-504"><a href="#cb14-504" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-conv-activation-map2}</span>
<span id="cb14-505"><a href="#cb14-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-506"><a href="#cb14-506" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}conv_activation_map2.jpg)</span>{width=600}</span>
<span id="cb14-507"><a href="#cb14-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-508"><a href="#cb14-508" aria-hidden="true" tabindex="-1"></a>Source: @johnson_eecs_2019</span>
<span id="cb14-509"><a href="#cb14-509" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-510"><a href="#cb14-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-511"><a href="#cb14-511" aria-hidden="true" tabindex="-1"></a>Finally, @fig-cnn-conv-activation-map3 illustrates how a convolutional layer is defined: it consists of a set of filters and their corresponding biases. Note that the output is essentially a $C_{out}$-dimensional image.</span>
<span id="cb14-512"><a href="#cb14-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-513"><a href="#cb14-513" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-conv-activation-map3}</span>
<span id="cb14-514"><a href="#cb14-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-515"><a href="#cb14-515" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}conv_activation_map3.jpg)</span>{width=600}</span>
<span id="cb14-516"><a href="#cb14-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-517"><a href="#cb14-517" aria-hidden="true" tabindex="-1"></a>Source: @johnson_eecs_2019</span>
<span id="cb14-518"><a href="#cb14-518" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-519"><a href="#cb14-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-520"><a href="#cb14-520" aria-hidden="true" tabindex="-1"></a>The data is processed in mini-batches, i.e., multiple images at once, as shown in @fig-cnn-conv-activation-map4.</span>
<span id="cb14-521"><a href="#cb14-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-522"><a href="#cb14-522" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-conv-activation-map4}</span>
<span id="cb14-523"><a href="#cb14-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-524"><a href="#cb14-524" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}conv_activation_map4.jpg)</span>{width=600}</span>
<span id="cb14-525"><a href="#cb14-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-526"><a href="#cb14-526" aria-hidden="true" tabindex="-1"></a>Source: @johnson_eecs_2019</span>
<span id="cb14-527"><a href="#cb14-527" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-528"><a href="#cb14-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-529"><a href="#cb14-529" aria-hidden="true" tabindex="-1"></a><span class="fu">### Padding, Stride, Kernel Size, Dilation</span></span>
<span id="cb14-530"><a href="#cb14-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-531"><a href="#cb14-531" aria-hidden="true" tabindex="-1"></a>To define a convolution, various hyperparameters need to be set. Some of the most important ones are:</span>
<span id="cb14-532"><a href="#cb14-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-533"><a href="#cb14-533" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Padding: How much padding is added to the input?</span>
<span id="cb14-534"><a href="#cb14-534" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Stride: What is the step size (stride) of the kernel?</span>
<span id="cb14-535"><a href="#cb14-535" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Kernel-Size: What is the kernel size?</span>
<span id="cb14-536"><a href="#cb14-536" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Dilation: What is the dilation rate?</span>
<span id="cb14-537"><a href="#cb14-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-538"><a href="#cb14-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-539"><a href="#cb14-539" aria-hidden="true" tabindex="-1"></a>**Stride** determines how the filters are convolved over the input activations, essentially the step size. If the stride is 1, the filter moves one pixel at a time to compute the next activation (see @fig-cnn-stride1). If the stride is greater, e.g., 2, it moves two pixels at a time, making the activation maps smaller in width and height (see @fig-cnn-stride2).</span>
<span id="cb14-540"><a href="#cb14-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-541"><a href="#cb14-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-542"><a href="#cb14-542" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-stride1}</span>
<span id="cb14-543"><a href="#cb14-543" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}stride1_example.jpg)</span>{width=600}</span>
<span id="cb14-544"><a href="#cb14-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-545"><a href="#cb14-545" aria-hidden="true" tabindex="-1"></a>Convolution with stride (1, 1). Left the input activations with (red) the individual positions of the center pixel of the kernel / filter.</span>
<span id="cb14-546"><a href="#cb14-546" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-547"><a href="#cb14-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-548"><a href="#cb14-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-549"><a href="#cb14-549" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-stride2}</span>
<span id="cb14-550"><a href="#cb14-550" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}stride2_example.jpg)</span>{width=600}</span>
<span id="cb14-551"><a href="#cb14-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-552"><a href="#cb14-552" aria-hidden="true" tabindex="-1"></a>Convolution with stride (2, 2).  Left the input activations with (red) the individual positions of the center pixel of the kernel / filter.</span>
<span id="cb14-553"><a href="#cb14-553" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-554"><a href="#cb14-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-555"><a href="#cb14-555" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title="Stride &gt; 1 often used!"}</span>
<span id="cb14-556"><a href="#cb14-556" aria-hidden="true" tabindex="-1"></a>Convolutions with stride provide a way to reduce the spatial dimensionality within a CNN and are commonly used. We will see later that other operations, such as pooling layers, also reduce spatial dimensionality. Using convolutions, however, is sometimes advantageous, since the downsampling is learned.</span>
<span id="cb14-557"><a href="#cb14-557" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-558"><a href="#cb14-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-559"><a href="#cb14-559" aria-hidden="true" tabindex="-1"></a>**Padding** refers to adding (typically) zeros to the border of the input activations before performing the convolution. This can be useful to ensure, for example, that the spatial dimensions of the activation maps are identical to those of the input activations. This is essential for segmentation tasks. @fig-cnn-padding illustrates how padding helps maintain the spatial resolution when padding the input activations.</span>
<span id="cb14-560"><a href="#cb14-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-561"><a href="#cb14-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-562"><a href="#cb14-562" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-padding}</span>
<span id="cb14-563"><a href="#cb14-563" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}padding_example_result.jpg)</span>{width=600}</span>
<span id="cb14-564"><a href="#cb14-564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-565"><a href="#cb14-565" aria-hidden="true" tabindex="-1"></a>Left: Input (Yellow) with Zero-Padding (white border), Middle: Filter, Right: Output.</span>
<span id="cb14-566"><a href="#cb14-566" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-567"><a href="#cb14-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-568"><a href="#cb14-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-569"><a href="#cb14-569" aria-hidden="true" tabindex="-1"></a>**Kernel-size** determines the spatial extend from which a value in the output activation map migh aggregate values. It is the receptive field of an output value with respect to the input values. @fig-cnn-kernel-size3x3 and @fig-cnn-kernel-size5x5 show convolutions with different kernel sizes. Note that the amount of padding increases with kernel-size. Kernels usually have an odd number of pixels in height and width to make sure they have a center pixel. Also, the number of parameters increases with kernel-size $(C_{in} \times k \times k) + C_{out}$, which is often an important consideration.</span>
<span id="cb14-570"><a href="#cb14-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-571"><a href="#cb14-571" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-kernel-size3x3}</span>
<span id="cb14-572"><a href="#cb14-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-573"><a href="#cb14-573" aria-hidden="true" tabindex="-1"></a><span class="al">![ ]({{&lt; meta params.images_path &gt;}}kernel_size_example_3x3.jpg)</span>{width=600}</span>
<span id="cb14-574"><a href="#cb14-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-575"><a href="#cb14-575" aria-hidden="true" tabindex="-1"></a>Convolution with zero-padding and $3 \times 3$ kernel that produces activations maps with unchanged spatial resolution.</span>
<span id="cb14-576"><a href="#cb14-576" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-577"><a href="#cb14-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-578"><a href="#cb14-578" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-kernel-size5x5}</span>
<span id="cb14-579"><a href="#cb14-579" aria-hidden="true" tabindex="-1"></a><span class="al">![ ]({{&lt; meta params.images_path &gt;}}kernel_size_example_5x5.jpg)</span>{width=600}</span>
<span id="cb14-580"><a href="#cb14-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-581"><a href="#cb14-581" aria-hidden="true" tabindex="-1"></a>Convolution with zero-padding and $5 \times 5$ kernel that produces activations maps with unchanged spatial resolution.</span>
<span id="cb14-582"><a href="#cb14-582" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-583"><a href="#cb14-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-584"><a href="#cb14-584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-585"><a href="#cb14-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-586"><a href="#cb14-586" aria-hidden="true" tabindex="-1"></a>::: {.callout-note appearance="simple"}</span>
<span id="cb14-587"><a href="#cb14-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-588"><a href="#cb14-588" aria-hidden="true" tabindex="-1"></a>**Question**</span>
<span id="cb14-589"><a href="#cb14-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-590"><a href="#cb14-590" aria-hidden="true" tabindex="-1"></a>What is the difference between using 3×3 vs 5×5 kernels? What is the same?</span>
<span id="cb14-591"><a href="#cb14-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-592"><a href="#cb14-592" aria-hidden="true" tabindex="-1"></a>**Think first**: Consider computational cost, receptive field, and parameter count.</span>
<span id="cb14-593"><a href="#cb14-593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-594"><a href="#cb14-594" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-595"><a href="#cb14-595" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Click for detailed comparison<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb14-596"><a href="#cb14-596" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-597"><a href="#cb14-597" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Aspect <span class="pp">|</span> 3×3 Kernel <span class="pp">|</span> 5×5 Kernel <span class="pp">|</span></span>
<span id="cb14-598"><a href="#cb14-598" aria-hidden="true" tabindex="-1"></a><span class="pp">|--------|------------|------------|</span></span>
<span id="cb14-599"><a href="#cb14-599" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Parameters** <span class="pp">|</span> 9 weights (plus bias) per input channel  <span class="pp">|</span> 25 weights (plus bias) per input channel <span class="pp">|</span></span>
<span id="cb14-600"><a href="#cb14-600" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Computation** <span class="pp">|</span> Less (fewer multiplications) <span class="pp">|</span> More (more multiplications) <span class="pp">|</span></span>
<span id="cb14-601"><a href="#cb14-601" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Receptive Field** <span class="pp">|</span> Smaller (grows slowly) <span class="pp">|</span> Larger (grows faster) <span class="pp">|</span></span>
<span id="cb14-602"><a href="#cb14-602" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Detail Capture** <span class="pp">|</span> Fine-grained features <span class="pp">|</span> Broader spatial patterns <span class="pp">|</span></span>
<span id="cb14-603"><a href="#cb14-603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-604"><a href="#cb14-604" aria-hidden="true" tabindex="-1"></a>**What's the same**:</span>
<span id="cb14-605"><a href="#cb14-605" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Both preserve translation equivariance</span>
<span id="cb14-606"><a href="#cb14-606" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Both use parameter sharing</span>
<span id="cb14-607"><a href="#cb14-607" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Both can learn spatial features</span>
<span id="cb14-608"><a href="#cb14-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-609"><a href="#cb14-609" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-610"><a href="#cb14-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-611"><a href="#cb14-611" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-612"><a href="#cb14-612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-613"><a href="#cb14-613" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title="Best Practice"}</span>
<span id="cb14-614"><a href="#cb14-614" aria-hidden="true" tabindex="-1"></a>**Modern CNN Design**: Use multiple 3×3 convolutions instead of larger kernels to get:</span>
<span id="cb14-615"><a href="#cb14-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-616"><a href="#cb14-616" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✅ Fewer parameters</span>
<span id="cb14-617"><a href="#cb14-617" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✅ More non-linearity (more activation functions)</span>
<span id="cb14-618"><a href="#cb14-618" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✅ Better gradient flow</span>
<span id="cb14-619"><a href="#cb14-619" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✅ Similar receptive field coverage</span>
<span id="cb14-620"><a href="#cb14-620" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-621"><a href="#cb14-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-622"><a href="#cb14-622" aria-hidden="true" tabindex="-1"></a>**Dilation** specifies how much the kernel is spread out and thus directly influences the receptive field. @fig-cnn-dilation shows how it works.</span>
<span id="cb14-623"><a href="#cb14-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-624"><a href="#cb14-624" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-dilation}</span>
<span id="cb14-625"><a href="#cb14-625" aria-hidden="true" tabindex="-1"></a><span class="al">![ ]({{&lt; meta params.images_path &gt;}}dilation_example.jpg)</span>{width=600}</span>
<span id="cb14-626"><a href="#cb14-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-627"><a href="#cb14-627" aria-hidden="true" tabindex="-1"></a>Illustration of how a kernel with dilation=2 (middle) is applied to the input activations (left) to produce activations (right). Note the reduced spatial dimensionality of the output activations.</span>
<span id="cb14-628"><a href="#cb14-628" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-629"><a href="#cb14-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-630"><a href="#cb14-630" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Questoin: Dilation?" appearance="simple"}</span>
<span id="cb14-631"><a href="#cb14-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-632"><a href="#cb14-632" aria-hidden="true" tabindex="-1"></a>Why would we use dilation $d &gt; 1$?</span>
<span id="cb14-633"><a href="#cb14-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-634"><a href="#cb14-634" aria-hidden="true" tabindex="-1"></a>**Consider**: What happens to the receptive field? What about computational cost?</span>
<span id="cb14-635"><a href="#cb14-635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-636"><a href="#cb14-636" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-637"><a href="#cb14-637" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Click for comprehensive answer<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb14-638"><a href="#cb14-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-639"><a href="#cb14-639" aria-hidden="true" tabindex="-1"></a>**Dilation increases receptive field without adding parameters or computation!**</span>
<span id="cb14-640"><a href="#cb14-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-641"><a href="#cb14-641" aria-hidden="true" tabindex="-1"></a>**Benefits of dilation**:</span>
<span id="cb14-642"><a href="#cb14-642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-643"><a href="#cb14-643" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Larger receptive field**: See more context without more layers</span>
<span id="cb14-644"><a href="#cb14-644" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Same parameter count**: Filter size stays the same</span>
<span id="cb14-645"><a href="#cb14-645" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Multi-scale features**: Different dilation rates capture different scales</span>
<span id="cb14-646"><a href="#cb14-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-647"><a href="#cb14-647" aria-hidden="true" tabindex="-1"></a>**Use cases**:</span>
<span id="cb14-648"><a href="#cb14-648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-649"><a href="#cb14-649" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Semantic segmentation**: Need large receptive fields while preserving spatial resolution</span>
<span id="cb14-650"><a href="#cb14-650" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Dense prediction tasks**: Want to see global context for local predictions</span>
<span id="cb14-651"><a href="#cb14-651" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Efficient architectures**: Alternative to deeper networks</span>
<span id="cb14-652"><a href="#cb14-652" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-653"><a href="#cb14-653" aria-hidden="true" tabindex="-1"></a>**Trade-offs**:</span>
<span id="cb14-654"><a href="#cb14-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-655"><a href="#cb14-655" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>✅ Larger receptive field with same parameters</span>
<span id="cb14-656"><a href="#cb14-656" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>❌ May miss fine-grained spatial relationships</span>
<span id="cb14-657"><a href="#cb14-657" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>❌ Can create "grid artifacts" if not used carefully</span>
<span id="cb14-658"><a href="#cb14-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-659"><a href="#cb14-659" aria-hidden="true" tabindex="-1"></a>**Example**: A 3×3 kernel with dilation=2 has the receptive field of a 5×5 kernel but only 9 parameters instead of 25!</span>
<span id="cb14-660"><a href="#cb14-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-661"><a href="#cb14-661" aria-hidden="true" tabindex="-1"></a>**Atrous/Dilated Convolutions** are especially popular in:</span>
<span id="cb14-662"><a href="#cb14-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-663"><a href="#cb14-663" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**DeepLab** (semantic segmentation)</span>
<span id="cb14-664"><a href="#cb14-664" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**WaveNet** (audio generation)</span>
<span id="cb14-665"><a href="#cb14-665" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Dilated ResNets** (dense prediction)</span>
<span id="cb14-666"><a href="#cb14-666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-667"><a href="#cb14-667" aria-hidden="true" tabindex="-1"></a>The key insight: **effectively expand the receptive field**!</span>
<span id="cb14-668"><a href="#cb14-668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-669"><a href="#cb14-669" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-670"><a href="#cb14-670" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-671"><a href="#cb14-671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-672"><a href="#cb14-672" aria-hidden="true" tabindex="-1"></a>::: {.callout-note appearance="simple"}</span>
<span id="cb14-673"><a href="#cb14-673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-674"><a href="#cb14-674" aria-hidden="true" tabindex="-1"></a>@dumoulin_guide_2016 has created some animations for better understanding of convolutions and published them here: <span class="co">[</span><span class="ot">https://github.com/vdumoulin/conv_arithmetic</span><span class="co">](https://github.com/vdumoulin/conv_arithmetic)</span>.</span>
<span id="cb14-675"><a href="#cb14-675" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-676"><a href="#cb14-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-677"><a href="#cb14-677" aria-hidden="true" tabindex="-1"></a>::: {.content-hidden unless-format="html"}</span>
<span id="cb14-678"><a href="#cb14-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-679"><a href="#cb14-679" aria-hidden="true" tabindex="-1"></a><span class="al">![Convolving a 3x3 kernel over a 5x5 input without padding and with stride 1x1. Source @dumoulin_guide_2016]({{&lt; meta params.images_path &gt;}}no_padding_no_strides.gif)</span>{width=200}</span>
<span id="cb14-680"><a href="#cb14-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-681"><a href="#cb14-681" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-682"><a href="#cb14-682" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-683"><a href="#cb14-683" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-684"><a href="#cb14-684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-685"><a href="#cb14-685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-686"><a href="#cb14-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-687"><a href="#cb14-687" aria-hidden="true" tabindex="-1"></a><span class="fu">### Calculations: Spatial Dimensionality of Activation Maps</span></span>
<span id="cb14-688"><a href="#cb14-688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-689"><a href="#cb14-689" aria-hidden="true" tabindex="-1"></a>You can calculate the dimensionality of the activation maps with the following formulas:</span>
<span id="cb14-690"><a href="#cb14-690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-691"><a href="#cb14-691" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$i$: Side length of the input activations (assumption: square inputs)</span>
<span id="cb14-692"><a href="#cb14-692" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$k$: Kernel size (assumption: square kernel)</span>
<span id="cb14-693"><a href="#cb14-693" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$o$: Side length of the output activation maps</span>
<span id="cb14-694"><a href="#cb14-694" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$s$: Stride (assumption: same stride along spatial dimensions)</span>
<span id="cb14-695"><a href="#cb14-695" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$p$: Number of paddings on each side (assumption: same number of paddings along spatial dimensions)</span>
<span id="cb14-696"><a href="#cb14-696" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$d$: Dilation rate.</span>
<span id="cb14-697"><a href="#cb14-697" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-698"><a href="#cb14-698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-699"><a href="#cb14-699" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb14-700"><a href="#cb14-700" aria-hidden="true" tabindex="-1"></a>o = \left\lfloor \frac{i + 2p - d\cdot(k-1) - 1}{s} + 1 \right\rfloor</span>
<span id="cb14-701"><a href="#cb14-701" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb14-702"><a href="#cb14-702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-703"><a href="#cb14-703" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- </span><span class="al">TODO</span><span class="co">: hide Answer part of the followig questions  --&gt;</span></span>
<span id="cb14-704"><a href="#cb14-704" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="🧮 Quiz: Output Shape Calculation" collapse="true"}</span>
<span id="cb14-705"><a href="#cb14-705" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-706"><a href="#cb14-706" aria-hidden="true" tabindex="-1"></a>**Problem**: You're designing a CNN layer with the following parameters:</span>
<span id="cb14-707"><a href="#cb14-707" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-708"><a href="#cb14-708" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Input size: $i = 32$ (height and width)</span>
<span id="cb14-709"><a href="#cb14-709" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Kernel size: $k = 3$</span>
<span id="cb14-710"><a href="#cb14-710" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Padding: $p = 1$</span>
<span id="cb14-711"><a href="#cb14-711" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Stride: $s = 2$</span>
<span id="cb14-712"><a href="#cb14-712" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Dilation: $d = 1$</span>
<span id="cb14-713"><a href="#cb14-713" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-714"><a href="#cb14-714" aria-hidden="true" tabindex="-1"></a>**Question**: What is the output size $o$?</span>
<span id="cb14-715"><a href="#cb14-715" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-716"><a href="#cb14-716" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-717"><a href="#cb14-717" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Click for step-by-step solution<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb14-718"><a href="#cb14-718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-719"><a href="#cb14-719" aria-hidden="true" tabindex="-1"></a>**Step-by-step solution**:</span>
<span id="cb14-720"><a href="#cb14-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-721"><a href="#cb14-721" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Apply the formula: $o = \lfloor \frac{i + 2p - d(k-1) - 1}{s} + 1 \rfloor$</span>
<span id="cb14-722"><a href="#cb14-722" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Substitute: $o = \lfloor \frac{32 + 2(1) - 1(3-1) - 1}{2} + 1 \rfloor$</span>
<span id="cb14-723"><a href="#cb14-723" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Simplify: $o = \lfloor \frac{32 + 2 - 2 - 1}{2} + 1 \rfloor$</span>
<span id="cb14-724"><a href="#cb14-724" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Calculate: $o = \lfloor \frac{31}{2} + 1 \rfloor = \lfloor 15.5 + 1 \rfloor = 16$</span>
<span id="cb14-725"><a href="#cb14-725" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-726"><a href="#cb14-726" aria-hidden="true" tabindex="-1"></a>**Answer**: $o = 16$</span>
<span id="cb14-727"><a href="#cb14-727" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-728"><a href="#cb14-728" aria-hidden="true" tabindex="-1"></a>**Intuition**: With stride 2, we're roughly halving the spatial dimensions, but padding helps maintain some size.</span>
<span id="cb14-729"><a href="#cb14-729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-730"><a href="#cb14-730" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-731"><a href="#cb14-731" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-732"><a href="#cb14-732" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-733"><a href="#cb14-733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-734"><a href="#cb14-734" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-735"><a href="#cb14-735" aria-hidden="true" tabindex="-1"></a><span class="fu">### Calculations: Number of Weights in a Convolutional Layer</span></span>
<span id="cb14-736"><a href="#cb14-736" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-737"><a href="#cb14-737" aria-hidden="true" tabindex="-1"></a>You can calculate the total number of weights in a convolutional layer with the following formula:</span>
<span id="cb14-738"><a href="#cb14-738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-739"><a href="#cb14-739" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$k$: Kernel size (assumption: square kernel, so kernel has dimensions $k \times k$)</span>
<span id="cb14-740"><a href="#cb14-740" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$C_{in}$: Number of input channels</span>
<span id="cb14-741"><a href="#cb14-741" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$C_{out}$: Number of output channels</span>
<span id="cb14-742"><a href="#cb14-742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-743"><a href="#cb14-743" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-744"><a href="#cb14-744" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb14-745"><a href="#cb14-745" aria-hidden="true" tabindex="-1"></a>\text{Total Weights} = (k \times k) \times C_{in} \times C_{out}</span>
<span id="cb14-746"><a href="#cb14-746" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb14-747"><a href="#cb14-747" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-748"><a href="#cb14-748" aria-hidden="true" tabindex="-1"></a>Each filter has a size of $k \times k \times C_{in}$, and there are $C_{out}$ filters in total, resulting in $(k \times k \times C_{in}) \times C_{out}$ weights.</span>
<span id="cb14-749"><a href="#cb14-749" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-750"><a href="#cb14-750" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-751"><a href="#cb14-751" aria-hidden="true" tabindex="-1"></a>**Biases:**</span>
<span id="cb14-752"><a href="#cb14-752" aria-hidden="true" tabindex="-1"></a>If each output channel has a bias term, add an additional $C_{out}$ weights for the biases.</span>
<span id="cb14-753"><a href="#cb14-753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-754"><a href="#cb14-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-755"><a href="#cb14-755" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb14-756"><a href="#cb14-756" aria-hidden="true" tabindex="-1"></a>\text{Total Parameters} = (k \times k \times C_{in} \times C_{out}) + C_{out}</span>
<span id="cb14-757"><a href="#cb14-757" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb14-758"><a href="#cb14-758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-759"><a href="#cb14-759" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="🧮 Quiz: Parameter Counting" collapse="true"}</span>
<span id="cb14-760"><a href="#cb14-760" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-761"><a href="#cb14-761" aria-hidden="true" tabindex="-1"></a>**Scenario**: Design a convolutional layer with:</span>
<span id="cb14-762"><a href="#cb14-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-763"><a href="#cb14-763" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Input: $3 \times 32 \times 32$ (RGB image)</span>
<span id="cb14-764"><a href="#cb14-764" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>64 output channels</span>
<span id="cb14-765"><a href="#cb14-765" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Kernel size: $5 \times 5$</span>
<span id="cb14-766"><a href="#cb14-766" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Include bias terms</span>
<span id="cb14-767"><a href="#cb14-767" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-768"><a href="#cb14-768" aria-hidden="true" tabindex="-1"></a>**Questions**:</span>
<span id="cb14-769"><a href="#cb14-769" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-770"><a href="#cb14-770" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>How many weights in the filters?</span>
<span id="cb14-771"><a href="#cb14-771" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>How many bias parameters?</span>
<span id="cb14-772"><a href="#cb14-772" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Total parameters?</span>
<span id="cb14-773"><a href="#cb14-773" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>If we used a fully connected layer instead, how many parameters would that require?</span>
<span id="cb14-774"><a href="#cb14-774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-775"><a href="#cb14-775" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-776"><a href="#cb14-776" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Click for solutions<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb14-777"><a href="#cb14-777" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-778"><a href="#cb14-778" aria-hidden="true" tabindex="-1"></a>**Solutions**:</span>
<span id="cb14-779"><a href="#cb14-779" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-780"><a href="#cb14-780" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Filter weights**: $(5 \times 5) \times 3 \times 64 = 4{,}800$</span>
<span id="cb14-781"><a href="#cb14-781" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Bias parameters**: $64$ (one per output channel)</span>
<span id="cb14-782"><a href="#cb14-782" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Total parameters**: $4{,}800 + 64 = 4{,}864$</span>
<span id="cb14-783"><a href="#cb14-783" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Fully connected alternative**: $(32 \times 32 \times 3) \times 64 = 196{,}608$</span>
<span id="cb14-784"><a href="#cb14-784" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-785"><a href="#cb14-785" aria-hidden="true" tabindex="-1"></a>**Insight**: CNNs use **40× fewer parameters** while being more effective for images!</span>
<span id="cb14-786"><a href="#cb14-786" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-787"><a href="#cb14-787" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-788"><a href="#cb14-788" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-789"><a href="#cb14-789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-790"><a href="#cb14-790" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- </span><span class="al">TODO</span><span class="co">: verify whether k=7 and justification really makes sense --&gt;</span></span>
<span id="cb14-791"><a href="#cb14-791" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ::: {.callout-note appearance="simple"}</span></span>
<span id="cb14-792"><a href="#cb14-792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-793"><a href="#cb14-793" aria-hidden="true" tabindex="-1"></a><span class="co">**Question**</span></span>
<span id="cb14-794"><a href="#cb14-794" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-795"><a href="#cb14-795" aria-hidden="true" tabindex="-1"></a><span class="co">**Real-World Scenario**: You're building a medical imaging system for chest X-ray analysis.</span></span>
<span id="cb14-796"><a href="#cb14-796" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-797"><a href="#cb14-797" aria-hidden="true" tabindex="-1"></a><span class="co">**Scenario:**</span></span>
<span id="cb14-798"><a href="#cb14-798" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-799"><a href="#cb14-799" aria-hidden="true" tabindex="-1"></a><span class="co">- Input X-rays: $1 \times 512 \times 512$ (grayscale)</span></span>
<span id="cb14-800"><a href="#cb14-800" aria-hidden="true" tabindex="-1"></a><span class="co">- First conv layer: 32 filters, $k=7 \times 7$, $s=2$, $p=3$, $d=1$</span></span>
<span id="cb14-801"><a href="#cb14-801" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-802"><a href="#cb14-802" aria-hidden="true" tabindex="-1"></a><span class="co">**Your Tasks:**</span></span>
<span id="cb14-803"><a href="#cb14-803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-804"><a href="#cb14-804" aria-hidden="true" tabindex="-1"></a><span class="co">1. What is the output activation map size $(C_{out}, H_{out}, W_{out})$?</span></span>
<span id="cb14-805"><a href="#cb14-805" aria-hidden="true" tabindex="-1"></a><span class="co">2. How many parameters does this layer have?</span></span>
<span id="cb14-806"><a href="#cb14-806" aria-hidden="true" tabindex="-1"></a><span class="co">3. Why might we choose $k=7$ for medical images vs $k=3$ for natural images?</span></span>
<span id="cb14-807"><a href="#cb14-807" aria-hidden="true" tabindex="-1"></a><span class="co">4. What's the receptive field of each output pixel?</span></span>
<span id="cb14-808"><a href="#cb14-808" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-809"><a href="#cb14-809" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;details&gt;</span></span>
<span id="cb14-810"><a href="#cb14-810" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;summary&gt;Click for detailed solutions&lt;/summary&gt;</span></span>
<span id="cb14-811"><a href="#cb14-811" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-812"><a href="#cb14-812" aria-hidden="true" tabindex="-1"></a><span class="co">**Solutions:**</span></span>
<span id="cb14-813"><a href="#cb14-813" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-814"><a href="#cb14-814" aria-hidden="true" tabindex="-1"></a><span class="co">1. **Output size**:</span></span>
<span id="cb14-815"><a href="#cb14-815" aria-hidden="true" tabindex="-1"></a><span class="co">   - $C_{out} = 32$ (number of filters)</span></span>
<span id="cb14-816"><a href="#cb14-816" aria-hidden="true" tabindex="-1"></a><span class="co">   - $H_{out} = W_{out} = \lfloor \frac{512 + 2(3) - 1(7-1) - 1}{2} \rfloor + 1 = 256$</span></span>
<span id="cb14-817"><a href="#cb14-817" aria-hidden="true" tabindex="-1"></a><span class="co">   - **Answer**: $(32, 256, 256)$</span></span>
<span id="cb14-818"><a href="#cb14-818" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-819"><a href="#cb14-819" aria-hidden="true" tabindex="-1"></a><span class="co">2. **Parameters**:</span></span>
<span id="cb14-820"><a href="#cb14-820" aria-hidden="true" tabindex="-1"></a><span class="co">   - Filter weights: $(7 \times 7) \times 1 \times 32 = 1{,}568$</span></span>
<span id="cb14-821"><a href="#cb14-821" aria-hidden="true" tabindex="-1"></a><span class="co">   - Biases: $32$</span></span>
<span id="cb14-822"><a href="#cb14-822" aria-hidden="true" tabindex="-1"></a><span class="co">   - **Total**: $1{,}600$ parameters</span></span>
<span id="cb14-823"><a href="#cb14-823" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-824"><a href="#cb14-824" aria-hidden="true" tabindex="-1"></a><span class="co">3. **Why $k=7$ for medical images?**</span></span>
<span id="cb14-825"><a href="#cb14-825" aria-hidden="true" tabindex="-1"></a><span class="co">   - Medical features often span larger spatial areas (organ boundaries, lesions)</span></span>
<span id="cb14-826"><a href="#cb14-826" aria-hidden="true" tabindex="-1"></a><span class="co">   - Need to capture broader anatomical context early</span></span>
<span id="cb14-827"><a href="#cb14-827" aria-hidden="true" tabindex="-1"></a><span class="co">   - Higher resolution medical images benefit from larger initial receptive fields</span></span>
<span id="cb14-828"><a href="#cb14-828" aria-hidden="true" tabindex="-1"></a><span class="co">   - Natural images have more fine-grained local features</span></span>
<span id="cb14-829"><a href="#cb14-829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-830"><a href="#cb14-830" aria-hidden="true" tabindex="-1"></a><span class="co">4. **Receptive field**: $7 \times 7$ (same as kernel size for first layer)</span></span>
<span id="cb14-831"><a href="#cb14-831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-832"><a href="#cb14-832" aria-hidden="true" tabindex="-1"></a><span class="co">**Key Insight**: Domain knowledge should inform architectural choices!</span></span>
<span id="cb14-833"><a href="#cb14-833" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;/details&gt;</span></span>
<span id="cb14-834"><a href="#cb14-834" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-835"><a href="#cb14-835" aria-hidden="true" tabindex="-1"></a><span class="co">::: --&gt;</span></span>
<span id="cb14-836"><a href="#cb14-836" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-837"><a href="#cb14-837" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ::: {.callout-tip title="Domain-Specific Design"}</span></span>
<span id="cb14-838"><a href="#cb14-838" aria-hidden="true" tabindex="-1"></a><span class="co">Different domains need different CNN architectures:</span></span>
<span id="cb14-839"><a href="#cb14-839" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-840"><a href="#cb14-840" aria-hidden="true" tabindex="-1"></a><span class="co">| Domain | Typical Kernel Sizes | Why? |</span></span>
<span id="cb14-841"><a href="#cb14-841" aria-hidden="true" tabindex="-1"></a><span class="co">|--------|---------------------|------|</span></span>
<span id="cb14-842"><a href="#cb14-842" aria-hidden="true" tabindex="-1"></a><span class="co">| **Natural Images** | 3×3, 1×1 | Fine details, efficiency |</span></span>
<span id="cb14-843"><a href="#cb14-843" aria-hidden="true" tabindex="-1"></a><span class="co">| **Medical Imaging** | 5×5, 7×7 | Larger anatomical features |</span></span>
<span id="cb14-844"><a href="#cb14-844" aria-hidden="true" tabindex="-1"></a><span class="co">| **Satellite Imagery** | 7×7, 9×9 | Geographic features at scale |</span></span>
<span id="cb14-845"><a href="#cb14-845" aria-hidden="true" tabindex="-1"></a><span class="co">| **Text Recognition** | Rectangular (1×3, 3×1) | Character and word shapes |</span></span>
<span id="cb14-846"><a href="#cb14-846" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-847"><a href="#cb14-847" aria-hidden="true" tabindex="-1"></a><span class="co">: {tbl-colwidths="[30,20,50]"}</span></span>
<span id="cb14-848"><a href="#cb14-848" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-849"><a href="#cb14-849" aria-hidden="true" tabindex="-1"></a><span class="co">Understanding your data is crucial for good architecture design!</span></span>
<span id="cb14-850"><a href="#cb14-850" aria-hidden="true" tabindex="-1"></a><span class="co">::: --&gt;</span></span>
<span id="cb14-851"><a href="#cb14-851" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-852"><a href="#cb14-852" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- </span><span class="al">TODO</span><span class="co">: figure out how to handel the Properties section here and some of this information that was already provided previously    --&gt;</span></span>
<span id="cb14-853"><a href="#cb14-853" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-854"><a href="#cb14-854" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-855"><a href="#cb14-855" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-856"><a href="#cb14-856" aria-hidden="true" tabindex="-1"></a><span class="fu">## Properties</span></span>
<span id="cb14-857"><a href="#cb14-857" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-858"><a href="#cb14-858" aria-hidden="true" tabindex="-1"></a><span class="fu">### Local (Sparse) Connectivity &amp; Parameter Sharing</span></span>
<span id="cb14-859"><a href="#cb14-859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-860"><a href="#cb14-860" aria-hidden="true" tabindex="-1"></a>Fully connected layers are impractical when working with high-dimensional inputs like images. If all neurons in a layer were connected to all previous neurons, the number of parameters to be estimated would increase massively, which is inefficient and leads to overfitting. Each neuron in a CNN is therefore only connected to a local region of the input volume (see @fig-cnn-conv-one-number). The spatial extent of this region is a hyperparameter and is called the receptive field of a neuron (kernel size) on the input volume. The connections along the depth (C) extend over the entire depth of the input volume $C_{in}$. The connections are therefore local along the spatial dimensions (width and height) but complete along the depth.</span>
<span id="cb14-861"><a href="#cb14-861" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-862"><a href="#cb14-862" aria-hidden="true" tabindex="-1"></a>Parameter sharing in convolutional layers is used to reduce the number of parameters. Since the filters are convolved over the inputs, the individual weights of the filters are identical over the spatial extent of the input volume. One of the main assumptions behind CNNs is the following: If it is useful to learn a specific (visual) feature at a certain position, then it is probably useful at other positions as well. In other words: If I learn filters that detect edges, corners, or cats, then it is a reasonable assumption that I want to do this throughout the image.</span>
<span id="cb14-863"><a href="#cb14-863" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-864"><a href="#cb14-864" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb14-865"><a href="#cb14-865" aria-hidden="true" tabindex="-1"></a>Sometimes parameter sharing does not make sense. This can be the case, for example, if we have centered structures in the images. Then you might want to learn position-dependent features. An example is images of faces that have been centered, where you might want to learn filters that detect the mouth only in the lower middle area (locally connected layers).</span>
<span id="cb14-866"><a href="#cb14-866" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-867"><a href="#cb14-867" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-868"><a href="#cb14-868" aria-hidden="true" tabindex="-1"></a>@lst-cnns-mlp shows the number of parameters of an MLP, while @lst-cnns-cnn shows the number of parameters of a CNN (each with two hidden layers) on the CIFAR10 dataset.</span>
<span id="cb14-869"><a href="#cb14-869" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-872"><a href="#cb14-872" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-873"><a href="#cb14-873" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb14-874"><a href="#cb14-874" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-875"><a href="#cb14-875" aria-hidden="true" tabindex="-1"></a><span class="co">#| lst-label: lst-cnns-mlp</span></span>
<span id="cb14-876"><a href="#cb14-876" aria-hidden="true" tabindex="-1"></a><span class="co">#| lst-cap: PyTorch example of an MLP.</span></span>
<span id="cb14-877"><a href="#cb14-877" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb14-878"><a href="#cb14-878" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb14-879"><a href="#cb14-879" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb14-880"><a href="#cb14-880" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchinfo</span>
<span id="cb14-881"><a href="#cb14-881" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-882"><a href="#cb14-882" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MLP(nn.Module):</span>
<span id="cb14-883"><a href="#cb14-883" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-884"><a href="#cb14-884" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb14-885"><a href="#cb14-885" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb14-886"><a href="#cb14-886" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.flatten <span class="op">=</span> nn.Flatten()</span>
<span id="cb14-887"><a href="#cb14-887" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layer1 <span class="op">=</span> nn.Linear(<span class="dv">3</span> <span class="op">*</span> <span class="dv">32</span> <span class="op">*</span> <span class="dv">32</span>, <span class="dv">64</span>)</span>
<span id="cb14-888"><a href="#cb14-888" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layer2 <span class="op">=</span> nn.Linear(<span class="dv">64</span>, <span class="dv">32</span>)</span>
<span id="cb14-889"><a href="#cb14-889" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_layer <span class="op">=</span> nn.Linear(<span class="dv">32</span>, <span class="dv">10</span>)</span>
<span id="cb14-890"><a href="#cb14-890" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-891"><a href="#cb14-891" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb14-892"><a href="#cb14-892" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.flatten(x)</span>
<span id="cb14-893"><a href="#cb14-893" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.hidden_layer1(x))</span>
<span id="cb14-894"><a href="#cb14-894" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.hidden_layer2(x))</span>
<span id="cb14-895"><a href="#cb14-895" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.output_layer(x)</span>
<span id="cb14-896"><a href="#cb14-896" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb14-897"><a href="#cb14-897" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-898"><a href="#cb14-898" aria-hidden="true" tabindex="-1"></a>net <span class="op">=</span> MLP()</span>
<span id="cb14-899"><a href="#cb14-899" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(torchinfo.summary(net, input_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">32</span>, <span class="dv">32</span>)))</span>
<span id="cb14-900"><a href="#cb14-900" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-901"><a href="#cb14-901" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-904"><a href="#cb14-904" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-905"><a href="#cb14-905" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb14-906"><a href="#cb14-906" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-907"><a href="#cb14-907" aria-hidden="true" tabindex="-1"></a><span class="co">#| lst-label: lst-cnns-cnn</span></span>
<span id="cb14-908"><a href="#cb14-908" aria-hidden="true" tabindex="-1"></a><span class="co">#| lst-cap: PyTorch example of a CNN.</span></span>
<span id="cb14-909"><a href="#cb14-909" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb14-910"><a href="#cb14-910" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb14-911"><a href="#cb14-911" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb14-912"><a href="#cb14-912" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchinfo</span>
<span id="cb14-913"><a href="#cb14-913" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-914"><a href="#cb14-914" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CNN(nn.Module):</span>
<span id="cb14-915"><a href="#cb14-915" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-916"><a href="#cb14-916" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb14-917"><a href="#cb14-917" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb14-918"><a href="#cb14-918" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(<span class="dv">3</span>, <span class="dv">16</span>, <span class="dv">7</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb14-919"><a href="#cb14-919" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(<span class="dv">16</span>, <span class="dv">16</span>, <span class="dv">3</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-920"><a href="#cb14-920" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.flatten <span class="op">=</span> nn.Flatten()</span>
<span id="cb14-921"><a href="#cb14-921" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_layer <span class="op">=</span> nn.Linear(<span class="dv">16</span> <span class="op">*</span> <span class="dv">8</span> <span class="op">*</span> <span class="dv">8</span> , <span class="dv">10</span>)</span>
<span id="cb14-922"><a href="#cb14-922" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-923"><a href="#cb14-923" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb14-924"><a href="#cb14-924" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.conv1(x))</span>
<span id="cb14-925"><a href="#cb14-925" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.conv2(x))</span>
<span id="cb14-926"><a href="#cb14-926" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.flatten(x)</span>
<span id="cb14-927"><a href="#cb14-927" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.output_layer(x)</span>
<span id="cb14-928"><a href="#cb14-928" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb14-929"><a href="#cb14-929" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-930"><a href="#cb14-930" aria-hidden="true" tabindex="-1"></a>cnn <span class="op">=</span> CNN()</span>
<span id="cb14-931"><a href="#cb14-931" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(torchinfo.summary(cnn, input_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">32</span>, <span class="dv">32</span>)))</span>
<span id="cb14-932"><a href="#cb14-932" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-933"><a href="#cb14-933" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-934"><a href="#cb14-934" aria-hidden="true" tabindex="-1"></a>We can see that the number of parameters of the CNN is much smaller. Feel free to compare performances.</span>
<span id="cb14-935"><a href="#cb14-935" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-936"><a href="#cb14-936" aria-hidden="true" tabindex="-1"></a>::: {.callout-note appearance="simple"}</span>
<span id="cb14-937"><a href="#cb14-937" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-938"><a href="#cb14-938" aria-hidden="true" tabindex="-1"></a>**Question**</span>
<span id="cb14-939"><a href="#cb14-939" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-940"><a href="#cb14-940" aria-hidden="true" tabindex="-1"></a>Why in @lst-cnns-cnn is <span class="in">`pading=3`</span> in <span class="in">`self.conv1 = nn.Conv2d(3, 16, 7, stride=2, padding=3)`</span>? Would <span class="in">`padding=0`</span>also be possible?</span>
<span id="cb14-941"><a href="#cb14-941" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-942"><a href="#cb14-942" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-943"><a href="#cb14-943" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Click for answer<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb14-944"><a href="#cb14-944" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-945"><a href="#cb14-945" aria-hidden="true" tabindex="-1"></a>**Answer:**</span>
<span id="cb14-946"><a href="#cb14-946" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-947"><a href="#cb14-947" aria-hidden="true" tabindex="-1"></a>The <span class="in">`padding=3`</span> is chosen to maintain the spatial dimensions when using <span class="in">`stride=2`</span>.</span>
<span id="cb14-948"><a href="#cb14-948" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-949"><a href="#cb14-949" aria-hidden="true" tabindex="-1"></a>Using the output size formula:</span>
<span id="cb14-950"><a href="#cb14-950" aria-hidden="true" tabindex="-1"></a>$$o = \left\lfloor \frac{i + 2p - d\cdot(k-1) - 1}{s} + 1 \right\rfloor$$</span>
<span id="cb14-951"><a href="#cb14-951" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-952"><a href="#cb14-952" aria-hidden="true" tabindex="-1"></a>With the given parameters:</span>
<span id="cb14-953"><a href="#cb14-953" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Input: $i = 32$</span>
<span id="cb14-954"><a href="#cb14-954" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Kernel: $k = 7$</span>
<span id="cb14-955"><a href="#cb14-955" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Stride: $s = 2$</span>
<span id="cb14-956"><a href="#cb14-956" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Dilation: $d = 1$</span>
<span id="cb14-957"><a href="#cb14-957" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Padding: $p = 3$</span>
<span id="cb14-958"><a href="#cb14-958" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-959"><a href="#cb14-959" aria-hidden="true" tabindex="-1"></a>$$o = \left\lfloor \frac{32 + 2(3) - 1(7-1) - 1}{2} + 1 \right\rfloor = \left\lfloor \frac{32 + 6 - 6 - 1}{2} + 1 \right\rfloor = \left\lfloor \frac{31}{2} + 1 \right\rfloor = 16$$</span>
<span id="cb14-960"><a href="#cb14-960" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-961"><a href="#cb14-961" aria-hidden="true" tabindex="-1"></a>**Would `padding=0` work?**</span>
<span id="cb14-962"><a href="#cb14-962" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-963"><a href="#cb14-963" aria-hidden="true" tabindex="-1"></a>Yes, but it would produce a different output size:</span>
<span id="cb14-964"><a href="#cb14-964" aria-hidden="true" tabindex="-1"></a>$$o = \left\lfloor \frac{32 + 0 - 6 - 1}{2} + 1 \right\rfloor = \left\lfloor \frac{25}{2} + 1 \right\rfloor = 13$$</span>
<span id="cb14-965"><a href="#cb14-965" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-966"><a href="#cb14-966" aria-hidden="true" tabindex="-1"></a>**Why `padding=3` is preferred here:**</span>
<span id="cb14-967"><a href="#cb14-967" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-968"><a href="#cb14-968" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Maintains a "nice" output dimension ($16 \times 16$) that's easy to work with in subsequent layers</span>
<span id="cb14-969"><a href="#cb14-969" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Preserves more spatial information from the edges of the input</span>
<span id="cb14-970"><a href="#cb14-970" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Common practice: use padding to keep output size as $\lfloor \frac{i}{s} \rfloor$ when possible</span>
<span id="cb14-971"><a href="#cb14-971" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-972"><a href="#cb14-972" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-973"><a href="#cb14-973" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-974"><a href="#cb14-974" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-975"><a href="#cb14-975" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-976"><a href="#cb14-976" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-977"><a href="#cb14-977" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-978"><a href="#cb14-978" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb14-979"><a href="#cb14-979" aria-hidden="true" tabindex="-1"></a>A convolution can also be expressed as a single matrix multiplication and is thus a special case of a linear transformation (such as in an MLP). @fig-cnn-conv-as-matrix-illustration shows how a linear layer in an MLP (top left) has different connections between each input $x_1, \cdots, x_6$ and neurons in a hidden layer $h_1, \cdots, h_6$ and the corresponding weight matrix (bottom left). On the right, the same is shown for a 1-D convolution with kernel size 3, however, note that the weight matrix is sparse (white values) and that the weights are shared (repeated patterns across the diagonal).</span>
<span id="cb14-980"><a href="#cb14-980" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-981"><a href="#cb14-981" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-conv-as-matrix-illustration}</span>
<span id="cb14-982"><a href="#cb14-982" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-983"><a href="#cb14-983" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}1d_example_weight_matrix_mlp_vs_cnn.png)</span>{width=600}</span>
<span id="cb14-984"><a href="#cb14-984" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-985"><a href="#cb14-985" aria-hidden="true" tabindex="-1"></a>Illustration of linear layer in an MLP (left column), including graph (top) and weight matrix (bottom). Convolutional layer (right) represented as a linear transformation. Source @prince_understanding_2023.</span>
<span id="cb14-986"><a href="#cb14-986" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-987"><a href="#cb14-987" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-988"><a href="#cb14-988" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-989"><a href="#cb14-989" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-990"><a href="#cb14-990" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-991"><a href="#cb14-991" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-992"><a href="#cb14-992" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Question: Linear Transformation" appearance="simple" collapse="true"}</span>
<span id="cb14-993"><a href="#cb14-993" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-994"><a href="#cb14-994" aria-hidden="true" tabindex="-1"></a>How should the linear transformation be defined to obtain the desired result in @fig-cnn-linear-transf-calc? How many parameters are needed? How could this be done with a convolution?</span>
<span id="cb14-995"><a href="#cb14-995" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-996"><a href="#cb14-996" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-997"><a href="#cb14-997" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-linear-transf-calc}</span>
<span id="cb14-998"><a href="#cb14-998" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-999"><a href="#cb14-999" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}linear_transf.png)</span>{width=800}</span>
<span id="cb14-1000"><a href="#cb14-1000" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1001"><a href="#cb14-1001" aria-hidden="true" tabindex="-1"></a>Input in 2-D (top left), the flattened version of it (bottom left), expected output (right), and unknown transformation (middle).</span>
<span id="cb14-1002"><a href="#cb14-1002" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-1003"><a href="#cb14-1003" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1004"><a href="#cb14-1004" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1005"><a href="#cb14-1005" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-1006"><a href="#cb14-1006" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1007"><a href="#cb14-1007" aria-hidden="true" tabindex="-1"></a><span class="fu">### Translation Equivariance</span></span>
<span id="cb14-1008"><a href="#cb14-1008" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1009"><a href="#cb14-1009" aria-hidden="true" tabindex="-1"></a>Convolutions are translation equivariant, as illustrated well in the following example:</span>
<span id="cb14-1010"><a href="#cb14-1010" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1011"><a href="#cb14-1011" aria-hidden="true" tabindex="-1"></a>::: {#fig-cnn-translation-equivariance}</span>
<span id="cb14-1012"><a href="#cb14-1012" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1013"><a href="#cb14-1013" aria-hidden="true" tabindex="-1"></a>{{&lt; video https://www.youtube.com/embed/qoWAFBYOtoU start="50" &gt;}}</span>
<span id="cb14-1014"><a href="#cb14-1014" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1015"><a href="#cb14-1015" aria-hidden="true" tabindex="-1"></a>Illustration of translation equivariance in convolutional layers.</span>
<span id="cb14-1016"><a href="#cb14-1016" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-1017"><a href="#cb14-1017" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1018"><a href="#cb14-1018" aria-hidden="true" tabindex="-1"></a>See @sec-cnn-invariance for more information.</span>
<span id="cb14-1019"><a href="#cb14-1019" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1020"><a href="#cb14-1020" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1021"><a href="#cb14-1021" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1022"><a href="#cb14-1022" aria-hidden="true" tabindex="-1"></a><span class="fu">### Stacking Convolutions</span></span>
<span id="cb14-1023"><a href="#cb14-1023" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1024"><a href="#cb14-1024" aria-hidden="true" tabindex="-1"></a>Multiple convolutions can be executed in sequence (stacking). Each convolution is performed on the activation maps of another, preceeding convolution. @fig-cnn-conv-stacking illustrates the process.</span>
<span id="cb14-1025"><a href="#cb14-1025" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1026"><a href="#cb14-1026" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-conv-stacking}</span>
<span id="cb14-1027"><a href="#cb14-1027" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1028"><a href="#cb14-1028" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}conv_stacking.jpg)</span>{width=600}</span>
<span id="cb14-1029"><a href="#cb14-1029" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1030"><a href="#cb14-1030" aria-hidden="true" tabindex="-1"></a>Source: @johnson_eecs_2019</span>
<span id="cb14-1031"><a href="#cb14-1031" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-1032"><a href="#cb14-1032" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1033"><a href="#cb14-1033" aria-hidden="true" tabindex="-1"></a>A convolution is therefore not only performed directly on the input (e.g., images) but is generally defined on inputs of dimensionality $C \times H \times W$.</span>
<span id="cb14-1034"><a href="#cb14-1034" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1035"><a href="#cb14-1035" aria-hidden="true" tabindex="-1"></a>**The principle of stacking convolutional layers and therefore convolutions are not just applied on raw pixel values from an image is important to understand!** While the filters in the first convolutional layer, which directly operate on the input image, typically learn basic features such as lines in different orientations or colors, filters in subsequent layers learn more complex features by combining the features from preceeding layers. This is similar to our visual cortex. @Zeiler2014 used a special technique to visualize patterns that filters in deeper layers are reacting to (see @fig-cnn-conv-zeiler).</span>
<span id="cb14-1036"><a href="#cb14-1036" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1037"><a href="#cb14-1037" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1038"><a href="#cb14-1038" aria-hidden="true" tabindex="-1"></a>However, non-linear activation functions must be used between the convolutions. Otherwise, the stacked convolution can be expressed with a simple convolution (similar to an MLP, which can be expressed with a linear transformation without activation functions).</span>
<span id="cb14-1039"><a href="#cb14-1039" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1040"><a href="#cb14-1040" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1041"><a href="#cb14-1041" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-conv-zeiler}</span>
<span id="cb14-1042"><a href="#cb14-1042" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1043"><a href="#cb14-1043" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}zeiler_learned_filters.png)</span>{width=600}</span>
<span id="cb14-1044"><a href="#cb14-1044" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1045"><a href="#cb14-1045" aria-hidden="true" tabindex="-1"></a>Shown are patterns the learned flters are reacting to for several layers (in gray) and the corresponding image patches that achieve maximum activation values for these patterns. Source: @Zeiler2014</span>
<span id="cb14-1046"><a href="#cb14-1046" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1047"><a href="#cb14-1047" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-1048"><a href="#cb14-1048" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1049"><a href="#cb14-1049" aria-hidden="true" tabindex="-1"></a><span class="fu">### Receptive Field  {#sec-cnn-receptive-field}</span></span>
<span id="cb14-1050"><a href="#cb14-1050" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1051"><a href="#cb14-1051" aria-hidden="true" tabindex="-1"></a>The receptive field defines which inputs influence the activations of a neuron. The receptive field of a neuron with respect to the input activations (the image pixels) is quite important. If the receptive field is smaller than the objects of interest, the network is unable to "see" the whole object. This is particularly problematic for object detection. See @fig-cnn-receptive-field-1 and @fig-cnn-receptive-field-4 to illustrate the concept.</span>
<span id="cb14-1052"><a href="#cb14-1052" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1053"><a href="#cb14-1053" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1054"><a href="#cb14-1054" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-receptive-field-1}</span>
<span id="cb14-1055"><a href="#cb14-1055" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}receptive_field1.jpg)</span>{width=600}</span>
<span id="cb14-1056"><a href="#cb14-1056" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1057"><a href="#cb14-1057" aria-hidden="true" tabindex="-1"></a>Receptive field with a 5x5 kernel.</span>
<span id="cb14-1058"><a href="#cb14-1058" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-1059"><a href="#cb14-1059" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1060"><a href="#cb14-1060" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1061"><a href="#cb14-1061" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-receptive-field-2}</span>
<span id="cb14-1062"><a href="#cb14-1062" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}receptive_field2.jpg)</span></span>
<span id="cb14-1063"><a href="#cb14-1063" aria-hidden="true" tabindex="-1"></a>Receptive field with a 5x5 kernel and two layers.</span>
<span id="cb14-1064"><a href="#cb14-1064" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-1065"><a href="#cb14-1065" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1066"><a href="#cb14-1066" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1067"><a href="#cb14-1067" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-receptive-field-3}</span>
<span id="cb14-1068"><a href="#cb14-1068" aria-hidden="true" tabindex="-1"></a><span class="al">![ ]({{&lt; meta params.images_path &gt;}}receptive_field3.jpg)</span></span>
<span id="cb14-1069"><a href="#cb14-1069" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1070"><a href="#cb14-1070" aria-hidden="true" tabindex="-1"></a>Receptive field with a 5x5 kernel and two layers.</span>
<span id="cb14-1071"><a href="#cb14-1071" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-1072"><a href="#cb14-1072" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1073"><a href="#cb14-1073" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1074"><a href="#cb14-1074" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-receptive-field-4}</span>
<span id="cb14-1075"><a href="#cb14-1075" aria-hidden="true" tabindex="-1"></a><span class="al">![ ]({{&lt; meta params.images_path &gt;}}receptive_field4.jpg)</span></span>
<span id="cb14-1076"><a href="#cb14-1076" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1077"><a href="#cb14-1077" aria-hidden="true" tabindex="-1"></a>Receptive field with a 5x5 kernel and two layers. The output activation at the center (right) aggregates values over the union of all blue boxes in the left figure and has thus a receptive field of $9 \times 9$ with respect to the input activations.</span>
<span id="cb14-1078"><a href="#cb14-1078" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-1079"><a href="#cb14-1079" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1080"><a href="#cb14-1080" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1081"><a href="#cb14-1081" aria-hidden="true" tabindex="-1"></a>The receptive field $r_0$ of a convolutional neural network can be calculated as follows:</span>
<span id="cb14-1082"><a href="#cb14-1082" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1083"><a href="#cb14-1083" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Define the recurrence relation:</span>
<span id="cb14-1084"><a href="#cb14-1084" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1085"><a href="#cb14-1085" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb14-1086"><a href="#cb14-1086" aria-hidden="true" tabindex="-1"></a>r_{l-1} = s_l \cdot r_l + (k_l - s_l)</span>
<span id="cb14-1087"><a href="#cb14-1087" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb14-1088"><a href="#cb14-1088" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1089"><a href="#cb14-1089" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$r_l$: Receptive field at layer $l$.</span>
<span id="cb14-1090"><a href="#cb14-1090" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$s_l$: Stride of layer $l$.</span>
<span id="cb14-1091"><a href="#cb14-1091" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$k_l$: Kernel size of layer $l$.</span>
<span id="cb14-1092"><a href="#cb14-1092" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1093"><a href="#cb14-1093" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Solving this recurrence relation, we get the closed-form solution for the receptive field at the input layer $r_0$:</span>
<span id="cb14-1094"><a href="#cb14-1094" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1095"><a href="#cb14-1095" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb14-1096"><a href="#cb14-1096" aria-hidden="true" tabindex="-1"></a>r_0 = \sum_{l=1}^L \left( (k_l - 1) \prod_{i=1}^{l-1} s_i \right) + 1</span>
<span id="cb14-1097"><a href="#cb14-1097" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb14-1098"><a href="#cb14-1098" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1099"><a href="#cb14-1099" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$L$: Total number of layers.</span>
<span id="cb14-1100"><a href="#cb14-1100" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$k_l$: Kernel size at layer $l$.</span>
<span id="cb14-1101"><a href="#cb14-1101" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$s_i$: Stride at layer $i$, from layer 1 up to $l-1$.</span>
<span id="cb14-1102"><a href="#cb14-1102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1103"><a href="#cb14-1103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1104"><a href="#cb14-1104" aria-hidden="true" tabindex="-1"></a>@araujo_computing_2019 have written a nice article about receptive fields and how to calculate them (also refer to this article for more details on the notation and justification).</span>
<span id="cb14-1105"><a href="#cb14-1105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1106"><a href="#cb14-1106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1107"><a href="#cb14-1107" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="🎯 Challenge Quiz: Receptive Field" collapse="true"}</span>
<span id="cb14-1108"><a href="#cb14-1108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1109"><a href="#cb14-1109" aria-hidden="true" tabindex="-1"></a>**Network Architecture**:</span>
<span id="cb14-1110"><a href="#cb14-1110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1111"><a href="#cb14-1111" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Layer 1: Conv2d(kernel=3, stride=1)</span>
<span id="cb14-1112"><a href="#cb14-1112" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Layer 2: Conv2d(kernel=3, stride=2)</span>
<span id="cb14-1113"><a href="#cb14-1113" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Layer 3: Conv2d(kernel=5, stride=1)</span>
<span id="cb14-1114"><a href="#cb14-1114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1115"><a href="#cb14-1115" aria-hidden="true" tabindex="-1"></a>**Question**: What is the receptive field of the final layer with respect to the input?</span>
<span id="cb14-1116"><a href="#cb14-1116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1117"><a href="#cb14-1117" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-1118"><a href="#cb14-1118" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Click for solution<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb14-1119"><a href="#cb14-1119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1120"><a href="#cb14-1120" aria-hidden="true" tabindex="-1"></a>**Solution using the formula**:</span>
<span id="cb14-1121"><a href="#cb14-1121" aria-hidden="true" tabindex="-1"></a>$$r_0 = \sum_{l=1}^3 \left( (k_l - 1) \prod_{i=1}^{l-1} s_i \right) + 1$$</span>
<span id="cb14-1122"><a href="#cb14-1122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1123"><a href="#cb14-1123" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$r_0 = (3-1) \cdot 1 + (3-1) \cdot 1 + (5-1) \cdot (1 \cdot 2) + 1$</span>
<span id="cb14-1124"><a href="#cb14-1124" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$r_0 = 2 + 2 + 8 + 1 = 13$</span>
<span id="cb14-1125"><a href="#cb14-1125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1126"><a href="#cb14-1126" aria-hidden="true" tabindex="-1"></a>**Answer**: $13 \times 13$ receptive field</span>
<span id="cb14-1127"><a href="#cb14-1127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1128"><a href="#cb14-1128" aria-hidden="true" tabindex="-1"></a>**Practical insight**: Each output pixel "sees" a $13 \times 13$ region of the input image.</span>
<span id="cb14-1129"><a href="#cb14-1129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1130"><a href="#cb14-1130" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-1131"><a href="#cb14-1131" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-1132"><a href="#cb14-1132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1133"><a href="#cb14-1133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1134"><a href="#cb14-1134" aria-hidden="true" tabindex="-1"></a><span class="fu">### Comparing MLPs and CNNs</span></span>
<span id="cb14-1135"><a href="#cb14-1135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1136"><a href="#cb14-1136" aria-hidden="true" tabindex="-1"></a>Now that we understand how CNNs work, let's compare them systematically to MLPs:</span>
<span id="cb14-1137"><a href="#cb14-1137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1138"><a href="#cb14-1138" aria-hidden="true" tabindex="-1"></a>:::{.callout-note title="Question: MLP vs CNNs" appearance="simple"}</span>
<span id="cb14-1139"><a href="#cb14-1139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1140"><a href="#cb14-1140" aria-hidden="true" tabindex="-1"></a>How do MLPs and CNNs differ in their approach to processing images?</span>
<span id="cb14-1141"><a href="#cb14-1141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1142"><a href="#cb14-1142" aria-hidden="true" tabindex="-1"></a>**Think first**: Consider parameter count, assumptions about data, and what each architecture "knows" about images.</span>
<span id="cb14-1143"><a href="#cb14-1143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1144"><a href="#cb14-1144" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-1145"><a href="#cb14-1145" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Click for detailed comparison<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb14-1146"><a href="#cb14-1146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1147"><a href="#cb14-1147" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> Aspect <span class="pp">|</span> MLP <span class="pp">|</span> CNN <span class="pp">|</span></span>
<span id="cb14-1148"><a href="#cb14-1148" aria-hidden="true" tabindex="-1"></a><span class="pp">|--------|-----|-----|</span></span>
<span id="cb14-1149"><a href="#cb14-1149" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Input** <span class="pp">|</span> Flattened vector <span class="pp">|</span> Preserve spatial structure (H×W×C) <span class="pp">|</span></span>
<span id="cb14-1150"><a href="#cb14-1150" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Connections** <span class="pp">|</span> Fully connected <span class="pp">|</span> Locally connected <span class="pp">|</span></span>
<span id="cb14-1151"><a href="#cb14-1151" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Parameter Sharing** <span class="pp">|</span> None (each weight unique) <span class="pp">|</span> Shared across spatial locations <span class="pp">|</span></span>
<span id="cb14-1152"><a href="#cb14-1152" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Translation Equivariance** <span class="pp">|</span> ❌ No <span class="pp">|</span> ✅ Yes (by design) <span class="pp">|</span></span>
<span id="cb14-1153"><a href="#cb14-1153" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Assumptions** <span class="pp">|</span> None about spatial structure <span class="pp">|</span> Locality + stationarity of statistics <span class="pp">|</span></span>
<span id="cb14-1154"><a href="#cb14-1154" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Parameter Count** <span class="pp">|</span> Very high <span class="pp">|</span> Much lower <span class="pp">|</span></span>
<span id="cb14-1155"><a href="#cb14-1155" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Learns Position** <span class="pp">|</span> Must learn same feature at each position <span class="pp">|</span> Learns once, applies everywhere <span class="pp">|</span></span>
<span id="cb14-1156"><a href="#cb14-1156" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> **Best For** <span class="pp">|</span> Tabular data, general patterns <span class="pp">|</span> Images, spatial data <span class="pp">|</span></span>
<span id="cb14-1157"><a href="#cb14-1157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1158"><a href="#cb14-1158" aria-hidden="true" tabindex="-1"></a>: {tbl-colwidths="<span class="co">[</span><span class="ot">20,40,40</span><span class="co">]</span>"}</span>
<span id="cb14-1159"><a href="#cb14-1159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1160"><a href="#cb14-1160" aria-hidden="true" tabindex="-1"></a>**Key Takeaway**: MLPs *can* theoretically learn to process images (universal approximation theorem), but CNNs bake in the right inductive biases, making learning vastly more efficient.</span>
<span id="cb14-1161"><a href="#cb14-1161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1162"><a href="#cb14-1162" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-1163"><a href="#cb14-1163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1164"><a href="#cb14-1164" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-1165"><a href="#cb14-1165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1166"><a href="#cb14-1166" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title="Deep Learning Insight: Inductive Bias"}</span>
<span id="cb14-1167"><a href="#cb14-1167" aria-hidden="true" tabindex="-1"></a>**Inductive bias** refers to the assumptions a learning algorithm makes to generalize from training data to unseen data.</span>
<span id="cb14-1168"><a href="#cb14-1168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1169"><a href="#cb14-1169" aria-hidden="true" tabindex="-1"></a>**MLPs**: Minimal inductive bias (very general, but requires lots of data)</span>
<span id="cb14-1170"><a href="#cb14-1170" aria-hidden="true" tabindex="-1"></a>**CNNs**: Strong inductive bias (assumes locality, translation equivariance → more efficient learning)</span>
<span id="cb14-1171"><a href="#cb14-1171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1172"><a href="#cb14-1172" aria-hidden="true" tabindex="-1"></a>The right inductive bias for your data is key to success! This is why:</span>
<span id="cb14-1173"><a href="#cb14-1173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1174"><a href="#cb14-1174" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>CNNs excel at images (spatial locality matters)</span>
<span id="cb14-1175"><a href="#cb14-1175" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Transformers excel at sequences (long-range dependencies matter)</span>
<span id="cb14-1176"><a href="#cb14-1176" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Graph Neural Networks excel at networks (graph structure matters)</span>
<span id="cb14-1177"><a href="#cb14-1177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1178"><a href="#cb14-1178" aria-hidden="true" tabindex="-1"></a>Choosing architecture = choosing which assumptions to bake in!</span>
<span id="cb14-1179"><a href="#cb14-1179" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-1180"><a href="#cb14-1180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1181"><a href="#cb14-1181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1182"><a href="#cb14-1182" aria-hidden="true" tabindex="-1"></a><span class="fu">## More Layers &amp; Operations</span></span>
<span id="cb14-1183"><a href="#cb14-1183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1184"><a href="#cb14-1184" aria-hidden="true" tabindex="-1"></a>CNNs often do not consist solely of convolutional layers but have other layers with specific purposes.</span>
<span id="cb14-1185"><a href="#cb14-1185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1186"><a href="#cb14-1186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1187"><a href="#cb14-1187" aria-hidden="true" tabindex="-1"></a><span class="fu">### Downsampling</span></span>
<span id="cb14-1188"><a href="#cb14-1188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1189"><a href="#cb14-1189" aria-hidden="true" tabindex="-1"></a>Spatial downsampling is often a desired property of convolutional layers. We can achieve that by choosing a stride $\gt 1$. However, there are other options, such as pooling layers. @fig-cnn-downsampling-pooling illustrates some pooling variants.</span>
<span id="cb14-1190"><a href="#cb14-1190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1191"><a href="#cb14-1191" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-downsampling-pooling}</span>
<span id="cb14-1192"><a href="#cb14-1192" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}downsampling_with_pooling.jpg)</span></span>
<span id="cb14-1193"><a href="#cb14-1193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1194"><a href="#cb14-1194" aria-hidden="true" tabindex="-1"></a>Shown are a) Sub-Sampling, b) Max Pooling, c) Average Pooling. Source:  @prince_understanding_2023</span>
<span id="cb14-1195"><a href="#cb14-1195" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-1196"><a href="#cb14-1196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1197"><a href="#cb14-1197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1198"><a href="#cb14-1198" aria-hidden="true" tabindex="-1"></a><span class="fu">### Upsampling {#sec-cnn-upsampling}</span></span>
<span id="cb14-1199"><a href="#cb14-1199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1200"><a href="#cb14-1200" aria-hidden="true" tabindex="-1"></a>Sometimes we need to increase the spatial dimensionality of our activation maps.</span>
<span id="cb14-1201"><a href="#cb14-1201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1202"><a href="#cb14-1202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1203"><a href="#cb14-1203" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-upsampling}</span>
<span id="cb14-1204"><a href="#cb14-1204" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}upsampling_options.jpg)</span></span>
<span id="cb14-1205"><a href="#cb14-1205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1206"><a href="#cb14-1206" aria-hidden="true" tabindex="-1"></a>Shown are  a) Duplicate, b) Max-Unpooling, c) Bilinear Interpolation . Source:  @prince_understanding_2023</span>
<span id="cb14-1207"><a href="#cb14-1207" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-1208"><a href="#cb14-1208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1209"><a href="#cb14-1209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1210"><a href="#cb14-1210" aria-hidden="true" tabindex="-1"></a>Upsampling can also be learned with Transposed Convolutions. More on that topic in _Image Segmentation_.</span>
<span id="cb14-1211"><a href="#cb14-1211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1212"><a href="#cb14-1212" aria-hidden="true" tabindex="-1"></a><span class="co"> &lt;!-- [image segmentation](segmentation.qmd#sec-segmentation-upsampling). --&gt;</span></span>
<span id="cb14-1213"><a href="#cb14-1213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1214"><a href="#cb14-1214" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- :::{#fig-cnn-transposed-convs}</span></span>
<span id="cb14-1215"><a href="#cb14-1215" aria-hidden="true" tabindex="-1"></a><span class="co">![ ]({{&lt; meta params.images_path &gt;}}transposed_convolution.jpg)</span></span>
<span id="cb14-1216"><a href="#cb14-1216" aria-hidden="true" tabindex="-1"></a><span class="co">::: --&gt;</span></span>
<span id="cb14-1217"><a href="#cb14-1217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1218"><a href="#cb14-1218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1219"><a href="#cb14-1219" aria-hidden="true" tabindex="-1"></a><span class="fu">### Changing the number of channels</span></span>
<span id="cb14-1220"><a href="#cb14-1220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1221"><a href="#cb14-1221" aria-hidden="true" tabindex="-1"></a>$1 \times 1$ convolutions have a kernel size of 1x1 and thus no spatial extent. These layers are often used in CNNs to change the number ($C$) of activation maps with few parameters. For example, activation maps of dimensionality ($C \times H \times W$) can be changed to a volume of ($C2 \times H \times W$) using only $C2 * (C + 1)$ parameters. This can be useful, for example, to save parameters before more complex layers or at the end of the CNN to adjust the size of the activation maps to the number of classes to be modeled (for classification problems) or to reduce to 3 color channels ($C2=3$) for image generation models. @fig-cnn-1x1-conv shows an example that uses only 6 weights (excluding bias) to reduce the number of channels to one.</span>
<span id="cb14-1222"><a href="#cb14-1222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1223"><a href="#cb14-1223" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-1x1-conv}</span>
<span id="cb14-1224"><a href="#cb14-1224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1225"><a href="#cb14-1225" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}1x1_conv_example.jpg)</span></span>
<span id="cb14-1226"><a href="#cb14-1226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1227"><a href="#cb14-1227" aria-hidden="true" tabindex="-1"></a>$1 \times 1$ convolution. Source: @prince_understanding_2023</span>
<span id="cb14-1228"><a href="#cb14-1228" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-1229"><a href="#cb14-1229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1230"><a href="#cb14-1230" aria-hidden="true" tabindex="-1"></a><span class="fu">### Global Average Pooling</span></span>
<span id="cb14-1231"><a href="#cb14-1231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1232"><a href="#cb14-1232" aria-hidden="true" tabindex="-1"></a>Global Average Pooling is often an important component. It computes the average of the activations along the depth dimension, reducing activation maps from (C x H x W) to (C x 1 x 1). This is useful for directly modeling logits in a classification problem with C classes, enabling architectures that completely eliminate fully-connected layers.</span>
<span id="cb14-1233"><a href="#cb14-1233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1234"><a href="#cb14-1234" aria-hidden="true" tabindex="-1"></a>::: {#fig-cnn-global-avg-pooling}</span>
<span id="cb14-1235"><a href="#cb14-1235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1236"><a href="#cb14-1236" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}global_average_pooling.jpg)</span></span>
<span id="cb14-1237"><a href="#cb14-1237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1238"><a href="#cb14-1238" aria-hidden="true" tabindex="-1"></a>Global Average pooling, input (left) and output (right).</span>
<span id="cb14-1239"><a href="#cb14-1239" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-1240"><a href="#cb14-1240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1241"><a href="#cb14-1241" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb14-1242"><a href="#cb14-1242" aria-hidden="true" tabindex="-1"></a>This can be achieved using PyTorchs <span class="co">[</span><span class="ot">https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html</span><span class="co">](https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html)</span></span>
<span id="cb14-1243"><a href="#cb14-1243" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-1244"><a href="#cb14-1244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1245"><a href="#cb14-1245" aria-hidden="true" tabindex="-1"></a><span class="fu">### Other Layers</span></span>
<span id="cb14-1246"><a href="#cb14-1246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1247"><a href="#cb14-1247" aria-hidden="true" tabindex="-1"></a>There are many other types of layers, also there are more flavours of convolutional layers.</span>
<span id="cb14-1248"><a href="#cb14-1248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1249"><a href="#cb14-1249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1250"><a href="#cb14-1250" aria-hidden="true" tabindex="-1"></a>One layer often used to reduce the number of parameters are **depthwise separable convolutions**. Instead of extending filters over the entire depth of the input activations, a separate filter (kernel) is used for each input channel, with the dimensionality ($1 \times k \times k$). @fig-cnn-depthwise shows an example. Subsequently, 1x1 convolutions are used to combine information across the input channels. See @fig-cnn-depthwise-separabel for a comparison of 'normal' convolutions and depthwise separable convolutions. Since 1x1 convolutions require fewer parameters, activation maps can be generated with fewer parameters.</span>
<span id="cb14-1251"><a href="#cb14-1251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1252"><a href="#cb14-1252" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-depthwise}</span>
<span id="cb14-1253"><a href="#cb14-1253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1254"><a href="#cb14-1254" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}depthwise.png)</span>{width=600}</span>
<span id="cb14-1255"><a href="#cb14-1255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1256"><a href="#cb14-1256" aria-hidden="true" tabindex="-1"></a>Source: <span class="co">[</span><span class="ot">https://paperswithcode.com/method/depthwise-convolution</span><span class="co">](https://paperswithcode.com/method/depthwise-convolution)</span></span>
<span id="cb14-1257"><a href="#cb14-1257" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-1258"><a href="#cb14-1258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1259"><a href="#cb14-1259" aria-hidden="true" tabindex="-1"></a>:::{#fig-cnn-depthwise-separabel}</span>
<span id="cb14-1260"><a href="#cb14-1260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1261"><a href="#cb14-1261" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}depthwise_separabel.png)</span>{width=600}</span>
<span id="cb14-1262"><a href="#cb14-1262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1263"><a href="#cb14-1263" aria-hidden="true" tabindex="-1"></a>Source: @yu_multi-scale_2016</span>
<span id="cb14-1264"><a href="#cb14-1264" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-1265"><a href="#cb14-1265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1266"><a href="#cb14-1266" aria-hidden="true" tabindex="-1"></a>Additionally, important layers include the diverse familiy of **normalization layers** which can improve convergence of the optimization procedure dramatically.</span>
<span id="cb14-1267"><a href="#cb14-1267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1268"><a href="#cb14-1268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1269"><a href="#cb14-1269" aria-hidden="true" tabindex="-1"></a><span class="fu">## PyTorch Examples</span></span>
<span id="cb14-1270"><a href="#cb14-1270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1273"><a href="#cb14-1273" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-1274"><a href="#cb14-1274" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb14-1275"><a href="#cb14-1275" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-1276"><a href="#cb14-1276" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb14-1277"><a href="#cb14-1277" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb14-1278"><a href="#cb14-1278" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> functional <span class="im">as</span> F</span>
<span id="cb14-1279"><a href="#cb14-1279" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchshow <span class="im">as</span> ts</span>
<span id="cb14-1280"><a href="#cb14-1280" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb14-1281"><a href="#cb14-1281" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb14-1282"><a href="#cb14-1282" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-1283"><a href="#cb14-1283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1286"><a href="#cb14-1286" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-1287"><a href="#cb14-1287" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb14-1288"><a href="#cb14-1288" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-1289"><a href="#cb14-1289" aria-hidden="true" tabindex="-1"></a><span class="co">#img = Image.open({{&lt; meta params.images_path &gt;}}'cat.jpg')</span></span>
<span id="cb14-1290"><a href="#cb14-1290" aria-hidden="true" tabindex="-1"></a>image_path <span class="op">=</span> <span class="st">"../../assets/images/cnns/cat.jpg"</span></span>
<span id="cb14-1291"><a href="#cb14-1291" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> Image.<span class="bu">open</span>(image_path)</span>
<span id="cb14-1292"><a href="#cb14-1292" aria-hidden="true" tabindex="-1"></a>img</span>
<span id="cb14-1293"><a href="#cb14-1293" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-1294"><a href="#cb14-1294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1297"><a href="#cb14-1297" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-1298"><a href="#cb14-1298" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb14-1299"><a href="#cb14-1299" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-1300"><a href="#cb14-1300" aria-hidden="true" tabindex="-1"></a>filter_ <span class="op">=</span> torch.tensor(</span>
<span id="cb14-1301"><a href="#cb14-1301" aria-hidden="true" tabindex="-1"></a>    [   [[<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>]], <span class="co"># R</span></span>
<span id="cb14-1302"><a href="#cb14-1302" aria-hidden="true" tabindex="-1"></a>        [[<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>]], <span class="co"># G</span></span>
<span id="cb14-1303"><a href="#cb14-1303" aria-hidden="true" tabindex="-1"></a>        [[<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>]], <span class="co"># B</span></span>
<span id="cb14-1304"><a href="#cb14-1304" aria-hidden="true" tabindex="-1"></a>    ]).unsqueeze(<span class="dv">0</span>).<span class="bu">float</span>()</span>
<span id="cb14-1305"><a href="#cb14-1305" aria-hidden="true" tabindex="-1"></a>ts.show(filter_, show_axis<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb14-1306"><a href="#cb14-1306" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-1307"><a href="#cb14-1307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1310"><a href="#cb14-1310" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-1311"><a href="#cb14-1311" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb14-1312"><a href="#cb14-1312" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-1313"><a href="#cb14-1313" aria-hidden="true" tabindex="-1"></a><span class="bu">input</span> <span class="op">=</span> torch.tensor(np.array(img)).unsqueeze(<span class="dv">0</span>).permute(<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">2</span>).<span class="bu">float</span>() <span class="co"># (N, C, H, W)</span></span>
<span id="cb14-1314"><a href="#cb14-1314" aria-hidden="true" tabindex="-1"></a><span class="bu">input</span> <span class="op">/=</span> <span class="fl">255.0</span></span>
<span id="cb14-1315"><a href="#cb14-1315" aria-hidden="true" tabindex="-1"></a><span class="bu">input</span> <span class="op">-=</span> <span class="fl">1.0</span></span>
<span id="cb14-1316"><a href="#cb14-1316" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> F.conv2d(<span class="bu">input</span>, filter_, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">0</span>, dilation<span class="op">=</span><span class="dv">1</span>, groups<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-1317"><a href="#cb14-1317" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-1318"><a href="#cb14-1318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1321"><a href="#cb14-1321" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-1322"><a href="#cb14-1322" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb14-1323"><a href="#cb14-1323" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-1324"><a href="#cb14-1324" aria-hidden="true" tabindex="-1"></a>ts.show(result)</span>
<span id="cb14-1325"><a href="#cb14-1325" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-1326"><a href="#cb14-1326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1327"><a href="#cb14-1327" aria-hidden="true" tabindex="-1"></a>2D-Convolution:</span>
<span id="cb14-1328"><a href="#cb14-1328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1331"><a href="#cb14-1331" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-1332"><a href="#cb14-1332" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb14-1333"><a href="#cb14-1333" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-1334"><a href="#cb14-1334" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> F.conv2d(<span class="bu">input</span>, filter_, stride<span class="op">=</span><span class="dv">6</span>, padding<span class="op">=</span><span class="dv">0</span>, dilation<span class="op">=</span><span class="dv">1</span>, groups<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-1335"><a href="#cb14-1335" aria-hidden="true" tabindex="-1"></a>ts.show(result)</span>
<span id="cb14-1336"><a href="#cb14-1336" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-1337"><a href="#cb14-1337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1338"><a href="#cb14-1338" aria-hidden="true" tabindex="-1"></a>Transposed convolution:</span>
<span id="cb14-1339"><a href="#cb14-1339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1342"><a href="#cb14-1342" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-1343"><a href="#cb14-1343" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb14-1344"><a href="#cb14-1344" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-1345"><a href="#cb14-1345" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> F.conv2d(<span class="bu">input</span>, filter_, stride<span class="op">=</span><span class="dv">6</span>, padding<span class="op">=</span><span class="dv">0</span>, dilation<span class="op">=</span><span class="dv">1</span>, groups<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-1346"><a href="#cb14-1346" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> F.conv_transpose2d(result, weight<span class="op">=</span>torch.ones_like(filter_))</span>
<span id="cb14-1347"><a href="#cb14-1347" aria-hidden="true" tabindex="-1"></a>ts.show(result)</span>
<span id="cb14-1348"><a href="#cb14-1348" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-1349"><a href="#cb14-1349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1350"><a href="#cb14-1350" aria-hidden="true" tabindex="-1"></a>Max-Pooling:</span>
<span id="cb14-1351"><a href="#cb14-1351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1354"><a href="#cb14-1354" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-1355"><a href="#cb14-1355" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb14-1356"><a href="#cb14-1356" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-1357"><a href="#cb14-1357" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> F.max_pool2d(<span class="bu">input</span>, kernel_size<span class="op">=</span><span class="dv">8</span>, stride<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb14-1358"><a href="#cb14-1358" aria-hidden="true" tabindex="-1"></a>ts.show(result)</span>
<span id="cb14-1359"><a href="#cb14-1359" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-1360"><a href="#cb14-1360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1361"><a href="#cb14-1361" aria-hidden="true" tabindex="-1"></a><span class="fu">## References</span></span>
<span id="cb14-1362"><a href="#cb14-1362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-1363"><a href="#cb14-1363" aria-hidden="true" tabindex="-1"></a>::: {#refs}</span>
<span id="cb14-1364"><a href="#cb14-1364" aria-hidden="true" tabindex="-1"></a>:::</span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© Copyright 2025, Marco Willi</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2025/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This page is built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>