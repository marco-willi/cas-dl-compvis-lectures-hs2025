---
title: "1 - Introduction"
params:
   images_path: "/assets/images/intro/"
---

::: {.callout-tip title="Learning Objectives"  collapse="true"}
After this lecture you should be able to:

- Distinguish core computer vision tasks (classification, detection, segmentation, generation) and their output structures.
- Describe key real-world application domains and practical challenges (occlusion, illumination, intra-class variation).
- Explain why deep learning reshaped computer vision (data, compute, architectures).
- Identify major historical milestones leading to modern CNNs and foundation models.

:::


::: {.callout-tip title="TLDR Recap" collapse="true"}

**Computer Vision Tasks:**

- **Classification**: Assign images to predefined classes
- **Detection**: Locate and recognize multiple objects with bounding boxes
- **Segmentation**: Assign each pixel to objects (instance) or classes (semantic)
- **Generation**: Transform or create images (translation, synthesis, super-resolution, text-to-image)
- **Keypoint Detection**: Localize specific points (e.g., human joints)

**Key Challenges:**

- Semantic gap (pixels â†’ meaning)
- Viewpoint, illumination, occlusion variations
- Intra-class variation and background clutter
- Context dependence

**Classical vs. Modern Approaches:**

- **Classical CV**: Hand-crafted features (HOG, SIFT, color histograms) + ML classifiers
  - Limited by manual feature design, poor scalability
- **Deep Learning**: End-to-end learned hierarchical representations
  - Automatic feature extraction, better scaling with data

**Historical Milestones:**

- **1959**: Hubel & Wiesel - receptive fields in visual cortex
- **1980**: Neocognitron - hierarchical neural network for vision
- **1986**: Backpropagation - training algorithm for neural networks
- **1998**: LeNet - first successful CNN for digit recognition
- **2012**: AlexNet/ImageNet breakthrough - 9.4% error reduction, marked the deep learning revolution
- **2014-2015**: VGGNet (depth), ResNet (skip connections) - enabled very deep networks
- **2020s**: Transformers and hybrid architectures competing with CNNs

**Why Deep Learning Won:**

- Large-scale datasets (ImageNet: 1.4M images, 1K classes)
- GPU computing power (parallel processing)
- Better architectures (CNNs exploit 2D structure)
- End-to-end optimization (learn features + classifier together)

**Applications:**

- Autonomous driving, medical imaging, facial recognition
- Photo editing, image generation, agricultural monitoring
- Embedded systems (mobile chips for on-device inference)


:::

## Motivation

Deep learning models have played a transformative role in computer vision over the last decade, significantly enhancing and expanding the capabilities to process visual data.

The integration of deep learning in computer vision has massively improved the accuracy and efficiency of visual recognition, classification, and analysis. This has opened up new possibilities in applications such as automated driving, facial recognition, and medical image analysis. These models are now embedded in a wide variety of products and services.


Biologists might need to analyze images from camera traps for species identification (see @fig-intro-kora-lynx).

::: {#fig-intro-kora-lynx width=400}
![Source: @breitenmoser-wursten_projekt_2024]({{< meta params.images_path >}}kora_lynx.png)

Image from a camera trap.
:::

Recent advances in generative deep learning enable anyone to create photorealistic synthetic images (see @fig-intro-synthetic-images). While these technologies have many positive applications (art, design, prototyping), they also raise concerns about misinformation and deepfakes, motivating research into synthetic media detection.


::: {#fig-intro-synthetic-images layout-ncol=2}
![[Source](https://x.com/TheInfiniteDude/status/1637211568692932608)]({{< meta params.images_path >}}sid_trump.jpg)

![[Source](https://x.com/cryptomattk/status/1687908457880367104)]({{< meta params.images_path >}}sid_pope.jpg)

Examples of AI-generated synthetic images that went viral online. These photorealistic images were created by generative models, not cameras.
:::


<!-- ![Camera Trap Image Analysis]({{< meta params.images_path >}}camtrap.jpg){width=400} -->


@fig-intro-google-lens shows an application where one can take a photo of an object of interest and let a model identify it, as well as search additional information about it. Photos can also be used for translation tasks whereby models detect letters (optical character recognition) and then translate it (using language models).

::: {#fig-intro-google-lens layout-ncol=2}

![Identification & Search]({{< meta params.images_path >}}google_lens_classification.png
)

![Translation]({{< meta params.images_path >}}google_lens_ocr.png)
:::

[Google Lens](https://search.google/ways-to-search/lens/)


One area with high commercial interest and competition is the development of self-driving capabilities. @fig-intro-self-driving shows a demonstration of the perception system required for autonomous driving, including detecting vehicles, pedestrians, cyclists, and understanding lane markings and traffic signs in real-time.

::: {#fig-intro-self-driving}

{{< video https://storage.googleapis.com/waymo-uploads/files/site-animations/waymo-driver/cameras.webm width=800 >}}

[Example from Waymo](https://waymo.com/waymo-driver/). Observe how the system simultaneously tracks multiple objects.

:::


Face recognition technology, such as Apple's Face ID (@fig-intro-face-id), has become ubiquitous in consumer devices. The system uses infrared depth sensing to create a 3D map of facial features, making it more secure than 2D image-based recognition and robust to different lighting conditions.

::: {#fig-intro-face-id}

{{< video https://www.youtube.com/embed/z-t1h0Y8vuM?si=qnEOYDmqyv8zGvMV start="50" width=800 height=600 >}}

[Example from Apple Face ID](https://support.apple.com/en-us/102381). Watch how the infrared dot projector creates a depth map of the face.
:::

Computer vision has also been very useful in agricultur. @fig-intro-minneapple shows an application where apples can be counted from image data. Furthermore, drones might be used to estimate crop yield, type and health.


::: {#fig-intro-minneapple width=400}

![Example from @hani_minneapple_2020]({{< meta params.images_path >}}minneapple.png)

:::

Deep learning has also been extensively applied in medical imaging. Many imaging modalities such as MRI, CT, X-ray, and ultrasound can be analyzed with computer vision techniques. @fig-intro-sam illustrates a medical segmentation model which can identify organs and anatomical structures at the pixel level, assisting radiologists in diagnosis and treatment planning.



::: {#fig-intro-sam width=400}

![Example from @ma_segment_2024.]({{< meta params.images_path >}}medsam.png)

:::


Photo-editing has been brought to new levels of sophistication with AI-based capabilities. @fig-intro-magic shows an impressive example from the Google Magic Editor, which can remove, move, and modify objects in photos while seamlessly filling in background details. The model must understand scene context, object boundaries, and realistic textures.


::: {#fig-intro-magic}

{{< video https://storage.googleapis.com/gweb-mobius-cdn/photos/uploads/6e54ed750f84538fd052b31818127f1e4df5711c.compressed.mp4 width=800 >}}

[Example from Google Magic Editor](https://www.google.com/intl/en/photos/editing/)

:::


::: {.callout-note appearance="simple"}

**ðŸ¤” Think About It**

What computer vision tasks do you think the model in @fig-intro-magic must perform to enable this editing capability?

<details>
<summary>Possible tasks involved</summary>

- Object detection and segmentation (identifying what to remove/move)
- Inpainting (filling in removed areas)
- Style transfer (matching lighting/texture)
- Scene understanding (maintaining realistic perspective)

</details>

:::


The increasingly better models and the ability to run them quickly and resource-efficiently on mobile devices have enabled such applications. @fig-intro-tensor-phone shows that special chips have been developed to process images with Deep Learning models quickly.

::: {#fig-intro-tensor-phone}

![]({{< meta params.images_path >}}tensor_phone.png){width=600}


From [Link](https://store.google.com/intl/en/ideas/pixel-camera-features/).
:::



## Computer Vision Tasks


### Image Classification

In image classification, an image is assigned to a predefined set of classes. In multi-class classification, there are $\gt 2$ classes; in binary classification, there are exactly 2 classes; and in multi-label classification, there are multiple sets of classes. @fig-intro-image_clssifier shows an example of multi-class classification.

::: {#fig-intro-image_clssifier width=600}
![]({{< meta params.images_path >}}image_classification_example.png)

Multi-Class Image Classification example: a model predicts the most likely class for an image along with confidence scores.
:::


### Object Detection

Object detection involves locating and recognizing multiple objects in an image. @fig-intro-yolo-example shows an example from the paper by @Redmon2016a. Each object is localized with a bounding box and assigned an object class with a confidence score.

::: {#fig-intro-yolo-example}

![]({{< meta params.images_path >}}yolo_object_detection_example.png){width=600}

Object Detection example (from @Redmon2016a). Bounding boxes localize the objects, showing the predicted class and confidence for each detection.
:::

### Segmentation

In segmentation, individual pixels are assigned to specific objects (instance segmentation) or classes (semantic segmentation). @fig-mask-rcnn-object-segmentation-example shows an example of instance segmentation from the paper by @he_mask_2018, where individual objects are detected and precisely localized at the pixel level.

::: {#fig-mask-rcnn-object-segmentation-example}

![]({{< meta params.images_path >}}mask_rcnn_object_segmentation_example.png){width=600}

Instance Segmentation example (from @he_mask_2018). Each person is individually segmented with pixel-level masks.
:::

::: {#fig-intro-semantic-segmentation}

{{< video https://www.youtube.com/embed/wfObVKKKJkE?si=GZ6m6vig54Z8XVie width=800 height=450 >}}

Semantic segmentation applied to aerial imagery of agricultural fields. Each pixel is classified into crop types, field boundaries, and other land categories.
:::


### Keypoint Detection

In keypoint detection, specific points (keypoints) on objectsâ€”typically human body jointsâ€”are localized. @fig-mask-rcnn-keypoint-detection-example shows an example of keypoint detection from the paper by @he_mask_2018, where individual people are detected and their joints localized. This can be used to transfer movements from a person to an avatar or to recognize human activities (action recognition).

::: {#fig-mask-rcnn-keypoint-detection-example}

![]({{< meta params.images_path >}}mask_rcnn_keypoint_detection_example.png){width=600}


Keypoint Detection example (from @he_mask_2018).
:::


### Image Generation

There are various applications where models transform input images into specific output images (image-to-image translation) or generate completely new images from scratch (unconditional generation). Below are example applications.


#### Image Manipulation

Interactive manipulation allows users to modify images by dragging points to reshape objects while maintaining realistic appearance.

::: {#fig-intro-image-manipulation}
{{< video https://vcai.mpi-inf.mpg.de/projects/DragGAN/data/DragGAN.mp4 width=800 >}}

[Source](https://vcai.mpi-inf.mpg.de/projects/DragGAN/), DragGAN by @pan_drag_2023
:::

#### Image Translation


@fig-intro-pix2pix-example shows an example of image generation from the paper by @image_to_image_isola2018, where images are generated conditioned on image inputs (translated). This can be used, for example, to convert a sketch of an object into a photorealistic depiction of it.

::: {#fig-intro-pix2pix-example width=600}

![]({{< meta params.images_path >}}pix2pix_examples.png)


Image Generation example (from @image_to_image_isola2018).
:::


#### Image Super Resolution

In the gaming industry, Deep Learning is used to upscale low-resolution images to high-resolution in real-time (image super resolution), as shown in @fig-intro-dssl. This enables higher frame rates while maintaining visual quality.

::: {#fig-intro-dssl  width=600}

![]({{< meta params.images_path >}}dssl.png)

Nvidia DLSS (Deep Learning Super Sampling) improves gaming performance. [Source](https://images.nvidia.com/aem-dam/Solutions/geforce/news/control-nvidia-dlss-2-0-update/deliver-us-the-moon-nvidia-dlss-2-0-performance-boost.png)
:::


#### Image Colorization

Image colorization can be learned with Deep Learning. This involves transforming a black-and-white image into an RGB image. @fig-intro-colorization-example shows an example.

::: {#fig-intro-colorization-example width=600}

![]({{< meta params.images_path >}}colorization_example.png)


Norwegian Bride (est late 1890s) from DeOldify: [Link](https://github.com/jantic/DeOldify)
:::


#### View Synthesis

In view synthesis, new camera viewpoints of a scene are generated from a 3D model. Neural Radiance Fields (NeRFs) are models that can generate novel views from a set of input images taken from known viewpoints. @fig-intro-nerf shows training data (input images from different angles) and the synthesized novel views the model can generate.

::: {#fig-intro-nerf width=600}

![]({{< meta params.images_path >}}nerf.png)


Neural Radiance Fields - example (from @mildenhall_nerf_2020).
:::

#### Unconditional Generation

In unconditional image generation, images are generated that resemble the training data distribution without specific input guidance. While you have no direct control over what specific image is generated, you can often manipulate latent variables to modify generated images or interpolate between samples. @fig-intro-stylegan3 shows photorealistic portraits generated by a model trained on human faces.

::: {#fig-intro-stylegan3 width=600}

![]({{< meta params.images_path >}}stylegan3_examples.png)


StyleGan3 (from @karras_alias-free_2021).
:::



#### Text to Image

@fig-stable-diffusion-example shows an example of image generation from the paper by @rombach_high-resolution_2022, where images are generated conditioned on text inputs.

::: {#fig-stable-diffusion-example width=600}

![]({{< meta params.images_path >}}stable_diffusion_example.png){width=600}


Image Generation example (from @rombach_high-resolution_2022.)
:::

On [civitai](https://civitai.com/), there are numerous examples and models to admire or download.



### Further tasks

There are many more computer vision tasks. The following list is not exhaustive:

:::: {.columns}

::: {.column width="50%"}
- Image Classification
- Object Detection (and Tracking)
- Image Segmentation
  - Semantic Segmentation
  - Instance Segmentation
- Optical Character Recognition (OCR)
- Pose Estimation
- Facial Recognition
- Action Recognition

:::

::: {.column width="50%"}
- Image Generation
  - Style Transfer
  - Image Inpainting
  - Super-Resolution
  - Text-to-Image (and more)
- Image Captioning
- 3D Reconstruction
- Image Retrieval
:::


::::

### What about videos?

Many of the image-level tasks can be extended to videos. After all, videos are nothing but a sequence of images.

The most recent impressive examples are from video generative models:


::: {#fig-intro-sora2}

{{< video https://openaiassets.blob.core.windows.net/$web/nf2/blog-final2/20250929_0337_New%20Video_simple_compose_01k6agr8ctey29mze208t29w38%20(1).mp4 width=800 >}}

[OpenAI Sora 2](https://openai.com/index/sora-2/) - Prompt: a man rides a horse which is on another horse
:::



## Challenges

We will now explore some challenges that must be overcome when analyzing images with machine learning models.

### Semantic Gap

The semantic gap refers to the discrepancy between the low-level pixel information that can be directly extracted from an image and the high-level semantic interpretation understood by humans. In simpler terms: an image consists of millions of pixel values, but deriving semantically meaningful concepts like "cat" or "sitting" from these raw numbers is extremely challenging.

::: {#fig-cat-number-grid}

![]({{< meta params.images_path >}}semantic_gap_comparison.png){width=600}

Illustration of the semantic gap: computers see numbers, humans see meaning.
:::

### Viewpoint

The semantic meaning of an image remains constant regardless of camera viewpoint, but the pixel values change dramatically. A model must learn to recognize the same object from different angles.

::: {#fig-intro-challenge-viewpoint}

![]({{< meta params.images_path >}}viewpoint_challenge.png){width=600}

The same cat from different viewpoints looks completely different at the pixel level.
:::

### Deformation

Objects, especially living beings, are often flexible and can appear in vastly different shapes and poses. Models must recognize a cat whether it's curled up sleeping or stretched out running.

::: {#fig-challenge-deformation}

![]({{< meta params.images_path >}}challenge_deformation.png){width=600}

Cats exhibit significant shape variation due to pose changes. [Source](http://cs231n.stanford.edu/)
:::

### Illumination

Changes in lighting conditions dramatically affect pixel values and can obscure object features. The same object under different lighting can produce vastly different images.

::: {#fig-challenge-illumination}

![]({{< meta params.images_path >}}challenge_illumination.png){width=600}

The same scene under different lighting conditions. [Source](http://cs231n.stanford.edu/)
:::

### Background Clutter

Background pixels can visually resemble the object of interest, making precise object localization and segmentation difficult. The model must learn to distinguish foreground from background.

::: {#fig-challenge-background}

![]({{< meta params.images_path >}}challenge_background.png){width=600}

Cat camouflaged against a similarly-textured background. [Source](http://cs231n.stanford.edu/)
:::

### Occlusion

Objects are often partially hidden by other objects in the scene. Models must learn to recognize objects even when only part of them is visible.

::: {#fig-challenge-occlusion}

![]({{< meta params.images_path >}}challenge_occlusion.png){width=600}

Partially occluded cat - only part of the object is visible. [Source](http://cs231n.stanford.edu/)
:::

### Intra-Class Variation

Objects belonging to the same class can exhibit enormous variability in appearance, color, size, and texture. All cats are cats, but they can look very different from each other.

::: {#fig-challenge-intra-class}

![]({{< meta params.images_path >}}challenge_intra_class_variation.jpg){width=600}

High intra-class variation: all are cats but with vastly different appearances. [Source](https://www.maxpixel.net/Cat-Kittens-Free-Float-Kitten-Rush-Cat-Puppy-555822)
:::

### Context Dependence

@fig-tiger-context shows that contextual information can be crucial for correct object classification. The surrounding scene provides important cues about what objects are likely to be present.

::: {#fig-tiger-context}

![]({{< meta params.images_path >}}tiger_context.jpg){width=600}

Is this a tiger? Context clues help determine the answer. [Source](https://www.linkedin.com/posts/ralph-aboujaoude-diaz-40838313_technology-artificialintelligence-computervision-activity-6912446088364875776-h-Iq?utm_source=linkedin_share&utm_medium=member_desktop_web)
:::


::: {#fig-blop-context}

![]({{< meta params.images_path >}}challenges_context_blop.png){width=600}

In the presence of image degradation a visual system (e.g. humans) heavily use context information. Even though the circled blops are identical, we interpret them as either: plate, bottle, cell phone, car, pedestrian or shoe. Source @torralba_using_2010
:::



## Machine Learning Basics

::: {.callout-note title="ðŸ“š Quick ML Refresher"}

This course uses machine learning concepts throughout. If you need a detailed refresher on ML fundamentals, see our [Machine Learning Basics](../background/machine_learning.qmd) background page.

**Quick summary**: We follow a data-driven approach:

1. **Collect** a dataset of images and labels
2. **Train** a model that learns to map images to labels
3. **Evaluate** the model on new data

```{python}
#| eval: false
#| echo: true

def train(images, labels):
 """ Train a Model """
 # Fit Model here
 return model

def predict(test_images, model):
 """ Predict """
 predictions = model(test_images)
 return predictions
```

**Key concepts:**

- **Model**: Function $f_{\theta}(\mathbf{x}) = \hat{y}$ with learnable parameters $\theta$
- **Optimization**: Find $\theta$ that minimizes loss $J(f_{\theta}(\mathbf{X}), \mathbf{y})$
- **Model Selection**: Use train/validation/test splits to avoid overfitting

:::


## Classical vs. Modern Computer Vision

### ML with Classical Computer Vision

Images typically have very high dimensionality. For example, an RGB image with a resolution of $800 \times 600$ has a dimensionality of $800 \times 600 \times 3 = 1,440,000$. Classical machine learning algorithms often struggle with such high dimensionalities:

- They are very slow or require a lot of memory.
- They cannot exploit the 2-D structure of images.
- They are very sensitive to slight changes in images (e.g., rotations).
- They can easily overfit, as the number of features is close to the number of observations (training set).

When modeling images with (classical) machine learning algorithms, methods from (classical) computer vision are often used. With the help of such methods, features can be extracted from images, and the algorithms can learn on these features. This avoids modeling high-dimensional raw data. @fig-intro-color-histograms, @fig-intro-hog-features, and @fig-intro-bow-features show various feature extraction methods.

@fig-intro-color-histograms shows that, for example, the distribution over the color spectrum can be extracted from an image. This could be an important feature.

::: {#fig-intro-color-histograms}

![]({{< meta params.images_path >}}color_histogram.png){width=600}


Color Histograms as Features (Source: @johnson_eecs_2022)
:::

@fig-intro-hog-features shows that techniques like Histogram of Oriented Gradients (HOG) @dalal_histograms_2005 can be used to extract structures from images. Such features were successfully used for pedestrian detection @dalal_histograms_2005.

::: {#fig-intro-hog-features}

![]({{< meta params.images_path >}}hog_features.png){width=600}


HOG as Features (Source: @johnson_eecs_2022)
:::

@fig-intro-bow-features shows another feature variant. Visual patches can be extracted from a dataset, clustered, and then used as descriptors.

::: {#fig-intro-bow-features}

![]({{< meta params.images_path >}}bag_of_words_features.png){width=600}


Bag of (visual) words Features (Source: @johnson_eecs_2022)
:::

Finally, all features can be combined, often more is better, as shown in @fig-intro-concat-features.

::: {#fig-intro-concat-features}

![]({{< meta params.images_path >}}features_concat.png){width=600}


Image Features (Source: @johnson_eecs_2022)
:::

Depending on the parameterization, HOG descriptors, for example, can be very large for images. The resulting feature vector from @fig-intro-concat-features can still be reduced in dimensionality, e.g., with Principal Component Analysis.

@fig-intro-cifar10 shows the CIFAR10 dataset. A well-known dataset for testing models, consisting of 10 classes with 6,000 images each.

::: {#fig-intro-cifar10}

![]({{< meta params.images_path >}}cifar10.jpg){width=600}


CIFAR10 Dataset [Source](https://www.cs.toronto.edu/~kriz/cifar.html)
:::

### Deep Learning

To accelerate research in computer vision and make progress more measurable, the PASCAL VOC Challenges were introduced @everingham_pascal_2007. These involved various tasks, such as detecting objects in photographs (@fig-intro-pascal).

::: {#fig-intro-pascal}

![]({{< meta params.images_path >}}pascal_voc_2007.png){width=600}


Images/illustrations from [Link](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/) and @johnson_eecs_2022. On the left, you see object annotations in images, and on the right, you see the development of Mean Average Precision over the years.
:::

The easy availability of images on the internet has made it possible to collect increasingly larger datasets. [ImageNet](https://www.image-net.org/) is such a very large, hierarchically annotated image dataset @deng_imagenet_2009 with over 1.4 million images, categorized into 1,000 object classes. @fig-intro-imagenet illustrates the dataset.

::: {#fig-intro-imagenet}

![]({{< meta params.images_path >}}imagenet.jpg){width=600}


[ImageNet](https://www.image-net.org/), [Image Source](https://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_full_1k.jpg), details in @deng_imagenet_2009
:::

Since 2010, challenges have been regularly conducted on the ImageNet dataset @russakovsky_imagenet_2015, such as image classification and object detection. @fig-intro-imagenet-perf shows the development of the error rate over time.

::: {#fig-intro-imagenet-perf}

![]({{< meta params.images_path >}}image_net_perf.png){width=600}


Source: @johnson_eecs_2022
:::

#### The ImageNet Breakthrough (2012)

The year 2012 marked a watershed moment in computer vision. Until then, the state-of-the-art in the ImageNet competition relied on classical approaches. In 2011, the winning team @perronnin_large-scale_2010 combined various hand-crafted feature extraction methods (SIFT features) with Support Vector Machines (SVMs), achieving a top-5 error rate of 25.8%.

Then came the breakthrough: Krizhevsky et al. @krizhevsky_imagenet_2012 implemented **AlexNet**, a convolutional neural network (CNN) with multiple layers, and achieved a top-5 error rate of just 16.4%â€”a dramatic 9.4 percentage point improvement. This wasn't just an incremental gain; it fundamentally demonstrated that deep learning could outperform decades of hand-crafted computer vision methods.

::: {#fig-intro-alexnet}

![]({{< meta params.images_path >}}alexnet.png){width=600}

AlexNet architecture @krizhevsky_imagenet_2012: 5 convolutional layers followed by 3 fully connected layers, trained on dual GPUs.
:::

**What made AlexNet work?**

- **Deep architecture**: 8 layers (5 convolutional + 3 fully connected)
- **GPU training**: Used dual GPUs to handle 60 million parameters
- **ReLU activations**: Faster training than traditional sigmoid/tanh
- **Dropout regularization**: Reduced overfitting

This success marked the end of classical computer vision methods in many areas and triggered an explosion of deep learning research.

While classical computer vision trains a machine learning model on features extracted with hand-crafted algorithms @fig-intro-classical-ml, the development is increasingly moving towards end-to-end learning. In this approach, one avoids as much as possible static/hand-designed components and learns everything, including feature extraction, with machine learning @fig-intro-end-to-end.

::: {#fig-intro-classical-ml}

![]({{< meta params.images_path >}}classical_ml.png){width=600}


Illustration from @johnson_eecs_2022
:::

::: {#fig-intro-end-to-end}

![]({{< meta params.images_path >}}end-to-end-ml.png){width=600}


Illustration from @johnson_eecs_2022
:::

Deep learning-based approaches have several key advantages over classical machine learning methods:

- **Automatic feature extraction**: No need for hand-crafted features like SIFT or HOG.
- **Hierarchical representations**: Learn features at multiple levels of abstraction (edges â†’ textures â†’ parts â†’ objects).
- **Better scaling**: Performance improves with more training data and compute.
- **End-to-end learning**: Unified optimization from raw inputs to task outputs.
- **Built-in invariances**: Architectures like CNNs are naturally invariant to certain transformations (e.g., translation).
- **Transfer learning**: Pre-trained models can be adapted to new tasks with limited data.

### Deep Learning History

We will now explore the most important milestones in deep learning for image analysis.

### Hubel and Wiesel

@hubel_receptive_1959 showed in experiments on cats that there are complex cells/neurons in the visual cortex that only respond to certain patterns. In addition to these complex cells, there are also simple cells that only respond to lines at a certain angle.

::: {#fig-huber-wiesel}

![]({{< meta params.images_path >}}huber_wiesel_cat_experiment.jpg){width=600}


Illustration [Source](https://link.springer.com/chapter/10.1007/978-3-030-28954-6_4/figures/1)
:::

### Neocognitron

@fukushima_neocognitron_1980 defined a model of a neural network that can recognize visual patterns. It already has the hierarchical structure of a modern convolutional neural network and is inspired by biological neural networks, particularly from the insights of @hubel_receptive_1959.

::: {#fig-neocognitron}

![]({{< meta params.images_path >}}neocogitron.png){width=600}


The Neocognitron @fukushima_neocognitron_1980.
:::

### Backpropagation

@rumelhart_learning_1986 introduced the backpropagation algorithm in the context of neural networks. This algorithm is used to train modern neural networks: it finds the parameters of an artificial neural network (ANN) to solve a specific task. Backpropagation is based on the chain rule from calculus and is also important for other machine learning models.

::: {#fig-rumalhart-backprop}

![]({{< meta params.images_path >}}rumelhart_backprop.png){width=600}


Backpropagation in neural networks @rumelhart_learning_1986.
:::


::: {.callout-tip}

A good video on backpropagation: [3Blue1Brown Backpropagation Calculus](https://www.youtube.com/watch?v=tIeHLnjs5U8)

:::



### Convolutional Neural Networks (CNNs)

@lecun_gradient-based_1998 implemented convolutional neural networks (CNNs) to recognize handwritten digits. The architecture is specialized for the 2-D structure of image data and trained a model very similar to modern CNNs, as shown in @fig-lecun-cnn.

::: {#fig-lecun-cnn}

![]({{< meta params.images_path >}}lecun_cnn.png){width=600}

LeNet-5 architecture @lecun_gradient-based_1998: the first successful CNN for digit recognition.
:::

After AlexNet's breakthrough in 2012 (see previous section), CNNs rapidly evolved and were applied to increasingly complex tasks. Notable developments include:

- **VGGNet (2014)**: Showed that depth mattersâ€”16-19 layers with small 3Ã—3 filters
- **ResNet (2015)**: Introduced skip connections, enabling networks with 50-152 layers
- **COCO Challenges**: Expanded beyond ImageNet to multi-task benchmarks (detection, segmentation, keypoints)
- **Mobile architectures**: Efficient CNNs for deployment on phones and embedded devices

### Other Architectures in Computer Vision

CNNs are still in use today (2025). Meanwhile, there are alternative architectures, such as transformer-based models (@dosovitskiy_image_2020), which are extremely successful in language modeling, or multilayer perceptron-based architectures (@liu_pay_2021). However, it has been shown that CNNs are still competitive and sometimes superior to alternative architectures (see @woo_convnext_2023). The question of which architecture type will prevail is open. Currently, it seems that CNNs and transformer-based models perform similarly (@smith_convnets_2023).

::: {.callout-tip title="Recap"}
Key takeaways:

- Computer vision tasks differ in output structure (class label, bounding boxes, pixel masks, generated images).
- Deep learning displaced feature-engineering pipelines through end-to-end optimization and data scaling.
- Core challenges (illumination, occlusion, intra-class variation, semantic gap) motivate inductive biases in architectures.
- Historical milestones (Hubel & Wiesel â†’ Neocognitron â†’ Backprop â†’ LeNet â†’ ImageNet/AlexNet) built foundations for modern CNNs.
- Emerging architectures (Transformers, hybrids) broaden modeling choices; no definitive winner yet.
:::

## References

::: {#refs}
:::
