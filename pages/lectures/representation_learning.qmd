---
title: "4 - Representation Learning & Foundation Models"
params:
   images_path: "/assets/images/representation_learning/"
---

::: {.callout-tip title="Learning Objectives" collapse="true"}
By the end of this lecture you should be able to:

- Explain what a representation is and why invariance & selectivity matter.
- Distinguish supervised, self-supervised (contrastive, masked, distillation), and multimodal representation objectives.
- Describe how layer depth changes semantic abstraction in CNNs & ViTs.
- Evaluate representations with linear probes, k-NN, clustering metrics, and retrieval.
- Choose a transfer strategy (feature extraction vs fine-tune vs adapters) for a new dataset.
- Justify when to favor DINOv3 vs CLIP vs supervised ViT for a downstream task.
:::


::: {.callout-tip title="TLDR Recap" collapse="true"}
Short summary:

- Foundation models deliver reusable semantic embeddings that dramatically lower labeled data requirements.
- DINOv3 refines self-distillation to yield robust, high-fidelity patch & global representations.
- Adaptation path: k-NN → Linear Probe → Adapters/LoRA → (optional) Full Fine-Tune.
- Objective choice influences invariances, spatial detail retention, and zero-shot capabilities.
:::

::: {.content-hidden}
$$
{{< include /assets/_macros.tex >}}
$$
:::

## Motivation


Different properties and challenges, such as the [semantic gap](intro.qmd#sec-intro-challenges-semantic-gap), are reasons modelling images is difficult, in particular extracting generic, semantically meaningful information is hard. Humans, however, excel at this task and can easily and  comprehensively describe images with relatively few bits of information. A good example is @fig-raw-elephant:

::: {#fig-raw-elephant layout-ncol=1}
![]({{< meta params.images_path >}}elephant.jpg){fig-alt="Raw RGB image used to illustrate pixel-space representations" width=100%}

Example Image [Source](https://unsplash.com/de/fotos/elefantenwanderung-tagsuber-QJbyG6O0ick).
:::

It is easy for humans to describe the image. For example:

```{python}
# | eval: false
# | echo: true
"""
An photograph of an elephant facing the camera.
The elephant has large tusks and ears and is standing in a flat savannah.
Is is bright day light with a cloudy sky.
"""
```

::: {.callout-note title="Information Content: Raw vs Natural Language" collapse="true"}

Let's quantify the dramatic compression achieved by semantic description:

**Raw Image (Pixel Representation):**

- Resolution: 640 × 360 pixels
- Channels: 3 (RGB)
- Bits per pixel: 8 bits per channel

```{python}
# | eval: true
# | echo: false
import pandas as pd
from IPython.display import HTML

# Natural language description
description = """
An photograph of an elephant facing the camera.
The elephant has large tusks and ears and is standing in a flat savannah.
Is is bright day light with a cloudy sky.
"""

# Calculate bits for natural language
char_count = len(description)
bits_utf8 = char_count * 8  # UTF-8 uses 8 bits per character
bits_ascii = char_count * 7  # ASCII uses 7 bits per character

# Raw image stats (from earlier calculation)
width, height, channels = 640, 360, 3
raw_bits = width * height * channels * 8

# Compression ratio
compression_ratio = raw_bits / bits_utf8

# Create comparison table
comparison_data = pd.DataFrame(
    {
        "Representation": [
            "Raw RGB Image",
            "Natural Language (UTF-8)",
            "Natural Language (ASCII)",
        ],
        "Size (bits)": [f"{raw_bits:,}", f"{bits_utf8:,}", f"{bits_ascii:,}"],
        "Size (KB)": [
            f"{raw_bits / 8 / 1024:.1f}",
            f"{bits_utf8 / 8 / 1024:.2f}",
            f"{bits_ascii / 8 / 1024:.2f}",
        ],
        "Compression Factor": [
            "1×",
            f"{compression_ratio:.0f}×",
            f"{raw_bits / bits_ascii:.0f}×",
        ],
    }
)

table_html = comparison_data.to_html(
    index=False,
    border=0,
    classes="table table-sm table-striped align-middle caption-top",
)

HTML(table_html)
```
:::


We can also say: By describing the image we **represent** the image with natural language, instead of the raw pixels.

This is exactly what **Representation Learning** is interested in: How can we find compact representations wich are significantly smaller than the raw, original representations while retaining _relevant_ information?

The idea is that such compact representations are well suited for down-strean tasks, such as image classification.

## What Is Representation Learning?

A representation is the output $\vect{z} = f(\vect{x})$ produced by an encoder $f$ applied to input $\vect{x}$; $\vect{z}$ retains task‑relevant structure while discarding nuisance variation. The encoder $f$ is the mapping; the representation $\vect{z}$ is its result.

Therefore, when we refer to representations we typically mean vector-valued (distributed) representations and not, for example, natural language descriptions.

### Intuition

Let's take a look at the (synthetic) images in @fig-rl-examples:

::: {#fig-rl-examples layout-nrow=3}

![Elephant - Day 1]({{< meta params.images_path >}}examples/elephant_day_01.png){fig-alt="Elephant in daylight savanna" width=45%}
![Elephant - Day 2]({{< meta params.images_path >}}examples/elephant_day_02.png){fig-alt="Elephant by watering hole in daylight" width=45%}

![Giraffe - Day]({{< meta params.images_path >}}examples/giraffe_day_01.png){fig-alt="Giraffe in daylight grassland" width=45%}
![Rhino - Night]({{< meta params.images_path >}}examples/rhino_night_01.png){fig-alt="Rhinoceros at nighttime" width=45%}

![Elephant - Night]({{< meta params.images_path >}}examples/elephant_night_01.png){fig-alt="Elephant at night with moonlight" width=45%}
![Car - Day]({{< meta params.images_path >}}examples/car_day_01.png){fig-alt="Modern car in bright daylight" width=45%}

Synthetic images illustrating representation learning challenges.
:::

Imagine you have to represent those in 2-D: $\vect{z} \in \mathbb{R}^2$. How would you proceed from a human point of view?

::: {.callout-note title="Representation Intuition"}

How would you represent the images of @fig-rl-examples in 2 dimensions? Why?

**Hint**:
Similar images might be close to each other.


<details>
<summary>Click for result</summary>

**Key Considerations:**

The images demonstrate different types of similarity challenges in representation learning:

1. **Same class, same conditions** (Elephant Day 1 & Elephant Day 2): These are most similar - both elephants photographed in daylight. They exhibit **intra-class variation** while maintaining the same semantic category.

2. **Same class, different conditions** (Elephant Day vs. Elephant Night): Moderate similarity - same animal class but dramatically different lighting conditions. This tests **illumination invariance**.

3. **Different classes, same conditions** (Elephants, Giraffe, Rhino by day): Animals share similar contexts (daytime, natural habitat) but different semantics, requiring **inter-class discrimination**.

4. **Different domain** (Animals vs. Car): Least similar - represents a complete **domain shift** from wildlife to manufactured objects.

**Ideal representation learning:** A good model should group the two daytime elephants closely, maintain reasonable similarity between day/night elephants, distinguish between different animal classes, and clearly separate animals from vehicles.

</details>

:::


### What are good representations?

Good representations transform raw data into a format that makes relevant patterns explicit and accessible. Think of them as **semantic coordinate systems** where distances reflect meaningful relationships.

::: {.callout-note title="Core Properties of Effective Representations"}

**Mathematical Framework:**

Given input $\vect{x} \in \mathcal{X}$ and encoder $f: \mathcal{X} \to \mathcal{Z}$, we want representation $\vect{z} = f(\vect{x})$ such that:

**1. Invariance & Selectivity Balance**

- **Invariance:** $\|f(\vect{x}) - f(g(\vect{x}))\|_2 < \epsilon$ for nuisance transform $g$ (lighting, pose)
- **Selectivity:** $\|f(\vect{x}_i) - f(\vect{x}_j)\|_2 > \delta$ for semantically different $\vect{x}_i, \vect{x}_j$
- *Trade-off:* Too much invariance → loss of discriminative details
- *Sweet spot:* Preserve task-relevant variations, discard irrelevant ones

**2. Geometric Structure Preservation**

- **Smooth manifold:** Similar semantic concepts cluster in representation space
- **Composability:** $f(\vect{x}_1) \oplus f(\vect{x}_2) \approx f(\text{combine}(\vect{x}_1, \vect{x}_2))$ for vector operations
- **Interpolability:** Linear interpolation $\alpha f(\vect{x}_1) + (1-\alpha) f(\vect{x}_2)$ yields meaningful intermediate concepts

**3. Transfer Efficiency**

- **Low sample complexity:** Few examples needed for downstream adaptation
- **Broad applicability:** Same $f(\cdot)$ works across multiple tasks/domains
- **Graceful degradation:** Performance degrades slowly with domain shift

**4. Computational Practicality**

- **Compact dimensionality:** $\text{dim}(\vect{z}) \ll \text{dim}(\vect{x})$ while preserving information
- **Fast computation:** Forward pass $f(\vect{x})$ efficient for real-time applications
- **Stable training:** Representation learning converges reliably

:::

::: {#fig-representation-quality-spectrum}
![]({{< meta params.images_path >}}representation_quality_spectrum.png){fig-alt="Spectrum from poor to excellent representations" width=100%}

**Representation Quality Spectrum:** From pixel-level chaos (left) through hand-crafted features (SIFT, HOG) to modern learned embeddings (right), showing increasing semantic organization and task performance.
:::


## Representation Learning and Similarities



Such a representation is much more compact, yet retains the most relevant information.

This begs the question:



::: {#fig-raw-elephant layout-ncol=1}
![]({{< meta params.images_path >}}elephant.jpg){fig-alt="Raw RGB image used to illustrate pixel-space representations" width=70%}

Example Image [Source](https://unsplash.com/de/fotos/elefantenwanderung-tagsuber-QJbyG6O0ick).
:::

A raw image is a 3D tensor of pixel intensities. Before any encoder is applied, it already contains structure (spatial locality, color correlations) but no task-specific abstractions. Inspecting its dimensionality highlights why learning compact representations matters.


:::: {.columns}

::: {.column width="50%"}

```{python}
#| eval: true
#| echo: false
#| label: tbl-rl-example-raw-dim
#| tbl-cap: "Raw elephant image dimensionality summary"
#| tbl-subtitle: "Single 224×224 crop used as running example"
#| tbl-colwidths: [30,30]
from pathlib import Path
from PIL import Image
import numpy as np
import pandas as pd
from IPython.display import HTML

img_path = Path("/workspace/assets/images/representation_learning/elephant.jpg")
with Image.open(img_path) as img:
    width, height = img.size
    channels = len(img.getbands())
    pixels = width * height
    values = pixels * channels
    arr_dtype = np.array(img).dtype

raw_stats = pd.DataFrame(
    {
        "Statistic": ["Width (px)", "Height (px)", "Channels", "Total Pixels", "Total Values", "Array dtype"],
        "Value": [width, height, channels, f"{pixels:,}", f"{values:,}", str(arr_dtype)],
    }
)

table_html = raw_stats.to_html(
    index=False,
    border=0,
    classes="table table-sm table-striped align-middle caption-top"
)

HTML(table_html)
```

:::

::::

Modalities differ vastly in raw state-space size:

```{python}
#| label: tbl-repr-modalities
#| echo: false
#| fig-cap: "Representative raw modality characteristics"
#| tbl-colwidths: [0.12, 0.22, 0.30, 0.36]
#| tbl-subtitle: "Interactive view with quick export"
#| layout-nrow: 1
import pandas as pd
from IPython.display import Markdown

data = [
    ("Images", "224×224 RGB", "≈150K pixels", "Local correlations, spatial invariance"),
    ("Text", "Tokens (512 seq)", "Variable (subword vocab ≈50K)", "Long-range dependencies"),
    ("Genomics", "Nucleotides (10K)", "4-letter alphabet", "Long-range, hierarchical motifs"),
    ("Graphs", "Nodes / Edges", "Irregular structure", "Permutation invariance"),
    ("Multimodal", "Image + Text", "Combined modalities", "Alignment & grounding"),
]

df_modalities = pd.DataFrame(data, columns=[
    "Modality",
    "Example Unit",
    "Typical Raw Dimensionality (Sample)",
    "Challenges",
])

try:
    from itables import show, options

    options.classes = "display nowrap compact stripe hover order-column"
    options.dom = "Bfrtip"
    options.buttons = ["copy", "csv", "excel"]
    options.searching = False
    options.lengthMenu = False
    show(df_modalities, index=False)
except Exception:
    Markdown(df_modalities.to_markdown(index=False))
```


### Pixel-Level vs Learned Similarities

Before diving into learning objectives, let's examine how different similarity measures organize the same images:

::: {#fig-similarity-comparison}
![]({{< meta params.images_path >}}rl_example_comprehensive_pixel_similarity.png){fig-alt="Comparison of pixel-level similarity measures" width=100%}

**Left:** Euclidean pixel distance creates sharp distinctions. **Middle:** Cosine pixel similarity shows high values across all images. **Right:** Difference map reveals how the metrics diverge.
:::

**Key Observation:** Both pixel-level measures fail to group semantically similar images (the two elephants), regardless of distance metric choice.

::: {#fig-mds-representations}
![]({{< meta params.images_path >}}rl_example_mds_comparison_complete.png){fig-alt="MDS projections comparing pixel and CNN representations" width=100%}

2D MDS projections reveal fundamental differences: **Left:** Scattered pixel-Euclidean layout (high stress). **Middle:** Tight pixel-cosine clustering but no semantic structure. **Right:** CNN embeddings create meaningful semantic neighborhoods with clear class separation.
:::

This motivates why we need **learned representations** that capture semantic similarity rather than superficial pixel patterns.

### The Invariance-Equivariance Spectrum

Rather than binary categories, think of a **continuum** of spatial sensitivity:

::: {#fig-invariance-equivariance-spectrum}
```{mermaid}
graph LR
    A["Full Invariance<br/>f(Tx) = f(x)<br/>📊 Classification"] --> B["Partial Invariance<br/>Small changes OK<br/>🔍 Retrieval"]
    B --> C["Local Equivariance<br/>f(Tx) ≈ T'f(x)<br/>🎯 Detection"]
    C --> D["Strict Equivariance<br/>f(Tx) = T'f(x)<br/>🎨 Segmentation"]

    style A fill:#FFE4E1
    style B fill:#F0F8FF
    style C fill:#F0FFF0
    style D fill:#FFF8DC
```

**Task-Representation Alignment:** Different tasks require different levels of spatial sensitivity. The key is matching representation properties to downstream requirements.
:::

::: {.callout-tip title="Interactive Demonstration" collapse="false"}
**Hands-on Exploration:** Create `demos/representation_quality_demo.ipynb` to:

1. **Invariance Visualization:** Apply increasing augmentation strength, plot embedding distances
2. **Semantic Interpolation:** Show smooth transitions between different object classes
3. **Transfer Efficiency:** Compare few-shot performance across different representation qualities
4. **Dimensionality Analysis:** Plot task performance vs representation dimension

**Key Questions to Explore:**
- At what augmentation strength do embeddings become invariant?
- Can you find semantic directions in the representation space?
- Which representations transfer best to new domains?
:::


::: {#fig-rl-augmentations}
![]({{< meta params.images_path >}}rl_example_augmentations.png){fig-alt="Semantic-preserving augmentations applied to an elephant image" width=100%}

Collage of semantic-preserving augmentations (crop, jitter, blur, noise) used to motivate invariance: each view retains the elephant identity despite appearance changes.
:::

::: {.callout-tip title="Try It Yourself!" collapse="false"}
**Interactive Demo:** Run the `demos/augmentation_collage.ipynb` notebook to:
- Generate your own augmentation collage
- Compare pixel-level vs CNN embedding similarities
- Visualize representation spaces with MDS projections

**Key Insight:** Notice how pixel-level similarity fails to capture semantic relationships, while learned CNN embeddings cluster augmented variants together and separate different object classes.
:::


**Why learn representations (vs hand-crafting)?**

The evolution from manual to learned features reflects three key limitations of hand-crafted approaches:

| Era | Approach | Examples | Limitations | Performance Ceiling |
|-----|----------|----------|-------------|-------------------|
| 1980s-2000s | Hand-crafted Features | SIFT, HOG, SURF, LBP | Domain-specific, brittle to variations | ~70% ImageNet |
| 2000s-2010s | Learned Shallow | Bag of Words, Fisher Vectors | Limited representational capacity | ~80% ImageNet |
| 2010s-Present | Deep Learning | CNNs, ViTs, Foundation Models | Requires large data, compute | >95% ImageNet |

: Evolution of visual representations {.striped .hover #tbl-repr-evolution}

**The Scale-Generalization Trade-off:** Manual engineering doesn't scale to the diversity of real-world data and tasks. Learned representations automatically discover relevant patterns from data, enabling generalization to unseen scenarios.

::: {.callout-warning title="Representation Pitfalls"}
**Common failure modes to avoid:**

1. **Over-invariance:** Aggressive augmentation removes task-critical information (e.g., color for flower classification)
2. **Under-selectivity:** Representations collapse, making different classes indistinguishable
3. **Brittleness:** High performance on training domain, catastrophic failure on test shifts
4. **Interpretability loss:** Complex embeddings become black boxes, hindering debugging
5. **Computational bloat:** Representations grow unnecessarily large without performance gains

**Mitigation strategies:** Balanced augmentation policies, regularization techniques, domain adaptation methods, and systematic evaluation protocols.
:::

### Advanced Representation Properties

Beyond basic invariance, modern representations exhibit sophisticated geometric and semantic structures:

::: {.callout-note title="Emergent Properties in Foundation Models"}
**Geometric Structure:**
- **Semantic directions:** Linear subspaces correspond to interpretable concepts (e.g., "style" vectors in image generation)
- **Compositional algebra:** $f(\text{king}) - f(\text{man}) + f(\text{woman}) \approx f(\text{queen})$ in language models
- **Hierarchical clustering:** Representations naturally organize into taxonomic structures

**Transfer Mechanisms:**
- **Feature reuse:** Early layers transfer across domains (edges → textures → parts)
- **Task-specific adaptation:** Late layers specialize while preserving general features
- **Few-shot emergence:** Quality representations enable learning from minimal examples

**Robustness Properties:**
- **Adversarial stability:** Embeddings remain consistent under small perturbations
- **Distributional shift tolerance:** Performance degrades gracefully with domain changes
- **Continual learning capability:** New tasks learned without catastrophic forgetting
:::

### Invariance vs Equivariance

- *Invariance:* $f(Tx) = f(x)$ (classification robustness).
- *Equivariance:* $f(Tx) = T' f(x)$ (segmentation, detection require spatial structure).
Convolutions approximate translation equivariance; self-attention is *structurally agnostic* and must learn inductive biases via data.

### Hierarchical Abstraction

Lower layers: edges, colors, local patterns → mid-layers: motifs/parts → deeper layers: object & global scene semantics.

::: {.panel-tabset #fig-zeiler-hierarchy}
### Layer 1
![]({{< meta params.images_path >}}zeiler_layer1.png){fig-alt="Layer 1: Gabor-like edges and color blobs" width=95%}

Edges, color opponency, and Gabor-like filters emerge first.

### Layer 2
![]({{< meta params.images_path >}}zeiler_layer2.png){fig-alt="Layer 2: Corners and simple textures" width=95%}

Edges combine into corners, contours, and early texture motifs.

### Layer 3
![]({{< meta params.images_path >}}zeiler_layer3.png){fig-alt="Layer 3: Texture motifs and repeated patterns" width=95%}

Mid-level texture repetitions (fur, mesh) and shape fragments appear.

### Layer 4
![]({{< meta params.images_path >}}zeiler_layer4.png){fig-alt="Layer 4: Object parts such as faces and wheels" width=95%}

Semantically meaningful object parts (faces, limbs, wheels) are isolated.

### Layer 5
![]({{< meta params.images_path >}}zeiler_layer5.png){fig-alt="Layer 5: Full object templates" width=95%}

Full object templates activate, providing category-level abstraction.
:::

Activation maximization reconstructions from Zeiler & Fergus (2014) illustrate how deeper CNN layers capture increasingly abstract concepts.

* Layer 1 focuses on oriented edges and color opponency — the digital analogue of Gabor filters.
* Layer 2 composes edges into corners, gradients, and early texture motifs.
* Layer 3 aggregates motifs into mid-level textures (fur, mesh) and shape fragments.
* Layer 4 responds to semantically meaningful parts (faces, limbs, wheels) aggregated across spatial context.
* Layer 5 fires on entire object templates, providing category-level abstraction that feeds classifier heads.

## Deep Image Representations

::: {#fig-representation-trajectory}
![]({{< meta params.images_path >}}representation_trajectory.png){fig-alt="Evolution from raw pixels to semantic representations" width=100%}

Representation evolution: **Raw RGB** → **Edge detection** (early CNN features) → **ResNet Grad-CAM** (mid-level semantic focus) → **ViT Attention** (global semantic relationships). Each stage reveals different aspects of visual understanding.
:::

::: {.callout-tip title="Try It Yourself!" collapse="false"}
**Interactive Demo:** Run the `demos/rl_examples.ipynb` notebook to:
- Generate your own representation trajectory visualization
- Compare Grad-CAM vs Vision Transformer attention patterns
- See how different architectures focus on different image regions

**Key Insight:** CNNs build local-to-global hierarchies, while ViTs can attend globally from the start, creating different semantic emphasis patterns.
:::

CNNs encode locality and translation inductive biases; Vision Transformers (ViTs) rely on larger data + patch embedding + positional encodings.

| Aspect | CNN | ViT |
|--------|-----|-----|
| Inductive Bias | Strong (locality, weight sharing) | Minimal (global token mixing) |
| Data Efficiency | Higher for small data | Needs large-scale pretraining |
| Receptive Field Growth | Layer stacking / dilation | Immediate global attention |
| Positional Handling | Implicit via convolution structure | Explicit positional embeddings |
| Adaptation (small data) | Often fine-tune later layers | Prefer frozen features + probe / adapters |

: CNN vs ViT representation properties {.striped .hover #tbl-repr-cnn-vit}

**Story:** Inductive bias trades off with flexibility. ViTs + massive self-supervised training (e.g., DINOv2/v3) *recover* desirable invariances without manual architectural constraints.

## 3. Transfer Learning

::: {#fig-transfer-strategy-flow}
```{mermaid}
flowchart TD
    A["New Dataset<br/>Size Assessment"] --> B{"< 1K samples?"}
    B -->|Yes| C["Linear Probe<br/>(Frozen Features)"]
    B -->|No| D{"< 10K samples?"}
    D -->|Yes| E["LoRA/Adapters<br/>(1-10% params)"]
    D -->|No| F{"Domain Shift?"}
    F -->|High| G["Partial Fine-tune<br/>(Top layers)"]
    F -->|Low| H["Full Fine-tune<br/>(All parameters)"]

    C --> I["Evaluate Gap"]
    I --> J{"Gap > 10%?"}
    J -->|Yes| E
    J -->|No| K["✓ Ship Linear Probe"]

    E --> L["Monitor Forgetting"]
    L --> M{"Performance Drop?"}
    M -->|Yes| N["Reduce LR/<br/>Add Regularization"]
    M -->|No| O["✓ Ship Adapter"]

    style K fill:#90EE90
    style O fill:#90EE90
    style G fill:#FFE4B5
    style H fill:#FFB6C1
```

Transfer learning decision flowchart: Start simple and escalate complexity only when performance gaps justify additional computational cost.
:::

Reusing a source representation $f_{\theta}$ trained on large dataset $D_s$ for target dataset $D_t$ (possibly small / domain shifted):

| Strategy | Frozen? | Trainable Params | When to Use | Risk |
|----------|---------|------------------|-------------|------|
| Feature Extraction + Linear Probe | Yes | ~Head only | Very small target | Underfitting complex tasks |
| Partial Fine-Tune (top blocks) | Partially | 10–40% | Medium data | Forgetting lower-level invariances |
| Full Fine-Tune | No | 100% | Large labeled target | Overfitting / compute |
| LoRA / Adapters | Base frozen | 1–10% | Medium data / resource-bound | Adapter configuration tuning |
| Prompt / Text Prompt (CLIP) | Base frozen | 0–Few tokens | Zero/Few-shot | Prompt sensitivity |

: Transfer strategy comparison {.striped .hover #tbl-repr-transfer}

::: {.callout-tip title="Heuristic"}
Start simple: Linear probe → If performance gap > X% vs baseline, move to LoRA/adapters → escalate to partial fine-tune → only full fine-tune when data & compute justify.
:::

### 3.1 Diagnostic Signals

- Linear probe saturates early + large gap to supervised SOTA ⇒ consider partial fine-tune.
- Training loss ↓ but validation plateaus ⇒ representation mismatch (try LoRA).
- Large domain color/style shift ⇒ consider small color-specific adapter or augment recipe adjustment.

<!-- TODO: Visual: Flowchart of decision path for adaptation strategy. -->

## 4. Self-Supervised Learning (SSL)

::: {#fig-ssl-paradigms}
```{mermaid}
graph TB
    subgraph "Contrastive Learning"
        A1["Image"] --> A2["Augment"]
        A2 --> A3["Encode"]
        A3 --> A4["Positive: Same image<br/>Negative: Different images"]
        A4 --> A5["InfoNCE Loss"]
    end

    subgraph "Self-Distillation (DINO)"
        B1["Image"] --> B2["Multi-crop<br/>Augmentation"]
        B2 --> B3["Student<br/>Encoder"]
        B2 --> B4["Teacher<br/>Encoder (EMA)"]
        B3 --> B5["Cross-entropy<br/>Distillation"]
        B4 --> B5
    end

    subgraph "Masked Modeling (MAE)"
        C1["Image"] --> C2["Random<br/>Masking (75%)"]
        C2 --> C3["Encoder<br/>(visible patches)"]
        C3 --> C4["Decoder<br/>Reconstruction"]
        C4 --> C5["Pixel/Token<br/>Loss"]
    end

    subgraph "Multimodal (CLIP)"
        D1["Image"] --> D3["Image<br/>Encoder"]
        D2["Text"] --> D4["Text<br/>Encoder"]
        D3 --> D5["Contrastive<br/>Alignment"]
        D4 --> D5
    end

    style A5 fill:#FFE4E1
    style B5 fill:#E0E6FF
    style C5 fill:#E6FFE6
    style D5 fill:#FFF8DC
```

Four major self-supervised learning paradigms with their core mechanisms and loss functions.
:::

Goal: Learn $f(x)$ without manual labels; leverage structure in data via pretext tasks.

| Paradigm | Core Idea | Positive Pair Definition | Example Models | Notable Strength | Typical Weakness |
|----------|-----------|--------------------------|----------------|------------------|------------------|
| Contrastive | Pull positives, push negatives | Augmented views of same image | SimCLR, MoCo | Strong invariance | Needs large batch / memory bank |
| Self-Distillation | Match teacher & student views | Teacher EMA vs student features | DINO v1/v2/v3 | Rich semantics w/o negatives | Augmentation sensitivity |
| Masked Modeling | Reconstruct masked content | Original vs masked patches | MAE, iBOT | Structural detail retention | Slower convergence |
| Multi-Modal Align | Align cross-modal embeddings | Paired image-text | CLIP, BLIP-2 | Zero / few-shot via text | Text bias & leakage |
| Predictive Coding | Predict future / context | Temporal or spatial neighbors | CPC variants | Sequential structure | Complexity of objectives |

: Major self-supervised paradigms {.striped .hover #tbl-repr-ssl}

### 4.1 Contrastive (InfoNCE)
Objective: maximize similarity of positive pair while minimizing others:
$$\mathcal{L}_{\text{InfoNCE}} = - \log \frac{\exp(\text{sim}(z_i, z_i^+)/\tau)}{\sum_{j=1}^N \exp(\text{sim}(z_i, z_j)/\tau)}$$
Temperature $\tau$ tunes concentration; similarity often cosine.

::: {.callout-note title="Quiz" collapse="true"}
**Question:** What happens if $\tau$ is extremely small?
**Answer (click to reveal):** <details><summary>Answer</summary>The loss focuses on the single hardest negative; training can become unstable.</details>
:::

### 4.2 Self-Distillation (DINO Family)
Teacher network (EMA of student) provides soft targets for multi-crop student views. Avoids explicit negatives; emergent clustering.

### 4.3 Masked Image Modeling (MAE / iBOT)
Randomly mask patches (40–75%), reconstruct pixel space or latent tokens; forces holistic context modeling.

### 4.4 Multimodal Alignment (CLIP)
Image encoder + text encoder; contrastive loss aligns matching pairs enabling text-driven zero-shot transfer.

<!-- TODO: Visual: Side-by-side small schematics of each SSL paradigm (contrastive, distillation, masked, multimodal). -->

## 5. Evaluating Representations

::: {.callout-note title="Interactive Evaluation Exercise" collapse="false"}
**Hands-on Challenge:** Load a pretrained model (ResNet-50, DINOv3) and evaluate it on a small dataset using multiple probes:

1. **k-NN baseline** (k=5): No training required
2. **Linear probe**: Train only a classifier head
3. **Clustering quality**: Measure NMI/ARI on embeddings
4. **Retrieval performance**: Query-gallery similarity ranking

**Discussion Points:**
- Which probe reveals representation quality best?
- When do you see large gaps between k-NN and linear probe?
- How does feature dimensionality affect each metric?

Try this with both ImageNet-supervised and self-supervised models to compare representation quality.
:::

No single metric suffices—use *battery* of probes.

| Method | Procedure | Signals | Pros | Cons |
|--------|-----------|---------|------|------|
| k-NN | Compare feature similarity | Local neighborhood structure | No training | Slow for large N |
| Linear Probe | Train logistic/regression on frozen features | Linear separability | Fast, comparable | Ignores non-linear capacity |
| Clustering (k-means) | Cluster embeddings, measure NMI/ARI | Unsupervised grouping quality | Label-free diagnostic | Sensitive to feature scaling |
| Retrieval mAP | Query vs gallery similarity ranking | Ranking robustness | Task-agnostic | Needs curated gallery |
| Few-shot Episodes | N-way K-shot average accuracy | Data efficiency | Mirrors target tasks | Variance with small K |

: Representation evaluation toolbox {.striped .hover #tbl-repr-eval}

::: {.callout-note title="Exercise Idea"}
Compute k-NN (k=5) accuracy, then a linear probe. Report relative improvement. Reflect: does larger gap imply under-expressive head or insufficient representation invariance?
:::

### 5.1 Failure Diagnostics

- High linear probe, poor retrieval ⇒ representation collapses fine-grained similarity.
- Good k-NN, poor clustering NMI ⇒ global structure fragmented; consider whitening or normalization.
- Poor few-shot but good full-data accuracy ⇒ representation needs adaptation (try LoRA).

## 6. Foundation Models (Bridge)

Full deep-dive in `lectures/foundation_models.qmd`. Here we contextualize *representations* origin:

| Model | Objective Category | Emergent Property | Transfer Sweet Spot |
|-------|--------------------|-------------------|---------------------|
| DINOv3 | Self-distillation | Semantic clusters & patch semantics | Few-shot classification, anomaly |
| CLIP | Contrastive multimodal | Language-aligned embeddings | Zero-shot, prompt-based tasks |
| MAE | Masked modeling | Fine structural detail | Dense prediction fine-tune |
| SAM Encoder | Promptable segmentation | High-resolution region semantics | Zero-shot segmentation |

: Selected foundation models & representation characteristics {.striped .hover #tbl-repr-foundation-overview}

### 6.1 Choosing Among DINOv3 vs CLIP

| Scenario | Recommended | Rationale |
|----------|------------|-----------|
| Few labeled images, no text prompts | DINOv3 | Strong linear probe performance |
| Need zero-shot open vocabulary | CLIP | Text prompts act as classifier |
| Downstream segmentation with prompts | SAM Encoder | Mask generalization |
| Style-rich generation/editing | Stable Diffusion (latent) | Generative prior |

: Scenario-based model selection {.striped .hover #tbl-repr-scenarios}

## 7. A Practical Recipe

::: {#fig-practical-recipe}
```{mermaid}
flowchart TD
    A["1. Select Backbone<br/>(DINOv3 ViT-B/L)"] --> B["2. Extract Embeddings<br/>(Normalize + Optional PCA)"]
    B --> C["3. k-NN Baseline<br/>(k=5, cosine distance)"]
    C --> D["4. Linear Probe<br/>(Early stopping)"]
    D --> E["5. Evaluate Clustering<br/>(NMI/ARI metrics)"]
    E --> F{"Gap > Target?"}
    F -->|No| G["✓ Deploy Linear Probe"]
    F -->|Yes| H["6. Add LoRA Adapters<br/>(r=8, attn matrices)"]
    H --> I["7. Monitor Forgetting<br/>(Cosine sim drift)"]
    I --> J["8. Document Cost<br/>(GPU hrs vs accuracy)"]
    J --> K["✓ Deploy Adapted Model"]

    style G fill:#90EE90
    style K fill:#90EE90

    classDef evaluation fill:#FFF8DC
    class C,D,E evaluation

    classDef adaptation fill:#E6FFE6
    class H,I adaptation
```

Systematic adaptation workflow: Start simple, escalate only when justified, and always monitor for catastrophic forgetting.
:::

::: {.callout-tip title="Implementation Checklist"}
**Before you start:**
- [ ] Baseline established (random features, ImageNet supervised)
- [ ] Validation split held out for hyperparameter tuning
- [ ] Compute budget allocated (linear probe: minutes, LoRA: hours, full fine-tune: days)

**During adaptation:**
- [ ] Learning curves monitored (train/val gap)
- [ ] Feature drift measured (cosine similarity pre/post)
- [ ] Multiple runs for statistical confidence

**After deployment:**
- [ ] Performance documented with confidence intervals
- [ ] Failure cases analyzed (where does the model struggle?)
- [ ] Computational cost vs accuracy trade-off quantified
:::

The systematic approach reduces trial-and-error and prevents over-engineering solutions.

### 7.1 Minimal PyTorch Sketch (Probe)

```python
# pseudo-code: linear probe on frozen backbone features
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Assume we precomputed (X_train, y_train), (X_val, y_val) via DINOv3 extractor
clf = LogisticRegression(max_iter=1000, n_jobs=-1)
clf.fit(X_train, y_train)
val_acc = accuracy_score(y_val, clf.predict(X_val))
print(f"Linear probe val accuracy: {val_acc:.3f}")
```

## 8. Interactive Learning & Reflection

::: {.callout-note title="Hands-On Exercise: Representation Comparison"}
**Setup:** Use the provided notebooks to compare different representation approaches:

1. **Run `demos/augmentation_collage.ipynb`:**
   - Generate augmentation matrices for semantic similarity
   - Compare ResNet-18 vs DINOv3 embeddings
   - Analyze MDS projections of representation spaces

2. **Run `demos/rl_examples.ipynb`:**
   - Visualize representation trajectories from pixels to semantics
   - Compare CNN Grad-CAM vs ViT attention patterns
   - Generate figures for your own dataset

**Discussion Questions:**
- Why do pixel-level similarities fail to capture semantic relationships?
- How do different SSL objectives (contrastive vs self-distillation) affect representation geometry?
- What trade-offs do you observe between CNN and ViT representations?
:::

::: {.callout-note title="Quiz 1: Invariance vs Equivariance"}
**Question:** Give one example where too much invariance harms performance.
<details><summary>Answer</summary>Face recognition losing fine-grained identity cues if heavy blurring / color jitter removes distinguishing patterns.</details>
:::

::: {.callout-note title="Quiz 2: Probing"}
**Question:** Why is k-NN often a stronger baseline than expected on high-quality representations?
<details><summary>Answer</summary>Because semantic neighborhoods are already well-formed; a parametric head adds little beyond nearest neighbor majority vote.</details>
:::

::: {.callout-note title="Quiz 3: SSL Paradigms"}
**Question:** What's the key advantage of self-distillation (DINO) over contrastive methods (SimCLR)?
<details><summary>Answer</summary>No need for explicit negatives or large batch sizes; emergent clustering arises from teacher-student dynamics.</details>
:::

::: {.callout-note title="Critical Thinking: Model Selection"}
**Scenario:** You have 500 labeled images of rare birds, no text descriptions, and need to classify 50 species. You can choose between:
- CLIP ViT-B/32 (trained on image-text pairs)
- DINOv3 ViT-B/14 (trained on images only)
- MAE ViT-B/16 (trained with masked reconstruction)

**Discussion:** Which would you choose and why? Consider:
- Training objective alignment with your task
- Data efficiency for few-shot scenarios
- Expected transfer performance for fine-grained classification

<details><summary>Reasoning</summary>
**Likely best: DINOv3** - Self-supervised on images, strong few-shot performance, good for fine-grained tasks.
**CLIP** might struggle without text descriptions and may have learned text-biased features.
**MAE** could work but typically needs more fine-tuning to reach peak performance.
</details>
:::

::: {.callout-tip title="Extension Project Ideas"}
**For deeper exploration:**

1. **Representation Geometry Analysis:**
   - Compute representation rank and effective dimensionality
   - Measure invariance to specific augmentation types
   - Analyze layer-wise semantic emergence

2. **Cross-Domain Transfer Study:**
   - Compare transfer from ImageNet → Medical images
   - Evaluate domain adaptation techniques (DANN, CORAL)
   - Measure forgetting during continual learning scenarios

3. **Efficiency vs Performance Trade-offs:**
   - Profile LoRA rank vs accuracy curves
   - Compare knowledge distillation to direct transfer
   - Analyze pruning effects on representation quality
:::

## 9. Recap

::: {.callout-tip title="Recap"}
- Representations trade off invariance, equivariance, and detail retention.
- Self-supervised objectives sculpt semantic geometry without labels.
- Evaluation requires multiple probes (k-NN + linear + few-shot) to avoid false confidence.
- DINOv3 & CLIP illustrate specialization (label-efficient vs open-vocabulary).
- Systematic adaptation flow reduces unnecessary fine-tuning cost.
:::



::: {#refs}
:::
