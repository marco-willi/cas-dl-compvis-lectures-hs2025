---
title: "4 - Representation Learning"
params:
   images_path: "/assets/images/representation_learning/"
---

::: {.callout-tip title="Learning Objectives" collapse="true"}
By the end of this lecture you should be able to:

- Explain what a representation is and why invariance & selectivity matter.
- Distinguish supervised, self-supervised (contrastive, masked, distillation), and multimodal representation objectives.
- Describe how layer depth changes semantic abstraction in CNNs & ViTs.
- Evaluate representations with linear probes, k-NN, clustering metrics, and retrieval.
- Choose a transfer strategy (feature extraction vs fine-tune vs adapters) for a new dataset.
- Justify when to favor DINOv3 vs CLIP vs supervised ViT for a downstream task.
:::


::: {.callout-tip title="TLDR Recap" collapse="true"}
Short summary:

- Foundation models deliver reusable semantic embeddings that dramatically lower labeled data requirements.
- DINOv3 refines self-distillation to yield robust, high-fidelity patch & global representations.
- Adaptation path: k-NN ‚Üí Linear Probe ‚Üí Adapters/LoRA ‚Üí (optional) Full Fine-Tune.
- Objective choice influences invariances, spatial detail retention, and zero-shot capabilities.
:::

::: {.content-hidden}
$$
{{< include /assets/_macros.tex >}}
$$
:::

## Motivation


Different properties and challenges, such as the [semantic gap](intro.qmd#sec-intro-challenges-semantic-gap), are reasons modelling images is difficult. In particular extracting generic, semantically meaningful information is hard. Humans, however, excel at this task and can easily and  comprehensively describe images with relatively few bits of information. A good example is @fig-raw-elephant:

::: {#fig-raw-elephant layout-ncol=1}
![]({{< meta params.images_path >}}elephant.jpg){fig-alt="Raw RGB image used to illustrate pixel-space representations" width=100%}

Example Image [Source](https://unsplash.com/de/fotos/elefantenwanderung-tagsuber-QJbyG6O0ick).
:::

It is easy for humans to describe the image. For example:

```{python}
# | eval: false
# | echo: true
"""
An photograph of an elephant facing the camera.
The elephant has large tusks and ears and is standing in a flat savannah.
Is is bright day light with a cloudy sky.
"""
```

Of course: **a lot** of information is lost, however, **practically** relevant information is preserved!

::: {.callout-note title="Information Content: Raw vs Natural Language" collapse="true"}

Let's quantify the dramatic compression achieved by semantic description:

**Raw Image (Pixel Representation):**

- Resolution: 640 √ó 360 pixels
- Channels: 3 (RGB)
- Bits per pixel: 8 bits per channel

```{python}
# | eval: true
# | echo: false
import pandas as pd
from IPython.display import HTML

# Natural language description
description = """
An photograph of an elephant facing the camera.
The elephant has large tusks and ears and is standing in a flat savannah.
Is is bright day light with a cloudy sky.
"""

# Calculate bits for natural language
char_count = len(description)
bits_utf8 = char_count * 8  # UTF-8 uses 8 bits per character
bits_ascii = char_count * 7  # ASCII uses 7 bits per character

# Raw image stats (from earlier calculation)
width, height, channels = 640, 360, 3
raw_bits = width * height * channels * 8

# Compression ratio
compression_ratio = raw_bits / bits_utf8

# Create comparison table
comparison_data = pd.DataFrame(
    {
        "Representation": [
            "Raw RGB Image",
            "Natural Language (UTF-8)",
            "Natural Language (ASCII)",
        ],
        "Size (bits)": [f"{raw_bits:,}", f"{bits_utf8:,}", f"{bits_ascii:,}"],
        "Size (KB)": [
            f"{raw_bits / 8 / 1024:.1f}",
            f"{bits_utf8 / 8 / 1024:.2f}",
            f"{bits_ascii / 8 / 1024:.2f}",
        ],
        "Compression Factor": [
            "1√ó",
            f"{compression_ratio:.0f}√ó",
            f"{raw_bits / bits_ascii:.0f}√ó",
        ],
    }
)

table_html = comparison_data.to_html(
    index=False,
    border=0,
    classes="table table-sm table-striped align-middle caption-top",
)

HTML(table_html)
```
:::


We can also say: By describing the image we **represent** the image with natural language, instead of the raw pixels.

This is exactly what **Representation Learning** is interested in: How can we find compact representations wich are significantly smaller than the raw, original representations while retaining _relevant_ information?

The idea is that such compact representations are well suited for down-stream tasks, such as image classification.

One motivation to represent images in a different space than with raw pixels (data space) is the enormous difference between the set of all *possible* images and the set of *natural* (real-world) images. This indicates that representing images in pixel space is not efficient. As illustrated in @fig-rl-set-of-images, the subset of natural images occupies only a tiny fraction of the entire image space.  For instance, for RGB images of size $256 \times 256$ pixels with 255 possible intensity values per channel, the total number of possible images is $255^{256 \times 256 \times 3}$, an astronomically large number, effectively infinite from a practical standpoint. This enables strong compression if we can find a more suitable space or coordinate system to represent images.


::: {#fig-rl-set-of-images}
![]({{< meta params.images_path >}}set_of_images.png){fig-alt="Set of example images illustrating representation learning concepts" width=600}

Illustrated are the set of all possible images and the subset of natural images.
:::


## What Is Representation Learning?

A representation is the output $\vect{z} = f(\vect{x})$ produced by an encoder $f$ applied to input $\vect{x}$; $\vect{z}$ retains task‚Äërelevant structure while discarding nuisance variation. The encoder $f$ is the mapping; the representation $\vect{z}$ is its result.

Therefore, when we refer to representations we typically mean vector-valued (distributed) representations and not, for example, natural language descriptions.



### Intuition

Let's take a look at the (synthetic) images in @fig-rl-examples:

::: {#fig-rl-examples layout-nrow=2}


![Elephant - Day 1]({{< meta params.images_path >}}examples/elephant_day_01.png){fig-alt="Elephant in daylight savanna"}

![Elephant - Day 1]({{< meta params.images_path >}}augmented/elephant_flip_horizontal.png){fig-alt="Elephant in daylight savanna"}

![Elephant - Day 1]({{< meta params.images_path >}}augmented/elephant_brightness_1.8x.png){fig-alt="Elephant in daylight savanna"}

![Elephant - Day 2]({{< meta params.images_path >}}examples/elephant_day_02.png){fig-alt="Elephant by watering hole in daylight"}

![Giraffe - Day]({{< meta params.images_path >}}examples/giraffe_day_01.png){fig-alt="Giraffe in daylight grassland"}

![Rhino - Night]({{< meta params.images_path >}}examples/rhino_night_01.png){fig-alt="Rhinoceros at nighttime"}

![Elephant - Night]({{< meta params.images_path >}}examples/elephant_night_01.png){fig-alt="Elephant at night with moonlight"}

![Car - Day]({{< meta params.images_path >}}examples/car_day_01.png){fig-alt="Modern car in bright daylight"}

Synthetic images illustrating representation learning challenges.
:::

Imagine you have to represent those in 2-D: $\vect{z} \in \mathbb{R}^2$. How would you proceed from a human point of view?


::: {#fig-representation-space}
![]({{< meta params.images_path >}}representation_space.png){fig-alt="Empty 2-dimensional representation space" width=50%}

Empty 2-dimensional representation space for mapping image similarity.
:::

::: {.callout-note title="Representation Intuition"}

How would you represent the images of @fig-rl-examples in 2 dimensions? Why?

**Hint**:
Similar images might should be close to each other. Exact positions do not matter, only distances.


<details>
<summary>Click for result</summary>

**Key Considerations:**

The images demonstrate different types of similarity challenges in representation learning:

1. **Same class, same conditions** (Elephant Day 1 & Elephant Day 2): These are most similar - both elephants photographed in daylight. They exhibit **intra-class variation** while maintaining the same semantic category.

2. **Same class, different conditions** (Elephant Day vs. Elephant Night): Moderate similarity - same animal class but dramatically different lighting conditions. This tests **illumination invariance**.

3. **Different classes, same conditions** (Elephants, Giraffe, Rhino by day): Animals share similar contexts (daytime, natural habitat) but different semantics, requiring **inter-class discrimination**.

4. **Different domain** (Animals vs. Car): Least similar - represents a complete **domain shift** from wildlife to manufactured objects.

**Ideal representation learning:** A good model should group the two daytime elephants closely, maintain reasonable similarity between day/night elephants, distinguish between different animal classes, and clearly separate animals from vehicles.

</details>

:::


### What are good representations?

Good representations transform raw data into a format that makes relevant patterns explicit and accessible. They represent a **semantic coordinate systems** where distances $\|\vect{z}_i - \vect{z}_j\|_2$ reflect meaningful relationships.

::: {#fig-data-representation-spaces}
![]({{< meta params.images_path >}}data_and_representation_space.png){fig-alt="Mapping from data space to representation space via encoder" width=80%}

**Data Space vs Representation Space:** The encoder $f$ transforms high-dimensional input samples $\vect{x}_i$ from data space into compact representations $\vect{z}_i$ in representation space, where semantic relationships become explicit through distances and clustering.
:::


**Mathematical Framework:**

Given input $\vect{x} \in \mathcal{X}$ and encoder $f: \mathcal{X} \to \mathcal{Z}$, we want representation $\vect{z} = f(\vect{x})$ such that:

#### Invariance & Selectivity Balance

- **Invariance:** $\|f(\vect{x}) - f(g(\vect{x}))\|_2 < \epsilon$ for nuisance transform $g$ (lighting, pose)
- **Selectivity:** $\|f(\vect{x}_i) - f(\vect{x}_j)\|_2 > \delta$ for semantically different $\vect{x}_i, \vect{x}_j$
- *Trade-off:* Too much invariance ‚Üí loss of discriminative details
- *Sweet spot:* Preserve task-relevant variations, discard irrelevant ones

#### Geometric Structure Preservation

- **Smooth manifold:** Similar semantic concepts cluster in representation space
- **Composability:** $f(\vect{x}_1) \oplus f(\vect{x}_2) \approx f(\text{combine}(\vect{x}_1, \vect{x}_2))$ for vector operations
- **Interpolability:** Linear interpolation $\alpha f(\vect{x}_1) + (1-\alpha) f(\vect{x}_2)$ yields meaningful intermediate concepts

#### Transfer Efficiency

- **Low sample complexity:** Few examples needed for downstream adaptation
- **Broad applicability:** Same $f(\cdot)$ works across multiple tasks/domains
- **Graceful degradation:** Performance degrades slowly with domain shift

#### Computational Practicality

- **Compact dimensionality:** $\text{dim}(\vect{z}) \ll \text{dim}(\vect{x})$ while preserving information
- **Fast computation:** Forward pass $f(\vect{x})$ efficient for real-time applications
- **Stable training:** Representation learning converges reliably


## Deep Image Representations

There are multiple ways to learn image representations with deep learning, using different learning paradigms and objectives.


### Supervised Learning

Supervised pretraining learns $f(\cdot)$ by predicting human labels (e.g., ImageNet classes) with a loss like cross-entropy.
Deep networks progressively transform pixels into higher-level semantics, and linear probes on the penultimate layer often separate classes.

::: {#fig-supervised-architecture}
![]({{< meta params.images_path >}}supervised_learning_representations.png){fig-alt="Supervised learning architecture showing encoder and classifier" width=600}

**Supervised Pre-training Architecture:** Input images are processed by an encoder (CNN or ViT) that extracts features, followed by a classification head trained with cross-entropy loss on human-provided labels. The penultimate layer embeddings serve as transferable representations for downstream tasks.
:::

### Weakly Supervised Learning

Weak supervision uses noisy/partial labels at scale‚Äîhashtags, alt-text, or image‚Äìtext pairs.
A landmark example is **CLIP** (see @radford_learning_2021), trained to align images and their captions via contrastive learning.
It enables zero-shot recognition by turning text prompts into classifiers (e.g., ‚Äúa photo of a cat‚Äù).
This reduces annotation cost and yields broadly transferable features. See @fig-clip-contrastive for an illustration.


::: {.callout-note title="CLIP Loss" collapse="true"}

**Composite objective.** CLIP jointly trains an **image encoder** $f_\theta$ and a **text encoder** $g_\phi$ to align matched image‚Äìtext pairs using a **symmetric contrastive (InfoNCE) loss** with a learned temperature (logit scale).

**Setup.** For a batch of $N$ paired samples $\{(x_i, y_i)\}_{i=1}^N$:

- Image features: $\tilde{\mathbf{v}}_i = \frac{f_\theta(x_i)}{\|f_\theta(x_i)\|}$
- Text features: $\tilde{\mathbf{t}}_i = \frac{g_\phi(y_i)}{\|g_\phi(y_i)\|}$
- Learned logit scale: $\alpha = \exp(s)$ with parameter $s$ (equivalently temperature $\tau = 1/\alpha$)
- Similarities (cosine scaled): $s_{ij} = \alpha \, \tilde{\mathbf{v}}_i^\top \tilde{\mathbf{t}}_j$

**Image-to-text loss (with in-batch negatives).**
\begin{equation}
\mathcal{L}_{i \to t}
= \frac{1}{N} \sum_{i=1}^{N}
-\log \frac{\exp\!\big(s_{ii}\big)}
{\sum_{j=1}^{N} \exp\!\big(s_{ij}\big)} \, .
\end{equation}

**Text-to-image loss (symmetric direction).**
\begin{equation}
\mathcal{L}_{t \to i}
= \frac{1}{N} \sum_{j=1}^{N}
-\log \frac{\exp\!\big(s_{jj}\big)}
{\sum_{i=1}^{N} \exp\!\big(s_{ij}\big)} \, .
\end{equation}

**Total objective.**
\begin{equation}
\mathcal{L}_{\text{CLIP}}
= \tfrac{1}{2} \left( \mathcal{L}_{i \to t} + \mathcal{L}_{t \to i} \right) \, .
\end{equation}

**Components (at a glance).**

- $f_\theta, g_\phi$: image/text encoders (e.g., ViT/CNN and Transformer).
- $\tilde{\mathbf{v}}_i, \tilde{\mathbf{t}}_i$: $\ell_2$-normalized embeddings (cosine similarity).
- $\alpha$ (or $1/\tau$): learned scaling controlling softness of the softmax.
- In-batch negatives: all non-matching pairs in the current batch act as negatives.
- Optimization: cross-entropy over the similarity matrix in both directions.

*Inference note:* zero-shot classification uses text prompts $y_c =$ ‚Äúa photo of a $\{class\}$‚Äù to form class prototypes $\tilde{\mathbf{t}}_c$ and picks $\arg\max_c \tilde{\mathbf{v}}^\top \tilde{\mathbf{t}}_c$.

:::

::: {#fig-clip-contrastive}
![]({{< meta params.images_path >}}clip_contrastive_pre_training.png){fig-alt="CLIP dual-encoder architecture with contrastive loss" width=600}

**CLIP Contrastive Pre-training:** Shown are the image and text encoders processing a batch of paired data. The resulting embeddings are compared via a similarity matrix where matching pairs (diagonal) are maximized and non-matching pairs (off-diagonal) are minimized through contrastive learning.
:::

### Self-Supervised Learning

Self-supervised learning (SSL) creates pretext tasks from raw data (no human labels) to shape $f(\cdot)$.
Two dominant families are *imputation/masked modeling* and *contrastive/self-distillation*.

#### Imputation

Masked image modeling hides random patches and trains the model to reconstruct them.
**Masked Autoencoding (MAE)** (see @he_masked_2021) shows that an asymmetric encoder‚Äìdecoder and high masking ratio (‚âà75%) scale well and produce strong features for transfer.
Intuition: to fill in what‚Äôs missing, the model must internalize global structure and semantics.

::: {.callout-note title="Masked Autoencoding Loss" collapse="true"}


**Setup.** Given an image $x$, MAE patchifies it and **masks** a large subset of patches $\mathcal{M}$ (typically $\sim 75\%$), keeping visible patches $\mathcal{V}$.

- Patchify: $X = \mathrm{Patchify}(x) \in \mathbb{R}^{P \times d}$ with $P$ patches and patch-dimension $d$.
- Split: $X = \{X_{\mathcal{V}}, X_{\mathcal{M}}\}$.
- **Encoder** $E_\theta$ processes only visible tokens $X_{\mathcal{V}}$ to produce latents $H = E_\theta(X_{\mathcal{V}})$.
- **Decoder** $D_\phi$ receives $H$ plus learned mask tokens at masked positions and predicts reconstructions $\hat{X}_{\mathcal{M}} = D_\phi(H, \text{mask tokens})$.

**Reconstruction loss (masked patches only).**
\begin{equation}
\mathcal{L}_{\text{MAE}}
=
\frac{1}{|\mathcal{M}|} \sum_{p \in \mathcal{M}}
\left\| \hat{X}_{p} - \tilde{X}_{p} \right\|_2^2 ,
\end{equation}
where $\tilde{X}$ are **normalized pixel targets** (e.g., per-channel mean/variance normalization) in patch space.

**Components (at a glance).**

- $X$: patchified image tokens; $\mathcal{V}/\mathcal{M}$: visible/masked index sets.
- $E_\theta$ (ViT encoder): operates on **visible** tokens only (asymmetric design).
- $D_\phi$ (lightweight decoder): reconstructs **masked** tokens from encoder latents $+$ mask tokens.
- $\hat{X}_{\mathcal{M}}$: decoder predictions at masked positions.
- $\tilde{X}_{\mathcal{M}}$: normalized pixel targets at masked positions.
- Loss averages **only over masked patches**, encouraging learning of global structure.

**Notes.**

- High mask ratio $+$ asymmetric encoder/decoder yields efficient pretraining and strong transfer.
- Some variants replace pixel MSE with feature/perceptual losses, but vanilla MAE uses MSE on normalized pixels.


:::

::: {#fig-mae-architecture}
![]({{< meta params.images_path >}}masked_auto_encoder.png){width=800}

**MAE Pre-training Pipeline:** A large fraction of image patches is randomly masked. The encoder processes only visible patches for efficiency. After encoding, lightweight mask tokens are inserted, and a small decoder reconstructs the original image pixels from the full token set. Post-training, the decoder is discarded; the encoder alone extracts features from complete (unmasked) images for downstream tasks.

:::


#### Contrastive Learning

Contrastive learning is a learning paradigm whereby a model learns to represent data points such that similar points are nearby and disimilar points are far apart.
The **SimCLR** (@chen_simple_2020) paper demonstrated how such a model can be learned and achieve high-accuracy on down-stream tasks.
@fig-ssl-contrastive illustrateds the concepts: From a given image create two _views_ (augmented versions) and contrast them with different images in a large batch of images (InfoNCE loss).

::: {.callout-note title="InfoNCE Loss" collapse="true"}

**Core Idea:** InfoNCE maximizes mutual information between positive pairs while contrasting against negative samples.

**Mathematical Form:**
$$
\mathcal{L}_{\text{InfoNCE}} = -\mathbb{E}\left[\log \frac{\exp(\text{sim}(z_i, z_i^+)/\tau)}{\exp(\text{sim}(z_i, z_i^+)/\tau) + \sum_{j=1}^{N-1} \exp(\text{sim}(z_i, z_j^-)/\tau)}\right]
$$

**Components:**

- $z_i$: Anchor embedding (encoded from image $x_i$)
- $z_i^+$: Positive embedding (different augmentation of same image)
- $z_j^-$: Negative embeddings (from different images in batch)
- $\text{sim}(\cdot, \cdot)$: Similarity function (typically cosine similarity)
- $\tau$: Temperature parameter controlling concentration

**Temperature Effect:**

- Small $\tau$ ($\to 0$): Focuses on hardest negatives, sharper distributions
- Large $\tau$ ($\to \infty$): Uniform weighting, softer distributions
- Typical values: $\tau \in [0.07, 0.5]$

**Intuition:** The loss encourages the model to:

1. Pull positive pairs close together in embedding space
2. Push negative pairs apart
3. Form tight clusters of semantically similar samples

**Practical Considerations:**

- Requires large batch sizes (e.g., 256-4096) for sufficient negatives
- Alternative: Memory banks (MoCo) or queue-based negatives
- Hard negative mining can improve learning efficiency

:::


::: {#fig-ssl-contrastive layout-ncol=2}

![]({{< meta params.images_path >}}self_supervised_contrastive1.png){fig-alt="Contrastive learning conceptual view showing similar and dissimilar samples"}

![]({{< meta params.images_path >}}self_supervised_contrastive2.png){fig-alt="Contrastive learning architectural view with encoder and projection head"}

**Left:** Learning representations by contrasting similar inputs (different views of same sample) against other inputs. **Right:** Architectural view showing the encoder and projection head pipeline.
:::

#### Self-Distillation

Self-distillation is an interesting concept: A **student** model tries to match the outputs of a **teacher** model‚Äôs distribution over a given image.
This principle was introduced in the BYOL paper (see @grill_bootstrap_2020) and led to the DINO-series of models: DINO (@caron_emerging_2021), DINOv2 (@oquab_dinov2_2024), and DINOv3 (@simeoni_dinov3_2025) are non-contrastive: a student matches the teacher‚Äôs distribution over crops (multi-crop), using centering and sharpening. This yields strong **global descriptors** and high-quality **local (patch) tokens**.

::: {.callout-note title="DINOv3 Loss" collapse="true"}

**Composite objective.** DINOv3 trains a student ViT against an EMA **teacher** with multi-crop views, combining a **global DINO loss**, a **local iBOT loss**, a **KoLeo uniformity regularizer**, and (in a refinement stage) a **Gram anchoring loss** on patch similarities:

\begin{align}
\mathcal{L}
&= \underbrace{\mathcal{L}_{\text{DINO}}}_{\text{global crops}}
+ \lambda_{\text{iBOT}} \underbrace{\mathcal{L}_{\text{iBOT}}}_{\text{masked local tokens}}
+ \lambda_{\text{KoLeo}} \underbrace{\mathcal{L}_{\text{KoLeo}}}_{\text{uniformity on }[\text{CLS}]}
+ \lambda_{\text{Gram}} \underbrace{\mathcal{L}_{\text{Gram}}}_{\text{refinement}}.
\end{align}

**Global DINO (distribution matching, no negatives).**
Student probabilities $p_s=\mathrm{softmax}(g_s(z_s)/\tau_s)$; teacher targets
$q_t=\mathrm{stopgrad}\!\left[\mathrm{softmax}\!\left((g_t(z_t)-c)/\tau_t\right)\right]$ with centering $c$ and temperatures $\tau_s,\tau_t$.
\begin{equation}
\mathcal{L}_{\text{DINO}} \;=\; - \sum_{k} q_t^{(k)} \log p_s^{(k)}
\quad \text{(summed over pairs of student/teacher crops).}
\end{equation}

**Local iBOT (masked token prediction).**
For masked student patches $m \in \mathcal{M}$, match teacher patch-level distributions:
\begin{equation}
\mathcal{L}_{\text{iBOT}} \;=\; - \frac{1}{|\mathcal{M}|}\sum_{m \in \mathcal{M}}\sum_{k} q_{t,m}^{(k)} \log p_{s,m}^{(k)} .
\end{equation}

**KoLeo (feature uniformity).**
Encourages spread (near-uniform) class-token embeddings; implemented as the KoLeo entropy estimator on mini-batches (schematic form):
\begin{equation}
\mathcal{L}_{\text{KoLeo}} \;\approx\; -\frac{1}{N}\sum_{i=1}^{N} \log d_i
\quad \text{with } d_i \text{ the NN distance of } z^{[\text{CLS}]}_i .
\end{equation}

**Gram anchoring (refinement step).**
Align student **patch-similarity structure** to a ‚ÄúGram teacher.‚Äù
Let $\tilde{Z}_s,\tilde{Z}_g \in \mathbb{R}^{P \times D}$ be $\ell_2$-normalized patch features (rows) for student and Gram-teacher on a global crop. With Gram matrices $G(\tilde{Z})=\tilde{Z}\tilde{Z}^\top$:
\begin{equation}
\mathcal{L}_{\text{Gram}} \;=\; \big\| G(\tilde{Z}_s) - G(\tilde{Z}_g) \big\|_F^2 .
\end{equation}

**Notes.**
- Teacher is an EMA of the student; teachers view only **global crops**, students see **global + local** crops; iBOT masks a subset of student patches.
- Gram anchoring is applied in a late **refinement** phase to restore **patch-level consistency** without hurting global performance.

:::

::: {#fig-dino-principle}

![]({{< meta params.images_path >}}dino_principle.png){fig-alt="DINO self-distillation principle showing teacher-student architecture with multi-crop strategy" width=600}

The student network processes various crops (global and local views) of the same image, attempting to match the output distribution of the momentum-updated teacher network. The teacher processes only global views and uses centering and sharpening to avoid collapse. No explicit negatives are required; the model learns by aligning predictions across different augmented views.
:::


::: {.callout-note title="DINOv3 Overview Video" collapse="false"}
Watch Meta's introduction to DINOv3, explaining the key improvements over DINOv2 and the self-distillation approach:

{{< video https://www.youtube.com/watch?v=-eOYWK6m3i8 >}}

:::


## Global vs Local Representations

**Global** representations summarize an image into a single embedding (e.g., a [CLS] token or pooled feature) that captures overall semantics.

**CLIP** is a prototypical global model: it aligns an image-wide vector with a text embedding, which is ideal for _zero-shot classification_, _retrieval_, and broad _semantic similarity_ at scale.

In contrast, the DINO family (incl. DINO/DINOv2/DINOv3) produces both strong global descriptors and rich local tokens via self-distillation and multi-crop training. These local features carry spatially resolved cues useful for tasks such as segmentation or obejct detection. The global token remains competitive for classification and retrieval.


::: {#fig-dinov3-local-features}
![]({{< meta params.images_path >}}dinov3_local_cosine_similarity_example.png){fig-alt="DINOv3 local feature similarity visualization showing query points and their cosine similarity maps" width=600}

Shown are the cosine similarities of feature embeddings at different locations across the input image (middle) for different query points (shown as red crosses). The visualization highlights semantic consistency across the image.
:::


## Using Representations

Learned image representations \(z=f(x)\) become versatile building blocks:

- you can **freeze** the encoder and train a light **linear probe** (fast, label-efficient)
- **fine-tune** the whole model (higher accuracy when you have labels)
- attach **adapters/LoRA** for parameter-efficient transfer.

 For **classification/retrieval**, global embeddings (e.g., CLIP) enable **zero-/few-shot** by comparing \(z\) to text or prototype vectors

 For **dense tasks** (segmentation, detection, correspondence), **local (patch) tokens** from ViTs (e.g., DINOv3) feed pixel-aligned heads.

 In practice: start with a robust pretrained encoder, evaluate with a **linear probe** and **k-NN**, then choose between full fine-tuning or adapters based on data, compute, and latency constraints (considering distillation, pruning, and quantization for deployment).


```{python}
#| eval: false
#| echo: true
import torch
from torch import nn
from transformers import AutoModel

model_id = "your-org/dinov3-base"  # TODO: replace with the real HF id (e.g., "facebook/dinov2-base")
backbone = AutoModel.from_pretrained(model_id)

# Freeze backbone for a linear probe
for p in backbone.parameters():
    p.requires_grad = False
backbone.eval()

# Helper: get a single global vector from token features (CLS or mean over patches)
class GlobalPool(nn.Module):
    def __init__(self, use_cls: bool = True):
        super().__init__()
        self.use_cls = use_cls
    def forward(self, last_hidden_state):  # (B, T, D)
        if self.use_cls:
            return last_hidden_state[:, 0]          # [CLS]
        return last_hidden_state.mean(dim=1)        # mean pool over tokens

pool = GlobalPool(use_cls=True)

# 2) Define and attach a linear head ------------------------------------------
embed_dim = getattr(backbone.config, "hidden_size", 768)  # typical config field
num_classes = 10                                          # <-- set to your dataset
head = nn.Linear(embed_dim, num_classes)

# 3) Create the composite "linear probe" model --------------------------------
class LinearProbe(nn.Module):
    def __init__(self, backbone, head, pool):
        super().__init__()
        self.backbone = backbone
        self.head = head
        self.pool = pool

    def forward(self, pixel_inputs):
        """
        pixel_inputs: preprocessed image batch as expected by the backbone's feature extractor.
        For HF ViTs, you'll typically pass pixel_values from a corresponding processor.
        """
        with torch.no_grad():  # ensure backbone stays frozen
            out = self.backbone(pixel_inputs, output_hidden_states=False)
            # Many HF vision models return .last_hidden_state for ViT-like backbones
            z = self.pool(out.last_hidden_state)  # (B, D)
        logits = self.head(z)                    # (B, C)
        return logits

model = LinearProbe(backbone, head, pool)
print(f"[info] Linear probe ready: embed_dim={embed_dim}, num_classes={num_classes}")


```

Such a representation is much more compact, yet retains the most relevant information.

This begs the question:


A raw image is a 3D tensor of pixel intensities. Before any encoder is applied, it already contains structure (spatial locality, color correlations) but no task-specific abstractions. Inspecting its dimensionality highlights why learning compact representations matters.


:::: {.columns}

::: {.column width="50%"}

```{python}
#| eval: true
#| echo: false
#| label: tbl-rl-example-raw-dim
#| tbl-cap: "Raw elephant image dimensionality summary"
#| tbl-subtitle: "Single 224√ó224 crop used as running example"
#| tbl-colwidths: [30,30]
from pathlib import Path
from PIL import Image
import numpy as np
import pandas as pd
from IPython.display import HTML

img_path = Path("/workspace/assets/images/representation_learning/elephant.jpg")
with Image.open(img_path) as img:
    width, height = img.size
    channels = len(img.getbands())
    pixels = width * height
    values = pixels * channels
    arr_dtype = np.array(img).dtype

raw_stats = pd.DataFrame(
    {
        "Statistic": ["Width (px)", "Height (px)", "Channels", "Total Pixels", "Total Values", "Array dtype"],
        "Value": [width, height, channels, f"{pixels:,}", f"{values:,}", str(arr_dtype)],
    }
)

table_html = raw_stats.to_html(
    index=False,
    border=0,
    classes="table table-sm table-striped align-middle caption-top"
)

HTML(table_html)
```

:::

::::

Modalities differ vastly in raw state-space size:

```{python}
#| label: tbl-repr-modalities
#| echo: false
#| fig-cap: "Representative raw modality characteristics"
#| tbl-colwidths: [0.12, 0.22, 0.30, 0.36]
#| tbl-subtitle: "Interactive view with quick export"
#| layout-nrow: 1
import pandas as pd
from IPython.display import Markdown

data = [
    ("Images", "224√ó224 RGB", "‚âà150K pixels", "Local correlations, spatial invariance"),
    ("Text", "Tokens (512 seq)", "Variable (subword vocab ‚âà50K)", "Long-range dependencies"),
    ("Genomics", "Nucleotides (10K)", "4-letter alphabet", "Long-range, hierarchical motifs"),
    ("Graphs", "Nodes / Edges", "Irregular structure", "Permutation invariance"),
    ("Multimodal", "Image + Text", "Combined modalities", "Alignment & grounding"),
]

df_modalities = pd.DataFrame(data, columns=[
    "Modality",
    "Example Unit",
    "Typical Raw Dimensionality (Sample)",
    "Challenges",
])

try:
    from itables import show, options

    options.classes = "display nowrap compact stripe hover order-column"
    options.dom = "Bfrtip"
    options.buttons = ["copy", "csv", "excel"]
    options.searching = False
    options.lengthMenu = False
    show(df_modalities, index=False)
except Exception:
    Markdown(df_modalities.to_markdown(index=False))
```


### Pixel-Level vs Learned Similarities

Before diving into learning objectives, let's examine how different similarity measures organize the same images:

::: {#fig-similarity-comparison}
![]({{< meta params.images_path >}}rl_example_comprehensive_pixel_similarity.png){fig-alt="Comparison of pixel-level similarity measures" width=100%}

**Left:** Euclidean pixel distance creates sharp distinctions. **Middle:** Cosine pixel similarity shows high values across all images. **Right:** Difference map reveals how the metrics diverge.
:::

**Key Observation:** Both pixel-level measures fail to group semantically similar images (the two elephants), regardless of distance metric choice.

::: {#fig-mds-representations}
![]({{< meta params.images_path >}}rl_example_mds_comparison_complete.png){fig-alt="MDS projections comparing pixel and CNN representations" width=100%}

2D MDS projections reveal fundamental differences: **Left:** Scattered pixel-Euclidean layout (high stress). **Middle:** Tight pixel-cosine clustering but no semantic structure. **Right:** CNN embeddings create meaningful semantic neighborhoods with clear class separation.
:::

This motivates why we need **learned representations** that capture semantic similarity rather than superficial pixel patterns.

### The Invariance-Equivariance Spectrum

Rather than binary categories, think of a **continuum** of spatial sensitivity:

::: {#fig-invariance-equivariance-spectrum}
```{mermaid}
graph LR
    A["Full Invariance<br/>f(Tx) = f(x)<br/>üìä Classification"] --> B["Partial Invariance<br/>Small changes OK<br/>üîç Retrieval"]
    B --> C["Local Equivariance<br/>f(Tx) ‚âà T'f(x)<br/>üéØ Detection"]
    C --> D["Strict Equivariance<br/>f(Tx) = T'f(x)<br/>üé® Segmentation"]

    style A fill:#FFE4E1
    style B fill:#F0F8FF
    style C fill:#F0FFF0
    style D fill:#FFF8DC
```

**Task-Representation Alignment:** Different tasks require different levels of spatial sensitivity. The key is matching representation properties to downstream requirements.
:::

::: {.callout-tip title="Interactive Demonstration" collapse="false"}
**Hands-on Exploration:** Create `demos/representation_quality_demo.ipynb` to:

1. **Invariance Visualization:** Apply increasing augmentation strength, plot embedding distances
2. **Semantic Interpolation:** Show smooth transitions between different object classes
3. **Transfer Efficiency:** Compare few-shot performance across different representation qualities
4. **Dimensionality Analysis:** Plot task performance vs representation dimension

**Key Questions to Explore:**
- At what augmentation strength do embeddings become invariant?
- Can you find semantic directions in the representation space?
- Which representations transfer best to new domains?
:::


::: {#fig-rl-augmentations}
![]({{< meta params.images_path >}}rl_example_augmentations.png){fig-alt="Semantic-preserving augmentations applied to an elephant image" width=100%}

Collage of semantic-preserving augmentations (crop, jitter, blur, noise) used to motivate invariance: each view retains the elephant identity despite appearance changes.
:::

::: {.callout-tip title="Try It Yourself!" collapse="false"}
**Interactive Demo:** Run the `demos/augmentation_collage.ipynb` notebook to:
- Generate your own augmentation collage
- Compare pixel-level vs CNN embedding similarities
- Visualize representation spaces with MDS projections

**Key Insight:** Notice how pixel-level similarity fails to capture semantic relationships, while learned CNN embeddings cluster augmented variants together and separate different object classes.
:::


**Why learn representations (vs hand-crafting)?**

The evolution from manual to learned features reflects three key limitations of hand-crafted approaches:

| Era | Approach | Examples | Limitations | Performance Ceiling |
|-----|----------|----------|-------------|-------------------|
| 1980s-2000s | Hand-crafted Features | SIFT, HOG, SURF, LBP | Domain-specific, brittle to variations | ~70% ImageNet |
| 2000s-2010s | Learned Shallow | Bag of Words, Fisher Vectors | Limited representational capacity | ~80% ImageNet |
| 2010s-Present | Deep Learning | CNNs, ViTs, Foundation Models | Requires large data, compute | >95% ImageNet |

: Evolution of visual representations {.striped .hover #tbl-repr-evolution}

**The Scale-Generalization Trade-off:** Manual engineering doesn't scale to the diversity of real-world data and tasks. Learned representations automatically discover relevant patterns from data, enabling generalization to unseen scenarios.

::: {.callout-warning title="Representation Pitfalls"}
**Common failure modes to avoid:**

1. **Over-invariance:** Aggressive augmentation removes task-critical information (e.g., color for flower classification)
2. **Under-selectivity:** Representations collapse, making different classes indistinguishable
3. **Brittleness:** High performance on training domain, catastrophic failure on test shifts
4. **Interpretability loss:** Complex embeddings become black boxes, hindering debugging
5. **Computational bloat:** Representations grow unnecessarily large without performance gains

**Mitigation strategies:** Balanced augmentation policies, regularization techniques, domain adaptation methods, and systematic evaluation protocols.
:::

### Advanced Representation Properties

Beyond basic invariance, modern representations exhibit sophisticated geometric and semantic structures:

::: {.callout-note title="Emergent Properties in Foundation Models"}
**Geometric Structure:**
- **Semantic directions:** Linear subspaces correspond to interpretable concepts (e.g., "style" vectors in image generation)
- **Compositional algebra:** $f(\text{king}) - f(\text{man}) + f(\text{woman}) \approx f(\text{queen})$ in language models
- **Hierarchical clustering:** Representations naturally organize into taxonomic structures

**Transfer Mechanisms:**
- **Feature reuse:** Early layers transfer across domains (edges ‚Üí textures ‚Üí parts)
- **Task-specific adaptation:** Late layers specialize while preserving general features
- **Few-shot emergence:** Quality representations enable learning from minimal examples

**Robustness Properties:**
- **Adversarial stability:** Embeddings remain consistent under small perturbations
- **Distributional shift tolerance:** Performance degrades gracefully with domain changes
- **Continual learning capability:** New tasks learned without catastrophic forgetting
:::

### Invariance vs Equivariance

- *Invariance:* $f(Tx) = f(x)$ (classification robustness).
- *Equivariance:* $f(Tx) = T' f(x)$ (segmentation, detection require spatial structure).
Convolutions approximate translation equivariance; self-attention is *structurally agnostic* and must learn inductive biases via data.

### Hierarchical Abstraction

Lower layers: edges, colors, local patterns ‚Üí mid-layers: motifs/parts ‚Üí deeper layers: object & global scene semantics.

::: {.panel-tabset #fig-zeiler-hierarchy}
### Layer 1
![]({{< meta params.images_path >}}zeiler_layer1.png){fig-alt="Layer 1: Gabor-like edges and color blobs" width=95%}

Edges, color opponency, and Gabor-like filters emerge first.

### Layer 2
![]({{< meta params.images_path >}}zeiler_layer2.png){fig-alt="Layer 2: Corners and simple textures" width=95%}

Edges combine into corners, contours, and early texture motifs.

### Layer 3
![]({{< meta params.images_path >}}zeiler_layer3.png){fig-alt="Layer 3: Texture motifs and repeated patterns" width=95%}

Mid-level texture repetitions (fur, mesh) and shape fragments appear.

### Layer 4
![]({{< meta params.images_path >}}zeiler_layer4.png){fig-alt="Layer 4: Object parts such as faces and wheels" width=95%}

Semantically meaningful object parts (faces, limbs, wheels) are isolated.

### Layer 5
![]({{< meta params.images_path >}}zeiler_layer5.png){fig-alt="Layer 5: Full object templates" width=95%}

Full object templates activate, providing category-level abstraction.
:::

Activation maximization reconstructions from Zeiler & Fergus (2014) illustrate how deeper CNN layers capture increasingly abstract concepts.

* Layer 1 focuses on oriented edges and color opponency ‚Äî the digital analogue of Gabor filters.
* Layer 2 composes edges into corners, gradients, and early texture motifs.
* Layer 3 aggregates motifs into mid-level textures (fur, mesh) and shape fragments.
* Layer 4 responds to semantically meaningful parts (faces, limbs, wheels) aggregated across spatial context.
* Layer 5 fires on entire object templates, providing category-level abstraction that feeds classifier heads.

## Deep Image Representations

::: {#fig-representation-trajectory}
![]({{< meta params.images_path >}}representation_trajectory.png){fig-alt="Evolution from raw pixels to semantic representations" width=100%}

Representation evolution: **Raw RGB** ‚Üí **Edge detection** (early CNN features) ‚Üí **ResNet Grad-CAM** (mid-level semantic focus) ‚Üí **ViT Attention** (global semantic relationships). Each stage reveals different aspects of visual understanding.
:::

::: {.callout-tip title="Try It Yourself!" collapse="false"}
**Interactive Demo:** Run the `demos/rl_examples.ipynb` notebook to:
- Generate your own representation trajectory visualization
- Compare Grad-CAM vs Vision Transformer attention patterns
- See how different architectures focus on different image regions

**Key Insight:** CNNs build local-to-global hierarchies, while ViTs can attend globally from the start, creating different semantic emphasis patterns.
:::

CNNs encode locality and translation inductive biases; Vision Transformers (ViTs) rely on larger data + patch embedding + positional encodings.

| Aspect | CNN | ViT |
|--------|-----|-----|
| Inductive Bias | Strong (locality, weight sharing) | Minimal (global token mixing) |
| Data Efficiency | Higher for small data | Needs large-scale pretraining |
| Receptive Field Growth | Layer stacking / dilation | Immediate global attention |
| Positional Handling | Implicit via convolution structure | Explicit positional embeddings |
| Adaptation (small data) | Often fine-tune later layers | Prefer frozen features + probe / adapters |

: CNN vs ViT representation properties {.striped .hover #tbl-repr-cnn-vit}

**Story:** Inductive bias trades off with flexibility. ViTs + massive self-supervised training (e.g., DINOv2/v3) *recover* desirable invariances without manual architectural constraints.

## 3. Transfer Learning

::: {#fig-transfer-strategy-flow}
```{mermaid}
flowchart TD
    A["New Dataset<br/>Size Assessment"] --> B{"< 1K samples?"}
    B -->|Yes| C["Linear Probe<br/>(Frozen Features)"]
    B -->|No| D{"< 10K samples?"}
    D -->|Yes| E["LoRA/Adapters<br/>(1-10% params)"]
    D -->|No| F{"Domain Shift?"}
    F -->|High| G["Partial Fine-tune<br/>(Top layers)"]
    F -->|Low| H["Full Fine-tune<br/>(All parameters)"]

    C --> I["Evaluate Gap"]
    I --> J{"Gap > 10%?"}
    J -->|Yes| E
    J -->|No| K["‚úì Ship Linear Probe"]

    E --> L["Monitor Forgetting"]
    L --> M{"Performance Drop?"}
    M -->|Yes| N["Reduce LR/<br/>Add Regularization"]
    M -->|No| O["‚úì Ship Adapter"]

    style K fill:#90EE90
    style O fill:#90EE90
    style G fill:#FFE4B5
    style H fill:#FFB6C1
```

Transfer learning decision flowchart: Start simple and escalate complexity only when performance gaps justify additional computational cost.
:::

Reusing a source representation $f_{\theta}$ trained on large dataset $D_s$ for target dataset $D_t$ (possibly small / domain shifted):

| Strategy | Frozen? | Trainable Params | When to Use | Risk |
|----------|---------|------------------|-------------|------|
| Feature Extraction + Linear Probe | Yes | ~Head only | Very small target | Underfitting complex tasks |
| Partial Fine-Tune (top blocks) | Partially | 10‚Äì40% | Medium data | Forgetting lower-level invariances |
| Full Fine-Tune | No | 100% | Large labeled target | Overfitting / compute |
| LoRA / Adapters | Base frozen | 1‚Äì10% | Medium data / resource-bound | Adapter configuration tuning |
| Prompt / Text Prompt (CLIP) | Base frozen | 0‚ÄìFew tokens | Zero/Few-shot | Prompt sensitivity |

: Transfer strategy comparison {.striped .hover #tbl-repr-transfer}

::: {.callout-tip title="Heuristic"}
Start simple: Linear probe ‚Üí If performance gap > X% vs baseline, move to LoRA/adapters ‚Üí escalate to partial fine-tune ‚Üí only full fine-tune when data & compute justify.
:::

### 3.1 Diagnostic Signals

- Linear probe saturates early + large gap to supervised SOTA ‚áí consider partial fine-tune.
- Training loss ‚Üì but validation plateaus ‚áí representation mismatch (try LoRA).
- Large domain color/style shift ‚áí consider small color-specific adapter or augment recipe adjustment.

<!-- TODO: Visual: Flowchart of decision path for adaptation strategy. -->

## 4. Self-Supervised Learning (SSL)

::: {#fig-ssl-paradigms}
```{mermaid}
graph TB
    subgraph "Contrastive Learning"
        A1["Image"] --> A2["Augment"]
        A2 --> A3["Encode"]
        A3 --> A4["Positive: Same image<br/>Negative: Different images"]
        A4 --> A5["InfoNCE Loss"]
    end

    subgraph "Self-Distillation (DINO)"
        B1["Image"] --> B2["Multi-crop<br/>Augmentation"]
        B2 --> B3["Student<br/>Encoder"]
        B2 --> B4["Teacher<br/>Encoder (EMA)"]
        B3 --> B5["Cross-entropy<br/>Distillation"]
        B4 --> B5
    end

    subgraph "Masked Modeling (MAE)"
        C1["Image"] --> C2["Random<br/>Masking (75%)"]
        C2 --> C3["Encoder<br/>(visible patches)"]
        C3 --> C4["Decoder<br/>Reconstruction"]
        C4 --> C5["Pixel/Token<br/>Loss"]
    end

    subgraph "Multimodal (CLIP)"
        D1["Image"] --> D3["Image<br/>Encoder"]
        D2["Text"] --> D4["Text<br/>Encoder"]
        D3 --> D5["Contrastive<br/>Alignment"]
        D4 --> D5
    end

    style A5 fill:#FFE4E1
    style B5 fill:#E0E6FF
    style C5 fill:#E6FFE6
    style D5 fill:#FFF8DC
```

Four major self-supervised learning paradigms with their core mechanisms and loss functions.
:::

Goal: Learn $f(x)$ without manual labels; leverage structure in data via pretext tasks.

| Paradigm | Core Idea | Positive Pair Definition | Example Models | Notable Strength | Typical Weakness |
|----------|-----------|--------------------------|----------------|------------------|------------------|
| Contrastive | Pull positives, push negatives | Augmented views of same image | SimCLR, MoCo | Strong invariance | Needs large batch / memory bank |
| Self-Distillation | Match teacher & student views | Teacher EMA vs student features | DINO v1/v2/v3 | Rich semantics w/o negatives | Augmentation sensitivity |
| Masked Modeling | Reconstruct masked content | Original vs masked patches | MAE, iBOT | Structural detail retention | Slower convergence |
| Multi-Modal Align | Align cross-modal embeddings | Paired image-text | CLIP, BLIP-2 | Zero / few-shot via text | Text bias & leakage |
| Predictive Coding | Predict future / context | Temporal or spatial neighbors | CPC variants | Sequential structure | Complexity of objectives |

: Major self-supervised paradigms {.striped .hover #tbl-repr-ssl}

### 4.1 Contrastive (InfoNCE)
Objective: maximize similarity of positive pair while minimizing others:
$$\mathcal{L}_{\text{InfoNCE}} = - \log \frac{\exp(\text{sim}(z_i, z_i^+)/\tau)}{\sum_{j=1}^N \exp(\text{sim}(z_i, z_j)/\tau)}$$
Temperature $\tau$ tunes concentration; similarity often cosine.

::: {.callout-note title="Quiz" collapse="true"}
**Question:** What happens if $\tau$ is extremely small?
**Answer (click to reveal):** <details><summary>Answer</summary>The loss focuses on the single hardest negative; training can become unstable.</details>
:::

### 4.2 Self-Distillation (DINO Family)
Teacher network (EMA of student) provides soft targets for multi-crop student views. Avoids explicit negatives; emergent clustering.

### 4.3 Masked Image Modeling (MAE / iBOT)
Randomly mask patches (40‚Äì75%), reconstruct pixel space or latent tokens; forces holistic context modeling.

### 4.4 Multimodal Alignment (CLIP)
Image encoder + text encoder; contrastive loss aligns matching pairs enabling text-driven zero-shot transfer.

<!-- TODO: Visual: Side-by-side small schematics of each SSL paradigm (contrastive, distillation, masked, multimodal). -->

## 5. Evaluating Representations

::: {.callout-note title="Interactive Evaluation Exercise" collapse="false"}
**Hands-on Challenge:** Load a pretrained model (ResNet-50, DINOv3) and evaluate it on a small dataset using multiple probes:

1. **k-NN baseline** (k=5): No training required
2. **Linear probe**: Train only a classifier head
3. **Clustering quality**: Measure NMI/ARI on embeddings
4. **Retrieval performance**: Query-gallery similarity ranking

**Discussion Points:**
- Which probe reveals representation quality best?
- When do you see large gaps between k-NN and linear probe?
- How does feature dimensionality affect each metric?

Try this with both ImageNet-supervised and self-supervised models to compare representation quality.
:::

No single metric suffices‚Äîuse *battery* of probes.

| Method | Procedure | Signals | Pros | Cons |
|--------|-----------|---------|------|------|
| k-NN | Compare feature similarity | Local neighborhood structure | No training | Slow for large N |
| Linear Probe | Train logistic/regression on frozen features | Linear separability | Fast, comparable | Ignores non-linear capacity |
| Clustering (k-means) | Cluster embeddings, measure NMI/ARI | Unsupervised grouping quality | Label-free diagnostic | Sensitive to feature scaling |
| Retrieval mAP | Query vs gallery similarity ranking | Ranking robustness | Task-agnostic | Needs curated gallery |
| Few-shot Episodes | N-way K-shot average accuracy | Data efficiency | Mirrors target tasks | Variance with small K |

: Representation evaluation toolbox {.striped .hover #tbl-repr-eval}

::: {.callout-note title="Exercise Idea"}
Compute k-NN (k=5) accuracy, then a linear probe. Report relative improvement. Reflect: does larger gap imply under-expressive head or insufficient representation invariance?
:::

### 5.1 Failure Diagnostics

- High linear probe, poor retrieval ‚áí representation collapses fine-grained similarity.
- Good k-NN, poor clustering NMI ‚áí global structure fragmented; consider whitening or normalization.
- Poor few-shot but good full-data accuracy ‚áí representation needs adaptation (try LoRA).

## 6. Foundation Models (Bridge)

Full deep-dive in `lectures/foundation_models.qmd`. Here we contextualize *representations* origin:

| Model | Objective Category | Emergent Property | Transfer Sweet Spot |
|-------|--------------------|-------------------|---------------------|
| DINOv3 | Self-distillation | Semantic clusters & patch semantics | Few-shot classification, anomaly |
| CLIP | Contrastive multimodal | Language-aligned embeddings | Zero-shot, prompt-based tasks |
| MAE | Masked modeling | Fine structural detail | Dense prediction fine-tune |
| SAM Encoder | Promptable segmentation | High-resolution region semantics | Zero-shot segmentation |

: Selected foundation models & representation characteristics {.striped .hover #tbl-repr-foundation-overview}

### 6.1 Choosing Among DINOv3 vs CLIP

| Scenario | Recommended | Rationale |
|----------|------------|-----------|
| Few labeled images, no text prompts | DINOv3 | Strong linear probe performance |
| Need zero-shot open vocabulary | CLIP | Text prompts act as classifier |
| Downstream segmentation with prompts | SAM Encoder | Mask generalization |
| Style-rich generation/editing | Stable Diffusion (latent) | Generative prior |

: Scenario-based model selection {.striped .hover #tbl-repr-scenarios}

## 7. A Practical Recipe

::: {#fig-practical-recipe}
```{mermaid}
flowchart TD
    A["1. Select Backbone<br/>(DINOv3 ViT-B/L)"] --> B["2. Extract Embeddings<br/>(Normalize + Optional PCA)"]
    B --> C["3. k-NN Baseline<br/>(k=5, cosine distance)"]
    C --> D["4. Linear Probe<br/>(Early stopping)"]
    D --> E["5. Evaluate Clustering<br/>(NMI/ARI metrics)"]
    E --> F{"Gap > Target?"}
    F -->|No| G["‚úì Deploy Linear Probe"]
    F -->|Yes| H["6. Add LoRA Adapters<br/>(r=8, attn matrices)"]
    H --> I["7. Monitor Forgetting<br/>(Cosine sim drift)"]
    I --> J["8. Document Cost<br/>(GPU hrs vs accuracy)"]
    J --> K["‚úì Deploy Adapted Model"]

    style G fill:#90EE90
    style K fill:#90EE90

    classDef evaluation fill:#FFF8DC
    class C,D,E evaluation

    classDef adaptation fill:#E6FFE6
    class H,I adaptation
```

Systematic adaptation workflow: Start simple, escalate only when justified, and always monitor for catastrophic forgetting.
:::

::: {.callout-tip title="Implementation Checklist"}
**Before you start:**
- [ ] Baseline established (random features, ImageNet supervised)
- [ ] Validation split held out for hyperparameter tuning
- [ ] Compute budget allocated (linear probe: minutes, LoRA: hours, full fine-tune: days)

**During adaptation:**
- [ ] Learning curves monitored (train/val gap)
- [ ] Feature drift measured (cosine similarity pre/post)
- [ ] Multiple runs for statistical confidence

**After deployment:**
- [ ] Performance documented with confidence intervals
- [ ] Failure cases analyzed (where does the model struggle?)
- [ ] Computational cost vs accuracy trade-off quantified
:::

The systematic approach reduces trial-and-error and prevents over-engineering solutions.

### 7.1 Minimal PyTorch Sketch (Probe)

```python
# pseudo-code: linear probe on frozen backbone features
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Assume we precomputed (X_train, y_train), (X_val, y_val) via DINOv3 extractor
clf = LogisticRegression(max_iter=1000, n_jobs=-1)
clf.fit(X_train, y_train)
val_acc = accuracy_score(y_val, clf.predict(X_val))
print(f"Linear probe val accuracy: {val_acc:.3f}")
```

## 8. Interactive Learning & Reflection

::: {.callout-note title="Hands-On Exercise: Representation Comparison"}
**Setup:** Use the provided notebooks to compare different representation approaches:

1. **Run `demos/augmentation_collage.ipynb`:**
   - Generate augmentation matrices for semantic similarity
   - Compare ResNet-18 vs DINOv3 embeddings
   - Analyze MDS projections of representation spaces

2. **Run `demos/rl_examples.ipynb`:**
   - Visualize representation trajectories from pixels to semantics
   - Compare CNN Grad-CAM vs ViT attention patterns
   - Generate figures for your own dataset

**Discussion Questions:**
- Why do pixel-level similarities fail to capture semantic relationships?
- How do different SSL objectives (contrastive vs self-distillation) affect representation geometry?
- What trade-offs do you observe between CNN and ViT representations?
:::

::: {.callout-note title="Quiz 1: Invariance vs Equivariance"}
**Question:** Give one example where too much invariance harms performance.
<details><summary>Answer</summary>Face recognition losing fine-grained identity cues if heavy blurring / color jitter removes distinguishing patterns.</details>
:::

::: {.callout-note title="Quiz 2: Probing"}
**Question:** Why is k-NN often a stronger baseline than expected on high-quality representations?
<details><summary>Answer</summary>Because semantic neighborhoods are already well-formed; a parametric head adds little beyond nearest neighbor majority vote.</details>
:::

::: {.callout-note title="Quiz 3: SSL Paradigms"}
**Question:** What's the key advantage of self-distillation (DINO) over contrastive methods (SimCLR)?
<details><summary>Answer</summary>No need for explicit negatives or large batch sizes; emergent clustering arises from teacher-student dynamics.</details>
:::

::: {.callout-note title="Critical Thinking: Model Selection"}
**Scenario:** You have 500 labeled images of rare birds, no text descriptions, and need to classify 50 species. You can choose between:
- CLIP ViT-B/32 (trained on image-text pairs)
- DINOv3 ViT-B/14 (trained on images only)
- MAE ViT-B/16 (trained with masked reconstruction)

**Discussion:** Which would you choose and why? Consider:
- Training objective alignment with your task
- Data efficiency for few-shot scenarios
- Expected transfer performance for fine-grained classification

<details><summary>Reasoning</summary>
**Likely best: DINOv3** - Self-supervised on images, strong few-shot performance, good for fine-grained tasks.
**CLIP** might struggle without text descriptions and may have learned text-biased features.
**MAE** could work but typically needs more fine-tuning to reach peak performance.
</details>
:::

::: {.callout-tip title="Extension Project Ideas"}
**For deeper exploration:**

1. **Representation Geometry Analysis:**
   - Compute representation rank and effective dimensionality
   - Measure invariance to specific augmentation types
   - Analyze layer-wise semantic emergence

2. **Cross-Domain Transfer Study:**
   - Compare transfer from ImageNet ‚Üí Medical images
   - Evaluate domain adaptation techniques (DANN, CORAL)
   - Measure forgetting during continual learning scenarios

3. **Efficiency vs Performance Trade-offs:**
   - Profile LoRA rank vs accuracy curves
   - Compare knowledge distillation to direct transfer
   - Analyze pruning effects on representation quality
:::

## 9. Recap

::: {.callout-tip title="Recap"}
- Representations trade off invariance, equivariance, and detail retention.
- Self-supervised objectives sculpt semantic geometry without labels.
- Evaluation requires multiple probes (k-NN + linear + few-shot) to avoid false confidence.
- DINOv3 & CLIP illustrate specialization (label-efficient vs open-vocabulary).
- Systematic adaptation flow reduces unnecessary fine-tuning cost.
:::



::: {#refs}
:::
