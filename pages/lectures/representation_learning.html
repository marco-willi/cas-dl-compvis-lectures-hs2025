<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Marco Willi">

<title>4 - Representation Learning – CAS Deep Learning - Computer Vision</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../pages/lectures/practical.html" rel="next">
<link href="../../pages/lectures/classification.html" rel="prev">
<link href="../..//assets/cv_logo_small.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-065a5179aebd64318d7ea99d77b64a9e.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-159896a7fbbc1299bfd73fe3af9cd852.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-97BR3FTSN1"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-97BR3FTSN1', { 'anonymize_ip': true});
</script>
<script src="https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js" integrity="sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../pages/lectures/intro.html">Lectures</a></li><li class="breadcrumb-item"><a href="../../pages/lectures/representation_learning.html">4 - Representation Learning</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-center sidebar-header">
      <a href="../../index.html" class="sidebar-logo-link">
      <img src="../../assets/cv_logo_small.png" alt="" class="sidebar-logo light-content py-0 d-lg-inline d-none">
      <img src="../../assets/cv_logo_small.png" alt="" class="sidebar-logo dark-content py-0 d-lg-inline d-none">
      </a>
      <div class="sidebar-tools-main">
    <a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2025" title="GitHub organization" class="quarto-navigation-tool px-1" aria-label="GitHub organization"><i class="bi bi-github"></i></a>
    <a href="" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-code-square"></i></a>
</div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Background &amp; Preparation</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/background/machine_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1 - Machine Learning Basics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/background/neural_networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2 - Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/background/frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3 - Software &amp; Hardware for Deep Learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Lectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1 - Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/cnns.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2 - Convolutional Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3 - Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/representation_learning.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">4 - Representation Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/practical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5 - Practical</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/recent_advances.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6 - Recent Advances</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Slides</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/cnns.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Convolutional Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/representation_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Representation Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/practical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Practical Considerations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/recent_advances.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Recent Advances</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false">
 <span class="menu-text">Resources</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/literature.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Books</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/links.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Helpful Links &amp; Resources</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Mathematical Notation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/student_projects.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Project Ideas</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#motivation" id="toc-motivation" class="nav-link active" data-scroll-target="#motivation"><span class="header-section-number">1</span> Motivation</a></li>
  <li><a href="#what-is-representation-learning" id="toc-what-is-representation-learning" class="nav-link" data-scroll-target="#what-is-representation-learning"><span class="header-section-number">2</span> What Is Representation Learning?</a>
  <ul class="collapse">
  <li><a href="#intuition" id="toc-intuition" class="nav-link" data-scroll-target="#intuition"><span class="header-section-number">2.1</span> Intuition</a></li>
  <li><a href="#what-are-good-representations" id="toc-what-are-good-representations" class="nav-link" data-scroll-target="#what-are-good-representations"><span class="header-section-number">2.2</span> What are good representations?</a></li>
  </ul></li>
  <li><a href="#deep-image-representations" id="toc-deep-image-representations" class="nav-link" data-scroll-target="#deep-image-representations"><span class="header-section-number">3</span> Deep Image Representations</a>
  <ul class="collapse">
  <li><a href="#supervised-learning" id="toc-supervised-learning" class="nav-link" data-scroll-target="#supervised-learning"><span class="header-section-number">3.1</span> Supervised Learning</a></li>
  <li><a href="#weakly-supervised-learning" id="toc-weakly-supervised-learning" class="nav-link" data-scroll-target="#weakly-supervised-learning"><span class="header-section-number">3.2</span> Weakly Supervised Learning</a></li>
  <li><a href="#self-supervised-learning" id="toc-self-supervised-learning" class="nav-link" data-scroll-target="#self-supervised-learning"><span class="header-section-number">3.3</span> Self-Supervised Learning</a></li>
  <li><a href="#global-vs-local-representations" id="toc-global-vs-local-representations" class="nav-link" data-scroll-target="#global-vs-local-representations"><span class="header-section-number">3.4</span> Global vs Local Representations</a></li>
  </ul></li>
  <li><a href="#comparing-representations" id="toc-comparing-representations" class="nav-link" data-scroll-target="#comparing-representations"><span class="header-section-number">4</span> Comparing Representations</a></li>
  <li><a href="#using-representations-a-practical-guide" id="toc-using-representations-a-practical-guide" class="nav-link" data-scroll-target="#using-representations-a-practical-guide"><span class="header-section-number">5</span> Using Representations: A Practical Guide</a></li>
  <li><a href="#quiz" id="toc-quiz" class="nav-link" data-scroll-target="#quiz"><span class="header-section-number">6</span> Quiz</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2025/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../pages/lectures/intro.html">Lectures</a></li><li class="breadcrumb-item"><a href="../../pages/lectures/representation_learning.html">4 - Representation Learning</a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">4 - Representation Learning</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Marco Willi </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Institute for Data Science I4DS, FHNW
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-tip callout-titled" title="Learning Objectives">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Learning Objectives
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>By the end of this lecture you should be able to:</p>
<ul>
<li><strong>Define</strong> representation learning and explain why transforming raw pixels into semantic embeddings enables efficient downstream task performance.</li>
<li><strong>Understand</strong> the balance between invariance (robustness to nuisance factors) and selectivity (discriminating semantically distinct inputs) in learned representations.</li>
<li><strong>Know</strong> supervised, self-supervised (contrastive, masked autoencoding, self-distillation), and weakly supervised (vision-language) learning paradigms.</li>
<li><strong>Distinguish</strong> between global representations (single embedding per image) and local representations (spatially-resolved patch tokens) and identify appropriate use cases for each.</li>
<li><strong>Select</strong> appropriate adaptation strategies (k-NN → linear probe → adapters/LoRA → partial fine-tuning → full fine-tuning) based on dataset size, computational resources, and task requirements.</li>
<li><strong>Justify</strong> model selection between DINOv3, CLIP, MAE, and supervised models for specific downstream tasks considering training objectives, data modalities, and few-shot vs zero-shot requirements.</li>
<li><strong>Apply</strong> practical transfer learning workflows using Hugging Face Transformers and timm libraries for real-world computer vision tasks.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="TLDR Recap">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>TLDR Recap
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Short summary:</p>
<ul>
<li><strong>Representation learning</strong> transforms high-dimensional raw data into compact, semantically meaningful embeddings where distances reflect task-relevant similarities, enabling efficient transfer to downstream tasks.</li>
<li><strong>Three learning paradigms</strong> yield distinct representations: (1) <strong>Supervised</strong> learning on labeled data (e.g., ImageNet ResNet) produces category-aligned features, (2) <strong>Self-supervised</strong> methods (SimCLR, MAE, DINO/v2/v3) learn without labels via contrastive learning, masked reconstruction, or self-distillation, (3) <strong>Weakly supervised</strong> approaches (CLIP) align vision and language for zero-shot capabilities.</li>
<li><strong>Global vs local representations</strong>: Global embeddings (e.g., [CLS] token) capture image-level semantics ideal for classification/retrieval; local patch tokens preserve spatial structure essential for dense prediction tasks like segmentation and detection.</li>
<li><strong>Practical adaptation hierarchy</strong>: Start simple and add complexity only when needed: (1) k-NN baseline (no training), (2) Linear probe (freeze encoder), (3) LoRA/Adapters (parameter-efficient), (4) Partial fine-tuning (unfreeze top layers), (5) Full fine-tuning (last resort for large datasets).</li>
<li><strong>Model selection heuristics</strong>: Use <strong>DINOv3</strong> for discriminative vision tasks and few-shot learning, <strong>CLIP</strong> for zero-shot classification and vision-language alignment, <strong>MAE</strong> when reconstruction pretraining aligns with your task, and <strong>supervised CNNs</strong> when computational efficiency is critical.</li>
<li><strong>Task-relevant invariance</strong> is crucial: Representations must preserve information necessary for downstream tasks (e.g., color for flower classification, orientation for medical imaging) while discarding true nuisance factors.</li>
</ul>
</div>
</div>
</div>
<section id="motivation" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="motivation"><span class="header-section-number">1</span> Motivation</h2>
<p>Different properties and challenges, such as the <a href="../../pages/lectures/intro.html#sec-intro-challenges-semantic-gap">semantic gap</a>, are reasons modelling images is difficult. In particular extracting generic, semantically meaningful information is hard. Humans, however, excel at this task and can easily and comprehensively describe images with relatively few bits of information. A good example is <a href="#fig-raw-elephant" class="quarto-xref">Figure&nbsp;1</a>:</p>
<div id="fig-raw-elephant" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-raw-elephant-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<img src="../../assets/images/representation_learning/elephant.jpg" class="img-fluid figure-img" style="width:100.0%" alt="Raw RGB image used to illustrate pixel-space representations">
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-raw-elephant-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Example Image <a href="https://unsplash.com/de/fotos/elefantenwanderung-tagsuber-QJbyG6O0ick">Source</a>.
</figcaption>
</figure>
</div>
<p>It is easy for humans to describe the image. For example:</p>
<div id="2dfc4b51" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co">A photograph of an elephant facing the camera.</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co">The elephant has large tusks and ears and is standing in a flat savannah.</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co">Is is bright day light with a cloudy sky.</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>Of course: <strong>a lot</strong> of information is lost, however, <strong>practically</strong> relevant information is preserved!</p>
<div class="callout callout-style-default callout-note callout-titled" title="Information Content: Raw vs Natural Language">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Information Content: Raw vs Natural Language
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Let’s quantify the dramatic compression achieved by semantic description:</p>
<p><strong>Raw Image (Pixel Representation):</strong></p>
<ul>
<li>Resolution: 640 × 360 pixels</li>
<li>Channels: 3 (RGB)</li>
<li>Bits per pixel: 8 bits per channel</li>
</ul>
<div id="3b30b048" class="cell" data-execution_count="2">
<div class="cell-output cell-output-display" data-execution_count="1">
<table class="dataframe table table-sm table-striped align-middle caption-top small">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">Representation</th>
<th data-quarto-table-cell-role="th">Size (bits)</th>
<th data-quarto-table-cell-role="th">Size (KB)</th>
<th data-quarto-table-cell-role="th">Compression Factor</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Raw RGB Image</td>
<td>5,529,600</td>
<td>675.0</td>
<td>1×</td>
</tr>
<tr class="even">
<td>Natural Language (UTF-8)</td>
<td>1,312</td>
<td>0.16</td>
<td>4215×</td>
</tr>
<tr class="odd">
<td>Natural Language (ASCII)</td>
<td>1,148</td>
<td>0.14</td>
<td>4817×</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</div>
<p>We can also say: By describing the image we <strong>represent</strong> the image with natural language, instead of the raw pixels.</p>
<p>This is exactly what <strong>Representation Learning</strong> is interested in: How can we find compact representations wich are significantly smaller than the raw, original representations while retaining <em>relevant</em> information?</p>
<p>The idea is that such compact representations are well suited for down-stream tasks, such as image classification.</p>
<p>One motivation to represent images in a different space than with raw pixels (data space) is the enormous difference between the set of all <em>possible</em> images and the set of <em>natural</em> (real-world) images. This indicates that representing images in pixel space is not efficient. As illustrated in <a href="#fig-rl-set-of-images" class="quarto-xref">Figure&nbsp;2</a>, the subset of natural images occupies only a tiny fraction of the entire image space. For instance, for RGB images of size <span class="math inline">\(256 \times 256\)</span> pixels with 255 possible intensity values per channel, the total number of possible images is <span class="math inline">\(255^{256 \times 256 \times 3}\)</span>, an astronomically large number, effectively infinite from a practical standpoint. This enables strong compression if we can find a more suitable space or coordinate system to represent images.</p>
<div id="fig-rl-set-of-images" class="quarto-float quarto-figure quarto-figure-center anchored" alt="Set of example images illustrating representation learning concepts">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rl-set-of-images-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/representation_learning/set_of_images.png" class="img-fluid figure-img" alt="Set of example images illustrating representation learning concepts" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rl-set-of-images-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Illustrated are the set of all possible images and the subset of natural images.
</figcaption>
</figure>
</div>
</section>
<section id="what-is-representation-learning" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="what-is-representation-learning"><span class="header-section-number">2</span> What Is Representation Learning?</h2>
<p>A representation is the output <span class="math inline">\(\mathbf{z} = f(\mathbf{x})\)</span> produced by an encoder <span class="math inline">\(f\)</span> applied to input <span class="math inline">\(\mathbf{x}\)</span>; <span class="math inline">\(\mathbf{z}\)</span> retains task‑relevant structure while discarding nuisance variation. The encoder <span class="math inline">\(f\)</span> is the mapping; the representation <span class="math inline">\(\mathbf{z}\)</span> is its result.</p>
<p>Therefore, when we refer to representations we typically mean vector-valued (distributed) representations and not, for example, natural language descriptions.</p>
<section id="intuition" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="intuition"><span class="header-section-number">2.1</span> Intuition</h3>
<p>Let’s take a look at the (synthetic) images in <a href="#fig-rl-examples" class="quarto-xref">Figure&nbsp;3</a>:</p>
<div id="fig-rl-examples" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rl-examples-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../assets/images/representation_learning/examples/elephant_day_01.png" class="img-fluid figure-img" alt="Elephant in daylight savanna"></p>
<figcaption>Elephant - Day 1</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../assets/images/representation_learning/augmented/elephant_flip_horizontal.png" class="img-fluid figure-img" alt="Elephant in daylight savanna"></p>
<figcaption>Elephant - Day 1</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../assets/images/representation_learning/augmented/elephant_brightness_1.8x.png" class="img-fluid figure-img" alt="Elephant in daylight savanna"></p>
<figcaption>Elephant - Day 1</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../assets/images/representation_learning/examples/elephant_day_02.png" class="img-fluid figure-img" alt="Elephant by watering hole in daylight"></p>
<figcaption>Elephant - Day 2</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../assets/images/representation_learning/examples/giraffe_day_01.png" class="img-fluid figure-img" alt="Giraffe in daylight grassland"></p>
<figcaption>Giraffe - Day</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../assets/images/representation_learning/examples/rhino_night_01.png" class="img-fluid figure-img" alt="Rhinoceros at nighttime"></p>
<figcaption>Rhino - Night</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../assets/images/representation_learning/examples/elephant_night_01.png" class="img-fluid figure-img" alt="Elephant at night with moonlight"></p>
<figcaption>Elephant - Night</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../assets/images/representation_learning/examples/car_day_01.png" class="img-fluid figure-img" alt="Modern car in bright daylight"></p>
<figcaption>Car - Day</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rl-examples-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Synthetic images illustrating representation learning challenges.
</figcaption>
</figure>
</div>
<p>Imagine you have to represent those in 2-D: <span class="math inline">\(\mathbf{z} \in \mathbb{R}^2\)</span>. How would you proceed from a human point of view?</p>
<div id="fig-representation-space" class="quarto-float quarto-figure quarto-figure-center anchored" alt="Empty 2-dimensional representation space">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-representation-space-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/representation_learning/representation_space.png" class="img-fluid figure-img" style="width:50.0%" alt="Empty 2-dimensional representation space">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-representation-space-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Empty 2-dimensional representation space for mapping image similarity.
</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Question: Representing Images">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Question: Representing Images
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>How would you represent the images of <a href="#fig-rl-examples" class="quarto-xref">Figure&nbsp;3</a> in 2 dimensions? Why?</p>
<p><strong>Hint</strong>: Similar images might should be close to each other. Exact positions do not matter, only distances.</p>
<details>
<summary>
Click for result
</summary>
<p><strong>Key Considerations:</strong></p>
<p>The images demonstrate different types of similarity challenges in representation learning:</p>
<ol type="1">
<li><p><strong>Same class, same conditions</strong> (Elephant Day 1 &amp; Elephant Day 2): These are most similar - both elephants photographed in daylight. They exhibit <strong>intra-class variation</strong> while maintaining the same semantic category.</p></li>
<li><p><strong>Same class, different conditions</strong> (Elephant Day vs.&nbsp;Elephant Night): Moderate similarity - same animal class but dramatically different lighting conditions. This tests <strong>illumination invariance</strong>.</p></li>
<li><p><strong>Different classes, same conditions</strong> (Elephants, Giraffe, Rhino by day): Animals share similar contexts (daytime, natural habitat) but different semantics, requiring <strong>inter-class discrimination</strong>.</p></li>
<li><p><strong>Different domain</strong> (Animals vs.&nbsp;Car): Least similar - represents a complete <strong>domain shift</strong> from wildlife to manufactured objects.</p></li>
</ol>
<p><strong>Ideal representation learning:</strong> A good model should group the two daytime elephants closely, maintain reasonable similarity between day/night elephants, distinguish between different animal classes, and clearly separate animals from vehicles.</p>
</details>
</div>
</div>
</div>
</section>
<section id="what-are-good-representations" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="what-are-good-representations"><span class="header-section-number">2.2</span> What are good representations?</h3>
<p>Good representations transform raw data into a format that makes relevant patterns explicit and accessible. They represent a <strong>semantic coordinate systems</strong> where distances <span class="math inline">\(\|\mathbf{z}_i - \mathbf{z}_j\|_2\)</span> reflect meaningful relationships.</p>
<div id="fig-data-representation-spaces" class="quarto-float quarto-figure quarto-figure-center anchored" alt="Mapping from data space to representation space via encoder">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-data-representation-spaces-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/representation_learning/data_and_representation_space.png" class="img-fluid figure-img" style="width:80.0%" alt="Mapping from data space to representation space via encoder">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-data-representation-spaces-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: <strong>Data Space vs Representation Space:</strong> The encoder <span class="math inline">\(f\)</span> transforms high-dimensional input samples <span class="math inline">\(\mathbf{x}_i\)</span> from data space into compact representations <span class="math inline">\(\mathbf{z}_i\)</span> in representation space, where semantic relationships become explicit through distances and clustering.
</figcaption>
</figure>
</div>
<p><strong>Mathematical Framework:</strong></p>
<p>Given input <span class="math inline">\(\mathbf{x} \in \mathcal{X}\)</span> and encoder <span class="math inline">\(f: \mathcal{X} \to \mathcal{Z}\)</span>, we want representation <span class="math inline">\(\mathbf{z} = f(\mathbf{x})\)</span> such that:</p>
<section id="invariance-selectivity-balance" class="level4" data-number="2.2.1">
<h4 data-number="2.2.1" class="anchored" data-anchor-id="invariance-selectivity-balance"><span class="header-section-number">2.2.1</span> Invariance &amp; Selectivity Balance</h4>
<ul>
<li><strong>Invariance:</strong> <span class="math inline">\(\|f(\mathbf{x}) - f(g(\mathbf{x}))\|_2 &lt; \epsilon\)</span> for nuisance transform <span class="math inline">\(g\)</span> (lighting, pose)</li>
<li><strong>Selectivity:</strong> <span class="math inline">\(\|f(\mathbf{x}_i) - f(\mathbf{x}_j)\|_2 &gt; \delta\)</span> for semantically different <span class="math inline">\(\mathbf{x}_i, \mathbf{x}_j\)</span></li>
<li><em>Trade-off:</em> Too much invariance → loss of discriminative details</li>
<li><em>Sweet spot:</em> Preserve task-relevant variations, discard irrelevant ones</li>
</ul>
</section>
<section id="geometric-structure-preservation" class="level4" data-number="2.2.2">
<h4 data-number="2.2.2" class="anchored" data-anchor-id="geometric-structure-preservation"><span class="header-section-number">2.2.2</span> Geometric Structure Preservation</h4>
<ul>
<li><strong>Smooth manifold:</strong> Similar semantic concepts cluster in representation space</li>
<li><strong>Composability:</strong> <span class="math inline">\(f(\mathbf{x}_1) \oplus f(\mathbf{x}_2) \approx f(\text{combine}(\mathbf{x}_1, \mathbf{x}_2))\)</span> for vector operations</li>
<li><strong>Interpolability:</strong> Linear interpolation <span class="math inline">\(\alpha f(\mathbf{x}_1) + (1-\alpha) f(\mathbf{x}_2)\)</span> yields meaningful intermediate concepts</li>
</ul>
</section>
<section id="transfer-efficiency" class="level4" data-number="2.2.3">
<h4 data-number="2.2.3" class="anchored" data-anchor-id="transfer-efficiency"><span class="header-section-number">2.2.3</span> Transfer Efficiency</h4>
<ul>
<li><strong>Low sample complexity:</strong> Few examples needed for downstream adaptation</li>
<li><strong>Broad applicability:</strong> Same <span class="math inline">\(f(\cdot)\)</span> works across multiple tasks/domains</li>
<li><strong>Graceful degradation:</strong> Performance degrades slowly with domain shift</li>
</ul>
</section>
<section id="computational-practicality" class="level4" data-number="2.2.4">
<h4 data-number="2.2.4" class="anchored" data-anchor-id="computational-practicality"><span class="header-section-number">2.2.4</span> Computational Practicality</h4>
<ul>
<li><strong>Compact dimensionality:</strong> <span class="math inline">\(\text{dim}(\mathbf{z}) \ll \text{dim}(\mathbf{x})\)</span> while preserving information</li>
<li><strong>Fast computation:</strong> Forward pass <span class="math inline">\(f(\mathbf{x})\)</span> efficient for real-time applications</li>
<li><strong>Stable training:</strong> Representation learning converges reliably</li>
</ul>
</section>
</section>
</section>
<section id="deep-image-representations" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="deep-image-representations"><span class="header-section-number">3</span> Deep Image Representations</h2>
<p>There are multiple ways to learn image representations with deep learning, using different learning paradigms and objectives.</p>
<section id="supervised-learning" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="supervised-learning"><span class="header-section-number">3.1</span> Supervised Learning</h3>
<p>Supervised pretraining learns <span class="math inline">\(f(\cdot)\)</span> by predicting human labels (e.g., ImageNet classes) with a loss like cross-entropy. Deep networks progressively transform pixels into higher-level semantics, and linear probes on the penultimate layer often separate classes.</p>
<div id="fig-supervised-architecture" class="quarto-float quarto-figure quarto-figure-center anchored" alt="Supervised learning architecture showing encoder and classifier">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-supervised-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/representation_learning/supervised_learning_representations.png" class="img-fluid figure-img" alt="Supervised learning architecture showing encoder and classifier" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-supervised-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: <strong>Supervised Pre-training Architecture:</strong> Input images are processed by an encoder (CNN or ViT) that extracts features, followed by a classification head trained with cross-entropy loss on human-provided labels. The penultimate layer embeddings serve as transferable representations for downstream tasks.
</figcaption>
</figure>
</div>
</section>
<section id="weakly-supervised-learning" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="weakly-supervised-learning"><span class="header-section-number">3.2</span> Weakly Supervised Learning</h3>
<p>Weak supervision uses noisy/partial labels at scale—hashtags, alt-text, or image–text pairs. A landmark example is <strong>CLIP</strong> (see <span class="citation" data-cites="radford_learning_2021">Radford et al. (<a href="#ref-radford_learning_2021" role="doc-biblioref">2021</a>)</span>), trained to align images and their captions via contrastive learning. It enables zero-shot recognition by turning text prompts into classifiers (e.g., “a photo of a cat”). This reduces annotation cost and yields broadly transferable features. See <a href="#fig-clip-contrastive" class="quarto-xref">Figure&nbsp;7</a> for an illustration.</p>
<div class="callout callout-style-default callout-note callout-titled" title="CLIP Loss">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>CLIP Loss
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Composite objective.</strong> CLIP jointly trains an <strong>image encoder</strong> <span class="math inline">\(f_\theta\)</span> and a <strong>text encoder</strong> <span class="math inline">\(g_\phi\)</span> to align matched image–text pairs using a <strong>symmetric contrastive (InfoNCE) loss</strong> with a learned temperature (logit scale).</p>
<p><strong>Setup.</strong> For a batch of <span class="math inline">\(N\)</span> paired samples <span class="math inline">\(\{(x_i, y_i)\}_{i=1}^N\)</span>:</p>
<ul>
<li>Image features: <span class="math inline">\(\tilde{\mathbf{v}}_i = \frac{f_\theta(x_i)}{\|f_\theta(x_i)\|}\)</span></li>
<li>Text features: <span class="math inline">\(\tilde{\mathbf{t}}_i = \frac{g_\phi(y_i)}{\|g_\phi(y_i)\|}\)</span></li>
<li>Learned logit scale: <span class="math inline">\(\alpha = \exp(s)\)</span> with parameter <span class="math inline">\(s\)</span> (equivalently temperature <span class="math inline">\(\tau = 1/\alpha\)</span>)</li>
<li>Similarities (cosine scaled): <span class="math inline">\(s_{ij} = \alpha \, \tilde{\mathbf{v}}_i^\top \tilde{\mathbf{t}}_j\)</span></li>
</ul>
<p><strong>Image-to-text loss (with in-batch negatives).</strong> <span class="math display">\[\begin{equation}
\mathcal{L}_{i \to t}
= \frac{1}{N} \sum_{i=1}^{N}
-\log \frac{\exp\!\big(s_{ii}\big)}
{\sum_{j=1}^{N} \exp\!\big(s_{ij}\big)} \, .
\end{equation}\]</span></p>
<p><strong>Text-to-image loss (symmetric direction).</strong> <span class="math display">\[\begin{equation}
\mathcal{L}_{t \to i}
= \frac{1}{N} \sum_{j=1}^{N}
-\log \frac{\exp\!\big(s_{jj}\big)}
{\sum_{i=1}^{N} \exp\!\big(s_{ij}\big)} \, .
\end{equation}\]</span></p>
<p><strong>Total objective.</strong> <span class="math display">\[\begin{equation}
\mathcal{L}_{\text{CLIP}}
= \tfrac{1}{2} \left( \mathcal{L}_{i \to t} + \mathcal{L}_{t \to i} \right) \, .
\end{equation}\]</span></p>
<p><strong>Components (at a glance).</strong></p>
<ul>
<li><span class="math inline">\(f_\theta, g_\phi\)</span>: image/text encoders (e.g., ViT/CNN and Transformer).</li>
<li><span class="math inline">\(\tilde{\mathbf{v}}_i, \tilde{\mathbf{t}}_i\)</span>: <span class="math inline">\(\ell_2\)</span>-normalized embeddings (cosine similarity).</li>
<li><span class="math inline">\(\alpha\)</span> (or <span class="math inline">\(1/\tau\)</span>): learned scaling controlling softness of the softmax.</li>
<li>In-batch negatives: all non-matching pairs in the current batch act as negatives.</li>
<li>Optimization: cross-entropy over the similarity matrix in both directions.</li>
</ul>
<p><em>Inference note:</em> zero-shot classification uses text prompts <span class="math inline">\(y_c =\)</span> “a photo of a <span class="math inline">\(\{class\}\)</span>” to form class prototypes <span class="math inline">\(\tilde{\mathbf{t}}_c\)</span> and picks <span class="math inline">\(\arg\max_c \tilde{\mathbf{v}}^\top \tilde{\mathbf{t}}_c\)</span>.</p>
</div>
</div>
</div>
<div id="fig-clip-contrastive" class="quarto-float quarto-figure quarto-figure-center anchored" alt="CLIP dual-encoder architecture with contrastive loss">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-clip-contrastive-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/representation_learning/clip_contrastive_pre_training.png" class="img-fluid figure-img" alt="CLIP dual-encoder architecture with contrastive loss" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-clip-contrastive-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: <strong>CLIP Contrastive Pre-training:</strong> Shown are the image and text encoders processing a batch of paired data. The resulting embeddings are compared via a similarity matrix where matching pairs (diagonal) are maximized and non-matching pairs (off-diagonal) are minimized through contrastive learning.
</figcaption>
</figure>
</div>
</section>
<section id="self-supervised-learning" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="self-supervised-learning"><span class="header-section-number">3.3</span> Self-Supervised Learning</h3>
<p>Self-supervised learning (SSL) creates pretext tasks from raw data (no human labels) to shape <span class="math inline">\(f(\cdot)\)</span>. Two dominant families are <em>imputation/masked modeling</em> and <em>contrastive/self-distillation</em>.</p>
<section id="imputation" class="level4" data-number="3.3.1">
<h4 data-number="3.3.1" class="anchored" data-anchor-id="imputation"><span class="header-section-number">3.3.1</span> Imputation</h4>
<p>Masked image modeling hides random patches and trains the model to reconstruct them. <strong>Masked Autoencoding (MAE)</strong> (see <span class="citation" data-cites="he_masked_2021">He et al. (<a href="#ref-he_masked_2021" role="doc-biblioref">2021</a>)</span>) shows that an asymmetric encoder–decoder and high masking ratio (≈75%) scale well and produce strong features for transfer. Intuition: to fill in what’s missing, the model must internalize global structure and semantics.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Masked Autoencoding Loss">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Masked Autoencoding Loss
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Setup.</strong> Given an image <span class="math inline">\(x\)</span>, MAE patchifies it and <strong>masks</strong> a large subset of patches <span class="math inline">\(\mathcal{M}\)</span> (typically <span class="math inline">\(\sim 75\%\)</span>), keeping visible patches <span class="math inline">\(\mathcal{V}\)</span>.</p>
<ul>
<li>Patchify: <span class="math inline">\(X = \mathrm{Patchify}(x) \in \mathbb{R}^{P \times d}\)</span> with <span class="math inline">\(P\)</span> patches and patch-dimension <span class="math inline">\(d\)</span>.</li>
<li>Split: <span class="math inline">\(X = \{X_{\mathcal{V}}, X_{\mathcal{M}}\}\)</span>.</li>
<li><strong>Encoder</strong> <span class="math inline">\(E_\theta\)</span> processes only visible tokens <span class="math inline">\(X_{\mathcal{V}}\)</span> to produce latents <span class="math inline">\(H = E_\theta(X_{\mathcal{V}})\)</span>.</li>
<li><strong>Decoder</strong> <span class="math inline">\(D_\phi\)</span> receives <span class="math inline">\(H\)</span> plus learned mask tokens at masked positions and predicts reconstructions <span class="math inline">\(\hat{X}_{\mathcal{M}} = D_\phi(H, \text{mask tokens})\)</span>.</li>
</ul>
<p><strong>Reconstruction loss (masked patches only).</strong> <span class="math display">\[\begin{equation}
\mathcal{L}_{\text{MAE}}
=
\frac{1}{|\mathcal{M}|} \sum_{p \in \mathcal{M}}
\left\| \hat{X}_{p} - \tilde{X}_{p} \right\|_2^2 ,
\end{equation}\]</span> where <span class="math inline">\(\tilde{X}\)</span> are <strong>normalized pixel targets</strong> (e.g., per-channel mean/variance normalization) in patch space.</p>
<p><strong>Components (at a glance).</strong></p>
<ul>
<li><span class="math inline">\(X\)</span>: patchified image tokens; <span class="math inline">\(\mathcal{V}/\mathcal{M}\)</span>: visible/masked index sets.</li>
<li><span class="math inline">\(E_\theta\)</span> (ViT encoder): operates on <strong>visible</strong> tokens only (asymmetric design).</li>
<li><span class="math inline">\(D_\phi\)</span> (lightweight decoder): reconstructs <strong>masked</strong> tokens from encoder latents <span class="math inline">\(+\)</span> mask tokens.</li>
<li><span class="math inline">\(\hat{X}_{\mathcal{M}}\)</span>: decoder predictions at masked positions.</li>
<li><span class="math inline">\(\tilde{X}_{\mathcal{M}}\)</span>: normalized pixel targets at masked positions.</li>
<li>Loss averages <strong>only over masked patches</strong>, encouraging learning of global structure.</li>
</ul>
<p><strong>Notes.</strong></p>
<ul>
<li>High mask ratio <span class="math inline">\(+\)</span> asymmetric encoder/decoder yields efficient pretraining and strong transfer.</li>
<li>Some variants replace pixel MSE with feature/perceptual losses, but vanilla MAE uses MSE on normalized pixels.</li>
</ul>
</div>
</div>
</div>
<div id="fig-mae-architecture" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mae-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/representation_learning/masked_auto_encoder.png" class="img-fluid figure-img" width="800">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mae-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: <strong>MAE Pre-training Pipeline:</strong> A large fraction of image patches is randomly masked. The encoder processes only visible patches for efficiency. After encoding, lightweight mask tokens are inserted, and a small decoder reconstructs the original image pixels from the full token set. Post-training, the decoder is discarded; the encoder alone extracts features from complete (unmasked) images for downstream tasks.
</figcaption>
</figure>
</div>
</section>
<section id="contrastive-learning" class="level4" data-number="3.3.2">
<h4 data-number="3.3.2" class="anchored" data-anchor-id="contrastive-learning"><span class="header-section-number">3.3.2</span> Contrastive Learning</h4>
<p>Contrastive learning is a learning paradigm whereby a model learns to represent data points such that similar points are nearby and disimilar points are far apart. The <strong>SimCLR</strong> (<span class="citation" data-cites="chen_simple_2020">Chen et al. (<a href="#ref-chen_simple_2020" role="doc-biblioref">2020</a>)</span>) paper demonstrated how such a model can be learned and achieve high-accuracy on down-stream tasks. <a href="#fig-ssl-contrastive" class="quarto-xref">Figure&nbsp;9</a> illustrateds the concepts: From a given image create two <em>views</em> (augmented versions) and contrast them with different images in a large batch of images (InfoNCE loss).</p>
<div class="callout callout-style-default callout-note callout-titled" title="InfoNCE Loss">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>InfoNCE Loss
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Core Idea:</strong> InfoNCE maximizes mutual information between positive pairs while contrasting against negative samples.</p>
<p><strong>Mathematical Form:</strong> <span class="math display">\[
\mathcal{L}_{\text{InfoNCE}} = -\mathbb{E}\left[\log \frac{\exp(\text{sim}(z_i, z_i^+)/\tau)}{\exp(\text{sim}(z_i, z_i^+)/\tau) + \sum_{j=1}^{N-1} \exp(\text{sim}(z_i, z_j^-)/\tau)}\right]
\]</span></p>
<p><strong>Components:</strong></p>
<ul>
<li><span class="math inline">\(z_i\)</span>: Anchor embedding (encoded from image <span class="math inline">\(x_i\)</span>)</li>
<li><span class="math inline">\(z_i^+\)</span>: Positive embedding (different augmentation of same image)</li>
<li><span class="math inline">\(z_j^-\)</span>: Negative embeddings (from different images in batch)</li>
<li><span class="math inline">\(\text{sim}(\cdot, \cdot)\)</span>: Similarity function (typically cosine similarity)</li>
<li><span class="math inline">\(\tau\)</span>: Temperature parameter controlling concentration</li>
</ul>
<p><strong>Temperature Effect:</strong></p>
<ul>
<li>Small <span class="math inline">\(\tau\)</span> (<span class="math inline">\(\to 0\)</span>): Focuses on hardest negatives, sharper distributions</li>
<li>Large <span class="math inline">\(\tau\)</span> (<span class="math inline">\(\to \infty\)</span>): Uniform weighting, softer distributions</li>
<li>Typical values: <span class="math inline">\(\tau \in [0.07, 0.5]\)</span></li>
</ul>
<p><strong>Intuition:</strong> The loss encourages the model to:</p>
<ol type="1">
<li>Pull positive pairs close together in embedding space</li>
<li>Push negative pairs apart</li>
<li>Form tight clusters of semantically similar samples</li>
</ol>
<p><strong>Practical Considerations:</strong></p>
<ul>
<li>Requires large batch sizes (e.g., 256-4096) for sufficient negatives</li>
<li>Alternative: Memory banks (MoCo) or queue-based negatives</li>
<li>Hard negative mining can improve learning efficiency</li>
</ul>
</div>
</div>
</div>
<div id="fig-ssl-contrastive" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ssl-contrastive-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="../../assets/images/representation_learning/self_supervised_contrastive1.png" class="img-fluid figure-img" alt="Contrastive learning conceptual view showing similar and dissimilar samples"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="../../assets/images/representation_learning/self_supervised_contrastive2.png" class="img-fluid figure-img" alt="Contrastive learning architectural view with encoder and projection head"></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ssl-contrastive-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: <strong>Left:</strong> Learning representations by contrasting similar inputs (different views of same sample) against other inputs. <strong>Right:</strong> Architectural view showing the encoder and projection head pipeline.
</figcaption>
</figure>
</div>
</section>
<section id="self-distillation" class="level4" data-number="3.3.3">
<h4 data-number="3.3.3" class="anchored" data-anchor-id="self-distillation"><span class="header-section-number">3.3.3</span> Self-Distillation</h4>
<p>Self-distillation is an interesting concept: A <strong>student</strong> model tries to match the outputs of a <strong>teacher</strong> model’s distribution over a given image. This principle was introduced in the BYOL paper (see <span class="citation" data-cites="grill_bootstrap_2020">Grill et al. (<a href="#ref-grill_bootstrap_2020" role="doc-biblioref">2020</a>)</span>) and led to the DINO-series of models: DINO (<span class="citation" data-cites="caron_emerging_2021">Caron et al. (<a href="#ref-caron_emerging_2021" role="doc-biblioref">2021</a>)</span>), DINOv2 (<span class="citation" data-cites="oquab_dinov2_2024">Oquab et al. (<a href="#ref-oquab_dinov2_2024" role="doc-biblioref">2024</a>)</span>), and DINOv3 (<span class="citation" data-cites="simeoni_dinov3_2025">Siméoni et al. (<a href="#ref-simeoni_dinov3_2025" role="doc-biblioref">2025</a>)</span>) are non-contrastive: a student matches the teacher’s distribution over crops (multi-crop), using centering and sharpening. This yields strong <strong>global descriptors</strong> and high-quality <strong>local (patch) tokens</strong>.</p>
<div class="callout callout-style-default callout-note callout-titled" title="DINOv3 Loss">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>DINOv3 Loss
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Composite objective.</strong> DINOv3 trains a student ViT against an EMA <strong>teacher</strong> with multi-crop views, combining a <strong>global DINO loss</strong>, a <strong>local iBOT loss</strong>, a <strong>KoLeo uniformity regularizer</strong>, and (in a refinement stage) a <strong>Gram anchoring loss</strong> on patch similarities:</p>
<p><span class="math display">\[\begin{align}
\mathcal{L}
&amp;= \underbrace{\mathcal{L}_{\text{DINO}}}_{\text{global crops}}
+ \lambda_{\text{iBOT}} \underbrace{\mathcal{L}_{\text{iBOT}}}_{\text{masked local tokens}}
+ \lambda_{\text{KoLeo}} \underbrace{\mathcal{L}_{\text{KoLeo}}}_{\text{uniformity on }[\text{CLS}]}
+ \lambda_{\text{Gram}} \underbrace{\mathcal{L}_{\text{Gram}}}_{\text{refinement}}.
\end{align}\]</span></p>
<p><strong>Global DINO (distribution matching, no negatives).</strong> Student probabilities <span class="math inline">\(p_s=\mathrm{softmax}(g_s(z_s)/\tau_s)\)</span>; teacher targets <span class="math inline">\(q_t=\mathrm{stopgrad}\!\left[\mathrm{softmax}\!\left((g_t(z_t)-c)/\tau_t\right)\right]\)</span> with centering <span class="math inline">\(c\)</span> and temperatures <span class="math inline">\(\tau_s,\tau_t\)</span>. <span class="math display">\[\begin{equation}
\mathcal{L}_{\text{DINO}} \;=\; - \sum_{k} q_t^{(k)} \log p_s^{(k)}
\quad \text{(summed over pairs of student/teacher crops).}
\end{equation}\]</span></p>
<p><strong>Local iBOT (masked token prediction).</strong> For masked student patches <span class="math inline">\(m \in \mathcal{M}\)</span>, match teacher patch-level distributions: <span class="math display">\[\begin{equation}
\mathcal{L}_{\text{iBOT}} \;=\; - \frac{1}{|\mathcal{M}|}\sum_{m \in \mathcal{M}}\sum_{k} q_{t,m}^{(k)} \log p_{s,m}^{(k)} .
\end{equation}\]</span></p>
<p><strong>KoLeo (feature uniformity).</strong> Encourages spread (near-uniform) class-token embeddings; implemented as the KoLeo entropy estimator on mini-batches (schematic form): <span class="math display">\[\begin{equation}
\mathcal{L}_{\text{KoLeo}} \;\approx\; -\frac{1}{N}\sum_{i=1}^{N} \log d_i
\quad \text{with } d_i \text{ the NN distance of } z^{[\text{CLS}]}_i .
\end{equation}\]</span></p>
<p><strong>Gram anchoring (refinement step).</strong> Align student <strong>patch-similarity structure</strong> to a “Gram teacher.” Let <span class="math inline">\(\tilde{Z}_s,\tilde{Z}_g \in \mathbb{R}^{P \times D}\)</span> be <span class="math inline">\(\ell_2\)</span>-normalized patch features (rows) for student and Gram-teacher on a global crop. With Gram matrices <span class="math inline">\(G(\tilde{Z})=\tilde{Z}\tilde{Z}^\top\)</span>: <span class="math display">\[\begin{equation}
\mathcal{L}_{\text{Gram}} \;=\; \big\| G(\tilde{Z}_s) - G(\tilde{Z}_g) \big\|_F^2 .
\end{equation}\]</span></p>
<p><strong>Notes.</strong></p>
<ul>
<li>Teacher is an EMA of the student; teachers view only <strong>global crops</strong>, students see <strong>global + local</strong> crops; iBOT masks a subset of student patches.</li>
<li>Gram anchoring is applied in a late <strong>refinement</strong> phase to restore <strong>patch-level consistency</strong> without hurting global performance.</li>
</ul>
</div>
</div>
</div>
<div id="fig-dino-principle" class="quarto-float quarto-figure quarto-figure-center anchored" alt="DINO self-distillation principle showing teacher-student architecture with multi-crop strategy">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-dino-principle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/representation_learning/dino_principle.png" class="img-fluid figure-img" alt="DINO self-distillation principle showing teacher-student architecture with multi-crop strategy" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dino-principle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: The student network processes various crops (global and local views) of the same image, attempting to match the output distribution of the momentum-updated teacher network. The teacher processes only global views and uses centering and sharpening to avoid collapse. No explicit negatives are required; the model learns by aligning predictions across different augmented views.
</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="DINOv3 Overview Video">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>DINOv3 Overview Video
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Watch Meta’s introduction to DINOv3, explaining the key improvements over DINOv2 and the self-distillation approach:</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/-eOYWK6m3i8" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
</div>
</div>
</section>
</section>
<section id="global-vs-local-representations" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="global-vs-local-representations"><span class="header-section-number">3.4</span> Global vs Local Representations</h3>
<p><strong>Global</strong> representations summarize an image into a single embedding (e.g., a [CLS] token or pooled feature) that captures overall semantics.</p>
<p><strong>CLIP</strong> is a prototypical global model: it aligns an image-wide vector with a text embedding, which is ideal for <em>zero-shot classification</em>, <em>retrieval</em>, and broad <em>semantic similarity</em> at scale.</p>
<p>In contrast, the DINO family (incl.&nbsp;DINO/DINOv2/DINOv3) produces both strong global descriptors and rich local tokens via self-distillation and multi-crop training. These local features carry spatially resolved cues useful for tasks such as segmentation or obejct detection. The global token remains competitive for classification and retrieval.</p>
<div id="fig-dinov3-local-features" class="quarto-float quarto-figure quarto-figure-center anchored" alt="DINOv3 local feature similarity visualization showing query points and their cosine similarity maps">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-dinov3-local-features-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/representation_learning/dinov3_local_cosine_similarity_example.png" class="img-fluid figure-img" alt="DINOv3 local feature similarity visualization showing query points and their cosine similarity maps" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dinov3-local-features-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: Shown are the cosine similarities of feature embeddings at different locations across the input image (middle) for different query points (shown as red crosses). The visualization highlights semantic consistency across the image.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="comparing-representations" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="comparing-representations"><span class="header-section-number">4</span> Comparing Representations</h2>
<p>The following illustrates different representations for the images in <a href="#fig-images-overview" class="quarto-xref">Figure&nbsp;12</a>.</p>
<div id="fig-images-overview" class="quarto-float quarto-figure quarto-figure-center anchored" alt="Example images to compare different representations">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-images-overview-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/representation_learning/images_overview.png" class="img-fluid figure-img" style="width:100.0%" alt="Example images to compare different representations">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-images-overview-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: Example images to compare different representations.
</figcaption>
</figure>
</div>
<p>In <a href="#fig-rl-similarity-resnet" class="quarto-xref">Figure&nbsp;14</a>, <a href="#fig-rl-similarity-clip" class="quarto-xref">Figure&nbsp;15</a>, and <a href="#fig-rl-similarity-dinov2" class="quarto-xref">Figure&nbsp;16</a> we show cosine similarities between normalized representations and compare to pixel-space similarity (<a href="#fig-rl-similarity-pixel-space" class="quarto-xref">Figure&nbsp;13</a>).</p>
<p><strong>Similarity Metrics:</strong></p>
<p>For images represented as vectors <span class="math inline">\(\mathbf{z}_i, \mathbf{z}_j \in \mathbb{R}^d\)</span>:</p>
<ul>
<li><p><strong>Euclidean Distance (Pixel Space):</strong> <span class="math display">\[d_{\text{L2}}(\mathbf{z}_i, \mathbf{z}_j) = \|\mathbf{z}_i - \mathbf{z}_j\|_2 = \sqrt{\sum_{k=1}^d (z_i^{(k)} - z_j^{(k)})^2}\]</span></p></li>
<li><p><strong>Cosine Similarity:</strong> <span class="math display">\[\text{sim}_{\cos}(\mathbf{z}_i, \mathbf{z}_j) = \frac{\mathbf{z}_i^\top \mathbf{z}_j}{\|\mathbf{z}_i\|_2 \|\mathbf{z}_j\|_2} = \frac{\sum_{k=1}^d z_i^{(k)} z_j^{(k)}}{\sqrt{\sum_{k=1}^d (z_i^{(k)})^2} \sqrt{\sum_{k=1}^d (z_j^{(k)})^2}}\]</span></p></li>
</ul>
<p>where <span class="math inline">\(\text{sim}_{\cos} \in [-1, 1]\)</span> with 1 indicating perfect alignment, 0 indicating orthogonality, and -1 indicating opposite directions.</p>
<div class="tabset-margin-container"></div><div id="fig-similarity-comparison-models" class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true" href="" aria-current="page">Pixel Space</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false" href="">ResNet-50</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-3" role="tab" aria-controls="tabset-1-3" aria-selected="false" href="">CLIP</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-4-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-4" role="tab" aria-controls="tabset-1-4" aria-selected="false" href="">DINOv2</a></li></ul>
<div id="fig-similarity-comparison-models" class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<div id="fig-rl-similarity-pixel-space" class="quarto-float quarto-figure quarto-figure-center anchored" alt="Pixel-space similarity matrix">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rl-similarity-pixel-space-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/representation_learning/similarity_pixel_space.png" class="img-fluid figure-img" alt="Pixel-space similarity matrix" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rl-similarity-pixel-space-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13: Raw pixel-level similarity fails to capture semantic relationships.
</figcaption>
</figure>
</div>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<div id="fig-rl-similarity-resnet" class="quarto-float quarto-figure quarto-figure-center anchored" alt="ResNet-50 supervised representation similarity">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rl-similarity-resnet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/representation_learning/similarity_resnet50.png" class="img-fluid figure-img" alt="ResNet-50 supervised representation similarity" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rl-similarity-resnet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14: Supervised CNN representations group semantically similar images.
</figcaption>
</figure>
</div>
</div>
<div id="tabset-1-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-3-tab">
<div id="fig-rl-similarity-clip" class="quarto-float quarto-figure quarto-figure-center anchored" alt="CLIP multimodal representation similarity">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rl-similarity-clip-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/representation_learning/similarity_clip.png" class="img-fluid figure-img" alt="CLIP multimodal representation similarity" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rl-similarity-clip-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15: Language-aligned embeddings from CLIP show text-driven semantic structure.
</figcaption>
</figure>
</div>
</div>
<div id="tabset-1-4" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-4-tab">
<div id="fig-rl-similarity-dinov2" class="quarto-float quarto-figure quarto-figure-center anchored" alt="DINOv2 self-supervised representation similarity">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rl-similarity-dinov2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/representation_learning/similarity_dinov2.png" class="img-fluid figure-img" alt="DINOv2 self-supervised representation similarity" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rl-similarity-dinov2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16: Self-distilled DINOv2 features reveal fine-grained semantic clusters.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Question: Which Representations Do You Favour? And Why?">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Question: Which Representations Do You Favour? And Why?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>Looking at the similarity matrices, which representation do you prefer and why?</p>
<details>
<summary>
Click for Discussion
</summary>
<p><strong>Pixel Space (<a href="#fig-rl-similarity-pixel-space" class="quarto-xref">Figure&nbsp;13</a>):</strong></p>
<ul>
<li>Not suitable for any semantic task</li>
<li>Captures low-level appearance variations (lighting, pose, exact pixel alignment)</li>
<li>Cannot distinguish semantic categories from superficial similarities</li>
</ul>
<p><strong>ResNet-50 Supervised (<a href="#fig-rl-similarity-resnet" class="quarto-xref">Figure&nbsp;14</a>):</strong></p>
<ul>
<li>Much better than pixel space but still limited</li>
<li>Shows semantic grouping (elephants cluster)</li>
<li>Trained on ImageNet classes, may not transfer optimally to fine-grained tasks</li>
<li>Some sensitivity to lightning (elephant night vs day)</li>
</ul>
<p><strong>CLIP Multimodal (<a href="#fig-rl-similarity-clip" class="quarto-xref">Figure&nbsp;15</a>):</strong></p>
<ul>
<li>Good semantic structure aligned with natural language concepts</li>
<li>Clear separation between major categories (animals vs.&nbsp;vehicles)</li>
<li>Limitation: Lower differentiation within categories</li>
<li>Best for: Broad semantic retrieval and zero-shot open-vocabulary tasks</li>
<li>Trade-off: Language alignment may sacrifice fine-grained visual distinctions</li>
</ul>
<p><strong>DINOv2 Self-Supervised (<a href="#fig-rl-similarity-dinov2" class="quarto-xref">Figure&nbsp;16</a>):</strong></p>
<ul>
<li>Sharp discrimination between semantic classes (Elephant vs Giraffe)</li>
<li>Strong within-category structure (elephant day variants)</li>
<li>Clear night/day distinctions while preserving identity</li>
<li>Best for: Few-shot learning, fine-grained classification, dense prediction tasks</li>
<li>Pure vision objective without language bias preserves visual details</li>
</ul>
<p><strong>Recommendation hierarchy:</strong></p>
<ol type="1">
<li><strong>DINOv2/v3</strong>: Default choice for vision-centric tasks requiring discriminative power</li>
<li><strong>CLIP</strong>: When zero-shot capabilities or text-image alignment is critical</li>
<li><strong>Supervised CNN</strong>: When computation is limited and broad categories suffice</li>
<li><strong>Pixel space</strong>: Never for semantic tasks (only for exact duplicate detection)</li>
</ol>
</details>
</div>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled" title="Task-Relevant Invariance">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Warning</span>Task-Relevant Invariance
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Critical consideration:</strong> Invariance to task-relevant attributes can render representations useless.</p>
<p><strong>Examples of harmful invariance:</strong></p>
<ul>
<li><strong>Color-invariant features for flower classification:</strong> If representations discard color information, distinguishing between red and yellow tulips becomes impossible.</li>
<li><strong>Rotation-invariant features for orientation tasks:</strong> Medical imaging often requires preserving anatomical orientation; rotation invariance destroys critical diagnostic information.</li>
<li><strong>Scale-invariant features for size estimation:</strong> Object measurement tasks require preserving absolute or relative size cues.</li>
<li><strong>Texture-invariant features for material classification:</strong> Distinguishing wood from metal requires texture sensitivity.</li>
</ul>
<p><strong>Design principle:</strong> Match your representation’s invariances to your task requirements. When in doubt, preserve information and let the downstream model learn to ignore irrelevant variations.</p>
</div>
</div>
</section>
<section id="using-representations-a-practical-guide" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="using-representations-a-practical-guide"><span class="header-section-number">5</span> Using Representations: A Practical Guide</h2>
<p>Always begin with the simplest approach and only add complexity when performance gaps justify it. For <strong>classification/retrieval</strong>, global embeddings (e.g., CLIP) enable <strong>zero-/few-shot</strong> learning. For <strong>dense tasks</strong> (segmentation, object detection), <strong>local (patch) tokens</strong> from ViTs (e.g., DINOv3) feed pixel-aligned heads.</p>
<p>Typical progression:</p>
<ol type="1">
<li><p><strong>k-NN baseline (no training)</strong> Think of this as “find the most similar examples.” We store the embeddings of labeled images and, for a new image, look up the <em>k</em> most similar ones; the majority label wins. It’s fast to set up, needs <strong>no training</strong>, and gives a quick reality check on representation quality. See <a href="#fig-rl-practical-knn" class="quarto-xref">Figure&nbsp;17</a> for illustrative code.</p></li>
<li><p><strong>Linear probe (freeze encoder, train head)</strong> Keep the powerful pretrained encoder fixed and train a tiny linear layer on top. If this small layer reaches good accuracy, it means the embeddings already separate classes well. It’s <strong>cheap, stable</strong>, and a great first serious baseline. See <a href="#fig-rl-practical-linear" class="quarto-xref">Figure&nbsp;18</a> for illustrative code.</p></li>
<li><p><strong>LoRA/Adapters (efficient parameter tuning)</strong> Instead of updating all model weights, add small “adapter” modules (or low-rank updates) and train <strong>only those</strong>. You keep most of the original knowledge while adapting to your task with far fewer trainable parameters—<strong>faster, lighter, and less prone to overfitting</strong> than full fine-tuning. See <a href="#fig-rl-practical-adapter" class="quarto-xref">Figure&nbsp;19</a> for illustrative code.</p></li>
<li><p><strong>Partial fine-tune (unfreeze top layers)</strong> Unfreeze just the <strong>last few layers</strong> of the encoder and train them (plus the head). This lets the model adjust higher-level features to your data while keeping the earlier, general features intact. It’s a good <strong>middle ground</strong> between speed and task fit. See <a href="#fig-rl-practical-fine-tune" class="quarto-xref">Figure&nbsp;20</a> for illustrative code.</p></li>
<li><p><strong>Full fine-tune (last resort)</strong> Train <strong>all</strong> the model’s weights on your dataset. This can yield the best alignment to your task but is <strong>computationally expensive</strong> and risks overfitting—especially with limited data. Use it when you have enough data/compute and simpler options plateau. See <a href="#fig-rl-practical-full-fine-tune" class="quarto-xref">Figure&nbsp;21</a> for illustrative code.</p></li>
</ol>
<div id="fig-rl-practical-knn" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="3">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rl-practical-knn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2" data-lst-cap="k-NN baseline (no training) — Hugging Face2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch, sklearn.neighbors <span class="im">as</span> sk</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModel, AutoImageProcessor</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>model_id <span class="op">=</span> <span class="st">"google/vit-base-patch16-224"</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>enc <span class="op">=</span> AutoModel.from_pretrained(model_id).<span class="bu">eval</span>()</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>proc <span class="op">=</span> AutoImageProcessor.from_pretrained(model_id)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> feats(imgs):  <span class="co"># imgs: list of PIL images</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    px <span class="op">=</span> proc(imgs, return_tensors<span class="op">=</span><span class="st">"pt"</span>)[<span class="st">"pixel_values"</span>]</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> enc(pixel_values<span class="op">=</span>px).last_hidden_state[:,<span class="dv">0</span>]  <span class="co"># [CLS]</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> z.cpu().numpy()</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>Xtr <span class="op">=</span> feats(train_images)<span class="op">;</span> Ytr <span class="op">=</span> train_labels</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>Xte <span class="op">=</span> feats(test_images)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>knn <span class="op">=</span> sk.KNeighborsClassifier(n_neighbors<span class="op">=</span><span class="dv">5</span>).fit(Xtr, Ytr)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>pred <span class="op">=</span> knn.predict(Xte)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-rl-practical-knn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17
</figcaption>
</figure>
</div>
<div id="fig-rl-practical-linear" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="4">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rl-practical-linear-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn, optim</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> timm</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>backbone <span class="op">=</span> timm.create_model(<span class="st">"vit_base_patch16_224"</span>, pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> p <span class="kw">in</span> backbone.parameters():</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    p.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>embed_dim <span class="op">=</span> backbone.num_features</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>head <span class="op">=</span> nn.Linear(embed_dim, num_classes)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(images):</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> backbone.forward_features(images)[:,<span class="dv">0</span>]  <span class="co"># [CLS]</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> head(z)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>opt <span class="op">=</span> optim.AdamW(head.parameters(), lr<span class="op">=</span><span class="fl">5e-4</span>)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="co"># training loop: logits = forward(images); loss = CE(logits, y); loss.backward(); opt.step()</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-rl-practical-linear-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;18
</figcaption>
</figure>
</div>
<div id="fig-rl-practical-adapter" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="5">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rl-practical-adapter-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModel</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> peft <span class="im">import</span> LoraConfig, get_peft_model</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>model_id <span class="op">=</span> <span class="st">"google/vit-base-patch16-224"</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModel.from_pretrained(model_id)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> LoraConfig(</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    r<span class="op">=</span><span class="dv">16</span>, lora_alpha<span class="op">=</span><span class="dv">32</span>, lora_dropout<span class="op">=</span><span class="fl">0.05</span>,</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    target_modules<span class="op">=</span>[<span class="st">"query"</span>,<span class="st">"key"</span>,<span class="st">"value"</span>,<span class="st">"fc"</span>],  <span class="co"># adjust to module names</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    bias<span class="op">=</span><span class="st">"none"</span>, task_type<span class="op">=</span><span class="st">"FEATURE_EXTRACTION"</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> get_peft_model(model, config)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>head <span class="op">=</span> torch.nn.Linear(model.config.hidden_size, num_classes)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(pixel_values):</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> model(pixel_values<span class="op">=</span>pixel_values).last_hidden_state[:,<span class="dv">0</span>]</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> head(z)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimizer over LoRA params + head params only</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-rl-practical-adapter-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;19
</figcaption>
</figure>
</div>
<div id="fig-rl-practical-fine-tune" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="6">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rl-practical-fine-tune-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> timm, torch</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> timm.create_model(<span class="st">"vit_base_patch16_224"</span>, pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Freeze everything</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> p <span class="kw">in</span> model.parameters():</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    p.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Unfreeze top transformer block(s)</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> p <span class="kw">in</span> model.blocks[<span class="op">-</span><span class="dv">2</span>:].parameters():</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    p.requires_grad <span class="op">=</span> <span class="va">True</span>  <span class="co"># last 2 blocks</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Replace / add classification head</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>model.head <span class="op">=</span> nn.Linear(model.num_features, num_classes)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimizer over unfrozen blocks + head</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>trainable <span class="op">=</span> [p <span class="cf">for</span> p <span class="kw">in</span> model.parameters() <span class="cf">if</span> p.requires_grad]</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>opt <span class="op">=</span> torch.optim.AdamW(trainable, lr<span class="op">=</span><span class="fl">1e-4</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-rl-practical-fine-tune-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;20
</figcaption>
</figure>
</div>
<div id="fig-rl-practical-full-fine-tune" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="7">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rl-practical-full-fine-tune-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForImageClassification</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>model_id <span class="op">=</span> <span class="st">"google/vit-base-patch16-224"</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForImageClassification.from_pretrained(</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    model_id, num_labels<span class="op">=</span>num_classes, ignore_mismatched_sizes<span class="op">=</span><span class="va">True</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> p <span class="kw">in</span> model.parameters():</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    p.requires_grad <span class="op">=</span> <span class="va">True</span>  <span class="co"># unfreeze all</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>opt <span class="op">=</span> torch.optim.AdamW(model.parameters(), lr<span class="op">=</span><span class="fl">5e-5</span>)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Standard supervised loop with processor(...)-&gt; pixel_values -&gt; model(pixel_values, labels=y)</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-rl-practical-full-fine-tune-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;21
</figcaption>
</figure>
</div>
</section>
<section id="quiz" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="quiz"><span class="header-section-number">6</span> Quiz</h2>
<div class="callout callout-style-default callout-note callout-titled" title="Question 1: Understanding Representation Learning">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Question 1: Understanding Representation Learning
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Question:</strong> Why do representations learned from raw pixels (pixel-space similarity) fail to capture semantic relationships between images?</p>
<details>
<summary>
Click for Answer
</summary>
<p><strong>Answer:</strong> Pixel-space representations measure low-level appearance differences (exact color values, lighting conditions, spatial alignment) rather than high-level semantic content. Two images of the same object under different lighting or poses will have large pixel-level distances despite being semantically identical. For example, in <a href="#fig-rl-similarity-pixel-space" class="quarto-xref">Figure&nbsp;13</a>, pixel distances cannot distinguish between “different elephants in similar lighting” vs “same elephant in different lighting”: both create similar pixel-level differences. Good representations compress the enormous space of possible images (<span class="math inline">\(255^{H \times W \times C}\)</span> combinations) into a semantic coordinate system where distances reflect meaning.</p>
</details>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Question 2: Invariance and Selectivity Trade-off">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Question 2: Invariance and Selectivity Trade-off
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Question:</strong> Explain the invariance-selectivity trade-off in representation learning. Provide one concrete example where too much invariance would harm performance on a specific task.</p>
<details>
<summary>
Click for Answer
</summary>
<p><strong>Answer:</strong></p>
<p><strong>The Trade-off:</strong> Good representations must balance two competing goals:</p>
<ul>
<li><strong>Invariance:</strong> <span class="math inline">\(\|f(\mathbf{x}) - f(g(\mathbf{x}))\|_2 &lt; \epsilon\)</span> for nuisance transforms <span class="math inline">\(g\)</span> (lighting, pose changes)</li>
<li><strong>Selectivity:</strong> <span class="math inline">\(\|f(\mathbf{x}_i) - f(\mathbf{x}_j)\|_2 &gt; \delta\)</span> for semantically different inputs</li>
</ul>
<p><strong>Concrete Example - Flower Classification:</strong> If representations are completely <strong>color-invariant</strong> (treating all colors as identical), distinguishing between red roses and yellow roses becomes impossible, even though color is the primary discriminative feature. The model would collapse all flower images to similar embeddings regardless of color, destroying task-critical information.</p>
<p><strong>Other Examples:</strong></p>
<ul>
<li><strong>Medical imaging:</strong> Rotation invariance destroys anatomical orientation information needed for diagnosis</li>
<li><strong>Object size estimation:</strong> Scale invariance removes absolute/relative size cues</li>
<li><strong>Material classification:</strong> Texture invariance prevents distinguishing wood from metal</li>
</ul>
<p><strong>Design Principle:</strong> Match invariances to task requirements — preserve information the downstream task needs, discard only true nuisance factors.</p>
</details>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Question 3: Comparing SSL Objectives">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Question 3: Comparing SSL Objectives
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Question:</strong> What is the key advantage of self-distillation methods (DINO/DINOv2/DINOv3) over contrastive learning methods (SimCLR)?</p>
<details>
<summary>
Click for Answer
</summary>
<p><strong>Answer:</strong></p>
<p><strong>Key Advantage:</strong> Self-distillation does <strong>not require explicit negative samples</strong> or large batch sizes.</p>
<p><strong>Contrastive Learning (SimCLR):</strong></p>
<ul>
<li>Requires large batches (256-4096) to provide sufficient negative pairs</li>
<li>InfoNCE loss: <span class="math inline">\(\mathcal{L} = -\log \frac{\exp(\text{sim}(z_i, z_i^+)/\tau)}{\exp(\text{sim}(z_i, z_i^+)/\tau) + \sum_{j=1}^{N-1} \exp(\text{sim}(z_i, z_j^-)/\tau)}\)</span></li>
<li>Computational cost and memory scale with batch size</li>
<li>Needs careful negative mining strategies</li>
</ul>
<p><strong>Self-Distillation (DINO):</strong></p>
<ul>
<li>Student network matches teacher’s probability distribution over crops</li>
<li>Teacher is EMA of student weights (no separate training)</li>
<li>Emergent clustering arises from teacher-student dynamics and centering/sharpening</li>
<li>Works with smaller batches, more memory-efficient</li>
<li><strong>Bonus:</strong> Produces both strong global descriptors AND high-quality local (patch) tokens</li>
</ul>
<p><strong>Result:</strong> DINO-family models achieve state-of-the-art transfer performance with simpler training dynamics and lower computational requirements.</p>
</details>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Question 4: Global vs Local Representations">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Question 4: Global vs Local Representations
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Question:</strong> When should you use global representations versus local representations? Give one specific task example for each.</p>
<details>
<summary>
Click for Answer
</summary>
<p><strong>Answer:</strong></p>
<p><strong>Global Representations</strong> (single embedding per image, e.g., [CLS] token):</p>
<ul>
<li><strong>Use when:</strong> Task requires image-level understanding without spatial localization</li>
<li><strong>Examples:</strong>
<ul>
<li><strong>Image classification:</strong> “Is this a cat or dog?” — only need overall category</li>
<li><strong>Image retrieval:</strong> “Find similar vacation photos” — semantic similarity at image level</li>
<li><strong>Zero-shot classification (CLIP):</strong> “Does this image contain a beach?” — image-text alignment</li>
</ul></li>
</ul>
<p><strong>Local Representations</strong> (spatially-resolved patch tokens, e.g., DINOv3 patch features):</p>
<ul>
<li><strong>Use when:</strong> Task requires pixel-level or region-level understanding</li>
<li><strong>Examples:</strong>
<ul>
<li><strong>Semantic segmentation:</strong> “Label every pixel as road/car/pedestrian” — need spatial structure</li>
<li><strong>Object detection:</strong> “Find all bounding boxes containing cars” — need localization</li>
<li><strong>Dense prediction:</strong> “Generate depth map for each pixel” — requires preserved spatial information</li>
</ul></li>
</ul>
<p><strong>Key Difference:</strong> Global embeddings through pooling (e.g., Global Average Pooling: <span class="math inline">\(\text{GAP}(X) = \frac{1}{H \times W}\sum_{i,j} X_{i,j}\)</span>) discard spatial structure for efficiency, while local features maintain the spatial grid of patch embeddings.</p>
<p><strong>Practical Tip:</strong> Many modern architectures (DINOv3, ViTs) provide <strong>both</strong> — use the [CLS] token for global tasks and patch tokens for dense tasks.</p>
</details>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Question 5: Adaptation Strategy Selection">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Question 5: Adaptation Strategy Selection
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Question:</strong> You have a dataset of 1,000 labeled medical images for a specialized classification task. Describe the recommended progression of adaptation strategies and explain why you should start with the simplest approach.</p>
<details>
<summary>
Click for Answer
</summary>
<p><strong>Answer:</strong></p>
<p><strong>Recommended Progression (from simplest to most complex):</strong></p>
<ol type="1">
<li><strong>k-NN Baseline (0 training):</strong>
<ul>
<li>Extract features with frozen pretrained model</li>
<li>Classify by finding k nearest neighbors in feature space</li>
<li><strong>Why first:</strong> Instant results, no hyperparameters, reveals representation quality</li>
<li><strong>Code:</strong> Just <code>sklearn.KNeighborsClassifier(n_neighbors=5).fit(features, labels)</code></li>
</ul></li>
<li><strong>Linear Probe (~1-10K parameters):</strong>
<ul>
<li>Freeze encoder, train only linear classification head</li>
<li><strong>Why next:</strong> If this works well, embeddings already separate classes</li>
<li><strong>Benefit:</strong> Fast, stable, minimal overfitting risk with 1K samples</li>
</ul></li>
<li><strong>LoRA/Adapters (~100K-1M parameters):</strong>
<ul>
<li>Add small adapter modules, train only those</li>
<li><strong>Why:</strong> Allows task-specific adaptation without full fine-tuning</li>
<li><strong>Trade-off:</strong> Better fit than linear probe, much cheaper than full fine-tuning</li>
</ul></li>
<li><strong>Partial Fine-tune (last 2-3 layers):</strong>
<ul>
<li>Unfreeze top transformer blocks + head</li>
<li><strong>Why:</strong> Adjust high-level features while keeping general low-level features</li>
<li><strong>Risk:</strong> Need careful learning rate tuning to avoid catastrophic forgetting</li>
</ul></li>
<li><strong>Full Fine-tune (all parameters) — Last Resort:</strong>
<ul>
<li><strong>Only if:</strong> Previous methods plateau AND you have sufficient data</li>
<li><strong>Risk:</strong> With only 1K samples, high overfitting risk</li>
<li><strong>Mitigation:</strong> Strong regularization, small learning rates (1e-5), early stopping</li>
</ul></li>
</ol>
<p><strong>Why Start Simple:</strong></p>
<ul>
<li><strong>Diagnostic value:</strong> If k-NN works, you know the representation is good</li>
<li><strong>Resource efficiency:</strong> Avoid wasting compute on complex methods when simple ones suffice</li>
<li><strong>Overfitting prevention:</strong> With limited data (1K samples), simpler models generalize better</li>
<li><strong>Baseline establishment:</strong> Each step provides a performance ceiling to beat</li>
</ul>
<p><strong>Medical Imaging Specific:</strong> Consider domain shift — medical images may differ significantly from natural images (ImageNet). If k-NN/linear probe fail, the pretrained features may not transfer well, suggesting need for more adaptation or domain-specific pretraining.</p>
</details>
</div>
</div>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-caron_emerging_2021" class="csl-entry" role="listitem">
Caron, Mathilde, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. 2021. <span>“Emerging <span>Properties</span> in <span>Self</span>-<span>Supervised</span> <span>Vision</span> <span>Transformers</span>.”</span> <a href="https://doi.org/10.48550/arXiv.2104.14294">https://doi.org/10.48550/arXiv.2104.14294</a>.
</div>
<div id="ref-chen_simple_2020" class="csl-entry" role="listitem">
Chen, Ting, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. <span>“A <span>Simple</span> <span>Framework</span> for <span>Contrastive</span> <span>Learning</span> of <span>Visual</span> <span>Representations</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2002.05709">https://doi.org/10.48550/arXiv.2002.05709</a>.
</div>
<div id="ref-grill_bootstrap_2020" class="csl-entry" role="listitem">
Grill, Jean-Bastien, Florian Strub, Florent Altché, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, et al. 2020. <span>“Bootstrap Your Own Latent: <span>A</span> New Approach to Self-Supervised <span>Learning</span>.”</span> arXiv. <a href="http://arxiv.org/abs/2006.07733">http://arxiv.org/abs/2006.07733</a>.
</div>
<div id="ref-he_masked_2021" class="csl-entry" role="listitem">
He, Kaiming, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. 2021. <span>“Masked <span>Autoencoders</span> <span>Are</span> <span>Scalable</span> <span>Vision</span> <span>Learners</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2111.06377">https://doi.org/10.48550/arXiv.2111.06377</a>.
</div>
<div id="ref-oquab_dinov2_2024" class="csl-entry" role="listitem">
Oquab, Maxime, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, et al. 2024. <span>“<span>DINOv2</span>: <span>Learning</span> <span>Robust</span> <span>Visual</span> <span>Features</span> Without <span>Supervision</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2304.07193">https://doi.org/10.48550/arXiv.2304.07193</a>.
</div>
<div id="ref-radford_learning_2021" class="csl-entry" role="listitem">
Radford, Alec, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, et al. 2021. <span>“Learning <span>Transferable</span> <span>Visual</span> <span>Models</span> <span>From</span> <span>Natural</span> <span>Language</span> <span>Supervision</span>.”</span> <em>arXiv:2103.00020 [Cs]</em>, February. <a href="http://arxiv.org/abs/2103.00020">http://arxiv.org/abs/2103.00020</a>.
</div>
<div id="ref-simeoni_dinov3_2025" class="csl-entry" role="listitem">
Siméoni, Oriane, Huy V. Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, et al. 2025. <span>“<span>DINOv3</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2508.10104">https://doi.org/10.48550/arXiv.2508.10104</a>.
</div>
</div>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/marco-willi\.github\.io\/cas-dl-compvis-lectures-hs2025\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../pages/lectures/classification.html" class="pagination-link" aria-label="3 - Image Classification">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">3 - Image Classification</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../pages/lectures/practical.html" class="pagination-link" aria-label="5 - Practical">
        <span class="nav-page-text">5 - Practical</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "4 - Representation Learning"</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="an">params:</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co">   images_path: "/assets/images/representation_learning/"</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title="Learning Objectives" collapse="true"}</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>By the end of this lecture you should be able to:</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Define** representation learning and explain why transforming raw pixels into semantic embeddings enables efficient downstream task performance.</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Understand** the balance between invariance (robustness to nuisance factors) and selectivity (discriminating semantically distinct inputs) in learned representations.</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Know** supervised, self-supervised (contrastive, masked autoencoding, self-distillation), and weakly supervised (vision-language) learning paradigms.</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Distinguish** between global representations (single embedding per image) and local representations (spatially-resolved patch tokens) and identify appropriate use cases for each.</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Select** appropriate adaptation strategies (k-NN → linear probe → adapters/LoRA → partial fine-tuning → full fine-tuning) based on dataset size, computational resources, and task requirements.</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Justify** model selection between DINOv3, CLIP, MAE, and supervised models for specific downstream tasks considering training objectives, data modalities, and few-shot vs zero-shot requirements.</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Apply** practical transfer learning workflows using Hugging Face Transformers and timm libraries for real-world computer vision tasks.</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title="TLDR Recap" collapse="true"}</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>Short summary:</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Representation learning** transforms high-dimensional raw data into compact, semantically meaningful embeddings where distances reflect task-relevant similarities, enabling efficient transfer to downstream tasks.</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Three learning paradigms** yield distinct representations: (1) **Supervised** learning on labeled data (e.g., ImageNet ResNet) produces category-aligned features, (2) **Self-supervised** methods (SimCLR, MAE, DINO/v2/v3) learn without labels via contrastive learning, masked reconstruction, or self-distillation, (3) **Weakly supervised** approaches (CLIP) align vision and language for zero-shot capabilities.</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Global vs local representations**: Global embeddings (e.g., <span class="co">[</span><span class="ot">CLS</span><span class="co">]</span> token) capture image-level semantics ideal for classification/retrieval; local patch tokens preserve spatial structure essential for dense prediction tasks like segmentation and detection.</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Practical adaptation hierarchy**: Start simple and add complexity only when needed: (1) k-NN baseline (no training), (2) Linear probe (freeze encoder), (3) LoRA/Adapters (parameter-efficient), (4) Partial fine-tuning (unfreeze top layers), (5) Full fine-tuning (last resort for large datasets).</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Model selection heuristics**: Use **DINOv3** for discriminative vision tasks and few-shot learning, **CLIP** for zero-shot classification and vision-language alignment, **MAE** when reconstruction pretraining aligns with your task, and **supervised CNNs** when computational efficiency is critical.</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Task-relevant invariance** is crucial: Representations must preserve information necessary for downstream tasks (e.g., color for flower classification, orientation for medical imaging) while discarding true nuisance factors.</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>::: {.content-hidden}</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>{{&lt; include /assets/_macros.tex &gt;}}</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a><span class="fu">## Motivation</span></span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>Different properties and challenges, such as the <span class="co">[</span><span class="ot">semantic gap</span><span class="co">](intro.qmd#sec-intro-challenges-semantic-gap)</span>, are reasons modelling images is difficult. In particular extracting generic, semantically meaningful information is hard. Humans, however, excel at this task and can easily and  comprehensively describe images with relatively few bits of information. A good example is @fig-raw-elephant:</span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>::: {#fig-raw-elephant layout-ncol=1}</span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}elephant.jpg)</span>{fig-alt="Raw RGB image used to illustrate pixel-space representations" width=100%}</span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a>Example Image <span class="co">[</span><span class="ot">Source</span><span class="co">](https://unsplash.com/de/fotos/elefantenwanderung-tagsuber-QJbyG6O0ick)</span>.</span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a>It is easy for humans to describe the image. For example:</span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a><span class="co"># | eval: false</span></span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a><span class="co"># | echo: true</span></span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a><span class="co">A photograph of an elephant facing the camera.</span></span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a><span class="co">The elephant has large tusks and ears and is standing in a flat savannah.</span></span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a><span class="co">Is is bright day light with a cloudy sky.</span></span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a>Of course: **a lot** of information is lost, however, **practically** relevant information is preserved!</span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Information Content: Raw vs Natural Language" collapse="true"}</span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true" tabindex="-1"></a>Let's quantify the dramatic compression achieved by semantic description:</span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true" tabindex="-1"></a>**Raw Image (Pixel Representation):**</span>
<span id="cb7-69"><a href="#cb7-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-70"><a href="#cb7-70" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Resolution: 640 × 360 pixels</span>
<span id="cb7-71"><a href="#cb7-71" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Channels: 3 (RGB)</span>
<span id="cb7-72"><a href="#cb7-72" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Bits per pixel: 8 bits per channel</span>
<span id="cb7-73"><a href="#cb7-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-76"><a href="#cb7-76" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-77"><a href="#cb7-77" aria-hidden="true" tabindex="-1"></a><span class="co"># | eval: true</span></span>
<span id="cb7-78"><a href="#cb7-78" aria-hidden="true" tabindex="-1"></a><span class="co"># | echo: false</span></span>
<span id="cb7-79"><a href="#cb7-79" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb7-80"><a href="#cb7-80" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> HTML</span>
<span id="cb7-81"><a href="#cb7-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-82"><a href="#cb7-82" aria-hidden="true" tabindex="-1"></a><span class="co"># Natural language description</span></span>
<span id="cb7-83"><a href="#cb7-83" aria-hidden="true" tabindex="-1"></a>description <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb7-84"><a href="#cb7-84" aria-hidden="true" tabindex="-1"></a><span class="st">A photograph of an elephant facing the camera.</span></span>
<span id="cb7-85"><a href="#cb7-85" aria-hidden="true" tabindex="-1"></a><span class="st">The elephant has large tusks and ears and is standing in a flat savannah.</span></span>
<span id="cb7-86"><a href="#cb7-86" aria-hidden="true" tabindex="-1"></a><span class="st">Is is bright day light with a cloudy sky.</span></span>
<span id="cb7-87"><a href="#cb7-87" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb7-88"><a href="#cb7-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-89"><a href="#cb7-89" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate bits for natural language</span></span>
<span id="cb7-90"><a href="#cb7-90" aria-hidden="true" tabindex="-1"></a>char_count <span class="op">=</span> <span class="bu">len</span>(description)</span>
<span id="cb7-91"><a href="#cb7-91" aria-hidden="true" tabindex="-1"></a>bits_utf8 <span class="op">=</span> char_count <span class="op">*</span> <span class="dv">8</span>  <span class="co"># UTF-8 uses 8 bits per character</span></span>
<span id="cb7-92"><a href="#cb7-92" aria-hidden="true" tabindex="-1"></a>bits_ascii <span class="op">=</span> char_count <span class="op">*</span> <span class="dv">7</span>  <span class="co"># ASCII uses 7 bits per character</span></span>
<span id="cb7-93"><a href="#cb7-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-94"><a href="#cb7-94" aria-hidden="true" tabindex="-1"></a><span class="co"># Raw image stats (from earlier calculation)</span></span>
<span id="cb7-95"><a href="#cb7-95" aria-hidden="true" tabindex="-1"></a>width, height, channels <span class="op">=</span> <span class="dv">640</span>, <span class="dv">360</span>, <span class="dv">3</span></span>
<span id="cb7-96"><a href="#cb7-96" aria-hidden="true" tabindex="-1"></a>raw_bits <span class="op">=</span> width <span class="op">*</span> height <span class="op">*</span> channels <span class="op">*</span> <span class="dv">8</span></span>
<span id="cb7-97"><a href="#cb7-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-98"><a href="#cb7-98" aria-hidden="true" tabindex="-1"></a><span class="co"># Compression ratio</span></span>
<span id="cb7-99"><a href="#cb7-99" aria-hidden="true" tabindex="-1"></a>compression_ratio <span class="op">=</span> raw_bits <span class="op">/</span> bits_utf8</span>
<span id="cb7-100"><a href="#cb7-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-101"><a href="#cb7-101" aria-hidden="true" tabindex="-1"></a><span class="co"># Create comparison table</span></span>
<span id="cb7-102"><a href="#cb7-102" aria-hidden="true" tabindex="-1"></a>comparison_data <span class="op">=</span> pd.DataFrame(</span>
<span id="cb7-103"><a href="#cb7-103" aria-hidden="true" tabindex="-1"></a>    {</span>
<span id="cb7-104"><a href="#cb7-104" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Representation"</span>: [</span>
<span id="cb7-105"><a href="#cb7-105" aria-hidden="true" tabindex="-1"></a>            <span class="st">"Raw RGB Image"</span>,</span>
<span id="cb7-106"><a href="#cb7-106" aria-hidden="true" tabindex="-1"></a>            <span class="st">"Natural Language (UTF-8)"</span>,</span>
<span id="cb7-107"><a href="#cb7-107" aria-hidden="true" tabindex="-1"></a>            <span class="st">"Natural Language (ASCII)"</span>,</span>
<span id="cb7-108"><a href="#cb7-108" aria-hidden="true" tabindex="-1"></a>        ],</span>
<span id="cb7-109"><a href="#cb7-109" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Size (bits)"</span>: [<span class="ss">f"</span><span class="sc">{</span>raw_bits<span class="sc">:,}</span><span class="ss">"</span>, <span class="ss">f"</span><span class="sc">{</span>bits_utf8<span class="sc">:,}</span><span class="ss">"</span>, <span class="ss">f"</span><span class="sc">{</span>bits_ascii<span class="sc">:,}</span><span class="ss">"</span>],</span>
<span id="cb7-110"><a href="#cb7-110" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Size (KB)"</span>: [</span>
<span id="cb7-111"><a href="#cb7-111" aria-hidden="true" tabindex="-1"></a>            <span class="ss">f"</span><span class="sc">{</span>raw_bits <span class="op">/</span> <span class="dv">8</span> <span class="op">/</span> <span class="dv">1024</span><span class="sc">:.1f}</span><span class="ss">"</span>,</span>
<span id="cb7-112"><a href="#cb7-112" aria-hidden="true" tabindex="-1"></a>            <span class="ss">f"</span><span class="sc">{</span>bits_utf8 <span class="op">/</span> <span class="dv">8</span> <span class="op">/</span> <span class="dv">1024</span><span class="sc">:.2f}</span><span class="ss">"</span>,</span>
<span id="cb7-113"><a href="#cb7-113" aria-hidden="true" tabindex="-1"></a>            <span class="ss">f"</span><span class="sc">{</span>bits_ascii <span class="op">/</span> <span class="dv">8</span> <span class="op">/</span> <span class="dv">1024</span><span class="sc">:.2f}</span><span class="ss">"</span>,</span>
<span id="cb7-114"><a href="#cb7-114" aria-hidden="true" tabindex="-1"></a>        ],</span>
<span id="cb7-115"><a href="#cb7-115" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Compression Factor"</span>: [</span>
<span id="cb7-116"><a href="#cb7-116" aria-hidden="true" tabindex="-1"></a>            <span class="st">"1×"</span>,</span>
<span id="cb7-117"><a href="#cb7-117" aria-hidden="true" tabindex="-1"></a>            <span class="ss">f"</span><span class="sc">{</span>compression_ratio<span class="sc">:.0f}</span><span class="ss">×"</span>,</span>
<span id="cb7-118"><a href="#cb7-118" aria-hidden="true" tabindex="-1"></a>            <span class="ss">f"</span><span class="sc">{</span>raw_bits <span class="op">/</span> bits_ascii<span class="sc">:.0f}</span><span class="ss">×"</span>,</span>
<span id="cb7-119"><a href="#cb7-119" aria-hidden="true" tabindex="-1"></a>        ],</span>
<span id="cb7-120"><a href="#cb7-120" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb7-121"><a href="#cb7-121" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-122"><a href="#cb7-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-123"><a href="#cb7-123" aria-hidden="true" tabindex="-1"></a>table_html <span class="op">=</span> comparison_data.to_html(</span>
<span id="cb7-124"><a href="#cb7-124" aria-hidden="true" tabindex="-1"></a>    index<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb7-125"><a href="#cb7-125" aria-hidden="true" tabindex="-1"></a>    border<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb7-126"><a href="#cb7-126" aria-hidden="true" tabindex="-1"></a>    classes<span class="op">=</span><span class="st">"table table-sm table-striped align-middle caption-top"</span>,</span>
<span id="cb7-127"><a href="#cb7-127" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-128"><a href="#cb7-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-129"><a href="#cb7-129" aria-hidden="true" tabindex="-1"></a>HTML(table_html)</span>
<span id="cb7-130"><a href="#cb7-130" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-131"><a href="#cb7-131" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-132"><a href="#cb7-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-133"><a href="#cb7-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-134"><a href="#cb7-134" aria-hidden="true" tabindex="-1"></a>We can also say: By describing the image we **represent** the image with natural language, instead of the raw pixels.</span>
<span id="cb7-135"><a href="#cb7-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-136"><a href="#cb7-136" aria-hidden="true" tabindex="-1"></a>This is exactly what **Representation Learning** is interested in: How can we find compact representations wich are significantly smaller than the raw, original representations while retaining _relevant_ information?</span>
<span id="cb7-137"><a href="#cb7-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-138"><a href="#cb7-138" aria-hidden="true" tabindex="-1"></a>The idea is that such compact representations are well suited for down-stream tasks, such as image classification.</span>
<span id="cb7-139"><a href="#cb7-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-140"><a href="#cb7-140" aria-hidden="true" tabindex="-1"></a>One motivation to represent images in a different space than with raw pixels (data space) is the enormous difference between the set of all *possible* images and the set of *natural* (real-world) images. This indicates that representing images in pixel space is not efficient. As illustrated in @fig-rl-set-of-images, the subset of natural images occupies only a tiny fraction of the entire image space.  For instance, for RGB images of size $256 \times 256$ pixels with 255 possible intensity values per channel, the total number of possible images is $255^{256 \times 256 \times 3}$, an astronomically large number, effectively infinite from a practical standpoint. This enables strong compression if we can find a more suitable space or coordinate system to represent images.</span>
<span id="cb7-141"><a href="#cb7-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-142"><a href="#cb7-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-143"><a href="#cb7-143" aria-hidden="true" tabindex="-1"></a>::: {#fig-rl-set-of-images}</span>
<span id="cb7-144"><a href="#cb7-144" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}set_of_images.png)</span>{fig-alt="Set of example images illustrating representation learning concepts" width=600}</span>
<span id="cb7-145"><a href="#cb7-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-146"><a href="#cb7-146" aria-hidden="true" tabindex="-1"></a>Illustrated are the set of all possible images and the subset of natural images.</span>
<span id="cb7-147"><a href="#cb7-147" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-148"><a href="#cb7-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-149"><a href="#cb7-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-150"><a href="#cb7-150" aria-hidden="true" tabindex="-1"></a><span class="fu">## What Is Representation Learning?</span></span>
<span id="cb7-151"><a href="#cb7-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-152"><a href="#cb7-152" aria-hidden="true" tabindex="-1"></a>A representation is the output $\vect{z} = f(\vect{x})$ produced by an encoder $f$ applied to input $\vect{x}$; $\vect{z}$ retains task‑relevant structure while discarding nuisance variation. The encoder $f$ is the mapping; the representation $\vect{z}$ is its result.</span>
<span id="cb7-153"><a href="#cb7-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-154"><a href="#cb7-154" aria-hidden="true" tabindex="-1"></a>Therefore, when we refer to representations we typically mean vector-valued (distributed) representations and not, for example, natural language descriptions.</span>
<span id="cb7-155"><a href="#cb7-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-156"><a href="#cb7-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-157"><a href="#cb7-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-158"><a href="#cb7-158" aria-hidden="true" tabindex="-1"></a><span class="fu">### Intuition</span></span>
<span id="cb7-159"><a href="#cb7-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-160"><a href="#cb7-160" aria-hidden="true" tabindex="-1"></a>Let's take a look at the (synthetic) images in @fig-rl-examples:</span>
<span id="cb7-161"><a href="#cb7-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-162"><a href="#cb7-162" aria-hidden="true" tabindex="-1"></a>::: {#fig-rl-examples layout-nrow=2}</span>
<span id="cb7-163"><a href="#cb7-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-164"><a href="#cb7-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-165"><a href="#cb7-165" aria-hidden="true" tabindex="-1"></a><span class="al">![Elephant - Day 1]({{&lt; meta params.images_path &gt;}}examples/elephant_day_01.png)</span>{fig-alt="Elephant in daylight savanna"}</span>
<span id="cb7-166"><a href="#cb7-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-167"><a href="#cb7-167" aria-hidden="true" tabindex="-1"></a><span class="al">![Elephant - Day 1]({{&lt; meta params.images_path &gt;}}augmented/elephant_flip_horizontal.png)</span>{fig-alt="Elephant in daylight savanna"}</span>
<span id="cb7-168"><a href="#cb7-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-169"><a href="#cb7-169" aria-hidden="true" tabindex="-1"></a><span class="al">![Elephant - Day 1]({{&lt; meta params.images_path &gt;}}augmented/elephant_brightness_1.8x.png)</span>{fig-alt="Elephant in daylight savanna"}</span>
<span id="cb7-170"><a href="#cb7-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-171"><a href="#cb7-171" aria-hidden="true" tabindex="-1"></a><span class="al">![Elephant - Day 2]({{&lt; meta params.images_path &gt;}}examples/elephant_day_02.png)</span>{fig-alt="Elephant by watering hole in daylight"}</span>
<span id="cb7-172"><a href="#cb7-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-173"><a href="#cb7-173" aria-hidden="true" tabindex="-1"></a><span class="al">![Giraffe - Day]({{&lt; meta params.images_path &gt;}}examples/giraffe_day_01.png)</span>{fig-alt="Giraffe in daylight grassland"}</span>
<span id="cb7-174"><a href="#cb7-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-175"><a href="#cb7-175" aria-hidden="true" tabindex="-1"></a><span class="al">![Rhino - Night]({{&lt; meta params.images_path &gt;}}examples/rhino_night_01.png)</span>{fig-alt="Rhinoceros at nighttime"}</span>
<span id="cb7-176"><a href="#cb7-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-177"><a href="#cb7-177" aria-hidden="true" tabindex="-1"></a><span class="al">![Elephant - Night]({{&lt; meta params.images_path &gt;}}examples/elephant_night_01.png)</span>{fig-alt="Elephant at night with moonlight"}</span>
<span id="cb7-178"><a href="#cb7-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-179"><a href="#cb7-179" aria-hidden="true" tabindex="-1"></a><span class="al">![Car - Day]({{&lt; meta params.images_path &gt;}}examples/car_day_01.png)</span>{fig-alt="Modern car in bright daylight"}</span>
<span id="cb7-180"><a href="#cb7-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-181"><a href="#cb7-181" aria-hidden="true" tabindex="-1"></a>Synthetic images illustrating representation learning challenges.</span>
<span id="cb7-182"><a href="#cb7-182" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-183"><a href="#cb7-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-184"><a href="#cb7-184" aria-hidden="true" tabindex="-1"></a>Imagine you have to represent those in 2-D: $\vect{z} \in \mathbb{R}^2$. How would you proceed from a human point of view?</span>
<span id="cb7-185"><a href="#cb7-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-186"><a href="#cb7-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-187"><a href="#cb7-187" aria-hidden="true" tabindex="-1"></a>::: {#fig-representation-space}</span>
<span id="cb7-188"><a href="#cb7-188" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}representation_space.png)</span>{fig-alt="Empty 2-dimensional representation space" width=50%}</span>
<span id="cb7-189"><a href="#cb7-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-190"><a href="#cb7-190" aria-hidden="true" tabindex="-1"></a>Empty 2-dimensional representation space for mapping image similarity.</span>
<span id="cb7-191"><a href="#cb7-191" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-192"><a href="#cb7-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-193"><a href="#cb7-193" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Question: Representing Images" collapse="false"}</span>
<span id="cb7-194"><a href="#cb7-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-195"><a href="#cb7-195" aria-hidden="true" tabindex="-1"></a>How would you represent the images of @fig-rl-examples in 2 dimensions? Why?</span>
<span id="cb7-196"><a href="#cb7-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-197"><a href="#cb7-197" aria-hidden="true" tabindex="-1"></a>**Hint**:</span>
<span id="cb7-198"><a href="#cb7-198" aria-hidden="true" tabindex="-1"></a>Similar images might should be close to each other. Exact positions do not matter, only distances.</span>
<span id="cb7-199"><a href="#cb7-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-200"><a href="#cb7-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-201"><a href="#cb7-201" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb7-202"><a href="#cb7-202" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Click for result<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb7-203"><a href="#cb7-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-204"><a href="#cb7-204" aria-hidden="true" tabindex="-1"></a>**Key Considerations:**</span>
<span id="cb7-205"><a href="#cb7-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-206"><a href="#cb7-206" aria-hidden="true" tabindex="-1"></a>The images demonstrate different types of similarity challenges in representation learning:</span>
<span id="cb7-207"><a href="#cb7-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-208"><a href="#cb7-208" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Same class, same conditions** (Elephant Day 1 &amp; Elephant Day 2): These are most similar - both elephants photographed in daylight. They exhibit **intra-class variation** while maintaining the same semantic category.</span>
<span id="cb7-209"><a href="#cb7-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-210"><a href="#cb7-210" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Same class, different conditions** (Elephant Day vs. Elephant Night): Moderate similarity - same animal class but dramatically different lighting conditions. This tests **illumination invariance**.</span>
<span id="cb7-211"><a href="#cb7-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-212"><a href="#cb7-212" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Different classes, same conditions** (Elephants, Giraffe, Rhino by day): Animals share similar contexts (daytime, natural habitat) but different semantics, requiring **inter-class discrimination**.</span>
<span id="cb7-213"><a href="#cb7-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-214"><a href="#cb7-214" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Different domain** (Animals vs. Car): Least similar - represents a complete **domain shift** from wildlife to manufactured objects.</span>
<span id="cb7-215"><a href="#cb7-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-216"><a href="#cb7-216" aria-hidden="true" tabindex="-1"></a>**Ideal representation learning:** A good model should group the two daytime elephants closely, maintain reasonable similarity between day/night elephants, distinguish between different animal classes, and clearly separate animals from vehicles.</span>
<span id="cb7-217"><a href="#cb7-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-218"><a href="#cb7-218" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb7-219"><a href="#cb7-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-220"><a href="#cb7-220" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-221"><a href="#cb7-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-222"><a href="#cb7-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-223"><a href="#cb7-223" aria-hidden="true" tabindex="-1"></a><span class="fu">### What are good representations?</span></span>
<span id="cb7-224"><a href="#cb7-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-225"><a href="#cb7-225" aria-hidden="true" tabindex="-1"></a>Good representations transform raw data into a format that makes relevant patterns explicit and accessible. They represent a **semantic coordinate systems** where distances $<span class="sc">\|</span>\vect{z}_i - \vect{z}_j<span class="sc">\|</span>_2$ reflect meaningful relationships.</span>
<span id="cb7-226"><a href="#cb7-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-227"><a href="#cb7-227" aria-hidden="true" tabindex="-1"></a>::: {#fig-data-representation-spaces}</span>
<span id="cb7-228"><a href="#cb7-228" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}data_and_representation_space.png)</span>{fig-alt="Mapping from data space to representation space via encoder" width=80%}</span>
<span id="cb7-229"><a href="#cb7-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-230"><a href="#cb7-230" aria-hidden="true" tabindex="-1"></a>**Data Space vs Representation Space:** The encoder $f$ transforms high-dimensional input samples $\vect{x}_i$ from data space into compact representations $\vect{z}_i$ in representation space, where semantic relationships become explicit through distances and clustering.</span>
<span id="cb7-231"><a href="#cb7-231" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-232"><a href="#cb7-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-233"><a href="#cb7-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-234"><a href="#cb7-234" aria-hidden="true" tabindex="-1"></a>**Mathematical Framework:**</span>
<span id="cb7-235"><a href="#cb7-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-236"><a href="#cb7-236" aria-hidden="true" tabindex="-1"></a>Given input $\vect{x} \in \mathcal{X}$ and encoder $f: \mathcal{X} \to \mathcal{Z}$, we want representation $\vect{z} = f(\vect{x})$ such that:</span>
<span id="cb7-237"><a href="#cb7-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-238"><a href="#cb7-238" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Invariance &amp; Selectivity Balance</span></span>
<span id="cb7-239"><a href="#cb7-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-240"><a href="#cb7-240" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Invariance:** $<span class="sc">\|</span>f(\vect{x}) - f(g(\vect{x}))<span class="sc">\|</span>_2 &lt; \epsilon$ for nuisance transform $g$ (lighting, pose)</span>
<span id="cb7-241"><a href="#cb7-241" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Selectivity:** $<span class="sc">\|</span>f(\vect{x}_i) - f(\vect{x}_j)<span class="sc">\|</span>_2 &gt; \delta$ for semantically different $\vect{x}_i, \vect{x}_j$</span>
<span id="cb7-242"><a href="#cb7-242" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>*Trade-off:* Too much invariance → loss of discriminative details</span>
<span id="cb7-243"><a href="#cb7-243" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>*Sweet spot:* Preserve task-relevant variations, discard irrelevant ones</span>
<span id="cb7-244"><a href="#cb7-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-245"><a href="#cb7-245" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Geometric Structure Preservation</span></span>
<span id="cb7-246"><a href="#cb7-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-247"><a href="#cb7-247" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Smooth manifold:** Similar semantic concepts cluster in representation space</span>
<span id="cb7-248"><a href="#cb7-248" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Composability:** $f(\vect{x}_1) \oplus f(\vect{x}_2) \approx f(\text{combine}(\vect{x}_1, \vect{x}_2))$ for vector operations</span>
<span id="cb7-249"><a href="#cb7-249" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Interpolability:** Linear interpolation $\alpha f(\vect{x}_1) + (1-\alpha) f(\vect{x}_2)$ yields meaningful intermediate concepts</span>
<span id="cb7-250"><a href="#cb7-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-251"><a href="#cb7-251" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Transfer Efficiency</span></span>
<span id="cb7-252"><a href="#cb7-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-253"><a href="#cb7-253" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Low sample complexity:** Few examples needed for downstream adaptation</span>
<span id="cb7-254"><a href="#cb7-254" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Broad applicability:** Same $f(\cdot)$ works across multiple tasks/domains</span>
<span id="cb7-255"><a href="#cb7-255" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Graceful degradation:** Performance degrades slowly with domain shift</span>
<span id="cb7-256"><a href="#cb7-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-257"><a href="#cb7-257" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Computational Practicality</span></span>
<span id="cb7-258"><a href="#cb7-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-259"><a href="#cb7-259" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Compact dimensionality:** $\text{dim}(\vect{z}) \ll \text{dim}(\vect{x})$ while preserving information</span>
<span id="cb7-260"><a href="#cb7-260" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Fast computation:** Forward pass $f(\vect{x})$ efficient for real-time applications</span>
<span id="cb7-261"><a href="#cb7-261" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Stable training:** Representation learning converges reliably</span>
<span id="cb7-262"><a href="#cb7-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-263"><a href="#cb7-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-264"><a href="#cb7-264" aria-hidden="true" tabindex="-1"></a><span class="fu">## Deep Image Representations</span></span>
<span id="cb7-265"><a href="#cb7-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-266"><a href="#cb7-266" aria-hidden="true" tabindex="-1"></a>There are multiple ways to learn image representations with deep learning, using different learning paradigms and objectives.</span>
<span id="cb7-267"><a href="#cb7-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-268"><a href="#cb7-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-269"><a href="#cb7-269" aria-hidden="true" tabindex="-1"></a><span class="fu">### Supervised Learning</span></span>
<span id="cb7-270"><a href="#cb7-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-271"><a href="#cb7-271" aria-hidden="true" tabindex="-1"></a>Supervised pretraining learns $f(\cdot)$ by predicting human labels (e.g., ImageNet classes) with a loss like cross-entropy.</span>
<span id="cb7-272"><a href="#cb7-272" aria-hidden="true" tabindex="-1"></a>Deep networks progressively transform pixels into higher-level semantics, and linear probes on the penultimate layer often separate classes.</span>
<span id="cb7-273"><a href="#cb7-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-274"><a href="#cb7-274" aria-hidden="true" tabindex="-1"></a>::: {#fig-supervised-architecture}</span>
<span id="cb7-275"><a href="#cb7-275" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}supervised_learning_representations.png)</span>{fig-alt="Supervised learning architecture showing encoder and classifier" width=600}</span>
<span id="cb7-276"><a href="#cb7-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-277"><a href="#cb7-277" aria-hidden="true" tabindex="-1"></a>**Supervised Pre-training Architecture:** Input images are processed by an encoder (CNN or ViT) that extracts features, followed by a classification head trained with cross-entropy loss on human-provided labels. The penultimate layer embeddings serve as transferable representations for downstream tasks.</span>
<span id="cb7-278"><a href="#cb7-278" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-279"><a href="#cb7-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-280"><a href="#cb7-280" aria-hidden="true" tabindex="-1"></a><span class="fu">### Weakly Supervised Learning</span></span>
<span id="cb7-281"><a href="#cb7-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-282"><a href="#cb7-282" aria-hidden="true" tabindex="-1"></a>Weak supervision uses noisy/partial labels at scale—hashtags, alt-text, or image–text pairs.</span>
<span id="cb7-283"><a href="#cb7-283" aria-hidden="true" tabindex="-1"></a>A landmark example is **CLIP** (see @radford_learning_2021), trained to align images and their captions via contrastive learning.</span>
<span id="cb7-284"><a href="#cb7-284" aria-hidden="true" tabindex="-1"></a>It enables zero-shot recognition by turning text prompts into classifiers (e.g., “a photo of a cat”).</span>
<span id="cb7-285"><a href="#cb7-285" aria-hidden="true" tabindex="-1"></a>This reduces annotation cost and yields broadly transferable features. See @fig-clip-contrastive for an illustration.</span>
<span id="cb7-286"><a href="#cb7-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-287"><a href="#cb7-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-288"><a href="#cb7-288" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="CLIP Loss" collapse="true"}</span>
<span id="cb7-289"><a href="#cb7-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-290"><a href="#cb7-290" aria-hidden="true" tabindex="-1"></a>**Composite objective.** CLIP jointly trains an **image encoder** $f_\theta$ and a **text encoder** $g_\phi$ to align matched image–text pairs using a **symmetric contrastive (InfoNCE) loss** with a learned temperature (logit scale).</span>
<span id="cb7-291"><a href="#cb7-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-292"><a href="#cb7-292" aria-hidden="true" tabindex="-1"></a>**Setup.** For a batch of $N$ paired samples $<span class="sc">\{</span>(x_i, y_i)<span class="sc">\}</span>_{i=1}^N$:</span>
<span id="cb7-293"><a href="#cb7-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-294"><a href="#cb7-294" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Image features: $\tilde{\mathbf{v}}_i = \frac{f_\theta(x_i)}{\|f_\theta(x_i)<span class="sc">\|</span>}$</span>
<span id="cb7-295"><a href="#cb7-295" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Text features: $\tilde{\mathbf{t}}_i = \frac{g_\phi(y_i)}{\|g_\phi(y_i)<span class="sc">\|</span>}$</span>
<span id="cb7-296"><a href="#cb7-296" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Learned logit scale: $\alpha = \exp(s)$ with parameter $s$ (equivalently temperature $\tau = 1/\alpha$)</span>
<span id="cb7-297"><a href="#cb7-297" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Similarities (cosine scaled): $s_{ij} = \alpha \, \tilde{\mathbf{v}}_i^\top \tilde{\mathbf{t}}_j$</span>
<span id="cb7-298"><a href="#cb7-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-299"><a href="#cb7-299" aria-hidden="true" tabindex="-1"></a>**Image-to-text loss (with in-batch negatives).**</span>
<span id="cb7-300"><a href="#cb7-300" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb7-301"><a href="#cb7-301" aria-hidden="true" tabindex="-1"></a>\mathcal{L}_{i \to t}</span>
<span id="cb7-302"><a href="#cb7-302" aria-hidden="true" tabindex="-1"></a>= \frac{1}{N} \sum_{i=1}^{N}</span>
<span id="cb7-303"><a href="#cb7-303" aria-hidden="true" tabindex="-1"></a>-\log \frac{\exp<span class="sc">\!</span>\big(s_{ii}\big)}</span>
<span id="cb7-304"><a href="#cb7-304" aria-hidden="true" tabindex="-1"></a>{\sum_{j=1}^{N} \exp<span class="sc">\!</span>\big(s_{ij}\big)} \, .</span>
<span id="cb7-305"><a href="#cb7-305" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb7-306"><a href="#cb7-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-307"><a href="#cb7-307" aria-hidden="true" tabindex="-1"></a>**Text-to-image loss (symmetric direction).**</span>
<span id="cb7-308"><a href="#cb7-308" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb7-309"><a href="#cb7-309" aria-hidden="true" tabindex="-1"></a>\mathcal{L}_{t \to i}</span>
<span id="cb7-310"><a href="#cb7-310" aria-hidden="true" tabindex="-1"></a>= \frac{1}{N} \sum_{j=1}^{N}</span>
<span id="cb7-311"><a href="#cb7-311" aria-hidden="true" tabindex="-1"></a>-\log \frac{\exp<span class="sc">\!</span>\big(s_{jj}\big)}</span>
<span id="cb7-312"><a href="#cb7-312" aria-hidden="true" tabindex="-1"></a>{\sum_{i=1}^{N} \exp<span class="sc">\!</span>\big(s_{ij}\big)} \, .</span>
<span id="cb7-313"><a href="#cb7-313" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb7-314"><a href="#cb7-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-315"><a href="#cb7-315" aria-hidden="true" tabindex="-1"></a>**Total objective.**</span>
<span id="cb7-316"><a href="#cb7-316" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb7-317"><a href="#cb7-317" aria-hidden="true" tabindex="-1"></a>\mathcal{L}_{\text{CLIP}}</span>
<span id="cb7-318"><a href="#cb7-318" aria-hidden="true" tabindex="-1"></a>= \tfrac{1}{2} \left( \mathcal{L}_{i \to t} + \mathcal{L}_{t \to i} \right) \, .</span>
<span id="cb7-319"><a href="#cb7-319" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb7-320"><a href="#cb7-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-321"><a href="#cb7-321" aria-hidden="true" tabindex="-1"></a>**Components (at a glance).**</span>
<span id="cb7-322"><a href="#cb7-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-323"><a href="#cb7-323" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$f_\theta, g_\phi$: image/text encoders (e.g., ViT/CNN and Transformer).</span>
<span id="cb7-324"><a href="#cb7-324" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\tilde{\mathbf{v}}_i, \tilde{\mathbf{t}}_i$: $\ell_2$-normalized embeddings (cosine similarity).</span>
<span id="cb7-325"><a href="#cb7-325" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\alpha$ (or $1/\tau$): learned scaling controlling softness of the softmax.</span>
<span id="cb7-326"><a href="#cb7-326" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>In-batch negatives: all non-matching pairs in the current batch act as negatives.</span>
<span id="cb7-327"><a href="#cb7-327" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Optimization: cross-entropy over the similarity matrix in both directions.</span>
<span id="cb7-328"><a href="#cb7-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-329"><a href="#cb7-329" aria-hidden="true" tabindex="-1"></a>*Inference note:* zero-shot classification uses text prompts $y_c =$ “a photo of a $<span class="sc">\{</span>class<span class="sc">\}</span>$” to form class prototypes $\tilde{\mathbf{t}}_c$ and picks $\arg\max_c \tilde{\mathbf{v}}^\top \tilde{\mathbf{t}}_c$.</span>
<span id="cb7-330"><a href="#cb7-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-331"><a href="#cb7-331" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-332"><a href="#cb7-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-333"><a href="#cb7-333" aria-hidden="true" tabindex="-1"></a>::: {#fig-clip-contrastive}</span>
<span id="cb7-334"><a href="#cb7-334" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}clip_contrastive_pre_training.png)</span>{fig-alt="CLIP dual-encoder architecture with contrastive loss" width=600}</span>
<span id="cb7-335"><a href="#cb7-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-336"><a href="#cb7-336" aria-hidden="true" tabindex="-1"></a>**CLIP Contrastive Pre-training:** Shown are the image and text encoders processing a batch of paired data. The resulting embeddings are compared via a similarity matrix where matching pairs (diagonal) are maximized and non-matching pairs (off-diagonal) are minimized through contrastive learning.</span>
<span id="cb7-337"><a href="#cb7-337" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-338"><a href="#cb7-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-339"><a href="#cb7-339" aria-hidden="true" tabindex="-1"></a><span class="fu">### Self-Supervised Learning</span></span>
<span id="cb7-340"><a href="#cb7-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-341"><a href="#cb7-341" aria-hidden="true" tabindex="-1"></a>Self-supervised learning (SSL) creates pretext tasks from raw data (no human labels) to shape $f(\cdot)$.</span>
<span id="cb7-342"><a href="#cb7-342" aria-hidden="true" tabindex="-1"></a>Two dominant families are *imputation/masked modeling* and *contrastive/self-distillation*.</span>
<span id="cb7-343"><a href="#cb7-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-344"><a href="#cb7-344" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Imputation</span></span>
<span id="cb7-345"><a href="#cb7-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-346"><a href="#cb7-346" aria-hidden="true" tabindex="-1"></a>Masked image modeling hides random patches and trains the model to reconstruct them.</span>
<span id="cb7-347"><a href="#cb7-347" aria-hidden="true" tabindex="-1"></a>**Masked Autoencoding (MAE)** (see @he_masked_2021) shows that an asymmetric encoder–decoder and high masking ratio (≈75%) scale well and produce strong features for transfer.</span>
<span id="cb7-348"><a href="#cb7-348" aria-hidden="true" tabindex="-1"></a>Intuition: to fill in what’s missing, the model must internalize global structure and semantics.</span>
<span id="cb7-349"><a href="#cb7-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-350"><a href="#cb7-350" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Masked Autoencoding Loss" collapse="true"}</span>
<span id="cb7-351"><a href="#cb7-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-352"><a href="#cb7-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-353"><a href="#cb7-353" aria-hidden="true" tabindex="-1"></a>**Setup.** Given an image $x$, MAE patchifies it and **masks** a large subset of patches $\mathcal{M}$ (typically $\sim 75\%$), keeping visible patches $\mathcal{V}$.</span>
<span id="cb7-354"><a href="#cb7-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-355"><a href="#cb7-355" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Patchify: $X = \mathrm{Patchify}(x) \in \mathbb{R}^{P \times d}$ with $P$ patches and patch-dimension $d$.</span>
<span id="cb7-356"><a href="#cb7-356" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Split: $X = <span class="sc">\{</span>X_{\mathcal{V}}, X_{\mathcal{M}}<span class="sc">\}</span>$.</span>
<span id="cb7-357"><a href="#cb7-357" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Encoder** $E_\theta$ processes only visible tokens $X_{\mathcal{V}}$ to produce latents $H = E_\theta(X_{\mathcal{V}})$.</span>
<span id="cb7-358"><a href="#cb7-358" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Decoder** $D_\phi$ receives $H$ plus learned mask tokens at masked positions and predicts reconstructions $\hat{X}_{\mathcal{M}} = D_\phi(H, \text{mask tokens})$.</span>
<span id="cb7-359"><a href="#cb7-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-360"><a href="#cb7-360" aria-hidden="true" tabindex="-1"></a>**Reconstruction loss (masked patches only).**</span>
<span id="cb7-361"><a href="#cb7-361" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb7-362"><a href="#cb7-362" aria-hidden="true" tabindex="-1"></a>\mathcal{L}_{\text{MAE}}</span>
<span id="cb7-363"><a href="#cb7-363" aria-hidden="true" tabindex="-1"></a>=</span>
<span id="cb7-364"><a href="#cb7-364" aria-hidden="true" tabindex="-1"></a>\frac{1}{|\mathcal{M}|} \sum_{p \in \mathcal{M}}</span>
<span id="cb7-365"><a href="#cb7-365" aria-hidden="true" tabindex="-1"></a>\left<span class="sc">\|</span> \hat{X}_{p} - \tilde{X}_{p} \right<span class="sc">\|</span>_2^2 ,</span>
<span id="cb7-366"><a href="#cb7-366" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb7-367"><a href="#cb7-367" aria-hidden="true" tabindex="-1"></a>where $\tilde{X}$ are **normalized pixel targets** (e.g., per-channel mean/variance normalization) in patch space.</span>
<span id="cb7-368"><a href="#cb7-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-369"><a href="#cb7-369" aria-hidden="true" tabindex="-1"></a>**Components (at a glance).**</span>
<span id="cb7-370"><a href="#cb7-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-371"><a href="#cb7-371" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$X$: patchified image tokens; $\mathcal{V}/\mathcal{M}$: visible/masked index sets.</span>
<span id="cb7-372"><a href="#cb7-372" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$E_\theta$ (ViT encoder): operates on **visible** tokens only (asymmetric design).</span>
<span id="cb7-373"><a href="#cb7-373" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$D_\phi$ (lightweight decoder): reconstructs **masked** tokens from encoder latents $+$ mask tokens.</span>
<span id="cb7-374"><a href="#cb7-374" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\hat{X}_{\mathcal{M}}$: decoder predictions at masked positions.</span>
<span id="cb7-375"><a href="#cb7-375" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\tilde{X}_{\mathcal{M}}$: normalized pixel targets at masked positions.</span>
<span id="cb7-376"><a href="#cb7-376" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Loss averages **only over masked patches**, encouraging learning of global structure.</span>
<span id="cb7-377"><a href="#cb7-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-378"><a href="#cb7-378" aria-hidden="true" tabindex="-1"></a>**Notes.**</span>
<span id="cb7-379"><a href="#cb7-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-380"><a href="#cb7-380" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>High mask ratio $+$ asymmetric encoder/decoder yields efficient pretraining and strong transfer.</span>
<span id="cb7-381"><a href="#cb7-381" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Some variants replace pixel MSE with feature/perceptual losses, but vanilla MAE uses MSE on normalized pixels.</span>
<span id="cb7-382"><a href="#cb7-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-383"><a href="#cb7-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-384"><a href="#cb7-384" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-385"><a href="#cb7-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-386"><a href="#cb7-386" aria-hidden="true" tabindex="-1"></a>::: {#fig-mae-architecture}</span>
<span id="cb7-387"><a href="#cb7-387" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}masked_auto_encoder.png)</span>{width=800}</span>
<span id="cb7-388"><a href="#cb7-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-389"><a href="#cb7-389" aria-hidden="true" tabindex="-1"></a>**MAE Pre-training Pipeline:** A large fraction of image patches is randomly masked. The encoder processes only visible patches for efficiency. After encoding, lightweight mask tokens are inserted, and a small decoder reconstructs the original image pixels from the full token set. Post-training, the decoder is discarded; the encoder alone extracts features from complete (unmasked) images for downstream tasks.</span>
<span id="cb7-390"><a href="#cb7-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-391"><a href="#cb7-391" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-392"><a href="#cb7-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-393"><a href="#cb7-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-394"><a href="#cb7-394" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Contrastive Learning</span></span>
<span id="cb7-395"><a href="#cb7-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-396"><a href="#cb7-396" aria-hidden="true" tabindex="-1"></a>Contrastive learning is a learning paradigm whereby a model learns to represent data points such that similar points are nearby and disimilar points are far apart.</span>
<span id="cb7-397"><a href="#cb7-397" aria-hidden="true" tabindex="-1"></a>The **SimCLR** (@chen_simple_2020) paper demonstrated how such a model can be learned and achieve high-accuracy on down-stream tasks.</span>
<span id="cb7-398"><a href="#cb7-398" aria-hidden="true" tabindex="-1"></a>@fig-ssl-contrastive illustrateds the concepts: From a given image create two _views_ (augmented versions) and contrast them with different images in a large batch of images (InfoNCE loss).</span>
<span id="cb7-399"><a href="#cb7-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-400"><a href="#cb7-400" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="InfoNCE Loss" collapse="true"}</span>
<span id="cb7-401"><a href="#cb7-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-402"><a href="#cb7-402" aria-hidden="true" tabindex="-1"></a>**Core Idea:** InfoNCE maximizes mutual information between positive pairs while contrasting against negative samples.</span>
<span id="cb7-403"><a href="#cb7-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-404"><a href="#cb7-404" aria-hidden="true" tabindex="-1"></a>**Mathematical Form:**</span>
<span id="cb7-405"><a href="#cb7-405" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-406"><a href="#cb7-406" aria-hidden="true" tabindex="-1"></a>\mathcal{L}_{\text{InfoNCE}} = -\mathbb{E}\left[\log \frac{\exp(\text{sim}(z_i, z_i^+)/\tau)}{\exp(\text{sim}(z_i, z_i^+)/\tau) + \sum_{j=1}^{N-1} \exp(\text{sim}(z_i, z_j^-)/\tau)}\right]</span>
<span id="cb7-407"><a href="#cb7-407" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb7-408"><a href="#cb7-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-409"><a href="#cb7-409" aria-hidden="true" tabindex="-1"></a>**Components:**</span>
<span id="cb7-410"><a href="#cb7-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-411"><a href="#cb7-411" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$z_i$: Anchor embedding (encoded from image $x_i$)</span>
<span id="cb7-412"><a href="#cb7-412" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$z_i^+$: Positive embedding (different augmentation of same image)</span>
<span id="cb7-413"><a href="#cb7-413" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$z_j^-$: Negative embeddings (from different images in batch)</span>
<span id="cb7-414"><a href="#cb7-414" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\text{sim}(\cdot, \cdot)$: Similarity function (typically cosine similarity)</span>
<span id="cb7-415"><a href="#cb7-415" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\tau$: Temperature parameter controlling concentration</span>
<span id="cb7-416"><a href="#cb7-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-417"><a href="#cb7-417" aria-hidden="true" tabindex="-1"></a>**Temperature Effect:**</span>
<span id="cb7-418"><a href="#cb7-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-419"><a href="#cb7-419" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Small $\tau$ ($\to 0$): Focuses on hardest negatives, sharper distributions</span>
<span id="cb7-420"><a href="#cb7-420" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Large $\tau$ ($\to \infty$): Uniform weighting, softer distributions</span>
<span id="cb7-421"><a href="#cb7-421" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Typical values: $\tau \in <span class="co">[</span><span class="ot">0.07, 0.5</span><span class="co">]</span>$</span>
<span id="cb7-422"><a href="#cb7-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-423"><a href="#cb7-423" aria-hidden="true" tabindex="-1"></a>**Intuition:** The loss encourages the model to:</span>
<span id="cb7-424"><a href="#cb7-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-425"><a href="#cb7-425" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Pull positive pairs close together in embedding space</span>
<span id="cb7-426"><a href="#cb7-426" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Push negative pairs apart</span>
<span id="cb7-427"><a href="#cb7-427" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Form tight clusters of semantically similar samples</span>
<span id="cb7-428"><a href="#cb7-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-429"><a href="#cb7-429" aria-hidden="true" tabindex="-1"></a>**Practical Considerations:**</span>
<span id="cb7-430"><a href="#cb7-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-431"><a href="#cb7-431" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Requires large batch sizes (e.g., 256-4096) for sufficient negatives</span>
<span id="cb7-432"><a href="#cb7-432" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Alternative: Memory banks (MoCo) or queue-based negatives</span>
<span id="cb7-433"><a href="#cb7-433" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Hard negative mining can improve learning efficiency</span>
<span id="cb7-434"><a href="#cb7-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-435"><a href="#cb7-435" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-436"><a href="#cb7-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-437"><a href="#cb7-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-438"><a href="#cb7-438" aria-hidden="true" tabindex="-1"></a>::: {#fig-ssl-contrastive layout-ncol=2}</span>
<span id="cb7-439"><a href="#cb7-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-440"><a href="#cb7-440" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}self_supervised_contrastive1.png)</span>{fig-alt="Contrastive learning conceptual view showing similar and dissimilar samples"}</span>
<span id="cb7-441"><a href="#cb7-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-442"><a href="#cb7-442" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}self_supervised_contrastive2.png)</span>{fig-alt="Contrastive learning architectural view with encoder and projection head"}</span>
<span id="cb7-443"><a href="#cb7-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-444"><a href="#cb7-444" aria-hidden="true" tabindex="-1"></a>**Left:** Learning representations by contrasting similar inputs (different views of same sample) against other inputs. **Right:** Architectural view showing the encoder and projection head pipeline.</span>
<span id="cb7-445"><a href="#cb7-445" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-446"><a href="#cb7-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-447"><a href="#cb7-447" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Self-Distillation</span></span>
<span id="cb7-448"><a href="#cb7-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-449"><a href="#cb7-449" aria-hidden="true" tabindex="-1"></a>Self-distillation is an interesting concept: A **student** model tries to match the outputs of a **teacher** model’s distribution over a given image.</span>
<span id="cb7-450"><a href="#cb7-450" aria-hidden="true" tabindex="-1"></a>This principle was introduced in the BYOL paper (see @grill_bootstrap_2020) and led to the DINO-series of models: DINO (@caron_emerging_2021), DINOv2 (@oquab_dinov2_2024), and DINOv3 (@simeoni_dinov3_2025) are non-contrastive: a student matches the teacher’s distribution over crops (multi-crop), using centering and sharpening. This yields strong **global descriptors** and high-quality **local (patch) tokens**.</span>
<span id="cb7-451"><a href="#cb7-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-452"><a href="#cb7-452" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="DINOv3 Loss" collapse="true"}</span>
<span id="cb7-453"><a href="#cb7-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-454"><a href="#cb7-454" aria-hidden="true" tabindex="-1"></a>**Composite objective.** DINOv3 trains a student ViT against an EMA **teacher** with multi-crop views, combining a **global DINO loss**, a **local iBOT loss**, a **KoLeo uniformity regularizer**, and (in a refinement stage) a **Gram anchoring loss** on patch similarities:</span>
<span id="cb7-455"><a href="#cb7-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-456"><a href="#cb7-456" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb7-457"><a href="#cb7-457" aria-hidden="true" tabindex="-1"></a>\mathcal{L}</span>
<span id="cb7-458"><a href="#cb7-458" aria-hidden="true" tabindex="-1"></a>&amp;= \underbrace{\mathcal{L}_{\text{DINO}}}_{\text{global crops}}</span>
<span id="cb7-459"><a href="#cb7-459" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>\lambda_{\text{iBOT}} \underbrace{\mathcal{L}_{\text{iBOT}}}_{\text{masked local tokens}}</span>
<span id="cb7-460"><a href="#cb7-460" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>\lambda_{\text{KoLeo}} \underbrace{\mathcal{L}_{\text{KoLeo}}}_{\text{uniformity on }<span class="co">[</span><span class="ot">\text{CLS}</span><span class="co">]</span>}</span>
<span id="cb7-461"><a href="#cb7-461" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>\lambda_{\text{Gram}} \underbrace{\mathcal{L}_{\text{Gram}}}_{\text{refinement}}.</span>
<span id="cb7-462"><a href="#cb7-462" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb7-463"><a href="#cb7-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-464"><a href="#cb7-464" aria-hidden="true" tabindex="-1"></a>**Global DINO (distribution matching, no negatives).**</span>
<span id="cb7-465"><a href="#cb7-465" aria-hidden="true" tabindex="-1"></a>Student probabilities $p_s=\mathrm{softmax}(g_s(z_s)/\tau_s)$; teacher targets</span>
<span id="cb7-466"><a href="#cb7-466" aria-hidden="true" tabindex="-1"></a>$q_t=\mathrm{stopgrad}<span class="sc">\!</span>\left<span class="co">[</span><span class="ot">\mathrm{softmax}\!\left((g_t(z_t)-c)/\tau_t\right)\right</span><span class="co">]</span>$ with centering $c$ and temperatures $\tau_s,\tau_t$.</span>
<span id="cb7-467"><a href="#cb7-467" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb7-468"><a href="#cb7-468" aria-hidden="true" tabindex="-1"></a>\mathcal{L}_{\text{DINO}} \;=\; - \sum_{k} q_t^{(k)} \log p_s^{(k)}</span>
<span id="cb7-469"><a href="#cb7-469" aria-hidden="true" tabindex="-1"></a>\quad \text{(summed over pairs of student/teacher crops).}</span>
<span id="cb7-470"><a href="#cb7-470" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb7-471"><a href="#cb7-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-472"><a href="#cb7-472" aria-hidden="true" tabindex="-1"></a>**Local iBOT (masked token prediction).**</span>
<span id="cb7-473"><a href="#cb7-473" aria-hidden="true" tabindex="-1"></a>For masked student patches $m \in \mathcal{M}$, match teacher patch-level distributions:</span>
<span id="cb7-474"><a href="#cb7-474" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb7-475"><a href="#cb7-475" aria-hidden="true" tabindex="-1"></a>\mathcal{L}_{\text{iBOT}} \;=\; - \frac{1}{|\mathcal{M}|}\sum_{m \in \mathcal{M}}\sum_{k} q_{t,m}^{(k)} \log p_{s,m}^{(k)} .</span>
<span id="cb7-476"><a href="#cb7-476" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb7-477"><a href="#cb7-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-478"><a href="#cb7-478" aria-hidden="true" tabindex="-1"></a>**KoLeo (feature uniformity).**</span>
<span id="cb7-479"><a href="#cb7-479" aria-hidden="true" tabindex="-1"></a>Encourages spread (near-uniform) class-token embeddings; implemented as the KoLeo entropy estimator on mini-batches (schematic form):</span>
<span id="cb7-480"><a href="#cb7-480" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb7-481"><a href="#cb7-481" aria-hidden="true" tabindex="-1"></a>\mathcal{L}_{\text{KoLeo}} \;\approx\; -\frac{1}{N}\sum_{i=1}^{N} \log d_i</span>
<span id="cb7-482"><a href="#cb7-482" aria-hidden="true" tabindex="-1"></a>\quad \text{with } d_i \text{ the NN distance of } z^{<span class="co">[</span><span class="ot">\text{CLS}</span><span class="co">]</span>}_i .</span>
<span id="cb7-483"><a href="#cb7-483" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb7-484"><a href="#cb7-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-485"><a href="#cb7-485" aria-hidden="true" tabindex="-1"></a>**Gram anchoring (refinement step).**</span>
<span id="cb7-486"><a href="#cb7-486" aria-hidden="true" tabindex="-1"></a>Align student **patch-similarity structure** to a “Gram teacher.”</span>
<span id="cb7-487"><a href="#cb7-487" aria-hidden="true" tabindex="-1"></a>Let $\tilde{Z}_s,\tilde{Z}_g \in \mathbb{R}^{P \times D}$ be $\ell_2$-normalized patch features (rows) for student and Gram-teacher on a global crop. With Gram matrices $G(\tilde{Z})=\tilde{Z}\tilde{Z}^\top$:</span>
<span id="cb7-488"><a href="#cb7-488" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb7-489"><a href="#cb7-489" aria-hidden="true" tabindex="-1"></a>\mathcal{L}_{\text{Gram}} \;=\; \big<span class="sc">\|</span> G(\tilde{Z}_s) - G(\tilde{Z}_g) \big<span class="sc">\|</span>_F^2 .</span>
<span id="cb7-490"><a href="#cb7-490" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb7-491"><a href="#cb7-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-492"><a href="#cb7-492" aria-hidden="true" tabindex="-1"></a>**Notes.**</span>
<span id="cb7-493"><a href="#cb7-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-494"><a href="#cb7-494" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Teacher is an EMA of the student; teachers view only **global crops**, students see **global + local** crops; iBOT masks a subset of student patches.</span>
<span id="cb7-495"><a href="#cb7-495" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Gram anchoring is applied in a late **refinement** phase to restore **patch-level consistency** without hurting global performance.</span>
<span id="cb7-496"><a href="#cb7-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-497"><a href="#cb7-497" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-498"><a href="#cb7-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-499"><a href="#cb7-499" aria-hidden="true" tabindex="-1"></a>::: {#fig-dino-principle}</span>
<span id="cb7-500"><a href="#cb7-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-501"><a href="#cb7-501" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}dino_principle.png)</span>{fig-alt="DINO self-distillation principle showing teacher-student architecture with multi-crop strategy" width=600}</span>
<span id="cb7-502"><a href="#cb7-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-503"><a href="#cb7-503" aria-hidden="true" tabindex="-1"></a>The student network processes various crops (global and local views) of the same image, attempting to match the output distribution of the momentum-updated teacher network. The teacher processes only global views and uses centering and sharpening to avoid collapse. No explicit negatives are required; the model learns by aligning predictions across different augmented views.</span>
<span id="cb7-504"><a href="#cb7-504" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-505"><a href="#cb7-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-506"><a href="#cb7-506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-507"><a href="#cb7-507" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="DINOv3 Overview Video" collapse="false"}</span>
<span id="cb7-508"><a href="#cb7-508" aria-hidden="true" tabindex="-1"></a>Watch Meta's introduction to DINOv3, explaining the key improvements over DINOv2 and the self-distillation approach:</span>
<span id="cb7-509"><a href="#cb7-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-510"><a href="#cb7-510" aria-hidden="true" tabindex="-1"></a>{{&lt; video https://www.youtube.com/watch?v=-eOYWK6m3i8 &gt;}}</span>
<span id="cb7-511"><a href="#cb7-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-512"><a href="#cb7-512" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-513"><a href="#cb7-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-514"><a href="#cb7-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-515"><a href="#cb7-515" aria-hidden="true" tabindex="-1"></a><span class="fu">### Global vs Local Representations</span></span>
<span id="cb7-516"><a href="#cb7-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-517"><a href="#cb7-517" aria-hidden="true" tabindex="-1"></a>**Global** representations summarize an image into a single embedding (e.g., a <span class="co">[</span><span class="ot">CLS</span><span class="co">]</span> token or pooled feature) that captures overall semantics.</span>
<span id="cb7-518"><a href="#cb7-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-519"><a href="#cb7-519" aria-hidden="true" tabindex="-1"></a>**CLIP** is a prototypical global model: it aligns an image-wide vector with a text embedding, which is ideal for _zero-shot classification_, _retrieval_, and broad _semantic similarity_ at scale.</span>
<span id="cb7-520"><a href="#cb7-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-521"><a href="#cb7-521" aria-hidden="true" tabindex="-1"></a>In contrast, the DINO family (incl. DINO/DINOv2/DINOv3) produces both strong global descriptors and rich local tokens via self-distillation and multi-crop training. These local features carry spatially resolved cues useful for tasks such as segmentation or obejct detection. The global token remains competitive for classification and retrieval.</span>
<span id="cb7-522"><a href="#cb7-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-523"><a href="#cb7-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-524"><a href="#cb7-524" aria-hidden="true" tabindex="-1"></a>::: {#fig-dinov3-local-features}</span>
<span id="cb7-525"><a href="#cb7-525" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}dinov3_local_cosine_similarity_example.png)</span>{fig-alt="DINOv3 local feature similarity visualization showing query points and their cosine similarity maps" width=600}</span>
<span id="cb7-526"><a href="#cb7-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-527"><a href="#cb7-527" aria-hidden="true" tabindex="-1"></a>Shown are the cosine similarities of feature embeddings at different locations across the input image (middle) for different query points (shown as red crosses). The visualization highlights semantic consistency across the image.</span>
<span id="cb7-528"><a href="#cb7-528" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-529"><a href="#cb7-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-530"><a href="#cb7-530" aria-hidden="true" tabindex="-1"></a><span class="fu">## Comparing Representations</span></span>
<span id="cb7-531"><a href="#cb7-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-532"><a href="#cb7-532" aria-hidden="true" tabindex="-1"></a>The following illustrates different representations for the images in @fig-images-overview.</span>
<span id="cb7-533"><a href="#cb7-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-534"><a href="#cb7-534" aria-hidden="true" tabindex="-1"></a>::: {#fig-images-overview}</span>
<span id="cb7-535"><a href="#cb7-535" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}images_overview.png)</span>{fig-alt="Example images to compare different representations" width=100%}</span>
<span id="cb7-536"><a href="#cb7-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-537"><a href="#cb7-537" aria-hidden="true" tabindex="-1"></a>Example images to compare different representations.</span>
<span id="cb7-538"><a href="#cb7-538" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-539"><a href="#cb7-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-540"><a href="#cb7-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-541"><a href="#cb7-541" aria-hidden="true" tabindex="-1"></a>In @fig-rl-similarity-resnet, @fig-rl-similarity-clip, and @fig-rl-similarity-dinov2 we show cosine similarities between normalized representations and compare to pixel-space similarity (@fig-rl-similarity-pixel-space).</span>
<span id="cb7-542"><a href="#cb7-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-543"><a href="#cb7-543" aria-hidden="true" tabindex="-1"></a>**Similarity Metrics:**</span>
<span id="cb7-544"><a href="#cb7-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-545"><a href="#cb7-545" aria-hidden="true" tabindex="-1"></a>For images represented as vectors $\vect{z}_i, \vect{z}_j \in \mathbb{R}^d$:</span>
<span id="cb7-546"><a href="#cb7-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-547"><a href="#cb7-547" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Euclidean Distance (Pixel Space):**</span>
<span id="cb7-548"><a href="#cb7-548" aria-hidden="true" tabindex="-1"></a>    $$d_{\text{L2}}(\vect{z}_i, \vect{z}_j) = \|\vect{z}_i - \vect{z}_j\|_2 = \sqrt{\sum_{k=1}^d (z_i^{(k)} - z_j^{(k)})^2}$$</span>
<span id="cb7-549"><a href="#cb7-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-550"><a href="#cb7-550" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Cosine Similarity:**</span>
<span id="cb7-551"><a href="#cb7-551" aria-hidden="true" tabindex="-1"></a>    $$\text{sim}_{\cos}(\vect{z}_i, \vect{z}_j) = \frac{\vect{z}_i^\top \vect{z}_j}{\|\vect{z}_i\|_2 \|\vect{z}_j\|_2} = \frac{\sum_{k=1}^d z_i^{(k)} z_j^{(k)}}{\sqrt{\sum_{k=1}^d (z_i^{(k)})^2} \sqrt{\sum_{k=1}^d (z_j^{(k)})^2}}$$</span>
<span id="cb7-552"><a href="#cb7-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-553"><a href="#cb7-553" aria-hidden="true" tabindex="-1"></a>where $\text{sim}_{\cos} \in <span class="co">[</span><span class="ot">-1, 1</span><span class="co">]</span>$ with 1 indicating perfect alignment, 0 indicating orthogonality, and -1 indicating opposite directions.</span>
<span id="cb7-554"><a href="#cb7-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-555"><a href="#cb7-555" aria-hidden="true" tabindex="-1"></a>::: {.panel-tabset #fig-similarity-comparison-models}</span>
<span id="cb7-556"><a href="#cb7-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-557"><a href="#cb7-557" aria-hidden="true" tabindex="-1"></a><span class="fu">### Pixel Space</span></span>
<span id="cb7-558"><a href="#cb7-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-559"><a href="#cb7-559" aria-hidden="true" tabindex="-1"></a>::: {#fig-rl-similarity-pixel-space}</span>
<span id="cb7-560"><a href="#cb7-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-561"><a href="#cb7-561" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}similarity_pixel_space.png)</span>{fig-alt="Pixel-space similarity matrix" width=600}</span>
<span id="cb7-562"><a href="#cb7-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-563"><a href="#cb7-563" aria-hidden="true" tabindex="-1"></a>Raw pixel-level similarity fails to capture semantic relationships.</span>
<span id="cb7-564"><a href="#cb7-564" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-565"><a href="#cb7-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-566"><a href="#cb7-566" aria-hidden="true" tabindex="-1"></a><span class="fu">### ResNet-50</span></span>
<span id="cb7-567"><a href="#cb7-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-568"><a href="#cb7-568" aria-hidden="true" tabindex="-1"></a>::: {#fig-rl-similarity-resnet}</span>
<span id="cb7-569"><a href="#cb7-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-570"><a href="#cb7-570" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}similarity_resnet50.png)</span>{fig-alt="ResNet-50 supervised representation similarity" width=600}</span>
<span id="cb7-571"><a href="#cb7-571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-572"><a href="#cb7-572" aria-hidden="true" tabindex="-1"></a>Supervised CNN representations group semantically similar images.</span>
<span id="cb7-573"><a href="#cb7-573" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-574"><a href="#cb7-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-575"><a href="#cb7-575" aria-hidden="true" tabindex="-1"></a><span class="fu">### CLIP</span></span>
<span id="cb7-576"><a href="#cb7-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-577"><a href="#cb7-577" aria-hidden="true" tabindex="-1"></a>::: {#fig-rl-similarity-clip}</span>
<span id="cb7-578"><a href="#cb7-578" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-579"><a href="#cb7-579" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}similarity_clip.png)</span>{fig-alt="CLIP multimodal representation similarity" width=600}</span>
<span id="cb7-580"><a href="#cb7-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-581"><a href="#cb7-581" aria-hidden="true" tabindex="-1"></a>Language-aligned embeddings from CLIP show text-driven semantic structure.</span>
<span id="cb7-582"><a href="#cb7-582" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-583"><a href="#cb7-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-584"><a href="#cb7-584" aria-hidden="true" tabindex="-1"></a><span class="fu">### DINOv2</span></span>
<span id="cb7-585"><a href="#cb7-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-586"><a href="#cb7-586" aria-hidden="true" tabindex="-1"></a>::: {#fig-rl-similarity-dinov2}</span>
<span id="cb7-587"><a href="#cb7-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-588"><a href="#cb7-588" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}similarity_dinov2.png)</span>{fig-alt="DINOv2 self-supervised representation similarity" width=600}</span>
<span id="cb7-589"><a href="#cb7-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-590"><a href="#cb7-590" aria-hidden="true" tabindex="-1"></a>Self-distilled DINOv2 features reveal fine-grained semantic clusters.</span>
<span id="cb7-591"><a href="#cb7-591" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-592"><a href="#cb7-592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-593"><a href="#cb7-593" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-594"><a href="#cb7-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-595"><a href="#cb7-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-596"><a href="#cb7-596" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Question: Which Representations Do You Favour? And Why?" collapse="false"}</span>
<span id="cb7-597"><a href="#cb7-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-598"><a href="#cb7-598" aria-hidden="true" tabindex="-1"></a>Looking at the similarity matrices, which representation do you prefer and why?</span>
<span id="cb7-599"><a href="#cb7-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-600"><a href="#cb7-600" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb7-601"><a href="#cb7-601" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Click for Discussion<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb7-602"><a href="#cb7-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-603"><a href="#cb7-603" aria-hidden="true" tabindex="-1"></a>**Pixel Space (@fig-rl-similarity-pixel-space):**</span>
<span id="cb7-604"><a href="#cb7-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-605"><a href="#cb7-605" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Not suitable for any semantic task</span>
<span id="cb7-606"><a href="#cb7-606" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Captures low-level appearance variations (lighting, pose, exact pixel alignment)</span>
<span id="cb7-607"><a href="#cb7-607" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Cannot distinguish semantic categories from superficial similarities</span>
<span id="cb7-608"><a href="#cb7-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-609"><a href="#cb7-609" aria-hidden="true" tabindex="-1"></a>**ResNet-50 Supervised (@fig-rl-similarity-resnet):**</span>
<span id="cb7-610"><a href="#cb7-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-611"><a href="#cb7-611" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Much better than pixel space but still limited</span>
<span id="cb7-612"><a href="#cb7-612" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Shows semantic grouping (elephants cluster)</span>
<span id="cb7-613"><a href="#cb7-613" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Trained on ImageNet classes, may not transfer optimally to fine-grained tasks</span>
<span id="cb7-614"><a href="#cb7-614" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Some sensitivity to lightning (elephant night vs day)</span>
<span id="cb7-615"><a href="#cb7-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-616"><a href="#cb7-616" aria-hidden="true" tabindex="-1"></a>**CLIP Multimodal (@fig-rl-similarity-clip):**</span>
<span id="cb7-617"><a href="#cb7-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-618"><a href="#cb7-618" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Good semantic structure aligned with natural language concepts</span>
<span id="cb7-619"><a href="#cb7-619" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Clear separation between major categories (animals vs. vehicles)</span>
<span id="cb7-620"><a href="#cb7-620" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Limitation: Lower differentiation within categories</span>
<span id="cb7-621"><a href="#cb7-621" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Best for: Broad semantic retrieval and zero-shot open-vocabulary tasks</span>
<span id="cb7-622"><a href="#cb7-622" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Trade-off: Language alignment may sacrifice fine-grained visual distinctions</span>
<span id="cb7-623"><a href="#cb7-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-624"><a href="#cb7-624" aria-hidden="true" tabindex="-1"></a>**DINOv2 Self-Supervised (@fig-rl-similarity-dinov2):**</span>
<span id="cb7-625"><a href="#cb7-625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-626"><a href="#cb7-626" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Sharp discrimination between semantic classes (Elephant vs Giraffe)</span>
<span id="cb7-627"><a href="#cb7-627" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Strong within-category structure (elephant day variants)</span>
<span id="cb7-628"><a href="#cb7-628" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Clear night/day distinctions while preserving identity</span>
<span id="cb7-629"><a href="#cb7-629" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Best for: Few-shot learning, fine-grained classification, dense prediction tasks</span>
<span id="cb7-630"><a href="#cb7-630" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Pure vision objective without language bias preserves visual details</span>
<span id="cb7-631"><a href="#cb7-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-632"><a href="#cb7-632" aria-hidden="true" tabindex="-1"></a>**Recommendation hierarchy:**</span>
<span id="cb7-633"><a href="#cb7-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-634"><a href="#cb7-634" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**DINOv2/v3**: Default choice for vision-centric tasks requiring discriminative power</span>
<span id="cb7-635"><a href="#cb7-635" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**CLIP**: When zero-shot capabilities or text-image alignment is critical</span>
<span id="cb7-636"><a href="#cb7-636" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Supervised CNN**: When computation is limited and broad categories suffice</span>
<span id="cb7-637"><a href="#cb7-637" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Pixel space**: Never for semantic tasks (only for exact duplicate detection)</span>
<span id="cb7-638"><a href="#cb7-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-639"><a href="#cb7-639" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb7-640"><a href="#cb7-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-641"><a href="#cb7-641" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-642"><a href="#cb7-642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-643"><a href="#cb7-643" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-644"><a href="#cb7-644" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning title="Task-Relevant Invariance"}</span>
<span id="cb7-645"><a href="#cb7-645" aria-hidden="true" tabindex="-1"></a>**Critical consideration:** Invariance to task-relevant attributes can render representations useless.</span>
<span id="cb7-646"><a href="#cb7-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-647"><a href="#cb7-647" aria-hidden="true" tabindex="-1"></a>**Examples of harmful invariance:**</span>
<span id="cb7-648"><a href="#cb7-648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-649"><a href="#cb7-649" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Color-invariant features for flower classification:** If representations discard color information, distinguishing between red and yellow tulips becomes impossible.</span>
<span id="cb7-650"><a href="#cb7-650" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Rotation-invariant features for orientation tasks:** Medical imaging often requires preserving anatomical orientation; rotation invariance destroys critical diagnostic information.</span>
<span id="cb7-651"><a href="#cb7-651" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Scale-invariant features for size estimation:** Object measurement tasks require preserving absolute or relative size cues.</span>
<span id="cb7-652"><a href="#cb7-652" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Texture-invariant features for material classification:** Distinguishing wood from metal requires texture sensitivity.</span>
<span id="cb7-653"><a href="#cb7-653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-654"><a href="#cb7-654" aria-hidden="true" tabindex="-1"></a>**Design principle:** Match your representation's invariances to your task requirements. When in doubt, preserve information and let the downstream model learn to ignore irrelevant variations.</span>
<span id="cb7-655"><a href="#cb7-655" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-656"><a href="#cb7-656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-657"><a href="#cb7-657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-658"><a href="#cb7-658" aria-hidden="true" tabindex="-1"></a><span class="fu">## Using Representations: A Practical Guide</span></span>
<span id="cb7-659"><a href="#cb7-659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-660"><a href="#cb7-660" aria-hidden="true" tabindex="-1"></a>Always begin with the simplest approach and only add complexity when performance gaps justify it.  For **classification/retrieval**, global embeddings (e.g., CLIP) enable **zero-/few-shot** learning. For **dense tasks** (segmentation, object detection), **local (patch) tokens** from ViTs (e.g., DINOv3) feed pixel-aligned heads.</span>
<span id="cb7-661"><a href="#cb7-661" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-662"><a href="#cb7-662" aria-hidden="true" tabindex="-1"></a>Typical progression:</span>
<span id="cb7-663"><a href="#cb7-663" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-664"><a href="#cb7-664" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**k-NN baseline (no training)**</span>
<span id="cb7-665"><a href="#cb7-665" aria-hidden="true" tabindex="-1"></a>   Think of this as “find the most similar examples.” We store the embeddings of labeled images and, for a new image, look up the *k* most similar ones; the majority label wins. It’s fast to set up, needs **no training**, and gives a quick reality check on representation quality. See @fig-rl-practical-knn for illustrative code.</span>
<span id="cb7-666"><a href="#cb7-666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-667"><a href="#cb7-667" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Linear probe (freeze encoder, train head)**</span>
<span id="cb7-668"><a href="#cb7-668" aria-hidden="true" tabindex="-1"></a>   Keep the powerful pretrained encoder fixed and train a tiny linear layer on top. If this small layer reaches good accuracy, it means the embeddings already separate classes well. It’s **cheap, stable**, and a great first serious baseline. See @fig-rl-practical-linear for illustrative code.</span>
<span id="cb7-669"><a href="#cb7-669" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-670"><a href="#cb7-670" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**LoRA/Adapters (efficient parameter tuning)**</span>
<span id="cb7-671"><a href="#cb7-671" aria-hidden="true" tabindex="-1"></a>   Instead of updating all model weights, add small “adapter” modules (or low-rank updates) and train **only those**. You keep most of the original knowledge while adapting to your task with far fewer trainable parameters—**faster, lighter, and less prone to overfitting** than full fine-tuning. See @fig-rl-practical-adapter for illustrative code.</span>
<span id="cb7-672"><a href="#cb7-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-673"><a href="#cb7-673" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Partial fine-tune (unfreeze top layers)**</span>
<span id="cb7-674"><a href="#cb7-674" aria-hidden="true" tabindex="-1"></a>   Unfreeze just the **last few layers** of the encoder and train them (plus the head). This lets the model adjust higher-level features to your data while keeping the earlier, general features intact. It’s a good **middle ground** between speed and task fit. See @fig-rl-practical-fine-tune for illustrative code.</span>
<span id="cb7-675"><a href="#cb7-675" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-676"><a href="#cb7-676" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Full fine-tune (last resort)**</span>
<span id="cb7-677"><a href="#cb7-677" aria-hidden="true" tabindex="-1"></a>   Train **all** the model’s weights on your dataset. This can yield the best alignment to your task but is **computationally expensive** and risks overfitting—especially with limited data. Use it when you have enough data/compute and simpler options plateau. See @fig-rl-practical-full-fine-tune for illustrative code.</span>
<span id="cb7-678"><a href="#cb7-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-679"><a href="#cb7-679" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-682"><a href="#cb7-682" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-683"><a href="#cb7-683" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb7-684"><a href="#cb7-684" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb7-685"><a href="#cb7-685" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-rl-practical-knn</span></span>
<span id="cb7-686"><a href="#cb7-686" aria-hidden="true" tabindex="-1"></a><span class="co">#| lst-cap: "k-NN baseline (no training) — Hugging Face2"</span></span>
<span id="cb7-687"><a href="#cb7-687" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "k-NN baseline (no training) — Hugging Face"</span></span>
<span id="cb7-688"><a href="#cb7-688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-689"><a href="#cb7-689" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch, sklearn.neighbors <span class="im">as</span> sk</span>
<span id="cb7-690"><a href="#cb7-690" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModel, AutoImageProcessor</span>
<span id="cb7-691"><a href="#cb7-691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-692"><a href="#cb7-692" aria-hidden="true" tabindex="-1"></a>model_id <span class="op">=</span> <span class="st">"google/vit-base-patch16-224"</span></span>
<span id="cb7-693"><a href="#cb7-693" aria-hidden="true" tabindex="-1"></a>enc <span class="op">=</span> AutoModel.from_pretrained(model_id).<span class="bu">eval</span>()</span>
<span id="cb7-694"><a href="#cb7-694" aria-hidden="true" tabindex="-1"></a>proc <span class="op">=</span> AutoImageProcessor.from_pretrained(model_id)</span>
<span id="cb7-695"><a href="#cb7-695" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-696"><a href="#cb7-696" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> feats(imgs):  <span class="co"># imgs: list of PIL images</span></span>
<span id="cb7-697"><a href="#cb7-697" aria-hidden="true" tabindex="-1"></a>    px <span class="op">=</span> proc(imgs, return_tensors<span class="op">=</span><span class="st">"pt"</span>)[<span class="st">"pixel_values"</span>]</span>
<span id="cb7-698"><a href="#cb7-698" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb7-699"><a href="#cb7-699" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> enc(pixel_values<span class="op">=</span>px).last_hidden_state[:,<span class="dv">0</span>]  <span class="co"># [CLS]</span></span>
<span id="cb7-700"><a href="#cb7-700" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> z.cpu().numpy()</span>
<span id="cb7-701"><a href="#cb7-701" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-702"><a href="#cb7-702" aria-hidden="true" tabindex="-1"></a>Xtr <span class="op">=</span> feats(train_images)<span class="op">;</span> Ytr <span class="op">=</span> train_labels</span>
<span id="cb7-703"><a href="#cb7-703" aria-hidden="true" tabindex="-1"></a>Xte <span class="op">=</span> feats(test_images)</span>
<span id="cb7-704"><a href="#cb7-704" aria-hidden="true" tabindex="-1"></a>knn <span class="op">=</span> sk.KNeighborsClassifier(n_neighbors<span class="op">=</span><span class="dv">5</span>).fit(Xtr, Ytr)</span>
<span id="cb7-705"><a href="#cb7-705" aria-hidden="true" tabindex="-1"></a>pred <span class="op">=</span> knn.predict(Xte)</span>
<span id="cb7-706"><a href="#cb7-706" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-707"><a href="#cb7-707" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-710"><a href="#cb7-710" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-711"><a href="#cb7-711" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb7-712"><a href="#cb7-712" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb7-713"><a href="#cb7-713" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-rl-practical-linear</span></span>
<span id="cb7-714"><a href="#cb7-714" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap:</span></span>
<span id="cb7-715"><a href="#cb7-715" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - "Linear probe (freeze encoder, train head) — timm"</span></span>
<span id="cb7-716"><a href="#cb7-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-717"><a href="#cb7-717" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb7-718"><a href="#cb7-718" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn, optim</span>
<span id="cb7-719"><a href="#cb7-719" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> timm</span>
<span id="cb7-720"><a href="#cb7-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-721"><a href="#cb7-721" aria-hidden="true" tabindex="-1"></a>backbone <span class="op">=</span> timm.create_model(<span class="st">"vit_base_patch16_224"</span>, pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-722"><a href="#cb7-722" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> p <span class="kw">in</span> backbone.parameters():</span>
<span id="cb7-723"><a href="#cb7-723" aria-hidden="true" tabindex="-1"></a>    p.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb7-724"><a href="#cb7-724" aria-hidden="true" tabindex="-1"></a>embed_dim <span class="op">=</span> backbone.num_features</span>
<span id="cb7-725"><a href="#cb7-725" aria-hidden="true" tabindex="-1"></a>head <span class="op">=</span> nn.Linear(embed_dim, num_classes)</span>
<span id="cb7-726"><a href="#cb7-726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-727"><a href="#cb7-727" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(images):</span>
<span id="cb7-728"><a href="#cb7-728" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb7-729"><a href="#cb7-729" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> backbone.forward_features(images)[:,<span class="dv">0</span>]  <span class="co"># [CLS]</span></span>
<span id="cb7-730"><a href="#cb7-730" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> head(z)</span>
<span id="cb7-731"><a href="#cb7-731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-732"><a href="#cb7-732" aria-hidden="true" tabindex="-1"></a>opt <span class="op">=</span> optim.AdamW(head.parameters(), lr<span class="op">=</span><span class="fl">5e-4</span>)</span>
<span id="cb7-733"><a href="#cb7-733" aria-hidden="true" tabindex="-1"></a><span class="co"># training loop: logits = forward(images); loss = CE(logits, y); loss.backward(); opt.step()</span></span>
<span id="cb7-734"><a href="#cb7-734" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-735"><a href="#cb7-735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-738"><a href="#cb7-738" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-739"><a href="#cb7-739" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb7-740"><a href="#cb7-740" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb7-741"><a href="#cb7-741" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-rl-practical-adapter</span></span>
<span id="cb7-742"><a href="#cb7-742" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap:</span></span>
<span id="cb7-743"><a href="#cb7-743" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - "LoRA / Adapters (parameter-efficient tuning) — Hugging Face + PEFT"</span></span>
<span id="cb7-744"><a href="#cb7-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-745"><a href="#cb7-745" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb7-746"><a href="#cb7-746" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModel</span>
<span id="cb7-747"><a href="#cb7-747" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> peft <span class="im">import</span> LoraConfig, get_peft_model</span>
<span id="cb7-748"><a href="#cb7-748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-749"><a href="#cb7-749" aria-hidden="true" tabindex="-1"></a>model_id <span class="op">=</span> <span class="st">"google/vit-base-patch16-224"</span></span>
<span id="cb7-750"><a href="#cb7-750" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModel.from_pretrained(model_id)</span>
<span id="cb7-751"><a href="#cb7-751" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-752"><a href="#cb7-752" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> LoraConfig(</span>
<span id="cb7-753"><a href="#cb7-753" aria-hidden="true" tabindex="-1"></a>    r<span class="op">=</span><span class="dv">16</span>, lora_alpha<span class="op">=</span><span class="dv">32</span>, lora_dropout<span class="op">=</span><span class="fl">0.05</span>,</span>
<span id="cb7-754"><a href="#cb7-754" aria-hidden="true" tabindex="-1"></a>    target_modules<span class="op">=</span>[<span class="st">"query"</span>,<span class="st">"key"</span>,<span class="st">"value"</span>,<span class="st">"fc"</span>],  <span class="co"># adjust to module names</span></span>
<span id="cb7-755"><a href="#cb7-755" aria-hidden="true" tabindex="-1"></a>    bias<span class="op">=</span><span class="st">"none"</span>, task_type<span class="op">=</span><span class="st">"FEATURE_EXTRACTION"</span></span>
<span id="cb7-756"><a href="#cb7-756" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-757"><a href="#cb7-757" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> get_peft_model(model, config)</span>
<span id="cb7-758"><a href="#cb7-758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-759"><a href="#cb7-759" aria-hidden="true" tabindex="-1"></a>head <span class="op">=</span> torch.nn.Linear(model.config.hidden_size, num_classes)</span>
<span id="cb7-760"><a href="#cb7-760" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-761"><a href="#cb7-761" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(pixel_values):</span>
<span id="cb7-762"><a href="#cb7-762" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> model(pixel_values<span class="op">=</span>pixel_values).last_hidden_state[:,<span class="dv">0</span>]</span>
<span id="cb7-763"><a href="#cb7-763" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> head(z)</span>
<span id="cb7-764"><a href="#cb7-764" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimizer over LoRA params + head params only</span></span>
<span id="cb7-765"><a href="#cb7-765" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-766"><a href="#cb7-766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-769"><a href="#cb7-769" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-770"><a href="#cb7-770" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb7-771"><a href="#cb7-771" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb7-772"><a href="#cb7-772" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-rl-practical-fine-tune</span></span>
<span id="cb7-773"><a href="#cb7-773" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap:</span></span>
<span id="cb7-774"><a href="#cb7-774" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - "Partial fine-tune (unfreeze top layers) — timm"</span></span>
<span id="cb7-775"><a href="#cb7-775" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-776"><a href="#cb7-776" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> timm, torch</span>
<span id="cb7-777"><a href="#cb7-777" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb7-778"><a href="#cb7-778" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-779"><a href="#cb7-779" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> timm.create_model(<span class="st">"vit_base_patch16_224"</span>, pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-780"><a href="#cb7-780" aria-hidden="true" tabindex="-1"></a><span class="co"># Freeze everything</span></span>
<span id="cb7-781"><a href="#cb7-781" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> p <span class="kw">in</span> model.parameters():</span>
<span id="cb7-782"><a href="#cb7-782" aria-hidden="true" tabindex="-1"></a>    p.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb7-783"><a href="#cb7-783" aria-hidden="true" tabindex="-1"></a><span class="co"># Unfreeze top transformer block(s)</span></span>
<span id="cb7-784"><a href="#cb7-784" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> p <span class="kw">in</span> model.blocks[<span class="op">-</span><span class="dv">2</span>:].parameters():</span>
<span id="cb7-785"><a href="#cb7-785" aria-hidden="true" tabindex="-1"></a>    p.requires_grad <span class="op">=</span> <span class="va">True</span>  <span class="co"># last 2 blocks</span></span>
<span id="cb7-786"><a href="#cb7-786" aria-hidden="true" tabindex="-1"></a><span class="co"># Replace / add classification head</span></span>
<span id="cb7-787"><a href="#cb7-787" aria-hidden="true" tabindex="-1"></a>model.head <span class="op">=</span> nn.Linear(model.num_features, num_classes)</span>
<span id="cb7-788"><a href="#cb7-788" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimizer over unfrozen blocks + head</span></span>
<span id="cb7-789"><a href="#cb7-789" aria-hidden="true" tabindex="-1"></a>trainable <span class="op">=</span> [p <span class="cf">for</span> p <span class="kw">in</span> model.parameters() <span class="cf">if</span> p.requires_grad]</span>
<span id="cb7-790"><a href="#cb7-790" aria-hidden="true" tabindex="-1"></a>opt <span class="op">=</span> torch.optim.AdamW(trainable, lr<span class="op">=</span><span class="fl">1e-4</span>)</span>
<span id="cb7-791"><a href="#cb7-791" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-792"><a href="#cb7-792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-795"><a href="#cb7-795" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb7-796"><a href="#cb7-796" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb7-797"><a href="#cb7-797" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb7-798"><a href="#cb7-798" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-rl-practical-full-fine-tune</span></span>
<span id="cb7-799"><a href="#cb7-799" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap:</span></span>
<span id="cb7-800"><a href="#cb7-800" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - "Full fine-tuning, unfreeze all layers."</span></span>
<span id="cb7-801"><a href="#cb7-801" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-802"><a href="#cb7-802" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb7-803"><a href="#cb7-803" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForImageClassification</span>
<span id="cb7-804"><a href="#cb7-804" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-805"><a href="#cb7-805" aria-hidden="true" tabindex="-1"></a>model_id <span class="op">=</span> <span class="st">"google/vit-base-patch16-224"</span></span>
<span id="cb7-806"><a href="#cb7-806" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForImageClassification.from_pretrained(</span>
<span id="cb7-807"><a href="#cb7-807" aria-hidden="true" tabindex="-1"></a>    model_id, num_labels<span class="op">=</span>num_classes, ignore_mismatched_sizes<span class="op">=</span><span class="va">True</span></span>
<span id="cb7-808"><a href="#cb7-808" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-809"><a href="#cb7-809" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> p <span class="kw">in</span> model.parameters():</span>
<span id="cb7-810"><a href="#cb7-810" aria-hidden="true" tabindex="-1"></a>    p.requires_grad <span class="op">=</span> <span class="va">True</span>  <span class="co"># unfreeze all</span></span>
<span id="cb7-811"><a href="#cb7-811" aria-hidden="true" tabindex="-1"></a>opt <span class="op">=</span> torch.optim.AdamW(model.parameters(), lr<span class="op">=</span><span class="fl">5e-5</span>)</span>
<span id="cb7-812"><a href="#cb7-812" aria-hidden="true" tabindex="-1"></a><span class="co"># Standard supervised loop with processor(...)-&gt; pixel_values -&gt; model(pixel_values, labels=y)</span></span>
<span id="cb7-813"><a href="#cb7-813" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb7-814"><a href="#cb7-814" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-815"><a href="#cb7-815" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-816"><a href="#cb7-816" aria-hidden="true" tabindex="-1"></a><span class="fu">## Quiz</span></span>
<span id="cb7-817"><a href="#cb7-817" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-818"><a href="#cb7-818" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Question 1: Understanding Representation Learning"}</span>
<span id="cb7-819"><a href="#cb7-819" aria-hidden="true" tabindex="-1"></a>**Question:** Why do representations learned from raw pixels (pixel-space similarity) fail to capture semantic relationships between images?</span>
<span id="cb7-820"><a href="#cb7-820" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-821"><a href="#cb7-821" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb7-822"><a href="#cb7-822" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Click for Answer<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb7-823"><a href="#cb7-823" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-824"><a href="#cb7-824" aria-hidden="true" tabindex="-1"></a>**Answer:** Pixel-space representations measure low-level appearance differences (exact color values, lighting conditions, spatial alignment) rather than high-level semantic content. Two images of the same object under different lighting or poses will have large pixel-level distances despite being semantically identical. For example, in @fig-rl-similarity-pixel-space, pixel distances cannot distinguish between "different elephants in similar lighting" vs "same elephant in different lighting": both create similar pixel-level differences. Good representations compress the enormous space of possible images ($255^{H \times W \times C}$ combinations) into a semantic coordinate system where distances reflect meaning.</span>
<span id="cb7-825"><a href="#cb7-825" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-826"><a href="#cb7-826" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb7-827"><a href="#cb7-827" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-828"><a href="#cb7-828" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-829"><a href="#cb7-829" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Question 2: Invariance and Selectivity Trade-off"}</span>
<span id="cb7-830"><a href="#cb7-830" aria-hidden="true" tabindex="-1"></a>**Question:** Explain the invariance-selectivity trade-off in representation learning. Provide one concrete example where too much invariance would harm performance on a specific task.</span>
<span id="cb7-831"><a href="#cb7-831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-832"><a href="#cb7-832" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb7-833"><a href="#cb7-833" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Click for Answer<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb7-834"><a href="#cb7-834" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-835"><a href="#cb7-835" aria-hidden="true" tabindex="-1"></a>**Answer:**</span>
<span id="cb7-836"><a href="#cb7-836" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-837"><a href="#cb7-837" aria-hidden="true" tabindex="-1"></a>**The Trade-off:** Good representations must balance two competing goals:</span>
<span id="cb7-838"><a href="#cb7-838" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-839"><a href="#cb7-839" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Invariance:** $<span class="sc">\|</span>f(\vect{x}) - f(g(\vect{x}))<span class="sc">\|</span>_2 &lt; \epsilon$ for nuisance transforms $g$ (lighting, pose changes)</span>
<span id="cb7-840"><a href="#cb7-840" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Selectivity:** $<span class="sc">\|</span>f(\vect{x}_i) - f(\vect{x}_j)<span class="sc">\|</span>_2 &gt; \delta$ for semantically different inputs</span>
<span id="cb7-841"><a href="#cb7-841" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-842"><a href="#cb7-842" aria-hidden="true" tabindex="-1"></a>**Concrete Example - Flower Classification:**</span>
<span id="cb7-843"><a href="#cb7-843" aria-hidden="true" tabindex="-1"></a>If representations are completely **color-invariant** (treating all colors as identical), distinguishing between red roses and yellow roses becomes impossible, even though color is the primary discriminative feature. The model would collapse all flower images to similar embeddings regardless of color, destroying task-critical information.</span>
<span id="cb7-844"><a href="#cb7-844" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-845"><a href="#cb7-845" aria-hidden="true" tabindex="-1"></a>**Other Examples:**</span>
<span id="cb7-846"><a href="#cb7-846" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-847"><a href="#cb7-847" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Medical imaging:** Rotation invariance destroys anatomical orientation information needed for diagnosis</span>
<span id="cb7-848"><a href="#cb7-848" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Object size estimation:** Scale invariance removes absolute/relative size cues</span>
<span id="cb7-849"><a href="#cb7-849" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Material classification:** Texture invariance prevents distinguishing wood from metal</span>
<span id="cb7-850"><a href="#cb7-850" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-851"><a href="#cb7-851" aria-hidden="true" tabindex="-1"></a>**Design Principle:** Match invariances to task requirements — preserve information the downstream task needs, discard only true nuisance factors.</span>
<span id="cb7-852"><a href="#cb7-852" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-853"><a href="#cb7-853" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb7-854"><a href="#cb7-854" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-855"><a href="#cb7-855" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-856"><a href="#cb7-856" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Question 3: Comparing SSL Objectives"}</span>
<span id="cb7-857"><a href="#cb7-857" aria-hidden="true" tabindex="-1"></a>**Question:** What is the key advantage of self-distillation methods (DINO/DINOv2/DINOv3) over contrastive learning methods (SimCLR)?</span>
<span id="cb7-858"><a href="#cb7-858" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-859"><a href="#cb7-859" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb7-860"><a href="#cb7-860" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Click for Answer<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb7-861"><a href="#cb7-861" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-862"><a href="#cb7-862" aria-hidden="true" tabindex="-1"></a>**Answer:**</span>
<span id="cb7-863"><a href="#cb7-863" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-864"><a href="#cb7-864" aria-hidden="true" tabindex="-1"></a>**Key Advantage:** Self-distillation does **not require explicit negative samples** or large batch sizes.</span>
<span id="cb7-865"><a href="#cb7-865" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-866"><a href="#cb7-866" aria-hidden="true" tabindex="-1"></a>**Contrastive Learning (SimCLR):**</span>
<span id="cb7-867"><a href="#cb7-867" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-868"><a href="#cb7-868" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Requires large batches (256-4096) to provide sufficient negative pairs</span>
<span id="cb7-869"><a href="#cb7-869" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>InfoNCE loss: $\mathcal{L} = -\log \frac{\exp(\text{sim}(z_i, z_i^+)/\tau)}{\exp(\text{sim}(z_i, z_i^+)/\tau) + \sum_{j=1}^{N-1} \exp(\text{sim}(z_i, z_j^-)/\tau)}$</span>
<span id="cb7-870"><a href="#cb7-870" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Computational cost and memory scale with batch size</span>
<span id="cb7-871"><a href="#cb7-871" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Needs careful negative mining strategies</span>
<span id="cb7-872"><a href="#cb7-872" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-873"><a href="#cb7-873" aria-hidden="true" tabindex="-1"></a>**Self-Distillation (DINO):**</span>
<span id="cb7-874"><a href="#cb7-874" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-875"><a href="#cb7-875" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Student network matches teacher's probability distribution over crops</span>
<span id="cb7-876"><a href="#cb7-876" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Teacher is EMA of student weights (no separate training)</span>
<span id="cb7-877"><a href="#cb7-877" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Emergent clustering arises from teacher-student dynamics and centering/sharpening</span>
<span id="cb7-878"><a href="#cb7-878" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Works with smaller batches, more memory-efficient</span>
<span id="cb7-879"><a href="#cb7-879" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Bonus:** Produces both strong global descriptors AND high-quality local (patch) tokens</span>
<span id="cb7-880"><a href="#cb7-880" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-881"><a href="#cb7-881" aria-hidden="true" tabindex="-1"></a>**Result:** DINO-family models achieve state-of-the-art transfer performance with simpler training dynamics and lower computational requirements.</span>
<span id="cb7-882"><a href="#cb7-882" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-883"><a href="#cb7-883" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb7-884"><a href="#cb7-884" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-885"><a href="#cb7-885" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-886"><a href="#cb7-886" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Question 4: Global vs Local Representations"}</span>
<span id="cb7-887"><a href="#cb7-887" aria-hidden="true" tabindex="-1"></a>**Question:** When should you use global representations versus local representations? Give one specific task example for each.</span>
<span id="cb7-888"><a href="#cb7-888" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-889"><a href="#cb7-889" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb7-890"><a href="#cb7-890" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Click for Answer<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb7-891"><a href="#cb7-891" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-892"><a href="#cb7-892" aria-hidden="true" tabindex="-1"></a>**Answer:**</span>
<span id="cb7-893"><a href="#cb7-893" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-894"><a href="#cb7-894" aria-hidden="true" tabindex="-1"></a>**Global Representations** (single embedding per image, e.g., <span class="co">[</span><span class="ot">CLS</span><span class="co">]</span> token):</span>
<span id="cb7-895"><a href="#cb7-895" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-896"><a href="#cb7-896" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Use when:** Task requires image-level understanding without spatial localization</span>
<span id="cb7-897"><a href="#cb7-897" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Examples:**</span>
<span id="cb7-898"><a href="#cb7-898" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**Image classification:** "Is this a cat or dog?" — only need overall category</span>
<span id="cb7-899"><a href="#cb7-899" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**Image retrieval:** "Find similar vacation photos" — semantic similarity at image level</span>
<span id="cb7-900"><a href="#cb7-900" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**Zero-shot classification (CLIP):** "Does this image contain a beach?" — image-text alignment</span>
<span id="cb7-901"><a href="#cb7-901" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-902"><a href="#cb7-902" aria-hidden="true" tabindex="-1"></a>**Local Representations** (spatially-resolved patch tokens, e.g., DINOv3 patch features):</span>
<span id="cb7-903"><a href="#cb7-903" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-904"><a href="#cb7-904" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Use when:** Task requires pixel-level or region-level understanding</span>
<span id="cb7-905"><a href="#cb7-905" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Examples:**</span>
<span id="cb7-906"><a href="#cb7-906" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**Semantic segmentation:** "Label every pixel as road/car/pedestrian" — need spatial structure</span>
<span id="cb7-907"><a href="#cb7-907" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**Object detection:** "Find all bounding boxes containing cars" — need localization</span>
<span id="cb7-908"><a href="#cb7-908" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**Dense prediction:** "Generate depth map for each pixel" — requires preserved spatial information</span>
<span id="cb7-909"><a href="#cb7-909" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-910"><a href="#cb7-910" aria-hidden="true" tabindex="-1"></a>**Key Difference:** Global embeddings through pooling (e.g., Global Average Pooling: $\text{GAP}(X) = \frac{1}{H \times W}\sum_{i,j} X_{i,j}$) discard spatial structure for efficiency, while local features maintain the spatial grid of patch embeddings.</span>
<span id="cb7-911"><a href="#cb7-911" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-912"><a href="#cb7-912" aria-hidden="true" tabindex="-1"></a>**Practical Tip:** Many modern architectures (DINOv3, ViTs) provide **both** — use the <span class="co">[</span><span class="ot">CLS</span><span class="co">]</span> token for global tasks and patch tokens for dense tasks.</span>
<span id="cb7-913"><a href="#cb7-913" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-914"><a href="#cb7-914" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb7-915"><a href="#cb7-915" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-916"><a href="#cb7-916" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-917"><a href="#cb7-917" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Question 5: Adaptation Strategy Selection"}</span>
<span id="cb7-918"><a href="#cb7-918" aria-hidden="true" tabindex="-1"></a>**Question:** You have a dataset of 1,000 labeled medical images for a specialized classification task. Describe the recommended progression of adaptation strategies and explain why you should start with the simplest approach.</span>
<span id="cb7-919"><a href="#cb7-919" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-920"><a href="#cb7-920" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb7-921"><a href="#cb7-921" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Click for Answer<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb7-922"><a href="#cb7-922" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-923"><a href="#cb7-923" aria-hidden="true" tabindex="-1"></a>**Answer:**</span>
<span id="cb7-924"><a href="#cb7-924" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-925"><a href="#cb7-925" aria-hidden="true" tabindex="-1"></a>**Recommended Progression (from simplest to most complex):**</span>
<span id="cb7-926"><a href="#cb7-926" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-927"><a href="#cb7-927" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**k-NN Baseline (0 training):**</span>
<span id="cb7-928"><a href="#cb7-928" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Extract features with frozen pretrained model</span>
<span id="cb7-929"><a href="#cb7-929" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Classify by finding k nearest neighbors in feature space</span>
<span id="cb7-930"><a href="#cb7-930" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Why first:** Instant results, no hyperparameters, reveals representation quality</span>
<span id="cb7-931"><a href="#cb7-931" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Code:** Just <span class="in">`sklearn.KNeighborsClassifier(n_neighbors=5).fit(features, labels)`</span></span>
<span id="cb7-932"><a href="#cb7-932" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-933"><a href="#cb7-933" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Linear Probe (~1-10K parameters):**</span>
<span id="cb7-934"><a href="#cb7-934" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Freeze encoder, train only linear classification head</span>
<span id="cb7-935"><a href="#cb7-935" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Why next:** If this works well, embeddings already separate classes</span>
<span id="cb7-936"><a href="#cb7-936" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Benefit:** Fast, stable, minimal overfitting risk with 1K samples</span>
<span id="cb7-937"><a href="#cb7-937" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-938"><a href="#cb7-938" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**LoRA/Adapters (~100K-1M parameters):**</span>
<span id="cb7-939"><a href="#cb7-939" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Add small adapter modules, train only those</span>
<span id="cb7-940"><a href="#cb7-940" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Why:** Allows task-specific adaptation without full fine-tuning</span>
<span id="cb7-941"><a href="#cb7-941" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Trade-off:** Better fit than linear probe, much cheaper than full fine-tuning</span>
<span id="cb7-942"><a href="#cb7-942" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-943"><a href="#cb7-943" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Partial Fine-tune (last 2-3 layers):**</span>
<span id="cb7-944"><a href="#cb7-944" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Unfreeze top transformer blocks + head</span>
<span id="cb7-945"><a href="#cb7-945" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Why:** Adjust high-level features while keeping general low-level features</span>
<span id="cb7-946"><a href="#cb7-946" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Risk:** Need careful learning rate tuning to avoid catastrophic forgetting</span>
<span id="cb7-947"><a href="#cb7-947" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-948"><a href="#cb7-948" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Full Fine-tune (all parameters) — Last Resort:**</span>
<span id="cb7-949"><a href="#cb7-949" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Only if:** Previous methods plateau AND you have sufficient data</span>
<span id="cb7-950"><a href="#cb7-950" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Risk:** With only 1K samples, high overfitting risk</span>
<span id="cb7-951"><a href="#cb7-951" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Mitigation:** Strong regularization, small learning rates (1e-5), early stopping</span>
<span id="cb7-952"><a href="#cb7-952" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-953"><a href="#cb7-953" aria-hidden="true" tabindex="-1"></a>**Why Start Simple:**</span>
<span id="cb7-954"><a href="#cb7-954" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-955"><a href="#cb7-955" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Diagnostic value:** If k-NN works, you know the representation is good</span>
<span id="cb7-956"><a href="#cb7-956" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Resource efficiency:** Avoid wasting compute on complex methods when simple ones suffice</span>
<span id="cb7-957"><a href="#cb7-957" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Overfitting prevention:** With limited data (1K samples), simpler models generalize better</span>
<span id="cb7-958"><a href="#cb7-958" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Baseline establishment:** Each step provides a performance ceiling to beat</span>
<span id="cb7-959"><a href="#cb7-959" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-960"><a href="#cb7-960" aria-hidden="true" tabindex="-1"></a>**Medical Imaging Specific:** Consider domain shift — medical images may differ significantly from natural images (ImageNet). If k-NN/linear probe fail, the pretrained features may not transfer well, suggesting need for more adaptation or domain-specific pretraining.</span>
<span id="cb7-961"><a href="#cb7-961" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-962"><a href="#cb7-962" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb7-963"><a href="#cb7-963" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb7-964"><a href="#cb7-964" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-965"><a href="#cb7-965" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-966"><a href="#cb7-966" aria-hidden="true" tabindex="-1"></a>::: {#refs}</span>
<span id="cb7-967"><a href="#cb7-967" aria-hidden="true" tabindex="-1"></a>:::</span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© Copyright 2025, Marco Willi</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2025/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This page is built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>