---
title: "3 - Image Classification"
params:
   images_path: "/assets/images/classification/"
execute:
  cache: false
  freeze: false
---

::: {.content-hidden}
$$
{{< include /assets/_macros.tex >}}
$$
:::

::: {.callout-tip title="Learning Objectives" collapse="true"}

- Define and recognize classification problems.
- Interpret logits, probabilities and the role of the softmax function.
- Explain cross-entropy and maximum likelihood on a high-level.
- Compare classic CNN architectures (AlexNet‚ÜíVGG‚ÜíResNet‚ÜíConvNeXt).
- Apply transfer learning and explain when to freeze vs fine-tune layers.
:::

::: {.callout-tip title="TLDR Recap" collapse="true"}

**Image Classification: Core Task**

- Assign images to predefined classes (multi-class: ‚â•2 classes, binary: 2 classes, multi-label: multiple class sets)
- Use CNNs to learn hierarchical features automatically from raw pixels
- Output: class probabilities or labels

**Softmax: Logits ‚Üí Probabilities**

$$P(Y = k \mid X = \vect{x}) = \frac{e^{z_k}}{\sum_{j=1}^K e^{z_j}}$$

- **Logits** $\vect{z}$: Raw model outputs (unbounded)
- **Softmax** $\sigma(\vect{z})$: Converts logits to valid probability distribution
- Properties: Always sums to 1, outputs in [0,1], differentiable

**Cross-Entropy Loss = Negative Log-Likelihood**

Maximum Likelihood approach:

$$L(\m{X}, \vect{y}, \theta) = -\sum_{i=1}^N \sum_{j=1}^K y^{(i)}_j \log P(Y = j\mid X = \vect{x}^{(i)})$$

- Minimizing negative log-likelihood = maximizing likelihood of correct predictions
- Equivalent to cross-entropy between true distribution $\vect{y}$ and predicted $\hat{\vect{y}}$
- Penalizes confident wrong predictions heavily

**Key Insight**: Softmax + Cross-Entropy arise naturally from probabilistic modeling (categorical distribution + maximum likelihood estimation)

**Classic CNN Architectures Evolution**

1. **AlexNet (2012)**: 5 conv + 3 FC layers, ReLU, dropout ‚Üí ImageNet breakthrough
2. **VGG (2014)**: Deeper (16-19 layers), small 3√ó3 kernels, simple design ‚Üí showed depth matters
3. **ResNet (2015)**: Residual connections (skip connections) ‚Üí enabled 50-152 layers, solved vanishing gradients
4. **ConvNeXt (2022)**: Modern CNN with Transformer-era tricks ‚Üí competes with Vision Transformers

**Residual Connections: The Key Innovation**

$$\vect{x}_{l+1} = \vect{x}_l + F(\vect{x}_l)$$

- Network learns residual $F(\vect{x})$ instead of full transformation
- Enables training very deep networks (>100 layers)
- Smoother loss landscape ‚Üí better optimization
- Default behavior: pass activations unchanged (identity mapping)

**Transfer Learning: Standing on Giants' Shoulders**

1. **Pre-training**: Train on large dataset (e.g., ImageNet: 1.2M images, 1000 classes)
2. **Transfer**: Adapt to your specific task with much less data
3. **Strategies**:
   - **Freeze early layers** (extract features only) when data is very scarce
   - **Fine-tune all layers** when you have moderate data
   - **Fine-tune only later layers** as middle ground

**Why Transfer Learning Works**:
- Early layers learn general features (edges, textures)
- Later layers learn task-specific features
- Pre-trained features transfer well across vision tasks

**Architecture Selection Strategy**

- ‚úÖ **Don't be a hero**: Use proven architectures first
- ‚úÖ **ResNet-50/101**: Excellent baseline for most tasks
- ‚úÖ **EfficientNet**: When model size/speed matters
- ‚úÖ **ConvNeXt**: State-of-the-art CNN performance

**Consider**:
- Accuracy requirements
- Inference speed (FLOPs)
- Model size (memory)
- Available training data

**Modern Components**

- **Normalization layers** (BatchNorm, LayerNorm): Stabilize training, enable deeper networks
- **Global Average Pooling**: Replace large FC layers ‚Üí fewer parameters
- **SE Blocks**: Learn channel-wise attention ‚Üí better feature recalibration
- **Depthwise Separable Convolutions**: Reduce parameters while maintaining performance

**Pre-processing Pipeline**

1. Resize/crop to fixed size (e.g., 224√ó224)
2. Scale pixel values: [0, 255] ‚Üí [0, 1]
3. Normalize: Subtract mean, divide by std per channel
4. Data augmentation (training): flips, crops, color jitter

**Best Practices**

- ‚úÖ Start with pre-trained models (transfer learning)
- ‚úÖ Use proven architectures (ResNet, ConvNeXt)
- ‚úÖ Apply proper preprocessing (resize, normalize)
- ‚úÖ Use cross-entropy loss for classification
- ‚úÖ Monitor both training and validation metrics
- ‚úÖ Consider calibration (temperature scaling, label smoothing)

**Key Takeaway**: Classification success comes from combining strong architectures (ResNets), probabilistic foundations (softmax + cross-entropy), transfer learning, and proper training practices.

:::



## Introduction

Image classification is a core task of computer vision. In image classification, an image is assigned to a **predefined set** of classes. There are different variants of image classification:

- in **multi-class classification** there are $> 2$ classes
- in **binary classification** there are 2 classes
- in **multi-label classification** there are several sets of classes.

@fig-classification-classification-illustration illustrates a multi-class classification task.

::: {#fig-classification-classification-illustration}
![]({{< meta params.images_path >}}classification_illustration.jpg)

Image Classification example.
:::

@fig-image-classifier-example shows an example from the paper by @krizhevsky_imagenet_2012, which achieved the best results in the 2012 ImageNet competition, demonstrating how well CNNs work. Note that each image had to be assigned to one of 1'000 classes.

::: {#fig-image-classifier-example}
![]({{< meta params.images_path >}}image_classification_example.png)

Image Classification example (from @krizhevsky_imagenet_2012).
:::

@fig-classification-camera-traps illustrates the challenge with images taken by camera traps, which need to be classified along animal species.

::: {#fig-classification-camera-traps}
![]({{< meta params.images_path >}}camera_traps.png)

Example images from camera traps.
:::

## Parametric Approach

In a parametric approach, we seek a model of the following form:

$$
\hat{y}^{(i)} = f(\theta, \vect{x}^{(i)})
$$

We want to find model parameters $\theta$ that output a score/prediction $\hat{y}^{(i)}$ for any data points $\vect{x}^{(i)}$ for each class $k \in K$. We then want to assess how good this score is with the help of a loss function.

Note: The model parameters $\theta$ are all the learnable parameters of a model, e.g. the filters of the convolutional layers.

### Softmax Classifier

With a softmax classifier, we interpret model predictions/scores as probabilities of class memberships: $P(Y=\vect{y}^{(i)}| X = \vect{x}^{(i)})$. We interpret the output of a model as the parameters of a [Categorical Distribution](https://en.wikipedia.org/wiki/Categorical_distribution) over all possible classes.

To obtain a valid probability distribution, the untransformed outputs $\vect{z}$, also called **logits**, of a model are transformed with the softmax function $\sigma(\vect{z})$:

$$
P(Y = k| X = \vect{x}^{(i)}) = \sigma(\vect{z})_k = \frac{e^{z_k}}{\sum_i^K e^{z_i}}
$$

@fig-classification-logits-to-probs shows an example of the effect of the softmax transformation.

```{python}
#| eval: true
#| echo: false
#| fig-width: 600
#| label: fig-classification-logits-to-probs
#| fig-cap:
#|   - "Logits (left) to probabilities with the Softmax function (right)."

from matplotlib import pyplot as plt
import numpy as np
import seaborn as sns

np.random.seed(123)
logits = np.random.normal(size=(10, )) * 2
softmax_output = np.exp(logits) / np.exp(logits).sum()

fig, ax = plt.subplots(ncols=2, figsize=(9, 3))
_ = sns.barplot(x=[i for i in range(0, 10)], y=logits, ax=ax[0]).set(
    title="Logits")
_ = sns.barplot(x=[i for i in range(0, 10)], y=softmax_output, ax=ax[1]).set(
    title="Softmax", ylim=(0, 1))

plt.tight_layout()
plt.show()
```


<!-- {{< embed /demos/logits_to_probabilities.ipynb#fig-demo-logits-to-probs >}} -->



::: {.callout-tip title="üéÆ Interactive Exploration" collapse="true"}
**Try different logit distributions** and see how softmax transforms them into probabilities!

```{python}
# | eval: true
# | echo: false
# | fig-width: 800

import numpy as np
import plotly.graph_objects as go
from plotly.subplots import make_subplots


def softmax(logits):
    """Compute softmax values for logits."""
    exp_logits = np.exp(logits - np.max(logits))  # numerical stability
    return exp_logits / np.sum(exp_logits)


def _padded_range(vals, frac=0.18, min_abs_pad=0.35):
    """Return a y-range padded enough so 'textposition=outside' is always visible."""
    vmin, vmax = float(np.min(vals)), float(np.max(vals))
    span = max(1e-9, vmax - vmin)
    pad = max(min_abs_pad, span * frac)
    return [vmin - pad, vmax + pad]


def create_interactive_logits_explorer():
    """Interactive exploration of logits ‚Üí probabilities."""
    n_classes = 5

    # Scenarios
    scenarios = {
        "Uniform Logits": [1.0, 1.0, 1.0, 1.0, 1.0],
        "Clear Winner": [5.0, 1.0, 0.5, 0.0, -0.5],
        "Close Competition": [2.0, 1.8, 1.5, 1.2, 1.0],
        "Very Confident": [10.0, 0.0, 0.0, 0.0, 0.0],
        "Negative Logits": [-0.5, -1.0, -1.5, -2.0, -2.5],
        "Mixed Range": [3.0, 0.0, -1.0, 0.5, 2.0],
        "Two Leaders": [4.0, 3.8, 0.0, 0.0, 0.0],
        "Random-ish": [1.5, -0.3, 2.1, 0.8, -0.8],
    }

    # Initial scenario
    initial_name = "Clear Winner"
    initial_logits = scenarios[initial_name]
    initial_probs = softmax(initial_logits)

    # Figure and subplots
    fig = make_subplots(
        rows=1,
        cols=2,
        subplot_titles=("Logits (Raw Model Outputs)", "Probabilities (After Softmax)"),
        horizontal_spacing=0.12,
    )

    # Nudge subplot titles slightly down to leave a lane for the dropdown above them
    for ann in fig.layout.annotations:
        if ann.text in ("Logits (Raw Model Outputs)", "Probabilities (After Softmax)"):
            ann.y = 1.0
            ann.yanchor = "bottom"

    # Colors for initial state
    max_logit_idx = int(np.argmax(initial_logits))
    logit_colors = [
        "lightblue" if i != max_logit_idx else "steelblue" for i in range(n_classes)
    ]

    max_prob_idx = int(np.argmax(initial_probs))
    prob_colors = [
        "lightcoral" if i != max_prob_idx else "crimson" for i in range(n_classes)
    ]

    # Left: logits bars
    fig.add_trace(
        go.Bar(
            x=[f"Class {i}" for i in range(n_classes)],
            y=initial_logits,
            name="Logits",
            marker_color=logit_colors,
            text=[f"{z:.2f}" for z in initial_logits],
            textposition="outside",
        ),
        row=1,
        col=1,
    )

    # Right: probability bars
    fig.add_trace(
        go.Bar(
            x=[f"Class {i}" for i in range(n_classes)],
            y=initial_probs,
            name="Probabilities",
            marker_color=prob_colors,
            text=[f"{p:.3f}" for p in initial_probs],
            textposition="outside",
        ),
        row=1,
        col=2,
    )

    # Dropdown buttons
    buttons = []
    for name, logits in scenarios.items():
        probs = softmax(logits)

        max_logit_idx = int(np.argmax(logits))
        logit_colors = [
            "lightblue" if i != max_logit_idx else "steelblue" for i in range(n_classes)
        ]

        max_prob_idx = int(np.argmax(probs))
        prob_colors = [
            "lightcoral" if i != max_prob_idx else "crimson" for i in range(n_classes)
        ]

        entropy = -np.sum(probs * np.log(probs + 1e-10))
        max_prob = float(np.max(probs))

        y1_range = _padded_range(logits)

        buttons.append(
            dict(
                label=f"{name} (max prob={max_prob:.3f}, H={entropy:.2f})",
                method="update",
                args=[
                    {
                        "y": [logits, probs],
                        "text": [
                            [f"{z:.2f}" for z in logits],
                            [f"{p:.3f}" for p in probs],
                        ],
                        "marker.color": [logit_colors, prob_colors],
                    },
                    {
                        "yaxis.range": y1_range,  # left subplot (logits)
                        "yaxis2.range": [0, 1.1],  # right subplot fixed
                        "annotations[2].text": f"Sum = {sum(probs):.6f}",  # keep "Sum=" annotation current
                    },
                ],
            )
        )

    # Layout: compact head space; dropdown above titles; opens upward
    fig.update_layout(
        updatemenus=[
            dict(
                type="dropdown",
                direction="down",
                x=0.5,
                xanchor="center",
                y=1.06,
                yanchor="bottom",  # sits just above titles
                buttons=buttons,
                pad=dict(t=4, r=6, l=6, b=10),
                bgcolor="white",
                bordercolor="#ddd",
                borderwidth=1,
            )
        ],
        height=380,  # compact but comfortable
        margin=dict(t=90, r=40, b=60, l=60),
        showlegend=False,
        uniformtext_minsize=10,
        uniformtext_mode="hide",
        yaxis1=dict(
            title="Logit Value", range=_padded_range(initial_logits), automargin=True
        ),
        yaxis2=dict(title="Probability", range=[0, 1.1], automargin=True),
    )

    # Sum annotation for probabilities (right subplot)
    fig.add_annotation(
        x=2,
        y=0.98,
        text=f"Sum = {sum(initial_probs):.6f}",
        showarrow=False,
        xref="x2",
        yref="y2",
        font=dict(size=12, color="green"),
        bgcolor="lightyellow",
        bordercolor="green",
        borderwidth=2,
        borderpad=4,
    )

    return fig


fig = create_interactive_logits_explorer()
fig.show()
```

**Key observations**:

- Logits can be any real number (negative, zero, or positive)
- Probabilities always sum to exactly 1.0 (see green box)
- **Entropy** measures uncertainty: lower = more confident, higher = more uncertain
- Try "Uniform Logits" vs "Very Confident" to see the extremes!
:::

## Loss Function

Now we look at how to define the loss function $L(\m{X}, \vect{y}, \theta)$ for a multi-class classification problem.

::: {.callout-note}
The loss function for a binary classification problem is identical. If we have a multi-label classification problem we have multiple outputs and thus multiple softmax functions. The principle is identical.
:::

### Likelihood

The likelihood of a data point $(\vect{x}^{(i)}, y^{(i)})$ is the **probability of observing a data point, given a specific model**:

$$
P(Y=y^{(i)}| X = \vect{x}^{(i)}) = f(\theta, \vect{x}^{(i)})
$$

This means we formulate a model with a probabilistic interpretation of predictions: $f(\theta, \vect{x}^{(i)}): \mathbb{R}^{n} \mapsto [0, 1]$

For a multi-class classification, the label vector is one-hot encoded $\vect{y}^{(i)} \in \{0, 1\}^K$, where the true class is coded with 1 and the rest with 0. The likelihood of a data point is defined as:

$$
\prod_{j=1}^K  P(Y = j| X = \vect{x}^{(i)})^{y^{(i)}_j}
$$

Since only one entry in $\vect{y}^{(i)}$ is 1, the likelihood is simply the prediction for the true class $P(Y = y^{(i)}| X = \vect{x}^{(i)})$.

[More Info](http://www.awebb.info/probability/2017/05/18/cross-entropy-and-log-likelihood.html)


::: {.callout-note appearance="simple" title="üìù Quiz: Cat vs Dog Likelihood" collapse="true"}

**Scenario**: Your CNN model classifies 5 images from our dataset. Let's explore how different prediction confidences affect likelihood!

**The Images**:
![Cat]({{< meta params.images_path >}}cats_and_dogs/cat_01.png){width=80}
![Dog]({{< meta params.images_path >}}cats_and_dogs/dog_01.png){width=80}
![Cat]({{< meta params.images_path >}}cats_and_dogs/cat_02.png){width=80}
![Dog]({{< meta params.images_path >}}cats_and_dogs/dog_02.png){width=80}
![Dog]({{< meta params.images_path >}}cats_and_dogs/dog_03.png){width=80}

**True Labels**: $\vect{y} = [\text{Cat}, \text{Dog}, \text{Cat}, \text{Dog}, \text{Dog}]$ or $[0, 1, 0, 1, 1]$ in binary encoding.

**Question 1**: Model A predicts: $P(\text{Cat}) = [0.9, 0.2, 0.8, 0.3, 0.1]$. What's the likelihood?

**Think first**: For each image, what's the probability of the *correct* class?

<details>
<summary>Click for result</summary>

```{python}
#| eval: true
#| echo: true

import numpy as np

# True labels: [Cat=0, Dog=1, Cat=0, Dog=1, Dog=1]
y_true = np.array([0, 1, 0, 1, 1])

# Model A predictions: P(Cat) for each image
p_cat = np.array([0.9, 0.2, 0.8, 0.3, 0.1])
p_dog = 1 - p_cat  # P(Dog) = 1 - P(Cat)

# Extract probability of correct class for each image
p_correct = np.where(y_true == 0, p_cat, p_dog)
print(f"P(correct class): {p_correct}")

# Likelihood = product of correct probabilities
likelihood = np.prod(p_correct)
print(f"Likelihood = {likelihood:.6f}")

# Individual contributions
for i, (true_label, prob) in enumerate(zip(y_true, p_correct)):
    label_name = "Cat" if true_label == 0 else "Dog"
    print(f"Image {i+1} ({label_name}): P = {prob:.1f}")
```

**Interpretation**: Model A is confident about the first cat (0.9) and the last dog (0.9), but uncertain about the middle images.

</details>

**Question 2**: Model B predicts: $P(\text{Cat}) = [0.8, 0.1, 0.9, 0.2, 0.1]$. Which model is better?

**Hint**: Compare the likelihoods. Higher likelihood = better model!

<details>
<summary>Click for result</summary>

```{python}
#| eval: true
#| echo: true

# Model B predictions
p_cat_B = np.array([0.8, 0.1, 0.9, 0.2, 0.1])
p_dog_B = 1 - p_cat_B

# Extract probability of correct class
p_correct_B = np.where(y_true == 0, p_cat_B, p_dog_B)
print(f"Model B - P(correct class): {p_correct_B}")

likelihood_B = np.prod(p_correct_B)
print(f"Model B - Likelihood = {likelihood_B:.6f}")

print(f"\nComparison:")
print(f"Model A: {likelihood:.6f}")
print(f"Model B: {likelihood_B:.6f}")
print(f"Model {'B' if likelihood_B > likelihood else 'A'} is better!")
```

**Key Insight**: Model B is more confident on the second cat (0.9 vs 0.8), leading to higher overall likelihood.

</details>

**Question 3**: What if we have a poorly calibrated model? Model C: $P(\text{Cat}) = [0.6, 0.4, 0.6, 0.4, 0.4]$

**Consider**: This model is not very confident about anything. How does this affect likelihood?

<details>
<summary>Click for result</summary>

```{python}
#| eval: true
#| echo: true

# Model C: low confidence predictions
p_cat_C = np.array([0.6, 0.4, 0.6, 0.4, 0.4])
p_dog_C = 1 - p_cat_C

p_correct_C = np.where(y_true == 0, p_cat_C, p_dog_C)
print(f"Model C - P(correct class): {p_correct_C}")

likelihood_C = np.prod(p_correct_C)
print(f"Model C - Likelihood = {likelihood_C:.6f}")

print(f"\nAll Models Comparison:")
print(f"Model A (mixed confidence): {likelihood:.6f}")
print(f"Model B (high confidence):  {likelihood_B:.6f}")
print(f"Model C (low confidence):   {likelihood_C:.6f}")

# Log-likelihoods (what we actually optimize)
print(f"\nNegative Log-Likelihoods (Cross-Entropy Loss):")
print(f"Model A: {-np.log(likelihood):.3f}")
print(f"Model B: {-np.log(likelihood_B):.3f}")
print(f"Model C: {-np.log(likelihood_C):.3f}")
print(f"Lower is better for loss!")
```

**Critical Observation**:
- **Confident correct predictions** (Model B) ‚Üí High likelihood ‚Üí Low loss ‚úÖ
- **Low confidence** (Model C) ‚Üí Low likelihood ‚Üí High loss ‚ùå
- **This is why we train models to be confident when correct!**

</details>


**Question 4**: What happens if we increase the dataset by ten fold? For example what happens with the likelihood of Model A if $P(\text{Cat}) = [0.9, 0.2, 0.8, 0.3, 0.1]$ is repeated 10 times?

<details>
<summary>Click for result</summary>

```{python}
#| eval: true
#| echo: true

# Repeat the dataset 10 times
p_cat_repeated = np.tile(p_cat, 10)
y_true_repeated = np.tile(y_true, 10)
p_dog_repeated = 1 - p_cat_repeated

p_correct_repeated = np.where(y_true_repeated == 0, p_cat_repeated, p_dog_repeated)
print(f"Dataset repeated 10x (50 samples total)")
print(f"P(correct class) for first 5: {p_correct_repeated[:5]}")

likelihood_10x = np.prod(p_correct_repeated)
print(f"\nLikelihood (10x dataset): {likelihood_10x:.2e}")
print(f"Original likelihood (1x):  {likelihood:.6f}")
print(f"\nRatio: {likelihood_10x / likelihood:.2e}")

# The key insight: likelihood shrinks exponentially!
print(f"\nNumerical underflow risk!")
print(f"This is why we use LOG-likelihood instead:")
print(f"Log-likelihood (10x): {np.sum(np.log(p_correct_repeated)):.3f}")
print(f"Log-likelihood (1x):  {np.sum(np.log(p_correct)):.3f}")
print(f"The log version just scales linearly (10x) without numerical issues!")
print(f"Lower is better for loss!")
```

**Critical Observation**:

- With 10x data, likelihood becomes TINY (underflow risk)
- But log-likelihood scales linearly ‚Üí numerically stable!

</details>


**Bonus Question**: What's the perfect model's likelihood?

<details>
<summary>Perfect Model Analysis</summary>

```{python}
#| eval: true
#| echo: true

# Perfect model: 100% confident and always correct
p_cat_perfect = np.array([1.0, 0.0, 1.0, 0.0, 0.0])  # Certain about each class
p_dog_perfect = 1 - p_cat_perfect

p_correct_perfect = np.where(y_true == 0, p_cat_perfect, p_dog_perfect)
print(f"Perfect Model - P(correct class): {p_correct_perfect}")

likelihood_perfect = np.prod(p_correct_perfect)
print(f"Perfect Model - Likelihood = {likelihood_perfect}")
print(f"Perfect Model - Negative Log-Likelihood = {-np.log(likelihood_perfect)}")

print(f"\nüéØ Maximum possible likelihood = 1.0")
print(f"üéØ Minimum possible loss = 0.0")
```

**The Goal**: Train your CNN to approach this perfect confidence on the training set (while generalizing to new data)!

</details>

:::


### Maximum Likelihood

The likelihood $P(\vect{y} | \theta, \m{X})$ of observing our entire dataset $(\m{X}, \vect{y})$, given the parameters $\theta$ and assuming that the data points $(\vect{x}^{(i)}, y^{(i)})$ are independent and identically distributed, can be calculated as:

\begin{equation}
\prod_{i=1}^N \prod_{j=1}^K  P(Y = j| X = \vect{x}^{(i)})^{y^{(i)}_j}
\end{equation}

Under the Maximum Likelihood approach, we seek the parameters $\theta$ that maximize the likelihood of observing the dataset.


### Negative Log-Likelihood

With Maximum Likelihood, we aim to choose the parameters $\theta$ such that $p(\vect{y} | \theta, \m{X})$ is maximized. However, this function can be complex to handle, and we can use some mathematical tricks. We take the logarithm of the likelihood so that the product of probabilities becomes a sum. Since the logarithm is a monotonically increasing function, we can maximize its logarithm instead of the function $p(\vect{y} | \theta, \m{X})$. Finally, we take the negative of the function, allowing us to minimize it.

\begin{align}
L(\m{X}, \vect{y}, \theta) &= - \log \prod_{i=1}^N \prod_{j=1}^K P(Y = j| X = \vect{x}^{(i)})^{y^{(i)}_j} \\
&= -\sum_{i=1}^N \sum_{j=1}^K y^{(i)}_j \log  P(Y = j| X = \vect{x}^{(i)})
\end{align}


::: {.callout-tip title="üí° Maximum Likelihood Intuition" collapse="true"}

**Visualize the connection: Likelihood ‚Üí Log-Likelihood ‚Üí Cross-Entropy Loss**:

```{python}
# | eval: true
# | echo: false
# | fig-width: 800


def create_likelihood_viz():
    """Visualize how likelihood changes with model quality (calibration)."""

    # Fixed dataset: 5 samples with true labels
    # True labels: [Cat, Dog, Cat, Dog, Dog]
    n_samples = 5
    y_true_binary = np.array([0, 1, 0, 1, 1])  # 0=Cat, 1=Dog

    # Model calibration parameter: varies from 0.5 (random) to 0.98 (well-calibrated)
    # This represents how well the model can predict the correct class
    # Think of it as: p(correct class) = calibration, p(wrong class) = 1 - calibration
    calibration_values = np.linspace(0.5, 0.98, 50)

    likelihoods = []
    log_likelihoods = []
    neg_log_likelihoods = []

    for calib in calibration_values:
        # Model predictions: always assign 'calib' probability to the TRUE class
        # For a binary problem: p(true_class) = calib, p(other_class) = 1 - calib
        # Example: if calib=0.8, true=Dog ‚Üí predict p(Dog)=0.8, p(Cat)=0.2
        #          if calib=0.8, true=Cat ‚Üí predict p(Cat)=0.8, p(Dog)=0.2

        # For likelihood, we only care about p(true_class) for each sample
        # All samples get probability = calib (since we're evaluating the true class)
        y_pred_probs = np.full(n_samples, calib)  # Each sample: p(true_class) = calib

        # Likelihood = product of all predicted probabilities for true classes
        # L(Œ∏) = p(y‚ÇÅ|x‚ÇÅ,Œ∏) √ó p(y‚ÇÇ|x‚ÇÇ,Œ∏) √ó ... √ó p(y‚ÇÖ|x‚ÇÖ,Œ∏)
        # With 5 samples: L = calib^5
        likelihood = np.prod(y_pred_probs)
        likelihoods.append(likelihood)

        # Log-likelihood = sum of log-probabilities (numerically stable)
        # log L(Œ∏) = log p(y‚ÇÅ|x‚ÇÅ,Œ∏) + log p(y‚ÇÇ|x‚ÇÇ,Œ∏) + ... + log p(y‚ÇÖ|x‚ÇÖ,Œ∏)
        log_likelihood = np.sum(np.log(y_pred_probs))
        log_likelihoods.append(log_likelihood)

        # Negative log-likelihood = what we minimize during training
        # -log L(Œ∏) = Cross-Entropy Loss
        neg_log_likelihoods.append(-log_likelihood)

    # Create subplots with more space for title
    fig = make_subplots(
        rows=3,
        cols=1,
        subplot_titles=(
            "Likelihood: L(Œ∏) = ‚àè·µ¢ p(y·µ¢|x·µ¢,Œ∏)  [Product of probabilities]",
            "Log-Likelihood: log L(Œ∏) = Œ£·µ¢ log p(y·µ¢|x·µ¢,Œ∏)  [Sum of log-probabilities]",
            "Negative Log-Likelihood: -log L(Œ∏) = Cross-Entropy Loss",
        ),
        vertical_spacing=0.15,
    )

    # Panel 1: Likelihood (gets very small!)
    fig.add_trace(
        go.Scatter(
            x=calibration_values,
            y=likelihoods,
            mode="lines",
            name="Likelihood",
            line=dict(color="blue", width=3),
            fill="tozeroy",
            hovertemplate="Calibration: %{x:.3f}<br>Likelihood: %{y:.6f}<extra></extra>",
        ),
        row=1,
        col=1,
    )

    # Panel 2: Log-likelihood (numerically stable)
    fig.add_trace(
        go.Scatter(
            x=calibration_values,
            y=log_likelihoods,
            mode="lines",
            name="Log-Likelihood",
            line=dict(color="green", width=3),
            fill="tozeroy",
            hovertemplate="Calibration: %{x:.3f}<br>Log-Likelihood: %{y:.3f}<extra></extra>",
        ),
        row=2,
        col=1,
    )

    # Panel 3: Negative log-likelihood (this is the LOSS we minimize)
    fig.add_trace(
        go.Scatter(
            x=calibration_values,
            y=neg_log_likelihoods,
            mode="lines",
            name="Negative Log-Likelihood",
            line=dict(color="red", width=3),
            fill="tozeroy",
            hovertemplate="Calibration: %{x:.3f}<br>NLL Loss: %{y:.3f}<extra></extra>",
        ),
        row=3,
        col=1,
    )

    # Update axes labels
    fig.update_xaxes(
        title_text="Model Calibration (probability assigned to correct class)",
        row=3,
        col=1,
    )
    fig.update_yaxes(title_text="Likelihood", row=1, col=1)
    fig.update_yaxes(title_text="Log-Likelihood", row=2, col=1)
    fig.update_yaxes(title_text="Loss", row=3, col=1)

    fig.update_layout(
        height=850,
        showlegend=False,
        title={
            "text": "Maximum Likelihood ‚Üî Cross-Entropy Connection<br><sub>Dataset: 5 samples (3 Dogs, 2 Cats)</sub>",
            "y": 1.0,
            "x": 0.5,
            "xanchor": "center",
            "yanchor": "top",
        },
    )

    # Add explanatory annotations
    fig.add_annotation(
        x=0.95,
        y=max(likelihoods) * 0.5,
        text="MAXIMIZE<br>THIS<br>(Training Goal)",
        showarrow=True,
        arrowhead=2,
        ax=-50,
        ay=0,
        row=1,
        col=1,
        font=dict(size=11, color="darkblue"),
    )

    fig.add_annotation(
        x=0.95,
        y=max(log_likelihoods) * 0.5,
        text="MAXIMIZE<br>THIS<br>(Equivalent Goal)",
        showarrow=True,
        arrowhead=2,
        ax=-50,
        ay=0,
        row=2,
        col=1,
        font=dict(size=11, color="darkgreen"),
    )

    fig.add_annotation(
        x=0.95,
        y=min(neg_log_likelihoods) * 1.5,
        text="MINIMIZE<br>THIS<br>(Actual Training Loss)",
        showarrow=True,
        arrowhead=2,
        ax=-50,
        ay=30,
        row=3,
        col=1,
        font=dict(size=11, color="darkred"),
    )

    return fig


fig = create_likelihood_viz()
fig.show()
```

**Critical Insights**:

- **X-axis**: Model calibration = probability assigned to the correct class (ranges from 0.5=random guessing to 0.98=well-calibrated)
- **Top panel**: Likelihood shrinks exponentially with dataset size ‚Üí numerical underflow risk!
- **Middle panel**: Log-likelihood converts products to sums ‚Üí numerically stable, same maximum
- **Bottom panel**: Negative log-likelihood (NLL) = Cross-Entropy Loss used in training
- **Key Connection**: max L(Œ∏) ‚ü∫ max log L(Œ∏) ‚ü∫ min -log L(Œ∏) ‚ü∫ min Cross-Entropy

The visualization shows: as model calibration improves (x increases), likelihood increases, log-likelihood increases, and loss (NLL) decreases.
:::

### Cross-Entropy

The loss function, derived with Maximum Likelihood, can also be viewed through the lens of cross-entropy between two discrete probability functions. Specifically, we can calculate and minimize the cross-entropy between the true distribution $\vect{y}^{(i)}$ and the predicted $\vect{\hat{y}}^{(i)}$. Cross-entropy comes from information theory and measures how many bits/nats on average are needed to describe an event of a probability distribution $p(x)$ when using the approximation $q(x)$.

\begin{equation}
CE = - \sum_{x \in X} p(x) \log q(x) \\
CE = - \sum_{i=1}^N \sum_{j=1}^K y_j^{(i)} \log \hat{y}_j^{(i)}
\end{equation}

It is evident that cross-entropy is identical to the negative log-likelihood.

::: {.callout-tip title="üìä Cross-Entropy Loss Landscape" collapse="true"}

**Visualize how cross-entropy penalizes predictions**:

```{python}
#| eval: true
#| echo: false
#| fig-width: 800

def create_ce_landscape():
    # For true label = 1
    p_pred = np.linspace(0.01, 0.99, 100)
    loss_true_1 = -np.log(p_pred)
    loss_true_0 = -np.log(1 - p_pred)

    fig = go.Figure()

    fig.add_trace(go.Scatter(
        x=p_pred,
        y=loss_true_1,
        mode='lines',
        name='True class = 1',
        line=dict(color='blue', width=3)
    ))

    fig.add_trace(go.Scatter(
        x=p_pred,
        y=loss_true_0,
        mode='lines',
        name='True class = 0',
        line=dict(color='red', width=3)
    ))

    fig.update_layout(
        title="Cross-Entropy Loss: How wrong predictions are penalized",
        xaxis_title="Predicted Probability for Class 1",
        yaxis_title="Cross-Entropy Loss",
        hovermode='x unified',
        height=500
    )

    # Add annotations
    fig.add_annotation(
        x=0.1, y=2.3,
        text="High penalty for<br>confident wrong prediction!",
        showarrow=True,
        arrowhead=2,
        ax=50, ay=-50
    )

    fig.add_annotation(
        x=0.9, y=2.3,
        text="High penalty for<br>confident wrong prediction!",
        showarrow=True,
        arrowhead=2,
        ax=-50, ay=-50
    )

    return fig

fig = create_ce_landscape()
fig.show()
```

**Key Insights**:

- Loss approaches **‚àû** when model is confidently wrong (p‚Üí0 for true class)
- Loss approaches **0** when model is confidently correct (p‚Üí1 for true class)
- **Asymmetric penalty**: Being wrong and confident is much worse than being uncertain
:::

```{python}
#| eval: true
#| echo: false
#| fig-width: 600
#| label: fig-classification-cross-entropy
#| fig-cap:
#|   - "True Distribution (left) and Predicted Distribution (right)."

from matplotlib import pyplot as plt
import numpy as np
import seaborn as sns

np.random.seed(123)
k = 5
logits = np.random.normal(size=(5, )) * 2
y_hat = np.exp(logits) / np.exp(logits).sum()
y_true = np.zeros_like(logits)
y_true[1] = 1

fig, ax = plt.subplots(ncols=2, figsize=(9, 3))
_ = sns.barplot(x=[i for i in range(0, k)], y=y_true, ax=ax[0]).set(
    title="True Distribution")
_ = sns.barplot(x=[i for i in range(0, k)], y=y_hat, ax=ax[1]).set(
    title="Predicted Distribution", ylim=(0, 1))

cross_entropy = f"{(- y_true * np.log(y_hat)).sum():.3f}"

plt.tight_layout()
plt.show()
```

@fig-classification-cross-entropy shows an example with a cross-entropy value of: `{python} cross_entropy`.

::: {.callout-tip title="üéØ Interactive Multi-Class Cross-Entropy" collapse="true"}
**See how different predictions map to loss values on the curve**:

```{python}
# | eval: true
# | echo: false
# | fig-width: 800


def create_multiclass_ce_demo():
    """Interactive multi-class cross-entropy demonstration."""

    # True label (one-hot)
    true_class = 1  # Class 1 (0-indexed)
    n_classes = 5

    # Create scenarios with different predictions
    scenarios = {
        "Perfect Prediction": [0.0, 1.0, 0.0, 0.0, 0.0],
        "Confident Correct": [0.05, 0.85, 0.05, 0.03, 0.02],
        "Uncertain Correct": [0.15, 0.35, 0.2, 0.15, 0.15],
        "Confident Wrong": [0.85, 0.05, 0.05, 0.03, 0.02],
        "Uniform (Random)": [0.2, 0.2, 0.2, 0.2, 0.2],
    }

    # Calculate cross-entropy for each scenario
    ce_losses = {}
    for name, probs in scenarios.items():
        ce = -np.log(probs[true_class] + 1e-10)
        ce_losses[name] = ce

    # Create visualization with 2 rows
    fig = make_subplots(
        rows=2,
        cols=1,
        subplot_titles=(
            "Predicted Probabilities",
            "Cross-Entropy Loss Landscape - Red Dot Shows Current Loss",
        ),
        row_heights=[0.45, 0.55],
        vertical_spacing=0.15,
    )

    # Initial scenario
    initial_name = "Confident Correct"
    initial_probs = scenarios[initial_name]

    # Row 1: Add probability bars
    colors = [
        "lightcoral" if i != true_class else "lightgreen" for i in range(n_classes)
    ]
    fig.add_trace(
        go.Bar(
            x=[f"Class {i}" for i in range(n_classes)],
            y=initial_probs,
            marker_color=colors,
            name="Probabilities",
            text=[f"{p:.2f}" for p in initial_probs],
            textposition="outside",
        ),
        row=1,
        col=1,
    )

    # Row 2: Add CE loss landscape
    p_pred = np.linspace(0.01, 0.99, 100)
    loss_curve = -np.log(p_pred)

    fig.add_trace(
        go.Scatter(
            x=p_pred,
            y=loss_curve,
            mode="lines",
            name="CE Loss Curve",
            line=dict(color="blue", width=3),
            showlegend=False,
        ),
        row=2,
        col=1,
    )

    # Add point for initial scenario on the loss landscape
    initial_pred = initial_probs[true_class]
    initial_loss = ce_losses[initial_name]

    fig.add_trace(
        go.Scatter(
            x=[initial_pred],
            y=[initial_loss],
            mode="markers",
            marker=dict(
                size=15,
                color="red",
                symbol="circle",
                line=dict(width=2, color="darkred"),
            ),
            name=initial_name,
            text=[f"{initial_name}<br>p={initial_pred:.3f}<br>Loss={initial_loss:.3f}"],
            hovertemplate="%{text}<extra></extra>",
            showlegend=False,
        ),
        row=2,
        col=1,
    )

    # Add true label indicator
    fig.add_annotation(
        x=f"Class {true_class}",
        y=1.05,
        text="‚Üê TRUE LABEL",
        showarrow=False,
        yref="y1",
        xref="x1",
        font=dict(size=12, color="green", family="Arial Black"),
    )

    # Build scenario buttons
    buttons = []
    for name, probs in scenarios.items():
        colors = [
            "lightcoral" if i != true_class else "lightgreen" for i in range(n_classes)
        ]
        pred_for_true = probs[true_class]
        loss = ce_losses[name]

        buttons.append(
            dict(
                label=name,
                method="update",
                args=[
                    {
                        "y": [
                            probs,  # bars (row 1)
                            loss_curve,  # CE curve (row 2)
                            [loss],  # red dot y (row 2)
                        ],
                        "text": [
                            [f"{p:.2f}" for p in probs],
                            None,
                            [f"{name}<br>p={pred_for_true:.3f}<br>Loss={loss:.3f}"],
                        ],
                        "x": [
                            [f"Class {i}" for i in range(n_classes)],
                            p_pred,
                            [pred_for_true],
                        ],
                    },
                    {
                        "title.text": f"Scenario: {name} | Probability for True Class = {pred_for_true:.3f} | Loss = {loss:.3f}"
                    },
                ],
            )
        )

    # --- Fix overlaps ---
    # 1) Move subplot titles slightly *down*; give row-2 (bottom) title extra downward shift
    for ann in fig.layout.annotations:
        if ann.text == "Predicted Probabilities":
            ann.y = 1.0
            ann.yanchor = "bottom"
            ann.yshift = -2
        elif ann.text == "Cross-Entropy Loss Landscape - Red Dot Shows Current Loss":
            ann.y = 1.0
            ann.yanchor = "bottom"
            ann.yshift = -10  # more space so it doesn't collide with yaxis2 title

    # 2) Increase standoff for yaxis2 title to keep it clear of the plot area and subtitles
    #    (this adds horizontal spacing between axis title and ticks/plot body)
    yaxis2_layout = dict(
        title="Cross-Entropy Loss", range=[0, 5], automargin=True, title_standoff=20
    )

    # 3) Place dropdown *below* the overall title, clearly separated (opens downward)
    #    Adjust top margin to reserve space for title + menu without overlap.
    fig.update_layout(
        updatemenus=[
            dict(
                type="dropdown",
                direction="down",  # opens downward
                x=0.5,
                xanchor="center",
                y=1.2,
                yanchor="top",  # just below the figure title
                buttons=buttons,
                pad=dict(t=4, r=6, l=6, b=10),
                bgcolor="white",
                bordercolor="#ddd",
                borderwidth=1,
            )
        ],
        height=700,
        margin=dict(t=120, r=40, b=60, l=70),  # a bit more left margin for yaxis title
        showlegend=False,
        yaxis1=dict(range=[0, 1.1], title="Probability", automargin=True),
        yaxis2=yaxis2_layout,
        xaxis2=dict(title="Predicted Probability for True Class (Class 1)"),
        title=dict(
            text=f"Scenario: {initial_name} | Probability for True Class = {initial_pred:.3f} | Loss = {initial_loss:.3f}",
            y=0.985,  # overall title near top
            yanchor="top",
            x=0.5,
            xanchor="center",
            pad=dict(t=6, b=6),
        ),
    )

    return fig


fig = create_multiclass_ce_demo()
fig.show()


```

**Key Takeaways**:

- **Green bar** = True class; **Red dot** = Current position on loss curve
- Watch how the red dot moves along the curve as predictions change
- Perfect prediction (p=1.0) ‚Üí Dot at left, Loss ‚âà 0
- Confident wrong (p=0.05) ‚Üí Dot at right, Loss ‚âà 3.0
:::


### Try it out yourself!

::: {.callout-tip}
## Train a CNN for image classification in your browser!

[CNN Explainer](https://poloclub.github.io/cnn-explainer/)
:::


## Architectures

Architecture refers to the complete definition of an untrained (deep-learning) model, i.e., the sequence and configuration of layers. The following are some important architectures that have been successfully used for image classification.

<!-- TODO: add a simple visualization that makes clear at the end is a vector of size k with the class logits -->
### AlexNet

CNNs became extremely popular after winning the [ImageNet](https://www.image-net.org/) Competition. @krizhevsky_imagenet_2012 implemented a CNN with multiple layers, known as the AlexNet architecture, as shown in @fig-classification-alexnet. ImageNet is a large, hierarchical image dataset @deng_imagenet_2009, which enabled efficient training of CNNs for the first time.

AlexNet consists of 5 convolutional layers and 3 fully-connected layers. The last layer is a 1000-way softmax output to model the classes in ImageNet.

The model was trained with two GPUs (GTX 580) with 3GB memory each. Since 3GB was insufficient to train the model, the architecture was split across the GPUs. Some layers were split between the GPUs, allowing a larger network to be trained.

@fig-classification-alexnet shows the detailed architecture, including kernel sizes, the number of filters per layer, activation map dimensions, and pooling layers. ReLU was used as the activation function. This representation is from the original paper and shows how the model was split across two GPUs.

::: {#fig-classification-alexnet}
![]({{< meta params.images_path >}}alexnet.png){width=600}

AlexNet @krizhevsky_imagenet_2012.
:::

Since the split across two GPUs is no longer necessary, the architecture is somewhat simplified. Modern implementations are shown in @fig-classification-alexnet2.

::: {#fig-classification-alexnet2}

![]({{< meta params.images_path >}}alexnet_illustration.jpg){width=400}

AlexNet @prince_understanding_2023
:::

@fig-classification-alexnet-table presents the operations in AlexNet in tabular form.

::: {#fig-classification-alexnet-table width=600}
![]({{< meta params.images_path >}}alexnet_table.jpg)

Source: @johnson_eecs_2019.
:::

We can also easily load AlexNet via torchvision.

```{python}
#| eval: true
#| echo: true

import torch
import torchvision.models as models
import torchinfo

alexnet = models.alexnet()
x = torch.zeros(1, 3, 224, 224, dtype=torch.float, requires_grad=False)
yhat = alexnet(x)

print(torchinfo.summary(alexnet, input_size=(1, 3, 224, 224)))
```

### VGG

@simonyan_very_2015 won the ImageNet Challenge in 2014 with their VGG architecture. They showed that smaller 3x3 kernels work significantly better and that deeper networks with 16-19 layers can be trained. @fig-classification-vgg shows the architecture as presented in the original paper. @fig-classification-vgg-arch visualizes the architecture.

::: {#fig-classification-vgg}
![]({{< meta params.images_path >}}vgg.png){width=600}

VGG @simonyan_very_2015.
:::

::: {#fig-classification-vgg-arch}

![]({{< meta params.images_path >}}vgg_illustration.jpg)

VGG @prince_understanding_2023

:::

VGG introduced a popular design element: A layer has the same number of filters as the previous layer unless the activation map dimensions are halved, in which case the number of filters is doubled (see also @fig-classification-vgg-arch). This was done to maintain the time complexity of the layers. VGG does not use normalization layers.

@fig-classification-vgg-design compares VGG with AlexNet.

::: {#fig-classification-vgg-design}
![]({{< meta params.images_path >}}vgg_design.jpg){width=600}

Source: @johnson_eecs_2019.
:::

### ResNet

@He2016 wondered whether CNNs could be improved simply by making them deeper, i.e., adding more layers. Their experiments showed that adding layers eventually stops being beneficial, and performance saturates and then rapidly degrades (see @fig-classification-resnet-motivation1). They noticed that the performance did not degrade due to overfitting (see @fig-classification-resnet-motivation2, which shows that training error is also poor), but because back-propagation becomes less effective and the weights do not optimize well. @He2016 hypothesized that deeper networks could not perform worse than shallower ones because the additional layers could simply pass on activations unchanged.

::: {#fig-classification-resnet-motivation1}
![]({{< meta params.images_path >}}resnet_test_error.jpg){width=600}

Source: @He2016
:::

::: {#fig-classification-resnet-motivation2}
![]({{< meta params.images_path >}}resnet_train_error.jpg){width=600}

Source: @He2016
:::

They then hypothesized that passing on activations unchanged as a default behavior could avoid this problem. They introduced identity mappings. @fig-classification-residual-connection shows such a connection. These connections are also called residual connections because the network only needs to learn the change in activations from layer $i$ to layer $i+1$, the residue.

::: {#fig-classification-residual-connection}
![]({{< meta params.images_path >}}residual_connection.jpg){width=600}

ResNet @He2016 (Graphic from @johnson_eecs_2019.)
:::

ResNet was otherwise inspired by VGG16. The convolutional layers consist of 3x3 kernels (except the first one), and the number of filters is doubled when the activation map dimensions are halved. ResNet uses convolutions with stride 2 for down-sampling and no max pooling. At the end, ResNet uses a global average pooling layer followed by a fully-connected layer with the number of classes. Variants of ResNet have 18, 34, 50, and over 100 layers. ResNet also uses batch normalization. @fig-classification-resnet-struk shows the architecture.

::: {#fig-classification-resnet-struk}
![]({{< meta params.images_path >}}resnet_arch_horiz.png){width=600}

Source: @He2016.
:::

ResNet is very popular and is still widely used today (there are now more modern variants). Recent studies of identity mappings (or more generally skip connections) have shown that the gradient surface of the loss function becomes smoother, allowing the network to be optimized better. This is impressively illustrated in @fig-classification-skip-loss-landscape.

::: {#fig-classification-skip-loss-landscape}
![]({{< meta params.images_path >}}loss_landscape_skip.png){width=600}

Source: @li_visualizing_2018.
:::

### ConvNext

One of the most modern CNN architectures was described in @liu_convnet_2022. This architecture uses tricks and implementation ideas accumulated over decades from various architectures. @fig-classification-convnext shows, starting from a modern version of ResNet, what has been adjusted to define this state-of-the-art architecture. Examples include: larger kernels, different activation functions, layer normalization instead of batch normalization, and depthwise separable convolutions.

::: {#fig-classification-convnext}
![]({{< meta params.images_path >}}convnext.png){width=600}

Convnext @liu_convnet_2022.
:::

There is already a new version of this architecture @woo_convnext_2023.

### ImageNet Performance

@fig-classification-image-net-performance shows the development of ImageNet performance. As of 2023, we are much closer to 100%, see [Link](https://paperswithcode.com/sota/image-classification-on-imagenet?metric=Top%205%20Accuracy).

::: {#fig-classification-image-net-performance}
![]({{< meta params.images_path >}}image_net_performance.jpg){width=600}

Graphic from @johnson_eecs_2019
:::

### Which Architecture?

Which architecture should be chosen for a specific problem? A common tip is: *Don't be a hero.*

One should rely on off-the-shelf architectures and not implement their own without a good reason.

Typically, ResNet-50 or ResNet-101 are good choices. However, there are also models that require significantly fewer parameters, such as Efficient Nets.

Important considerations are also the requirements regarding accuracy, performance (FLOPs), and model size (memory).

## Further Variations

There are countless other architectures and especially specific layers or layer blocks that can be used in various architectures. The following are Squeeze/Excite Networks and the important class of normalization layers, which are often used and can significantly improve models or accelerate the optimization process.

### Squeeze/Excite Networks

Squeeze-and-Excite Networks (SE-Networks) were introduced in 2019 @hu_squeeze-and-excitation_2019. These include so-called Squeeze and Excite blocks (SE blocks), which allow the scaling of activation maps of a layer. This scaling is learnable through a few additional parameters. In practice, significant performance gains have been observed. @fig-classification-se-networks shows an illustration.

::: {#fig-classification-se-networks}
![]({{< meta params.images_path >}}se_networks.png){width=800}

Source: @hu_squeeze-and-excitation_2019.
:::


These SE blocks can be easily applied to arbitrary activation maps. @fig-classification-se-networks shows an input feature map $\tensor{X} \in \mathbb{R}^{H' \times W' \times C'}$ that is transformed with $F_{tr}$ (e.g., with a convolutional layer). This results in the activation maps $\tensor{U} \in \mathbb{R}^{H \times W \times C}$.

The transformation $F_{sq}(\cdot)$ applies the squeeze operation, a global average pooling, to generate a description of each channel by aggregating spatial information of the activation map $\tensor{U}$. This results in a vector $z \in \mathbb{R}^{1 \times 1 \times C}$.

The excitation operation $F_{ex}(\cdot, W)$ uses a gating mechanism with parameters $W$, implemented with two fully-connected layers and activation functions. The result is $s \in \mathbb{R}^{1 \times 1 \times C}$, the channel weights. It is called gating because the weights range from $[0, 1]$ and thus control how much information of a channel flows through (gatekeeping).

Finally, $F_{scale}(\cdot)$ scales the activation maps $\tensor{U}$ with the channel weights.

The operation can be described as follows:

\begin{align}
U &= F_{tr}(X) \\
z &= F_{sq}(U) = \text{GlobalAvgPool}(U) \\
s &= F_{ex}(z, W) = \sigma(\mathbf{W}_2 g(\mathbf{W}_1 z)) \\
\hat{X} &= F_{scale}(U, s) = U \odot s
\end{align}

where $g()$ represents the ReLU function, and $\sigma$ represents the sigmoid function.

### Normalization Layers

Normalization layers normalize activation maps to improve parameter learning. There are many variations, as shown in @fig-classification-normalization. In modern architectures, normalization layers are typically used. Popular is, for example, layer normalization (see @ba_layer_2016). The general form of normalization is given in equation @eq-classification-normalization. The parameters $\gamma$ and $\beta$ are learned, while the means $E[x]$ and variances $\sigma^2[x]$ are estimated from the activations. Why normalization layers work and which ones to prefer are still subjects of research and are often empirically tested in practice (considered as hyper-parameters). It is believed that the cost function becomes smoother overall, allowing the network to train faster and better @santurkar_how_2019.


$$
y = \frac{x - E[x]}{\sqrt{\sigma^2[x] + \epsilon}} * \gamma + \beta
$$ {#eq-classification-normalization}

::: {#fig-classification-normalization}
![]({{< meta params.images_path >}}normalization.png){width=300}

Source: @qiao_micro-batch_2020.
:::


::: {.callout-note}
Normalization layers are very common and used in almost any architecture. While batch normalization has traditionally been used in CNNs, newer models rely on layer normalization.
:::


## Pre-Processing

Typically, images are pre-processed before being processed by the CNN. This is done to make the optimization process converge faster. Common steps include:

- Resizing/cropping to a fixed size, e.g., 224x224x3. This is necessary for the images to be processed in a batch (they must have the same dimensions).
- Scaling: Images are scaled from the interval [0, 255] to the interval [0, 1].
- Normalization: Normalization is often done along the color channels so that the mean value of the pixel values is 0.

In `torchvision`, the following pre-processing steps are used: [Link](https://github.com/pytorch/vision/blob/main/torchvision/transforms/_presets.py)

## Transfer Learning

Transfer Learning refers to the process of adapting a trained model for Task A to Task B. Adapting pre-trained models often leads to better results and also reduces the number of training iterations. Deep learning frameworks often provide easy access to such models (also known as Model Zoo). For example, from PyTorch: [Link](https://pytorch.org/vision/stable/models.html).

```{python}
#| eval: false
#| echo: true

from torchvision.models import resnet50, ResNet50_Weights
resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)
```

## PyTorch Example

### Architecture

Example of a CNN architecture.

```{python}
#| eval: false
#| echo: true

import torch
import torch.nn as nn
import torch.nn.functional as F


class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = torch.flatten(x, 1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

net = Net()
```

### Loss Function

```{python}
#| eval: false
#| echo: true

import torch.optim as optim

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
```

## References

::: {#refs}
:::
