---
title: "Image Classification"
params:
   images_path: "/assets/images/classification/"
execute:
  cache: false
  freeze: false
---

::: {.content-hidden}
$$
{{< include /assets/_macros.tex >}}
$$
:::

::: {.callout-tip title="Learning Objectives" collapse="true"}
After this lecture you should be able to:

- Define multi-class vs multi-label classification and their output encodings.
- Derive the softmax and cross-entropy from a maximum likelihood perspective.
- Interpret logits, probabilities, and calibration (temperature, label smoothing high-level).
- Compare classic CNN architectures (AlexNet→VGG→ResNet→ConvNeXt) and choose a baseline.
- Apply transfer learning and explain when to freeze vs fine-tune layers.
:::

::: {.callout-tip title="TLDR Recap" collapse="true"}

**Image Classification: Core Task**

- Assign images to predefined classes (multi-class: ≥2 classes, binary: 2 classes, multi-label: multiple class sets)
- Use CNNs to learn hierarchical features automatically from raw pixels
- Output: class probabilities or labels

**Softmax: Logits → Probabilities**

$$P(Y = k \mid X = \vect{x}) = \frac{e^{z_k}}{\sum_{j=1}^K e^{z_j}}$$

- **Logits** $\vect{z}$: Raw model outputs (unbounded)
- **Softmax** $\sigma(\vect{z})$: Converts logits to valid probability distribution
- Properties: Always sums to 1, outputs in [0,1], differentiable

**Cross-Entropy Loss = Negative Log-Likelihood**

Maximum Likelihood approach:

$$L(\m{X}, \vect{y}, \theta) = -\sum_{i=1}^N \sum_{j=1}^K y^{(i)}_j \log P(Y = j\mid X = \vect{x}^{(i)})$$

- Minimizing negative log-likelihood = maximizing likelihood of correct predictions
- Equivalent to cross-entropy between true distribution $\vect{y}$ and predicted $\hat{\vect{y}}$
- Penalizes confident wrong predictions heavily

**Key Insight**: Softmax + Cross-Entropy arise naturally from probabilistic modeling (categorical distribution + maximum likelihood estimation)

**Classic CNN Architectures Evolution**

1. **AlexNet (2012)**: 5 conv + 3 FC layers, ReLU, dropout → ImageNet breakthrough
2. **VGG (2014)**: Deeper (16-19 layers), small 3×3 kernels, simple design → showed depth matters
3. **ResNet (2015)**: Residual connections (skip connections) → enabled 50-152 layers, solved vanishing gradients
4. **ConvNeXt (2022)**: Modern CNN with Transformer-era tricks → competes with Vision Transformers

**Residual Connections: The Key Innovation**

$$\vect{x}_{l+1} = \vect{x}_l + F(\vect{x}_l)$$

- Network learns residual $F(\vect{x})$ instead of full transformation
- Enables training very deep networks (>100 layers)
- Smoother loss landscape → better optimization
- Default behavior: pass activations unchanged (identity mapping)

**Transfer Learning: Standing on Giants' Shoulders**

1. **Pre-training**: Train on large dataset (e.g., ImageNet: 1.2M images, 1000 classes)
2. **Transfer**: Adapt to your specific task with much less data
3. **Strategies**:
   - **Freeze early layers** (extract features only) when data is very scarce
   - **Fine-tune all layers** when you have moderate data
   - **Fine-tune only later layers** as middle ground

**Why Transfer Learning Works**:
- Early layers learn general features (edges, textures)
- Later layers learn task-specific features
- Pre-trained features transfer well across vision tasks

**Architecture Selection Strategy**

✅ **Don't be a hero**: Use proven architectures first
✅ **ResNet-50/101**: Excellent baseline for most tasks
✅ **EfficientNet**: When model size/speed matters
✅ **ConvNeXt**: State-of-the-art CNN performance

**Consider**:
- Accuracy requirements
- Inference speed (FLOPs)
- Model size (memory)
- Available training data

**Modern Components**

- **Normalization layers** (BatchNorm, LayerNorm): Stabilize training, enable deeper networks
- **Global Average Pooling**: Replace large FC layers → fewer parameters
- **SE Blocks**: Learn channel-wise attention → better feature recalibration
- **Depthwise Separable Convolutions**: Reduce parameters while maintaining performance

**Pre-processing Pipeline**

1. Resize/crop to fixed size (e.g., 224×224)
2. Scale pixel values: [0, 255] → [0, 1]
3. Normalize: Subtract mean, divide by std per channel
4. Data augmentation (training): flips, crops, color jitter

**Best Practices**

✅ Start with pre-trained models (transfer learning)
✅ Use proven architectures (ResNet, ConvNeXt)
✅ Apply proper preprocessing (resize, normalize)
✅ Use cross-entropy loss for classification
✅ Monitor both training and validation metrics
✅ Consider calibration (temperature scaling, label smoothing)

**Key Takeaway**: Classification success comes from combining strong architectures (ResNets), probabilistic foundations (softmax + cross-entropy), transfer learning, and proper training practices.

:::



## Introduction

Image classification is a core task of Computer Vision. In image classification, an image is assigned to a predefined set of classes. In multi-class classification, there are ≥ 2 classes; in binary classification, there are 2 classes; and in multi-label classification, there are several sets of classes. @fig-classification-classification-illustration illustrates the problem in multi-class classification.

::: {#fig-classification-classification-illustration}
![]({{< meta params.images_path >}}classification_illustration.jpg){width=600}

Image Classification example.
:::

@fig-image-classifier-example shows an example from the paper by @krizhevsky_imagenet_2012, which achieved the best results in the 2012 ImageNet competition, demonstrating how well CNNs work.

::: {#fig-image-classifier-example}
![]({{< meta params.images_path >}}image_classification_example.png){width=600}

Image Classification example (from @krizhevsky_imagenet_2012).
:::

@fig-classification-camera-traps illustrates the challenge with images taken by camera traps, which need to be classified along animal species.

::: {#fig-classification-camera-traps}
![]({{< meta params.images_path >}}camera_traps.png){width=600}

Example images from camera traps.
:::

## Parametric Approach

In a parametric approach, we seek a model of the following form:

$$
\hat{y}^{(i)} = f(\theta, \vect{x}^{(i)})
$$

We want to find model parameters $\theta$ that output a score/prediction $\hat{y}^{(i)}$ for any data points $\vect{x}^{(i)}$ for each class $k \in K$. We then want to assess how good this score is with the help of a loss function.

### Softmax Classifier

With a Softmax Classifier, we interpret model predictions/scores as probabilities of class memberships: $P(Y=\vect{y}^{(i)}| X = \vect{x}^{(i)})$. We interpret the output as a [Categorical Distribution](https://en.wikipedia.org/wiki/Categorical_distribution) over all possible classes.

To obtain a valid probability distribution, the untransformed outputs $\vect{z}$, also called logits, of a model are transformed with the softmax function $\sigma(\vect{z})$:

$$
P(Y = k \mid X = \vect{x}^{(i)}) = \sigma(\vect{z})_k = \frac{e^{z_k}}{\sum_{j=1}^K e^{z_j}}
$$

@fig-classification-logits-to-probs shows an example of the effect of the softmax transformation.

```{python}
#| eval: true
#| echo: false
#| fig-width: 600
#| label: fig-classification-logits-to-probs
#| fig-cap:
#|   - "Logits (left) to probabilities with the Softmax function (right)."

from matplotlib import pyplot as plt
import numpy as np
import seaborn as sns

np.random.seed(123)
logits = np.random.normal(size=(10, )) * 2
softmax_output = np.exp(logits) / np.exp(logits).sum()

fig, ax = plt.subplots(ncols=2, figsize=(9, 3))
_ = sns.barplot(x=[i for i in range(0, 10)], y=logits, ax=ax[0]).set(
    title="Logits")
_ = sns.barplot(x=[i for i in range(0, 10)], y=softmax_output, ax=ax[1]).set(
    title="Softmax", ylim=(0, 1))

plt.tight_layout()
plt.show()
```


<!-- {{< embed /demos/logits_to_probabilities.ipynb#fig-demo-logits-to-probs >}} -->



::: {.callout-tip title="🎮 Interactive Exploration" collapse="false"}
**Try different logit distributions** and see how softmax transforms them into probabilities!

```{python}
#| eval: true
#| echo: false
#| fig-width: 800

import plotly.graph_objects as go
from plotly.subplots import make_subplots

def softmax(logits):
    """Compute softmax values for logits."""
    exp_logits = np.exp(logits - np.max(logits))  # Subtract max for numerical stability
    return exp_logits / np.sum(exp_logits)

def create_interactive_logits_explorer():
    """Interactive exploration of logits to probabilities transformation."""

    n_classes = 5

    # Create different scenarios with interesting logit distributions
    scenarios = {
        "Uniform Logits": [1.0, 1.0, 1.0, 1.0, 1.0],
        "Clear Winner": [5.0, 1.0, 0.5, 0.0, -0.5],
        "Close Competition": [2.0, 1.8, 1.5, 1.2, 1.0],
        "Very Confident": [10.0, 0.0, 0.0, 0.0, 0.0],
        "Negative Logits": [-0.5, -1.0, -1.5, -2.0, -2.5],
        "Mixed Range": [3.0, 0.0, -1.0, 0.5, 2.0],
        "Two Leaders": [4.0, 3.8, 0.0, 0.0, 0.0],
        "Random-ish": [1.5, -0.3, 2.1, 0.8, -0.8]
    }

    # Initial scenario
    initial_name = "Clear Winner"
    initial_logits = scenarios[initial_name]
    initial_probs = softmax(initial_logits)

    # Create figure
    fig = make_subplots(
        rows=1, cols=2,
        subplot_titles=("Logits (Raw Model Outputs)", "Probabilities (After Softmax)"),
        horizontal_spacing=0.12
    )

    # Determine colors for initial state
    max_logit_idx = np.argmax(initial_logits)
    logit_colors = ['lightblue' if i != max_logit_idx else 'steelblue' for i in range(n_classes)]

    max_prob_idx = np.argmax(initial_probs)
    prob_colors = ['lightcoral' if i != max_prob_idx else 'crimson' for i in range(n_classes)]

    # Add initial logits bar chart
    fig.add_trace(
        go.Bar(
            x=[f'Class {i}' for i in range(n_classes)],
            y=initial_logits,
            name="Logits",
            marker_color=logit_colors,
            text=[f"{z:.2f}" for z in initial_logits],
            textposition='outside'
        ),
        row=1, col=1
    )

    # Add initial probabilities bar chart
    fig.add_trace(
        go.Bar(
            x=[f'Class {i}' for i in range(n_classes)],
            y=initial_probs,
            name="Probabilities",
            marker_color=prob_colors,
            text=[f"{p:.3f}" for p in initial_probs],
            textposition='outside'
        ),
        row=1, col=2
    )

    # Create dropdown buttons for scenarios
    buttons = []
    for name, logits in scenarios.items():
        probs = softmax(logits)

        # Determine colors
        max_logit_idx = np.argmax(logits)
        logit_colors = ['lightblue' if i != max_logit_idx else 'steelblue' for i in range(n_classes)]

        max_prob_idx = np.argmax(probs)
        prob_colors = ['lightcoral' if i != max_prob_idx else 'crimson' for i in range(n_classes)]

        # Calculate statistics
        entropy = -np.sum(probs * np.log(probs + 1e-10))
        max_prob = np.max(probs)

        button = dict(
            label=f"{name} (max prob={max_prob:.3f}, Entropy (H)={entropy:.2f})",
            method="update",
            args=[
                {
                    "y": [logits, probs],
                    "text": [
                        [f"{z:.2f}" for z in logits],
                        [f"{p:.3f}" for p in probs]
                    ],
                    "marker.color": [logit_colors, prob_colors]
                },
                {
                    "annotations[2].text": f"Sum = {sum(probs):.6f}"
                }
            ]
        )
        buttons.append(button)

    # Calculate initial stats
    initial_entropy = -np.sum(initial_probs * np.log(initial_probs + 1e-10))
    initial_max_prob = np.max(initial_probs)

    fig.update_layout(
        updatemenus=[
            dict(
                type="dropdown",
                direction="down",
                x=0.5,
                xanchor="center",
                y=1.15,
                yanchor="top",
                buttons=buttons
            )
        ],
        height=300,
        showlegend=False,
        yaxis1=dict(title="Logit Value"),
        yaxis2=dict(title="Probability", range=[0, 1.05])
    )

    # Add sum annotation for probabilities
    fig.add_annotation(
        x=2,
        y=0.98,
        text=f"Sum = {sum(initial_probs):.6f}",
        showarrow=False,
        xref="x2",
        yref="y2",
        font=dict(size=12, color="green"),
        bgcolor="lightyellow",
        bordercolor="green",
        borderwidth=2,
        borderpad=4
    )

    return fig

fig = create_interactive_logits_explorer()
fig.show()
```

**Key observations**:

- Logits can be any real number (negative, zero, or positive)
- Probabilities always sum to exactly 1.0 (see green box)
- **Entropy** measures uncertainty: lower = more confident, higher = more uncertain
- Try "Uniform Logits" vs "Very Confident" to see the extremes!
:::

## Loss Function

Now we look at how to define the loss function $L(\m{X}, \vect{y}, \theta)$ for a multi-class classification problem.

### Likelihood

The likelihood of a data point $(\vect{x}^{(i)}, y^{(i)})$ is the probability of observing/realizing a data point, given a model with certain parameters:

$$
P(Y=y^{(i)}| X = \vect{x}^{(i)}) = f(\theta, \vect{x}^{(i)})
$$

This means we formulate a model with a probabilistic interpretation of predictions: $f(\theta, \vect{x}^{(i)}): \mathbb{R}^{n} \mapsto [0, 1]$

For a multi-class classification, the label vector is one-hot encoded $\vect{y}^{(i)} \in \{0, 1\}^K$, where the true class is coded with 1 and the rest with 0. The likelihood of a data point is defined as:

$$
\prod_{j=1}^K  P(Y = j| X = \vect{x}^{(i)})^{y^{(i)}_j}
$$

Since only one entry in $\vect{y}^{(i)}$ is 1, the likelihood is simply the prediction for the true class $P(Y = y^{(i)}| X = \vect{x}^{(i)})$.

[More Info](http://www.awebb.info/probability/2017/05/18/cross-entropy-and-log-likelihood.html)

```{python}
#| eval: true
#| echo: false
import numpy as np

# Example For Likelihood Calculation

y_true = np.array(
    [0, 1, 0, 1, 1]
)

y_pred = np.array(
    [0.1, 0.8, 0.2, 0.7, 0.9]
)

# Perfect Prediction
#y_pred = np.array([0, 1, 0, 1, 1])

# Larger Dataset
#y_true = np.repeat(y_true, 10, axis=0)
#y_pred = np.repeat(y_pred, 10, axis=0)


def calculate_likelihood(y_true: float, y_pred: float) -> float:
    return (y_pred ** y_true) * ((1 - y_pred) ** (1 - y_true))

def display_likelihood(y_true, y_pred):
    sample_likelihoods = [calculate_likelihood(float(yt), float(yp)) for yt, yp in zip(y_true, y_pred)]
    likelihood = np.prod(sample_likelihoods)
    print(f"Total Likelihood: {likelihood:.6f}")
```

::: {.callout-note appearance="simple" title="📝 Quiz: Understanding Likelihood"}

**Scenario**: Given ground truth $\vect{y} = [0, 1, 0, 1, 1]$, we'll explore how different predictions affect the likelihood.

**Question 1**: For predictions $\hat{\vect{y}} = [0.1, 0.8, 0.2, 0.7, 0.9]$, what is the likelihood?

**Think first**: Remember, likelihood is the product of probabilities for the correct classes. Higher is better!

<details>
<summary>Click for result</summary>

```{python}
#| eval: true
#| echo: true

y_pred = np.array([0.1, 0.8, 0.2, 0.7, 0.9])
display_likelihood(y_true, y_pred)
```

**Interpretation**: Each correct prediction contributes to the total likelihood. The likelihood is the product of all individual likelihoods.

</details>

**Question 2**: Does likelihood increase for $\hat{\vect{y}} = [0.1, 0.9, 0.2, 0.7, 0.9]$?

**Hint**: Compare this to the previous prediction. What changed?

<details>
<summary>Click for result</summary>

```{python}
#| eval: true
#| echo: true

y_pred = np.array([0.1, 0.9, 0.2, 0.7, 0.9])
display_likelihood(y_true, y_pred)
```

**Key Insight**: The second prediction (0.9 vs 0.8) is more confident for the true class, increasing overall likelihood.

</details>

**Question 3**: What happens if we increase the dataset by 10× (copying the vector)?

**Consider**: Likelihood is a product. What happens when you multiply many probabilities < 1?

<details>
<summary>Click for result and explanation</summary>

```{python}
#| eval: true
#| echo: true

y_pred = np.array([0.1, 0.8, 0.2, 0.7, 0.9])

y_true = np.repeat(y_true, 10, axis=0)
y_pred = np.repeat(y_pred, 10, axis=0)

display_likelihood(y_true, y_pred)
```

**Critical Observation**: The likelihood becomes extremely small! This is why we use **log-likelihood** in practice:

- Products become sums: $\log(ab) = \log(a) + \log(b)$
- Avoids numerical underflow
- Easier to optimize
- Equivalent for optimization (log is monotonic)

**This motivates the negative log-likelihood loss function!**

</details>

:::


### Maximum Likelihood

The likelihood $P(\vect{y} | \theta, \m{X})$ of observing our entire dataset $(\m{X}, \vect{y})$, given the parameters $\theta$ and assuming that the data points $(\vect{x}^{(i)}, y^{(i)})$ are independent and identically distributed, can be calculated as:

\begin{equation}
\prod_{i=1}^N \prod_{j=1}^K  P(Y = j| X = \vect{x}^{(i)})^{y^{(i)}_j}
\end{equation}

Under the Maximum Likelihood approach, we seek the parameters $\theta$ that maximize the likelihood of observing the dataset.

::: {.callout-tip title="💡 Maximum Likelihood Intuition" collapse="true"}
**Visualize the connection: Likelihood → Log-Likelihood → Cross-Entropy Loss**:

```{python}
#| eval: true
#| echo: false
#| fig-width: 800

def create_likelihood_viz():
    """Visualize how likelihood changes with model quality (calibration)."""

    # Fixed dataset: 5 samples with true labels
    # True labels: [Cat, Dog, Cat, Dog, Dog]
    n_samples = 5
    y_true_binary = np.array([0, 1, 0, 1, 1])  # 0=Cat, 1=Dog

    # Model calibration parameter: varies from 0.5 (random) to 0.98 (well-calibrated)
    # This represents how well the model can predict the correct class
    # Think of it as: p(correct class) = calibration, p(wrong class) = 1 - calibration
    calibration_values = np.linspace(0.5, 0.98, 50)

    likelihoods = []
    log_likelihoods = []
    neg_log_likelihoods = []

    for calib in calibration_values:
        # Model predictions: always assign 'calib' probability to the TRUE class
        # For a binary problem: p(true_class) = calib, p(other_class) = 1 - calib
        # Example: if calib=0.8, true=Dog → predict p(Dog)=0.8, p(Cat)=0.2
        #          if calib=0.8, true=Cat → predict p(Cat)=0.8, p(Dog)=0.2

        # For likelihood, we only care about p(true_class) for each sample
        # All samples get probability = calib (since we're evaluating the true class)
        y_pred_probs = np.full(n_samples, calib)  # Each sample: p(true_class) = calib

        # Likelihood = product of all predicted probabilities for true classes
        # L(θ) = p(y₁|x₁,θ) × p(y₂|x₂,θ) × ... × p(y₅|x₅,θ)
        # With 5 samples: L = calib^5
        likelihood = np.prod(y_pred_probs)
        likelihoods.append(likelihood)

        # Log-likelihood = sum of log-probabilities (numerically stable)
        # log L(θ) = log p(y₁|x₁,θ) + log p(y₂|x₂,θ) + ... + log p(y₅|x₅,θ)
        log_likelihood = np.sum(np.log(y_pred_probs))
        log_likelihoods.append(log_likelihood)

        # Negative log-likelihood = what we minimize during training
        # -log L(θ) = Cross-Entropy Loss
        neg_log_likelihoods.append(-log_likelihood)

    # Create subplots with more space for title
    fig = make_subplots(
        rows=3, cols=1,
        subplot_titles=(
            "Likelihood: L(θ) = ∏ᵢ p(yᵢ|xᵢ,θ)  [Product of probabilities]",
            "Log-Likelihood: log L(θ) = Σᵢ log p(yᵢ|xᵢ,θ)  [Sum of log-probabilities]",
            "Negative Log-Likelihood: -log L(θ) = Cross-Entropy Loss"
        ),
        vertical_spacing=0.15
    )

    # Panel 1: Likelihood (gets very small!)
    fig.add_trace(
        go.Scatter(
            x=calibration_values,
            y=likelihoods,
            mode='lines',
            name='Likelihood',
            line=dict(color='blue', width=3),
            fill='tozeroy',
            hovertemplate='Calibration: %{x:.3f}<br>Likelihood: %{y:.6f}<extra></extra>'
        ),
        row=1, col=1
    )

    # Panel 2: Log-likelihood (numerically stable)
    fig.add_trace(
        go.Scatter(
            x=calibration_values,
            y=log_likelihoods,
            mode='lines',
            name='Log-Likelihood',
            line=dict(color='green', width=3),
            fill='tozeroy',
            hovertemplate='Calibration: %{x:.3f}<br>Log-Likelihood: %{y:.3f}<extra></extra>'
        ),
        row=2, col=1
    )

    # Panel 3: Negative log-likelihood (this is the LOSS we minimize)
    fig.add_trace(
        go.Scatter(
            x=calibration_values,
            y=neg_log_likelihoods,
            mode='lines',
            name='Negative Log-Likelihood',
            line=dict(color='red', width=3),
            fill='tozeroy',
            hovertemplate='Calibration: %{x:.3f}<br>NLL Loss: %{y:.3f}<extra></extra>'
        ),
        row=3, col=1
    )

    # Update axes labels
    fig.update_xaxes(title_text="Model Calibration (probability assigned to correct class)", row=3, col=1)
    fig.update_yaxes(title_text="Likelihood", row=1, col=1)
    fig.update_yaxes(title_text="Log-Likelihood", row=2, col=1)
    fig.update_yaxes(title_text="Loss", row=3, col=1)

    fig.update_layout(
        height=850,
        showlegend=False,
        title={
            'text': "Maximum Likelihood ↔ Cross-Entropy Connection<br><sub>Dataset: 5 samples (3 Dogs, 2 Cats)</sub>",
            'y': 0.98,
            'x': 0.5,
            'xanchor': 'center',
            'yanchor': 'top'
        }
    )

    # Add explanatory annotations
    fig.add_annotation(
        x=0.95, y=max(likelihoods)*0.5,
        text="MAXIMIZE<br>THIS<br>(Training Goal)",
        showarrow=True,
        arrowhead=2,
        ax=-50, ay=0,
        row=1, col=1,
        font=dict(size=11, color='darkblue')
    )

    fig.add_annotation(
        x=0.95, y=max(log_likelihoods)*0.5,
        text="MAXIMIZE<br>THIS<br>(Equivalent Goal)",
        showarrow=True,
        arrowhead=2,
        ax=-50, ay=0,
        row=2, col=1,
        font=dict(size=11, color='darkgreen')
    )

    fig.add_annotation(
        x=0.95, y=min(neg_log_likelihoods)*1.5,
        text="MINIMIZE<br>THIS<br>(Actual Training Loss)",
        showarrow=True,
        arrowhead=2,
        ax=-50, ay=30,
        row=3, col=1,
        font=dict(size=11, color='darkred')
    )

    return fig

fig = create_likelihood_viz()
fig.show()
```

**Critical Insights**:

- **X-axis**: Model calibration = probability assigned to the correct class (ranges from 0.5=random guessing to 0.98=well-calibrated)
- **Top panel**: Likelihood shrinks exponentially with dataset size → numerical underflow risk!
- **Middle panel**: Log-likelihood converts products to sums → numerically stable, same maximum
- **Bottom panel**: Negative log-likelihood (NLL) = Cross-Entropy Loss used in training
- **Key Connection**: max L(θ) ⟺ max log L(θ) ⟺ min -log L(θ) ⟺ min Cross-Entropy

The visualization shows: as model calibration improves (x increases), likelihood increases, log-likelihood increases, and loss (NLL) decreases.
:::

### Negative Log-Likelihood

With Maximum Likelihood, we aim to choose the parameters $\theta$ such that $p(\vect{y} | \theta, \m{X})$ is maximized. However, this function can be complex to handle, and we can use some mathematical tricks. We take the logarithm of the likelihood so that the product of probabilities becomes a sum. Since the logarithm is a monotonically increasing function, we can maximize its logarithm instead of the function $p(\vect{y} | \theta, \m{X})$. Finally, we take the negative of the function, allowing us to minimize it.

\begin{equation}
L(\m{X}, \vect{y}, \theta) = - \log \prod_{i=1}^N \prod_{j=1}^K P(Y = j| X = \vect{x}^{(i)})^{y^{(i)}_j} \\
L(\m{X}, \vect{y}, \theta) = -\sum_{i=1}^N \sum_{j=1}^K y^{(i)}_j \log  P(Y = j| X = \vect{x}^{(i)})
\end{equation}

### Cross-Entropy

The loss function, derived with Maximum Likelihood, can also be viewed through the lens of cross-entropy between two discrete probability functions. Specifically, we can calculate and minimize the cross-entropy between the true distribution $\vect{y}^{(i)}$ and the predicted $\vect{\hat{y}}^{(i)}$. Cross-entropy comes from information theory and measures how many bits/nats on average are needed to describe an event of a probability distribution $p(x)$ when using the approximation $q(x)$.

\begin{equation}
CE = - \sum_{x \in X} p(x) \log q(x) \\
CE = - \sum_{i=1}^N \sum_{j=1}^K y_j^{(i)} \log \hat{y}_j^{(i)}
\end{equation}

It is evident that cross-entropy is identical to the negative log-likelihood.

::: {.callout-tip title="📊 Cross-Entropy Loss Landscape" collapse="false"}
**Visualize how cross-entropy penalizes predictions**:

```{python}
#| eval: true
#| echo: false
#| fig-width: 800

def create_ce_landscape():
    # For true label = 1
    p_pred = np.linspace(0.01, 0.99, 100)
    loss_true_1 = -np.log(p_pred)
    loss_true_0 = -np.log(1 - p_pred)

    fig = go.Figure()

    fig.add_trace(go.Scatter(
        x=p_pred,
        y=loss_true_1,
        mode='lines',
        name='True class = 1',
        line=dict(color='blue', width=3)
    ))

    fig.add_trace(go.Scatter(
        x=p_pred,
        y=loss_true_0,
        mode='lines',
        name='True class = 0',
        line=dict(color='red', width=3)
    ))

    fig.update_layout(
        title="Cross-Entropy Loss: How wrong predictions are penalized",
        xaxis_title="Predicted Probability for Class 1",
        yaxis_title="Cross-Entropy Loss",
        hovermode='x unified',
        height=500
    )

    # Add annotations
    fig.add_annotation(
        x=0.1, y=2.3,
        text="High penalty for<br>confident wrong prediction!",
        showarrow=True,
        arrowhead=2,
        ax=50, ay=-50
    )

    fig.add_annotation(
        x=0.9, y=2.3,
        text="High penalty for<br>confident wrong prediction!",
        showarrow=True,
        arrowhead=2,
        ax=-50, ay=-50
    )

    return fig

fig = create_ce_landscape()
fig.show()
```

**Key Insights**:

- Loss approaches **∞** when model is confidently wrong (p→0 for true class)
- Loss approaches **0** when model is confidently correct (p→1 for true class)
- **Asymmetric penalty**: Being wrong and confident is much worse than being uncertain
:::

```{python}
#| eval: true
#| echo: false
#| fig-width: 600
#| label: fig-classification-cross-entropy
#| fig-cap:
#|   - "True Distribution (left) and Predicted Distribution (right)."

from matplotlib import pyplot as plt
import numpy as np
import seaborn as sns

np.random.seed(123)
k = 5
logits = np.random.normal(size=(5, )) * 2
y_hat = np.exp(logits) / np.exp(logits).sum()
y_true = np.zeros_like(logits)
y_true[1] = 1

fig, ax = plt.subplots(ncols=2, figsize=(9, 3))
_ = sns.barplot(x=[i for i in range(0, k)], y=y_true, ax=ax[0]).set(
    title="True Distribution")
_ = sns.barplot(x=[i for i in range(0, k)], y=y_hat, ax=ax[1]).set(
    title="Predicted Distribution", ylim=(0, 1))

cross_entropy = f"{(- y_true * np.log(y_hat)).sum():.3f}"

plt.tight_layout()
plt.show()
```

@fig-classification-cross-entropy shows an example with a cross-entropy value of: `{python} cross_entropy`.

::: {.callout-tip title="🎯 Interactive Multi-Class Cross-Entropy" collapse="false"}
**See how different predictions map to loss values on the curve**:

```{python}
#| eval: true
#| echo: false
#| fig-width: 800

def create_multiclass_ce_demo():
    """Interactive multi-class cross-entropy demonstration."""

    # True label (one-hot)
    true_class = 1  # Class 1 (0-indexed)
    n_classes = 5

    # Create scenarios with different predictions
    scenarios = {
        "Perfect Prediction": [0.0, 1.0, 0.0, 0.0, 0.0],
        "Confident Correct": [0.05, 0.85, 0.05, 0.03, 0.02],
        "Uncertain Correct": [0.15, 0.35, 0.2, 0.15, 0.15],
        "Confident Wrong": [0.85, 0.05, 0.05, 0.03, 0.02],
        "Uniform (Random)": [0.2, 0.2, 0.2, 0.2, 0.2]
    }

    # Calculate cross-entropy for each scenario
    ce_losses = {}
    for name, probs in scenarios.items():
        ce = -np.log(probs[true_class] + 1e-10)
        ce_losses[name] = ce

    # Create visualization with 2 rows
    fig = make_subplots(
        rows=2, cols=1,
        subplot_titles=(
            "Predicted Probabilities",
            "Cross-Entropy Loss Landscape - Red Dot Shows Current Loss"
        ),
        row_heights=[0.45, 0.55],
        vertical_spacing=0.15
    )

    # Initial scenario
    initial_name = "Confident Correct"
    initial_probs = scenarios[initial_name]

    # Row 1: Add probability bars
    colors = ['lightcoral' if i != true_class else 'lightgreen' for i in range(n_classes)]
    fig.add_trace(
        go.Bar(
            x=[f'Class {i}' for i in range(n_classes)],
            y=initial_probs,
            marker_color=colors,
            name="Probabilities",
            text=[f"{p:.2f}" for p in initial_probs],
            textposition='outside'
        ),
        row=1, col=1
    )

    # Row 2: Add CE loss landscape
    p_pred = np.linspace(0.01, 0.99, 100)
    loss_curve = -np.log(p_pred)

    fig.add_trace(
        go.Scatter(
            x=p_pred,
            y=loss_curve,
            mode='lines',
            name='CE Loss Curve',
            line=dict(color='blue', width=3),
            showlegend=False
        ),
        row=2, col=1
    )

    # Add point for initial scenario on the loss landscape
    initial_pred = initial_probs[true_class]
    initial_loss = ce_losses[initial_name]

    fig.add_trace(
        go.Scatter(
            x=[initial_pred],
            y=[initial_loss],
            mode='markers',
            marker=dict(
                size=15,
                color='red',
                symbol='circle',
                line=dict(width=2, color='darkred')
            ),
            name=initial_name,
            text=[f"{initial_name}<br>p={initial_pred:.3f}<br>Loss={initial_loss:.3f}"],
            hovertemplate='%{text}<extra></extra>',
            showlegend=False
        ),
        row=2, col=1
    )

    # Add true label indicator
    fig.add_annotation(
        x=f'Class {true_class}',
        y=1.05,
        text="← TRUE LABEL",
        showarrow=False,
        yref="y1",
        xref="x1",
        font=dict(size=12, color="green", family="Arial Black")
    )

    # Create buttons for scenarios
    buttons = []
    for name, probs in scenarios.items():
        colors = ['lightcoral' if i != true_class else 'lightgreen' for i in range(n_classes)]
        pred_for_true = probs[true_class]
        loss = ce_losses[name]

        button = dict(
            label=name,
            method="update",
            args=[
                {
                    "y": [
                        probs,
                        loss_curve,
                        [loss]
                    ],
                    "text": [
                        [f"{p:.2f}" for p in probs],
                        None,
                        [f"{name}<br>p={pred_for_true:.3f}<br>Loss={loss:.3f}"]
                    ],
                    "x": [
                        [f'Class {i}' for i in range(n_classes)],
                        p_pred,
                        [pred_for_true]
                    ]
                },
                {"title": f"Scenario: {name} | Probability for True Class = {pred_for_true:.3f} | Loss = {loss:.3f}"}
            ]
        )
        buttons.append(button)

    fig.update_layout(
        updatemenus=[
            dict(
                type="dropdown",
                direction="down",
                x=0.5,
                xanchor="center",
                y=1.08,
                yanchor="top",
                buttons=buttons
            )
        ],
        height=700,
        showlegend=False,
        yaxis1=dict(range=[0, 1.1], title="Probability"),
        yaxis2=dict(title="Cross-Entropy Loss", range=[0, 5]),
        xaxis2=dict(title="Predicted Probability for True Class (Class 1)"),
        title=f"Scenario: {initial_name} | Probability for True Class = {initial_pred:.3f} | Loss = {initial_loss:.3f}"
    )

    return fig

fig = create_multiclass_ce_demo()
fig.show()
```

**Key Takeaways**:

- **Green bar** = True class; **Red dot** = Current position on loss curve
- Watch how the red dot moves along the curve as predictions change
- Perfect prediction (p=1.0) → Dot at left, Loss ≈ 0
- Confident wrong (p=0.05) → Dot at right, Loss ≈ 3.0
:::

## Architectures

Architecture refers to the complete definition of an untrained (deep-learning) model, i.e., the sequence and configuration of layers. The following are some important architectures that have been successfully used for image classification.

<!-- TODO: add a simple visualization that makes clear at the end is a vector of size k with the class logits -->
### AlexNet

CNNs became extremely popular after winning the [ImageNet](https://www.image-net.org/) Competition. @krizhevsky_imagenet_2012 implemented a CNN with multiple layers, known as the AlexNet architecture, as shown in @fig-classification-alexnet. ImageNet is a large, hierarchical image dataset @deng_imagenet_2009, which enabled efficient training of CNNs for the first time.

AlexNet consists of 5 convolutional layers and 3 fully-connected layers. The last layer is a 1000-way softmax output to model the classes in ImageNet.

The model was trained with two GPUs (GTX 580) with 3GB memory each. Since 3GB was insufficient to train the model, the architecture was split across the GPUs. Some layers were split between the GPUs, allowing a larger network to be trained.

@fig-classification-alexnet shows the detailed architecture, including kernel sizes, the number of filters per layer, activation map dimensions, and pooling layers. ReLU was used as the activation function. This representation is from the original paper and shows how the model was split across two GPUs.

::: {#fig-classification-alexnet}
![]({{< meta params.images_path >}}alexnet.png){width=600}

AlexNet @krizhevsky_imagenet_2012.
:::

Since the split across two GPUs is no longer necessary, the architecture is somewhat simplified. Modern implementations are shown in @fig-classification-alexnet2.

::: {#fig-classification-alexnet2 width=400}

![]({{< meta params.images_path >}}alexnet_illustration.jpg)

AlexNet @prince_understanding_2023
:::

@fig-classification-alexnet-table presents the operations in AlexNet in tabular form.

::: {#fig-classification-alexnet-table width=600}
![]({{< meta params.images_path >}}alexnet_table.jpg)

Source: @johnson_eecs_2019.
:::

We can also easily load AlexNet via torchvision.

```{python}
#| eval: true
#| echo: true

import torch
import torchvision.models as models
import torchinfo

alexnet = models.alexnet()
x = torch.zeros(1, 3, 224, 224, dtype=torch.float, requires_grad=False)
yhat = alexnet(x)

print(torchinfo.summary(alexnet, input_size=(1, 3, 224, 224)))
```

### VGG

@simonyan_very_2015 won the ImageNet Challenge in 2014 with their VGG architecture. They showed that smaller 3x3 kernels work significantly better and that deeper networks with 16-19 layers can be trained. @fig-classification-vgg shows the architecture as presented in the original paper. @fig-classification-vgg-arch visualizes the architecture.

::: {#fig-classification-vgg}
![]({{< meta params.images_path >}}vgg.png){width=600}

VGG @simonyan_very_2015.
:::

::: {#fig-classification-vgg-arch}

![]({{< meta params.images_path >}}vgg_illustration.jpg)

VGG @prince_understanding_2023

:::

VGG introduced a popular design element: A layer has the same number of filters as the previous layer unless the activation map dimensions are halved, in which case the number of filters is doubled (see also @fig-classification-vgg-arch). This was done to maintain the time complexity of the layers. VGG does not use normalization layers.

@fig-classification-vgg-design compares VGG with AlexNet.

::: {#fig-classification-vgg-design}
![]({{< meta params.images_path >}}vgg_design.jpg){width=600}

Source: @johnson_eecs_2019.
:::

### ResNet

@He2016 wondered whether CNNs could be improved simply by making them deeper, i.e., adding more layers. Their experiments showed that adding layers eventually stops being beneficial, and performance saturates and then rapidly degrades (see @fig-classification-resnet-motivation1). They noticed that the performance did not degrade due to overfitting (see @fig-classification-resnet-motivation2, which shows that training error is also poor), but because back-propagation becomes less effective and the weights do not optimize well. @He2016 hypothesized that deeper networks could not perform worse than shallower ones because the additional layers could simply pass on activations unchanged.

::: {#fig-classification-resnet-motivation1}
![]({{< meta params.images_path >}}resnet_test_error.jpg){width=600}

Source: @He2016
:::

::: {#fig-classification-resnet-motivation2}
![]({{< meta params.images_path >}}resnet_train_error.jpg){width=600}

Source: @He2016
:::

They then hypothesized that passing on activations unchanged as a default behavior could avoid this problem. They introduced identity mappings. @fig-classification-residual-connection shows such a connection. These connections are also called residual connections because the network only needs to learn the change in activations from layer $i$ to layer $i+1$, the residue.

::: {#fig-classification-residual-connection}
![]({{< meta params.images_path >}}residual_connection.jpg){width=600}

ResNet @He2016 (Graphic from @johnson_eecs_2019.)
:::

ResNet was otherwise inspired by VGG16. The convolutional layers consist of 3x3 kernels (except the first one), and the number of filters is doubled when the activation map dimensions are halved. ResNet uses convolutions with stride 2 for down-sampling and no max pooling. At the end, ResNet uses a global average pooling layer followed by a fully-connected layer with the number of classes. Variants of ResNet have 18, 34, 50, and over 100 layers. ResNet also uses batch normalization. @fig-classification-resnet-struk shows the architecture.

::: {#fig-classification-resnet-struk}
![]({{< meta params.images_path >}}resnet_arch_horiz.png){width=600}

Source: @He2016.
:::

ResNet is very popular and is still widely used today (there are now more modern variants). Recent studies of identity mappings (or more generally skip connections) have shown that the gradient surface of the loss function becomes smoother, allowing the network to be optimized better. This is impressively illustrated in @fig-classification-skip-loss-landscape.

::: {#fig-classification-skip-loss-landscape}
![]({{< meta params.images_path >}}loss_landscape_skip.png){width=600}

Source: @li_visualizing_2018.
:::

### ConvNext

One of the most modern CNN architectures was described in @liu_convnet_2022. This architecture uses tricks and implementation ideas accumulated over decades from various architectures. @fig-classification-convnext shows, starting from a modern version of ResNet, what has been adjusted to define this state-of-the-art architecture. Examples include: larger kernels, different activation functions, layer normalization instead of batch normalization, and depthwise separable convolutions.

::: {#fig-classification-convnext}
![]({{< meta params.images_path >}}convnext.png){width=600}

Convnext @liu_convnet_2022.
:::

There is already a new version of this architecture @woo_convnext_2023.

### ImageNet Performance

@fig-classification-image-net-performance shows the development of ImageNet performance. As of 2023, we are much closer to 100%, see [Link](https://paperswithcode.com/sota/image-classification-on-imagenet?metric=Top%205%20Accuracy).

::: {#fig-classification-image-net-performance}
![]({{< meta params.images_path >}}image_net_performance.jpg){width=600}

Graphic from @johnson_eecs_2019
:::

### Which Architecture?

Which architecture should be chosen for a specific problem? A common tip is: *Don't be a hero.*

One should rely on off-the-shelf architectures and not implement their own without a good reason.

Typically, ResNet-50 or ResNet-101 are good choices. However, there are also models that require significantly fewer parameters, such as Efficient Nets.

Important considerations are also the requirements regarding accuracy, performance (FLOPs), and model size (memory).

## Further Variations

There are countless other architectures and especially specific layers or layer blocks that can be used in various architectures. The following are Squeeze/Excite Networks and the important class of normalization layers, which are often used and can significantly improve models or accelerate the optimization process.

### Squeeze/Excite Networks

Squeeze-and-Excite Networks (SE-Networks) were introduced in 2019 @hu_squeeze-and-excitation_2019. These include so-called Squeeze and Excite blocks (SE blocks), which allow the scaling of activation maps of a layer. This scaling is learnable through a few additional parameters. In practice, significant performance gains have been observed. @fig-classification-se-networks shows an illustration.

::: {#fig-classification-se-networks}
![]({{< meta params.images_path >}}se_networks.png){width=800}

Source: @hu_squeeze-and-excitation_2019.
:::


These SE blocks can be easily applied to arbitrary activation maps. @fig-classification-se-networks shows an input feature map $\tensor{X} \in \mathbb{R}^{H' \times W' \times C'}$ that is transformed with $F_{tr}$ (e.g., with a convolutional layer). This results in the activation maps $\tensor{U} \in \mathbb{R}^{H \times W \times C}$.

The transformation $F_{sq}(\cdot)$ applies the squeeze operation, a global average pooling, to generate a description of each channel by aggregating spatial information of the activation map $\tensor{U}$. This results in a vector $z \in \mathbb{R}^{1 \times 1 \times C}$.

The excitation operation $F_{ex}(\cdot, W)$ uses a gating mechanism with parameters $W$, implemented with two fully-connected layers and activation functions. The result is $s \in \mathbb{R}^{1 \times 1 \times C}$, the channel weights. It is called gating because the weights range from $[0, 1]$ and thus control how much information of a channel flows through (gatekeeping).

Finally, $F_{scale}(\cdot)$ scales the activation maps $\tensor{U}$ with the channel weights.

The operation can be described as follows:

\begin{equation}
U = F_{tr}(X) \\
z = F_{sq}(U) = \text{GlobalAvgPool}(U) \\
s = F_{ex}(z, W) = \sigma(\mathbf{W}_2 g(\mathbf{W}_1 z)) \\
\hat{X} = F_{scale}(U, s) = U \odot s
\end{equation}

where $g()$ represents the ReLU function, and $\sigma$ represents the sigmoid function.

### Normalization Layers

Normalization layers normalize activation maps to improve parameter learning. There are many variations, as shown in @fig-classification-normalization. In modern architectures, normalization layers are typically used. Popular is, for example, layer normalization (see @ba_layer_2016). The general form of normalization is given in equation @eq-classification-normalization. The parameters $\gamma$ and $\beta$ are learned, while the means $E[x]$ and variances $\sigma^2[x]$ are estimated from the activations. Why normalization layers work and which ones to prefer are still subjects of research and are often empirically tested in practice (considered as hyper-parameters). It is believed that the cost function becomes smoother overall, allowing the network to train faster and better @santurkar_how_2019.


$$
y = \frac{x - E[x]}{\sqrt{\sigma^2[x] + \epsilon}} * \gamma + \beta
$$ {#eq-classification-normalization}

::: {#fig-classification-normalization}
![]({{< meta params.images_path >}}normalization.png){width=300}

Source: @qiao_micro-batch_2020.
:::

## Pre-Processing

Typically, images are pre-processed before being processed by the CNN. This is done to make the optimization process converge faster. Common steps include:

- Resizing/cropping to a fixed size, e.g., 224x224x3. This is necessary for the images to be processed in a batch (they must have the same dimensions).
- Scaling: Images are scaled from the interval [0, 255] to the interval [0, 1].
- Normalization: Normalization is often done along the color channels so that the mean value of the pixel values is 0.

In `torchvision`, the following pre-processing steps are used: [Link](https://github.com/pytorch/vision/blob/main/torchvision/transforms/_presets.py)

## Transfer Learning

Transfer Learning refers to the process of adapting a trained model for Task A to Task B. Adapting pre-trained models often leads to better results and also reduces the number of training iterations. Deep learning frameworks often provide easy access to such models (also known as Model Zoo). For example, from PyTorch: https://pytorch.org/vision/stable/models.html.

```{python}
#| eval: false
#| echo: true

from torchvision.models import resnet50, ResNet50_Weights
resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)
```

## PyTorch Example

### Architecture

Example of a CNN architecture.

```{python}
#| eval: false
#| echo: true

import torch
import torch.nn as nn
import torch.nn.functional as F


class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = torch.flatten(x, 1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

net = Net()
```

### Loss Function

```{python}
#| eval: false
#| echo: true

import torch.optim as optim

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
```



::: {.callout-tip title="Recap"}
Key takeaways:

- Softmax + cross-entropy arises naturally from maximum likelihood with a categorical distribution.
- Logits provide a numerically stable pre-softmax representation (prefer `log_softmax` + `nll_loss` internally).
- Architecture choice should start from proven baselines (ResNet / ConvNeXt) before experimentation.
- Transfer learning offers data efficiency; freezing early layers can regularize when data is scarce.
- Proper preprocessing (resize, normalize) and calibration techniques (label smoothing, temperature) improve robustness.
:::

## References

::: {#refs}
:::
