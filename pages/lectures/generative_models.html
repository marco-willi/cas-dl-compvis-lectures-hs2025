<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Marco Willi">

<title>8 - Generative Models – CAS Deep Learning - Computer Vision</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../pages/slides/intro.html" rel="next">
<link href="../../pages/lectures/intro_genai.html" rel="prev">
<link href="../..//assets/cv_logo_small.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-597958c53c93a607afca12fd375c57ed.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-159896a7fbbc1299bfd73fe3af9cd852.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-97BR3FTSN1"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-97BR3FTSN1', { 'anonymize_ip': true});
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../pages/lectures/intro.html">Lectures</a></li><li class="breadcrumb-item"><a href="../../pages/lectures/generative_models.html">8 - Generative Models</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-center sidebar-header">
      <a href="../../index.html" class="sidebar-logo-link">
      <img src="../../assets/cv_logo_small.png" alt="" class="sidebar-logo light-content py-0 d-lg-inline d-none">
      <img src="../../assets/cv_logo_small.png" alt="" class="sidebar-logo dark-content py-0 d-lg-inline d-none">
      </a>
      <div class="sidebar-tools-main">
    <a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2025" title="GitHub organization" class="quarto-navigation-tool px-1" aria-label="GitHub organization"><i class="bi bi-github"></i></a>
    <a href="" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-code-square"></i></a>
</div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Background &amp; Preparation</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/background/machine_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1 - Machine Learning Basics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/background/neural_networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2 - Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/background/frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3 - Software &amp; Hardware for Deep Learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Lectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1 - Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/cnns.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2 - Convolutional Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3 - Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/representation_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4 - Representation Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/practical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5 - Practical</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/recent_advances.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6 - Recent Advances</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/intro_genai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">7 - Intro to Generative AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/generative_models.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">8 - Generative Models</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Slides</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/cnns.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Convolutional Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/representation_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Representation Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/practical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Practical Considerations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/recent_advances.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Recent Advances</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/intro_genai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Intro to Generative AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/generative_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Generative Models</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/quiz.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Quiz &amp; Exam Prep</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false">
 <span class="menu-text">Resources</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/literature.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Books</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/links.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Helpful Links &amp; Resources</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Mathematical Notation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/student_projects.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Project Ideas</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#modelling-approaches" id="toc-modelling-approaches" class="nav-link active" data-scroll-target="#modelling-approaches"><span class="header-section-number">1</span> Modelling Approaches</a>
  <ul class="collapse">
  <li><a href="#autoregressive-models" id="toc-autoregressive-models" class="nav-link" data-scroll-target="#autoregressive-models"><span class="header-section-number">1.1</span> Autoregressive Models</a></li>
  <li><a href="#diffusion-models" id="toc-diffusion-models" class="nav-link" data-scroll-target="#diffusion-models"><span class="header-section-number">1.2</span> Diffusion Models</a></li>
  <li><a href="#generative-adversarial-networks" id="toc-generative-adversarial-networks" class="nav-link" data-scroll-target="#generative-adversarial-networks"><span class="header-section-number">1.3</span> Generative Adversarial Networks</a></li>
  <li><a href="#variational-autoencoders" id="toc-variational-autoencoders" class="nav-link" data-scroll-target="#variational-autoencoders"><span class="header-section-number">1.4</span> Variational Autoencoders</a></li>
  <li><a href="#comparing-gans-vaes-and-diffusion-models" id="toc-comparing-gans-vaes-and-diffusion-models" class="nav-link" data-scroll-target="#comparing-gans-vaes-and-diffusion-models"><span class="header-section-number">1.5</span> Comparing GANs, VAEs, and Diffusion Models</a></li>
  </ul></li>
  <li><a href="#conditional-generation" id="toc-conditional-generation" class="nav-link" data-scroll-target="#conditional-generation"><span class="header-section-number">2</span> Conditional Generation</a></li>
  <li><a href="#pre-trained-models" id="toc-pre-trained-models" class="nav-link" data-scroll-target="#pre-trained-models"><span class="header-section-number">3</span> Pre-Trained Models</a>
  <ul class="collapse">
  <li><a href="#stable-diffusion-practical-state-of-the-art" id="toc-stable-diffusion-practical-state-of-the-art" class="nav-link" data-scroll-target="#stable-diffusion-practical-state-of-the-art"><span class="header-section-number">3.1</span> Stable Diffusion: Practical State-of-the-Art</a></li>
  <li><a href="#why-latent-diffusion" id="toc-why-latent-diffusion" class="nav-link" data-scroll-target="#why-latent-diffusion"><span class="header-section-number">3.2</span> Why Latent Diffusion?</a></li>
  <li><a href="#conditioning-on-text-and-more" id="toc-conditioning-on-text-and-more" class="nav-link" data-scroll-target="#conditioning-on-text-and-more"><span class="header-section-number">3.3</span> Conditioning on Text (and More)</a></li>
  <li><a href="#fine-tuning-with-lora" id="toc-fine-tuning-with-lora" class="nav-link" data-scroll-target="#fine-tuning-with-lora"><span class="header-section-number">3.4</span> Fine-Tuning with LoRA</a></li>
  <li><a href="#hardware-requirements" id="toc-hardware-requirements" class="nav-link" data-scroll-target="#hardware-requirements"><span class="header-section-number">3.5</span> Hardware Requirements</a></li>
  <li><a href="#open-source-ecosystem" id="toc-open-source-ecosystem" class="nav-link" data-scroll-target="#open-source-ecosystem"><span class="header-section-number">3.6</span> Open-Source Ecosystem</a></li>
  <li><a href="#closed-source-platforms" id="toc-closed-source-platforms" class="nav-link" data-scroll-target="#closed-source-platforms"><span class="header-section-number">3.7</span> Closed-Source Platforms</a></li>
  </ul></li>
  <li><a href="#conclusions-discussions" id="toc-conclusions-discussions" class="nav-link" data-scroll-target="#conclusions-discussions"><span class="header-section-number">4</span> Conclusions &amp; Discussions</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">5</span> References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2025/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../pages/lectures/intro.html">Lectures</a></li><li class="breadcrumb-item"><a href="../../pages/lectures/generative_models.html">8 - Generative Models</a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">8 - Generative Models</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Marco Willi </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Institute for Data Science I4DS, FHNW
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-tip callout-titled" title="Learning Objectives">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Learning Objectives
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>After this lecture you should be able to:</p>
<ul>
<li>Explain the core mechanisms of major generative model families: autoregressive, diffusion, GANs, and VAEs</li>
<li>Compare tradeoffs between these approaches (sampling speed, quality, coverage, likelihood computation)</li>
<li>Understand how conditional generation extends models to accept additional inputs (text, images, class labels)</li>
<li>Utilize pre-trained models (Stable Diffusion) and efficient fine-tuning techniques (LoRA) for practical applications</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="TLDR Recap">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>TLDR Recap
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Four major generative model families make different tradeoffs: <strong>Autoregressive</strong> (exact likelihood, slow sequential sampling), <strong>Diffusion</strong> (SOTA quality, iterative denoising), <strong>GANs</strong> (fast generation, adversarial training, mode collapse risk), <strong>VAEs</strong> (structured latent space, approximate likelihood). <strong>Conditional generation</strong> extends models to accept text, images, or labels as input. <strong>Stable Diffusion</strong> runs diffusion in VAE latent space for efficiency, enabling practical high-resolution generation. <strong>LoRA</strong> fine-tuning adapts pre-trained models with minimal compute. Societal considerations include synthetic data pollution, authenticity challenges, and embedded biases.</p>
</div>
</div>
</div>
<section id="modelling-approaches" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="modelling-approaches"><span class="header-section-number">1</span> Modelling Approaches</h2>
<p>Over the past decade, several families of generative models have emerged, each with distinct philosophies and trade-offs. We introduce a few of the most important ones in the following sections, however, note that this list is not exhaustive.</p>
<section id="autoregressive-models" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="autoregressive-models"><span class="header-section-number">1.1</span> Autoregressive Models</h3>
<p>Autoregressive models generate data sequentially, predicting one element at a time conditioned on all previous elements. This sequential factorization turns the complex problem of modeling high-dimensional distributions into a series of simpler conditional distributions.</p>
<div id="fig-genai-autoregressive-overview" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-genai-autoregressive-overview-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/genai/autoregressive_overview.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-genai-autoregressive-overview-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Autoregressive generation: Models predict each element sequentially, conditioning on all previously generated elements. Figure from <span class="citation" data-cites="torralba_foundations_2024">Torralba, Isola, and Freeman (<a href="#ref-torralba_foundations_2024" role="doc-biblioref">2024</a>)</span>.
</figcaption>
</figure>
</div>
<p>For an image with pixels <span class="math inline">\(x_1, x_2, \ldots, x_n\)</span>, the joint distribution factors as:</p>
<p><span class="math display">\[
p(x_1, x_2, \ldots, x_n) = p(x_1) \cdot p(x_2|x_1) \cdot p(x_3|x_1,x_2) \cdots p(x_n|x_1,\ldots,x_{n-1})
\]</span></p>
<p>Each conditional <span class="math inline">\(p(x_i|x_1,\ldots,x_{i-1})\)</span> is modeled by a neural network (typically a masked CNN or Transformer). During generation, we sample from these conditionals sequentially:</p>
<ol type="1">
<li>Sample <span class="math inline">\(x_1 \sim p(x_1)\)</span></li>
<li>Sample <span class="math inline">\(x_2 \sim p(x_2|x_1)\)</span></li>
<li>Sample <span class="math inline">\(x_3 \sim p(x_3|x_1,x_2)\)</span></li>
<li>Continue until all pixels generated</li>
</ol>
<p><strong>Key advantage</strong>: Exact likelihood computation—we can evaluate <span class="math inline">\(p(x)\)</span> for any sample.</p>
<p><strong>Key limitation</strong>: Sequential generation is slow—cannot parallelize pixel prediction.</p>
</section>
<section id="diffusion-models" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="diffusion-models"><span class="header-section-number">1.2</span> Diffusion Models</h3>
<p>Diffusion models <span class="citation" data-cites="ho_denoising_2020">Ho, Jain, and Abbeel (<a href="#ref-ho_denoising_2020" role="doc-biblioref">2020</a>)</span> <span class="citation" data-cites="sohl-dickstein_deep_2015">Sohl-Dickstein et al. (<a href="#ref-sohl-dickstein_deep_2015" role="doc-biblioref">2015</a>)</span> have emerged as the state-of-the-art approach for high-quality image generation, powering systems like Stable Diffusion. The core insight is remarkably simple: learn to reverse a gradual noising process.</p>
<p><a href="#fig-genai-diffusion-forward-fcv" class="quarto-xref">Figure&nbsp;2</a> shows the <strong>forward diffusion</strong> process, whereby Gaussian noise is gradually added to an image (from left to right).</p>
<div id="fig-genai-diffusion-forward-fcv" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-genai-diffusion-forward-fcv-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/genai/forward_diffusion_fcv.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-genai-diffusion-forward-fcv-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Forward diffusion process. From <span class="citation" data-cites="torralba_foundations_2024">Torralba, Isola, and Freeman (<a href="#ref-torralba_foundations_2024" role="doc-biblioref">2024</a>)</span>.
</figcaption>
</figure>
</div>
<p>The Gaussian noise is added to data over <span class="math inline">\(T\)</span> steps (typically 1000), following a pre-specified noise schedule. This is a deterministic process, thus no neural network is involved.</p>
<p>The forward process at step <span class="math inline">\(t\)</span> is defined by:</p>
<p><span class="math display">\[\begin{equation}
z_t = \sqrt{1-\beta_t} z_{t-1} + \sqrt{\beta_t} \epsilon_t, \quad \epsilon_t \sim \mathcal{N}(0, I)
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\beta_t\)</span> controls the noise level (<strong>the noise schedule</strong>). This process gradually degrades the data <span class="math inline">\(z_0 = x\)</span> into pure noise <span class="math inline">\(z_T \approx \mathcal{N}(0, I)\)</span> over <span class="math inline">\(T\)</span> steps. Common noise schedules (linear, cosine) start with small <span class="math inline">\(\beta_t\)</span> values and gradually increase, ensuring smooth degradation from data to noise.</p>
<p>A crucial mathematical property: we can <strong>directly jump to any timestep</strong> <span class="math inline">\(z_t\)</span> without iterating through all intermediate steps. This enables efficient training, where we randomly sample timesteps and train the denoiser on all noise levels simultaneously.</p>
<p>The core idea is to train a neural network to model the <strong>reverse diffusion</strong> process, thus going in the opposite direction and denoise an image. <a href="#fig-genai-diffusion-reverse-fcv" class="quarto-xref">Figure&nbsp;3</a> shows the idea.</p>
<div id="fig-genai-diffusion-reverse-fcv" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-genai-diffusion-reverse-fcv-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/genai/reverse_diffusion_fcv.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-genai-diffusion-reverse-fcv-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Reverse diffusion process visualization. From <span class="citation" data-cites="torralba_foundations_2024">Torralba, Isola, and Freeman (<a href="#ref-torralba_foundations_2024" role="doc-biblioref">2024</a>)</span>.
</figcaption>
</figure>
</div>
<p>A neural network learns to gradually remove noise, step by step. Starting from pure noise <span class="math inline">\(z_T \sim \mathcal{N}(0, I)\)</span>, the model denoises iteratively to produce realistic samples.</p>
<p>The neural network learns to predict the noise <span class="math inline">\(\epsilon_t\)</span> that was added at each timestep. Crucially, the network is <strong>conditioned on the timestep</strong> <span class="math inline">\(t\)</span>, since the noise level varies dramatically from early (almost clean) to late (almost pure noise) steps. The training objective is simple:</p>
<p><span class="math display">\[\begin{equation}
\mathcal{L} = \mathbb{E}_{t, x, \epsilon} \| \epsilon - f_\theta(z_t, t) \|^2
\end{equation}\]</span></p>
<p>Train the network to predict the noise <span class="math inline">\(\epsilon\)</span> that was added to create <span class="math inline">\(z_t\)</span> from <span class="math inline">\(x\)</span>.</p>
<div id="fig-genai-diffusion-algorithm" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-genai-diffusion-algorithm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/genai/diffusion_algorithm.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-genai-diffusion-algorithm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Training and sampling algorithms: Training samples random timesteps and noise; sampling iteratively denoises from <span class="math inline">\(T\)</span> to 0. Guidance techniques (not shown) can further improve sample quality.
</figcaption>
</figure>
</div>
<p>The denoising network typically uses a <strong>U-Net architecture</strong>—a convolutional encoder-decoder with skip connections. The U-Net processes noisy images at multiple resolutions, with timestep <span class="math inline">\(t\)</span> embedded and injected throughout the network.</p>
<div id="fig-genai-diffusion-unet" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-genai-diffusion-unet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/genai/diffusion_unet.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-genai-diffusion-unet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: U-Net architecture for diffusion models: Downsampling path (left) captures context, upsampling path (right) enables precise localization, and skip connections (horizontal) preserve spatial details. Timestep embeddings condition the network on noise level. From <span class="citation" data-cites="prince_understanding_2023">Prince (<a href="#ref-prince_understanding_2023" role="doc-biblioref">2023</a>)</span>.
</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Pros and Cons of Diffusion Models">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Pros and Cons of Diffusion Models
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Pros</strong></p>
<ul>
<li>High-quality, diverse outputs</li>
<li>Stable and reliable training</li>
</ul>
<p><strong>Cons</strong></p>
<ul>
<li>Slow inference time</li>
<li>High compute (training + inference) cost</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Interesting Note on the Noise">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Interesting Note on the Noise
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>While the standard diffusion framework has proven highly successful, researchers have explored numerous extensions and modifications:</p>
<p><strong>Cold Diffusion</strong> <span class="citation" data-cites="bansal_cold_2022">(<a href="#ref-bansal_cold_2022" role="doc-biblioref">Bansal et al. 2022</a>)</span> demonstrates that the Gaussian noise assumption is not fundamental to diffusion models. The key insight: any degradation process that can be inverted works. Instead of adding Gaussian noise, Cold Diffusion applies transformations like blurring, pixelation, or masking—yet still learns to reverse them through the same denoising framework.</p>
<div id="fig-genai-cold-diffusion" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-genai-cold-diffusion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/genai/cold_diffusion.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-genai-cold-diffusion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Cold Diffusion: Various degradation operators (blur, downsampling, masking) can replace Gaussian noise in the diffusion framework. The model learns to reverse these transformations through the same training procedure. From <span class="citation" data-cites="bansal_cold_2022">Bansal et al. (<a href="#ref-bansal_cold_2022" role="doc-biblioref">2022</a>)</span>.
</figcaption>
</figure>
</div>
<p>This generalization reveals that diffusion’s success stems from the gradual, multi-step inversion strategy rather than the specific choice of Gaussian noise. The framework adapts to diverse degradation operators, opening possibilities for task-specific degradations that may better suit particular domains or applications.</p>
</div>
</div>
</div>
</section>
<section id="generative-adversarial-networks" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="generative-adversarial-networks"><span class="header-section-number">1.3</span> Generative Adversarial Networks</h3>
<p>GANs, see <span class="citation" data-cites="goodfellow_generative_2014">Goodfellow et al. (<a href="#ref-goodfellow_generative_2014" role="doc-biblioref">2014</a>)</span>, frame generative modeling as an <strong>adversarial game</strong> between two neural networks: a <strong>generator</strong> <span class="math inline">\(g(\mathbf{z})\)</span> that creates fake samples, and a <strong>discriminator</strong> <span class="math inline">\(d(\mathbf{x})\)</span> that tries to distinguish real data from generated fakes (see <a href="#fig-genai-gan-idea" class="quarto-xref">Figure&nbsp;7</a>). This adversarial setup drives both networks to improve: the generator learns to create increasingly realistic samples, while the discriminator becomes better at spotting fakes.</p>
<div id="fig-genai-gan-idea" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-genai-gan-idea-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/genai/gan-architecture.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-genai-gan-idea-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: GAN idea: The generator creates fake samples from random noise <span class="math inline">\(z \sim \mathcal{N}(0, I)\)</span>, while the discriminator judges whether samples are real or fake. From <a href="https://microsoft.github.io/GenStudio/">Link</a>
</figcaption>
</figure>
</div>
<p>The GAN training objective can be expressed as a minimax game:</p>
<p><span class="math display">\[
\min_{g} \max_{d} \mathbb{E}_{\mathbf{x} \sim p_{data}}[\log d(\mathbf{x})] + \mathbb{E}_{\mathbf{z} \sim p_z}[\log(1 - d(g(\mathbf{z})))]
\]</span></p>
<p>The discriminator <span class="math inline">\(d\)</span> maximizes its ability to correctly classify real (<span class="math inline">\(d(\mathbf{x}) \approx 1\)</span>) vs.&nbsp;fake (<span class="math inline">\(d(g(\mathbf{z})) \approx 0\)</span>) samples. The generator <span class="math inline">\(G\)</span> minimizes the discriminator’s success, trying to produce samples where <span class="math inline">\(d(g(\mathbf{z})) \approx 1\)</span> (fooling the discriminator). See <a href="#fig-genai-gan-learning" class="quarto-xref">Figure&nbsp;8</a> for an illustration.</p>
<p>In practice, training alternates between: 1. <strong>Discriminator step</strong>: Update <span class="math inline">\(d\)</span> to better distinguish real from fake 2. <strong>Generator step</strong>: Update <span class="math inline">\(g\)</span> to better fool the current discriminator</p>
<div id="fig-genai-gan-learning" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-genai-gan-learning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/genai/gan_learning.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-genai-gan-learning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: GAN learning process: As training progresses, the generator’s distribution (green) moves closer to the real data distribution (black dots). The discriminator’s decision boundary (blue) adapts accordingly. From <span class="citation" data-cites="foster_generative_2023">Foster (<a href="#ref-foster_generative_2023" role="doc-biblioref">2023</a>)</span>
</figcaption>
</figure>
</div>
</section>
<section id="variational-autoencoders" class="level3" data-number="1.4">
<h3 data-number="1.4" class="anchored" data-anchor-id="variational-autoencoders"><span class="header-section-number">1.4</span> Variational Autoencoders</h3>
<p>Variational Autoencoders (VAEs) <span class="citation" data-cites="Kingma2014">(<a href="#ref-Kingma2014" role="doc-biblioref">Kingma and Welling 2014</a>)</span> take a different approach to generative modeling, building on the autoencoder architecture (see <a href="#fig-genai-vae-illustration" class="quarto-xref">Figure&nbsp;9</a>) but with a crucial probabilistic twist.</p>
<div id="fig-genai-vae-illustration" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-genai-vae-illustration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/genai/vae_illustration.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-genai-vae-illustration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: VAE architecture: The encoder maps inputs to latent distributions <span class="math inline">\(q(z|x)\)</span>, while the decoder reconstructs from latent samples. From <a href="https://lilianweng.github.io/posts/2018-08-12-vae/">Source</a>.
</figcaption>
</figure>
</div>
<p>A VAE learns two networks that perform (approximate) inverse operations:</p>
<ul>
<li><strong>Encoder</strong> <span class="math inline">\(q_{\psi}(z|x)\)</span>: Maps data <span class="math inline">\(x\)</span> to a distribution over latent codes <span class="math inline">\(z\)</span></li>
<li><strong>Decoder</strong> <span class="math inline">\(p_{\theta}(x|z)\)</span>: Maps latent codes <span class="math inline">\(z\)</span> back to data <span class="math inline">\(x\)</span></li>
</ul>
<p>Unlike standard autoencoders, VAEs enforce structure on the latent space through probabilistic modeling (see <a href="#fig-genai-vae-illustration2" class="quarto-xref">Figure&nbsp;10</a>), ensuring that randomly sampled latent codes produce meaningful outputs.</p>
<div id="fig-genai-vae-illustration2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-genai-vae-illustration2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/genai/vae_illustration2.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-genai-vae-illustration2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: VAE computational graph: The encoder outputs mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span> for each latent dimension. The <strong>reparameterization trick</strong> (<span class="math inline">\(z = \mu + \epsilon \cdot \sigma\)</span>, where <span class="math inline">\(\epsilon \sim \mathcal{N}(0, 1)\)</span>) enables gradient-based optimization. From <a href="https://lilianweng.github.io/posts/2018-08-12-vae/">Source</a>.
</figcaption>
</figure>
</div>
<p>VAEs optimize the <strong>Evidence Lower Bound (ELBO)</strong>, which consists of two terms:</p>
<p><span class="math display">\[
\text{ELBO} = \underbrace{\mathbb{E}_{z \sim q_{\psi}(z|x)}[\log p_{\theta}(x|z)]}_{\text{Reconstruction}} - \underbrace{\text{KL}(q_{\psi}(z|x) \| p(z))}_{\text{Regularization}}
\]</span></p>
<p>The <strong>reconstruction term</strong> ensures the decoder can recover the input from the latent code. The <strong>KL regularization term</strong> (typically with prior <span class="math inline">\(p(z) = \mathcal{N}(0, I)\)</span>) prevents the latent space from developing “gaps and tendrils”—ensuring that random samples from <span class="math inline">\(\mathcal{N}(0, I)\)</span> decode to meaningful outputs.</p>
<p>This balance is key to VAE behavior: the reconstruction loss wants to use the latent space flexibly, while KL regularization pushes latent codes toward a standard normal distribution. The result is a <strong>structured, continuous latent space</strong> where interpolation and attribute manipulation work smoothly.</p>
<div id="fig-genai-vae-latent-space" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-genai-vae-latent-space-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/genai/vae_latent_space_face.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-genai-vae-latent-space-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: VAE latent space: Traversing the latent space produces smooth interpolations between faces, demonstrating the structured representation learned by the model. From <span class="citation" data-cites="prince_understanding_2023">Prince (<a href="#ref-prince_understanding_2023" role="doc-biblioref">2023</a>)</span>.
</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Pros and Cons of VAEs">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Pros and Cons of VAEs
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Pros</strong></p>
<ul>
<li>Principled probabilistic foundation</li>
<li>Latent space structure: can interpolate in latent space and interpret factors</li>
<li>Stable training</li>
<li>Can be used for representation learning</li>
</ul>
<p><strong>Cons</strong></p>
<ul>
<li>Blurry outputs (more complex variants can reduce this)</li>
</ul>
</div>
</div>
</section>
<section id="comparing-gans-vaes-and-diffusion-models" class="level3" data-number="1.5">
<h3 data-number="1.5" class="anchored" data-anchor-id="comparing-gans-vaes-and-diffusion-models"><span class="header-section-number">1.5</span> Comparing GANs, VAEs, and Diffusion Models</h3>
<p>Having explored each model family, how do they actually behave differently? Let’s examine their behavior on simple 1D and 2D distributions to build intuition about their strengths and failure modes.</p>
<section id="d-distribution-comparison" class="level4" data-number="1.5.1">
<h4 data-number="1.5.1" class="anchored" data-anchor-id="d-distribution-comparison"><span class="header-section-number">1.5.1</span> 1D Distribution Comparison</h4>
<div id="fig-genai-1d-true" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-genai-1d-true-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/genai/1d_comparison/true_distribution.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-genai-1d-true-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: True 1D data distribution: A bimodal distribution that the generative models must learn to reproduce.
</figcaption>
</figure>
</div>
<div class="tabset-margin-container"></div><div id="fig-genai-1d-example" class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true" href="" aria-current="page">GAN</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false" href="">VAE</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-3" role="tab" aria-controls="tabset-1-3" aria-selected="false" href="">Diffusion</a></li></ul>
<div id="fig-genai-1d-example" class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<div id="fig-genai-1d-comparison" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-genai-1d-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../assets/images/genai/1d_comparison/gan_training_progress.png" class="img-fluid figure-img"></p>
<figcaption>GAN</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-genai-1d-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13
</figcaption>
</figure>
</div>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<div id="fig-genai-1d-example-vae" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-genai-1d-example-vae-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../assets/images/genai/1d_comparison/vae_training_progress.png" class="img-fluid figure-img"></p>
<figcaption>VAE</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-genai-1d-example-vae-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14
</figcaption>
</figure>
</div>
</div>
<div id="tabset-1-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-3-tab">
<div id="fig-genai-1d-example-gan-diffusion" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-genai-1d-example-gan-diffusion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../assets/images/genai/1d_comparison/diffusion_training_progress.png" class="img-fluid figure-img"></p>
<figcaption>Diffusion</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-genai-1d-example-gan-diffusion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15
</figcaption>
</figure>
</div>
<p>Training progression on 1D bimodal distribution. Each panel shows how the learned distribution (orange) approaches the true distribution (blue) over training iterations.</p>
</div>
</div>
</div>
<div id="fig-genai-1d-final" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-genai-1d-final-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/genai/1d_comparison/final_comparison.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-genai-1d-final-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16: Final comparison: All three models after convergence, showing their different biases in capturing the bimodal distribution.
</figcaption>
</figure>
</div>
</section>
<section id="d-distribution-comparison-1" class="level4" data-number="1.5.2">
<h4 data-number="1.5.2" class="anchored" data-anchor-id="d-distribution-comparison-1"><span class="header-section-number">1.5.2</span> 2D Distribution Comparison</h4>
<div id="fig-genai-2d-true" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-genai-2d-true-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/genai/2d_comparison/true_distribution.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-genai-2d-true-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17: True 2D data distribution: A complex spiral pattern in 2D space.
</figcaption>
</figure>
</div>
<div class="tabset-margin-container"></div><div id="fig-genai-2d-comparison" class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-2-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-1" role="tab" aria-controls="tabset-2-1" aria-selected="true" href="">GAN</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-2-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-2" role="tab" aria-controls="tabset-2-2" aria-selected="false" href="">VAE</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-2-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-3" role="tab" aria-controls="tabset-2-3" aria-selected="false" href="">Diffusion</a></li></ul>
<div id="fig-genai-2d-comparison" class="tab-content">
<div id="tabset-2-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-2-1-tab">
<div id="fig-genai-2d-example-gan" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-genai-2d-example-gan-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../assets/images/genai/2d_comparison/gan_training_progress.png" class="img-fluid figure-img"></p>
<figcaption>GAN</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-genai-2d-example-gan-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;18
</figcaption>
</figure>
</div>
</div>
<div id="tabset-2-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-2-2-tab">
<div id="fig-genai-2d-example-vae" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-genai-2d-example-vae-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../assets/images/genai/2d_comparison/vae_training_progress.png" class="img-fluid figure-img"></p>
<figcaption>VAE</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-genai-2d-example-vae-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;19
</figcaption>
</figure>
</div>
</div>
<div id="tabset-2-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-2-3-tab">
<div id="fig-genai-2d-example-gan-diffusion" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-genai-2d-example-gan-diffusion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../assets/images/genai/2d_comparison/diffusion_training_progress.png" class="img-fluid figure-img"></p>
<figcaption>Diffusion</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-genai-2d-example-gan-diffusion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;20
</figcaption>
</figure>
</div>
<p>Training progression on 2D spiral distribution. The models must learn a complex curved manifold.</p>
</div>
</div>
</div>
<div id="fig-genai-2d-final" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-genai-2d-final-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/genai/2d_comparison/final_comparison.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-genai-2d-final-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;21: Final 2D comparison: Converged distributions overlaid on the true spiral. Notice how each model type handles the complex geometry differently.
</figcaption>
</figure>
</div>
</section>
</section>
</section>
<section id="conditional-generation" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="conditional-generation"><span class="header-section-number">2</span> Conditional Generation</h2>
<p>So far, we have discussed <strong>unconditional generation</strong>: models that learn <span class="math inline">\(p(\mathbf{x})\)</span> or generate samples from the full training distribution. But many applications require <strong>conditional generation</strong>: producing outputs that match specific requirements or inputs.</p>
<p>Conditional generation models learn <span class="math inline">\(p(\mathbf{x}|\mathbf{c})\)</span>, where <span class="math inline">\(\mathbf{c}\)</span> is a conditioning signal:</p>
<ul>
<li><strong>Text prompt</strong>: “A photo of a cat wearing sunglasses”</li>
<li><strong>Class label</strong>: Generate a digit “7” or a face with “smiling” attribute</li>
<li><strong>Image</strong>: Translate sketches to photos, colorize grayscale images</li>
<li><strong>Other modalities</strong>: Depth maps, segmentation masks, audio, etc.</li>
</ul>
<p>The core modification is simple: provide the condition <span class="math inline">\(\mathbf{c}\)</span> as additional input to the model. However, the architectural integration varies by model family.</p>
<p>One example are conditional GANs. As shown in <a href="#fig-genai-gan-conditional" class="quarto-xref">Figure&nbsp;22</a>. In this case typically only the generator receives the conditioning information <span class="math inline">\(g(\mathbf{z}, \mathbf{c})\)</span> and <span class="math inline">\(d(\mathbf{x})\)</span>. The discriminator does not require <span class="math inline">\(\mathbf{c}\)</span> in this case because input-output consistency is enforced with other losses that compare input and output (e.g.&nbsp;<span class="math inline">\(L_2\)</span> norm).</p>
<div id="fig-genai-gan-conditional" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-genai-gan-conditional-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/genai/gan_conditional_generation.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-genai-gan-conditional-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;22: Conditional GAN architecture: Generator receives the conditioning input, enabling controlled generation. From <span class="citation" data-cites="prince_understanding_2023">Prince (<a href="#ref-prince_understanding_2023" role="doc-biblioref">2023</a>)</span>.
</figcaption>
</figure>
</div>
<p>Often, the discriminator needs to be conditioned on <span class="math inline">\(\mathbf{c}\)</span> as well. The discriminator thus must judge whether <span class="math inline">\(\mathbf{x}\)</span> matches the condition <span class="math inline">\(\mathbf{c}\)</span>, not just whether <span class="math inline">\(\mathbf{x}\)</span> looks realistic. This forces the generator to respect the conditioning signal to fool the discriminator.</p>
<p><strong>Training objective</strong>: <span class="math display">\[\begin{equation}
\min_{g} \max_{d} \mathbb{E}_{x, c}[\log d(\mathbf{x}, \mathbf{c})] + \mathbb{E}_{z, c}[\log(1 - d(g(\mathbf{z}, \mathbf{c}), \mathbf{c}))]
\end{equation}\]</span></p>
<p><strong>Common conditioning mechanisms</strong>:</p>
<ul>
<li><strong>Concatenation</strong>: Concatenate <span class="math inline">\(\mathbf{c}\)</span> with <span class="math inline">\(\mathbf{z}\)</span> for generator, with <span class="math inline">\(\mathbf{x}\)</span> for discriminator</li>
<li><strong>Class embeddings</strong>: For discrete labels, learn embedding vectors</li>
<li><strong>Cross-attention</strong>: For complex conditions (text, images), use attention mechanisms</li>
</ul>
<p>Beyond GANs, other model families support conditioning:</p>
<ul>
<li><strong>Conditional VAEs</strong>: Condition both encoder and decoder on <span class="math inline">\(c\)</span>, useful for disentangling style from content</li>
<li><strong>Autoregressive models</strong>: Condition each step on <span class="math inline">\(c\)</span>, e.g., PixelCNN for class-conditional generation</li>
<li><strong>ControlNet</strong>: Add spatial conditioning (edges, depth, pose) to pre-trained diffusion models without retraining from scratch</li>
</ul>
</section>
<section id="pre-trained-models" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="pre-trained-models"><span class="header-section-number">3</span> Pre-Trained Models</h2>
<p>Training generative models from scratch requires massive datasets (millions of images), significant compute resources (weeks on multiple GPUs), and careful hyperparameter tuning. For most practical applications, <strong>pre-trained models</strong> provide a much more accessible starting point.</p>
<p>The landscape has been transformed by models like <strong>Stable Diffusion</strong>, which bring state-of-the-art generation capabilities to consumer hardware. These models are trained on billions of image-text pairs and can generate high-quality, diverse images from text prompts—with minimal customization required for many use cases.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Pros and Cons of Pre-Trained Models">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Pros and Cons of Pre-Trained Models
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Pros</strong></p>
<ul>
<li>High-quality image generation out-of-the-box</li>
<li>Generate images via natural language prompts</li>
<li>Easy to customize or fine-tune with minimal investment (LoRA, DreamBooth)</li>
<li>Active community sharing models, techniques, and improvements</li>
</ul>
<p><strong>Cons</strong></p>
<ul>
<li>Not suitable if your data is from a highly specific domain (e.g., medical imaging, satellite imagery)</li>
<li>May not work well for very specific output constraints (e.g., exact branding requirements)</li>
<li>Biases from training data may propagate to outputs</li>
<li>Licensing considerations for commercial use</li>
</ul>
</div>
</div>
<section id="stable-diffusion-practical-state-of-the-art" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="stable-diffusion-practical-state-of-the-art"><span class="header-section-number">3.1</span> Stable Diffusion: Practical State-of-the-Art</h3>
<p><strong>Stable Diffusion</strong> <span class="citation" data-cites="rombach_high-resolution_2022">(<a href="#ref-rombach_high-resolution_2022" role="doc-biblioref">Rombach et al. 2022</a>)</span> is an open-source text-to-image model that has become the de facto standard for accessible, high-quality image generation. See <a href="#fig-genai-stable-diffusion-xl-examples" class="quarto-xref">Figure&nbsp;23</a> for some examples.</p>
<div id="fig-genai-stable-diffusion-xl-examples" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-genai-stable-diffusion-xl-examples-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/genai/pract_stable_diffusion_xl_examples.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-genai-stable-diffusion-xl-examples-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;23: Stable Diffusion XL: Example generations showing diverse styles and subjects from text prompts. <a href="https://huggingface.co/stabilityai/stable-diffusion-3.5-large">Source</a>.
</figcaption>
</figure>
</div>
<p>Unlike pure pixel-space diffusion (which is computationally expensive), Stable Diffusion uses <strong>latent diffusion</strong>—running the diffusion process in a compressed latent space.</p>
<div id="fig-genai-diffusion-sd-architecture" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-genai-diffusion-sd-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/genai/diffusion_sd_architecture_genbook.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-genai-diffusion-sd-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;24: Stable Diffusion pipeline: Text is encoded via CLIP, diffusion runs in VAE latent space (much smaller than pixel space), and the VAE decoder produces the final image. Adapted from GenBook.
</figcaption>
</figure>
</div>
</section>
<section id="why-latent-diffusion" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="why-latent-diffusion"><span class="header-section-number">3.2</span> Why Latent Diffusion?</h3>
<p>Running diffusion directly on high-resolution images (e.g., 512×512×3) is computationally expensive—the U-Net must process millions of values at each denoising step. <strong>Latent Diffusion</strong> solves this by:</p>
<ol type="1">
<li><strong>Pre-training a VAE</strong> to compress images into a lower-dimensional latent space (e.g., 64×64×4)</li>
<li><strong>Running diffusion in the latent space</strong> (much faster!)</li>
<li><strong>Decoding</strong> the final latent representation back to pixel space</li>
</ol>
<p>This provides an <strong>8× reduction</strong> in memory and computation, making high-resolution generation practical on consumer GPUs.</p>
</section>
<section id="conditioning-on-text-and-more" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="conditioning-on-text-and-more"><span class="header-section-number">3.3</span> Conditioning on Text (and More)</h3>
<p>Stable Diffusion supports <strong>text-conditional generation</strong> via cross-attention to CLIP text embeddings. The U-Net attends to text features at multiple scales, enabling precise control over generated content. Beyond text, the framework supports conditioning on:</p>
<ul>
<li><strong>Images</strong>: For inpainting, outpainting, and image-to-image translation</li>
<li><strong>Depth maps</strong>: For 3D-aware generation</li>
<li><strong>Segmentation masks</strong>: For spatial control</li>
<li><strong>Style embeddings</strong>: For artistic control</li>
</ul>
<div id="fig-genai-diffusion-latent-diff-architecture" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-genai-diffusion-latent-diff-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/genai/diffusion_latent_diff_architecture.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-genai-diffusion-latent-diff-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;25: Latent Diffusion architecture: The denoising U-Net operates in VAE latent space and uses cross-attention to condition on text (or other modalities). From <span class="citation" data-cites="rombach_high-resolution_2022">Rombach et al. (<a href="#ref-rombach_high-resolution_2022" role="doc-biblioref">2022</a>)</span>.
</figcaption>
</figure>
</div>
</section>
<section id="fine-tuning-with-lora" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="fine-tuning-with-lora"><span class="header-section-number">3.4</span> Fine-Tuning with LoRA</h3>
<p>While pre-trained models are powerful, you often want to adapt them to specific styles, subjects, or domains. Traditional fine-tuning updates all model parameters (billions of weights!), requiring massive memory and compute. <strong>LoRA (Low-Rank Adaptation)</strong>, see <a href="#fig-genai-lora-architecture" class="quarto-xref">Figure&nbsp;26</a>, offers a far more efficient alternative.</p>
<p><strong>Key idea</strong>: Instead of updating the full weight matrix <span class="math inline">\(W\)</span>, LoRA learns a low-rank update <span class="math inline">\(\Delta W = BA\)</span> where:</p>
<ul>
<li><span class="math inline">\(B \in \mathbb{R}^{d \times r}\)</span> and <span class="math inline">\(A \in \mathbb{R}^{r \times k}\)</span></li>
<li><span class="math inline">\(r \ll \min(d, k)\)</span> (typically <span class="math inline">\(r = 4\)</span> to <span class="math inline">\(16\)</span>)</li>
<li>Only <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are trained; original weights <span class="math inline">\(W\)</span> remain frozen</li>
</ul>
<p>This reduces trainable parameters by <strong>1000×</strong> or more, enabling fine-tuning on a single consumer GPU in hours rather than days. LoRA adapters can be easily shared, combined, and switched, creating a thriving ecosystem of specialized models.</p>
<p><strong>Popular use cases</strong>:</p>
<ul>
<li><strong>Style transfer</strong>: Train on artworks to mimic specific artistic styles</li>
<li><strong>Subject learning</strong>: DreamBooth + LoRA to generate images of specific people/objects</li>
<li><strong>Domain adaptation</strong>: Adapt to specialized domains (anime, architecture, product photography)</li>
</ul>
<div id="fig-genai-lora-architecture" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-genai-lora-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/genai/lora.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-genai-lora-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;26: LoRA (Low-Rank Adaptation): Instead of fine-tuning all weights <span class="math inline">\(W\)</span>, LoRA trains low-rank matrices <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> where <span class="math inline">\(\Delta W = BA\)</span>. This drastically reduces trainable parameters. From <span class="citation" data-cites="hu_lora_2021">Hu et al. (<a href="#ref-hu_lora_2021" role="doc-biblioref">2021</a>)</span>.
</figcaption>
</figure>
</div>
</section>
<section id="hardware-requirements" class="level3" data-number="3.5">
<h3 data-number="3.5" class="anchored" data-anchor-id="hardware-requirements"><span class="header-section-number">3.5</span> Hardware Requirements</h3>
<p>The computational demands vary significantly between inference and training:</p>
<section id="inference-image-generation" class="level4" data-number="3.5.1">
<h4 data-number="3.5.1" class="anchored" data-anchor-id="inference-image-generation"><span class="header-section-number">3.5.1</span> Inference (Image Generation)</h4>
<ul>
<li><strong>Minimum</strong>: 4-6 GB VRAM (GPU)
<ul>
<li>Stable Diffusion 1.5 at 512×512 resolution</li>
<li>Longer generation times, limited batch sizes</li>
</ul></li>
<li><strong>Recommended</strong>: 8-12 GB VRAM
<ul>
<li>Stable Diffusion XL at 1024×1024</li>
<li>Comfortable batch sizes, faster generation</li>
</ul></li>
<li><strong>High-end</strong>: 16+ GB VRAM
<ul>
<li>Multiple models loaded simultaneously</li>
<li>Video generation, 3D-aware models</li>
</ul></li>
</ul>
<p><strong>CPU-only</strong> inference is possible but <strong>extremely slow</strong> (minutes per image vs.&nbsp;seconds). Many optimization techniques exist: quantization, attention slicing, model offloading.</p>
</section>
<section id="fine-tuning-lora" class="level4" data-number="3.5.2">
<h4 data-number="3.5.2" class="anchored" data-anchor-id="fine-tuning-lora"><span class="header-section-number">3.5.2</span> Fine-Tuning (LoRA)</h4>
<ul>
<li><strong>Minimum</strong>: 12-16 GB VRAM
<ul>
<li>LoRA fine-tuning with small batch sizes</li>
<li>Gradient checkpointing, mixed precision required</li>
</ul></li>
<li><strong>Recommended</strong>: 24 GB VRAM
<ul>
<li>Comfortable batch sizes for faster convergence</li>
<li>Multiple LoRA ranks simultaneously</li>
</ul></li>
</ul>
</section>
<section id="full-training-from-scratch" class="level4" data-number="3.5.3">
<h4 data-number="3.5.3" class="anchored" data-anchor-id="full-training-from-scratch"><span class="header-section-number">3.5.3</span> Full Training (From Scratch)</h4>
<ul>
<li><strong>Minimum</strong>: Multiple A100 GPUs (40-80 GB each)</li>
<li><strong>Typical</strong>: Clusters with 100+ GPUs</li>
<li><strong>Duration</strong>: Days to weeks</li>
<li><strong>Cost</strong>: $10,000s to $100,000s</li>
</ul>
<p>For most researchers and practitioners, pre-trained models + LoRA fine-tuning offer the best trade-off.</p>
</section>
</section>
<section id="open-source-ecosystem" class="level3" data-number="3.6">
<h3 data-number="3.6" class="anchored" data-anchor-id="open-source-ecosystem"><span class="header-section-number">3.6</span> Open-Source Ecosystem</h3>
<p>The open-source community has created a rich ecosystem of tools, models, and resources:</p>
<section id="huggingface" class="level4" data-number="3.6.1">
<h4 data-number="3.6.1" class="anchored" data-anchor-id="huggingface"><span class="header-section-number">3.6.1</span> HuggingFace</h4>
<p><a href="https://huggingface.co/">HuggingFace</a> is the central hub for open-source generative models:</p>
<ul>
<li><p><strong>Diffusers library</strong>: Unified Python API for diffusion models with extensive documentation</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> diffusers <span class="im">import</span> StableDiffusionPipeline</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>pipe <span class="op">=</span> StableDiffusionPipeline.from_pretrained(<span class="st">"stabilityai/stable-diffusion-2-1"</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> pipe(<span class="st">"A photo of a cat wearing sunglasses"</span>).images[<span class="dv">0</span>]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p><strong>Model Hub</strong>: 100,000+ models including base models, fine-tunes, and LoRA adapters</p></li>
<li><p><strong>Spaces</strong>: Interactive demos for trying models before downloading</p></li>
<li><p><strong>Documentation</strong>: Comprehensive guides for inference, fine-tuning, and training</p></li>
</ul>
<p><strong>Best for</strong>: Programmatic access, research, production deployments, custom workflows</p>
</section>
<section id="civitai" class="level4" data-number="3.6.2">
<h4 data-number="3.6.2" class="anchored" data-anchor-id="civitai"><span class="header-section-number">3.6.2</span> CivitAI</h4>
<p><a href="https://civitai.com/">CivitAI</a> is a community-driven platform specializing in Stable Diffusion models:</p>
<ul>
<li><strong>Extensive model library</strong>: Thousands of community-created models, LoRAs, and embeddings</li>
<li><strong>Style-focused</strong>: Strong emphasis on artistic styles, anime, photorealism</li>
<li><strong>Preview images</strong>: See example outputs before downloading</li>
<li><strong>Version control</strong>: Track model updates and improvements</li>
</ul>
<p><strong>Best for</strong>: Exploring diverse styles, finding specialized models, community engagement</p>
</section>
<section id="automatic1111-webui" class="level4" data-number="3.6.3">
<h4 data-number="3.6.3" class="anchored" data-anchor-id="automatic1111-webui"><span class="header-section-number">3.6.3</span> Automatic1111 WebUI</h4>
<p><a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui">Automatic1111</a> provides a feature-rich web interface for local generation:</p>
<ul>
<li><strong>Local execution</strong>: Run models on your own hardware with full control</li>
<li><strong>Advanced features</strong>: Inpainting, img2img, ControlNet, depth guidance</li>
<li><strong>Extensions ecosystem</strong>: Hundreds of community plugins</li>
<li><strong>Batch processing</strong>: Generate and iterate efficiently</li>
</ul>
<p><strong>Best for</strong>: Interactive experimentation, full control over generation, no API costs</p>
</section>
</section>
<section id="closed-source-platforms" class="level3" data-number="3.7">
<h3 data-number="3.7" class="anchored" data-anchor-id="closed-source-platforms"><span class="header-section-number">3.7</span> Closed-Source Platforms</h3>
<p>Commercial platforms offer convenient APIs and frontier models:</p>
<section id="openai" class="level4" data-number="3.7.1">
<h4 data-number="3.7.1" class="anchored" data-anchor-id="openai"><span class="header-section-number">3.7.1</span> OpenAI</h4>
<ul>
<li><strong>Latest model</strong>: 4o Image with enhanced prompt following and coherence</li>
<li><strong>API access</strong>: Programmatic generation, inpainting, variations</li>
<li><strong>Pricing</strong>: Pay per image generated</li>
<li><strong>Strengths</strong>: Strong text understanding, safe outputs, reliable API</li>
</ul>
</section>
<section id="google-gemini-imagen" class="level4" data-number="3.7.2">
<h4 data-number="3.7.2" class="anchored" data-anchor-id="google-gemini-imagen"><span class="header-section-number">3.7.2</span> Google Gemini (Imagen)</h4>
<ul>
<li><strong>High photorealism</strong>: Particularly strong at realistic images</li>
<li><strong>Multimodal</strong>: Integration with Gemini’s broader capabilities</li>
<li><strong>Controlled access</strong>: Available through Google Cloud</li>
<li><strong>Strengths</strong>: Photo-quality outputs, factual accuracy</li>
</ul>
</section>
<section id="replicate" class="level4" data-number="3.7.3">
<h4 data-number="3.7.3" class="anchored" data-anchor-id="replicate"><span class="header-section-number">3.7.3</span> Replicate</h4>
<ul>
<li><strong>Hybrid platform</strong>: Both open- and closed-source models via unified API</li>
<li><strong>Pay-per-use</strong>: No infrastructure management, pay only for inference</li>
<li><strong>Host custom models</strong>: Deploy your own fine-tuned models</li>
<li><strong>Wide selection</strong>: SDXL, Midjourney alternatives, domain-specific models</li>
</ul>
</section>
<section id="midjourney" class="level4" data-number="3.7.4">
<h4 data-number="3.7.4" class="anchored" data-anchor-id="midjourney"><span class="header-section-number">3.7.4</span> Midjourney</h4>
<ul>
<li><strong>Artistic focus</strong>: Highly curated, aesthetic, and stylized outputs</li>
<li><strong>Discord interface</strong>: Community-driven prompt engineering</li>
<li><strong>No API</strong>: Interface only, no programmatic access</li>
<li><strong>Strengths</strong>: Artistic coherence, aesthetic quality, strong defaults</li>
</ul>
<p><strong>Choosing a platform</strong>:</p>
<ul>
<li><strong>For prototyping</strong>: HuggingFace Spaces or Replicate (fast, no setup)</li>
<li><strong>For production</strong>: HuggingFace Diffusers or OpenAI API (reliable, scalable)</li>
<li><strong>For experimentation</strong>: Automatic1111 (full control, no API costs)</li>
<li><strong>For art/design</strong>: Midjourney or CivitAI models (aesthetic quality)</li>
</ul>
</section>
</section>
</section>
<section id="conclusions-discussions" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="conclusions-discussions"><span class="header-section-number">4</span> Conclusions &amp; Discussions</h2>
<p>An interesting thought: What happens if models are trained (inadvertedly) on synhetic data? <span class="citation" data-cites="alemohammad_self-consuming_2023">Alemohammad et al. (<a href="#ref-alemohammad_self-consuming_2023" role="doc-biblioref">2023</a>)</span> studies this and found remarkable degradation, see <a href="#fig-genai-deterioration-on-pollution" class="quarto-xref">Figure&nbsp;27</a>.</p>
<div id="fig-genai-deterioration-on-pollution" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-genai-deterioration-on-pollution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/genai/deterioration_on_pollution.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-genai-deterioration-on-pollution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;27: Example: Deterioration of generated image quality under pollution or domain shift. From <span class="citation" data-cites="alemohammad_self-consuming_2023">Alemohammad et al. (<a href="#ref-alemohammad_self-consuming_2023" role="doc-biblioref">2023</a>)</span>.
</figcaption>
</figure>
</div>
<p>Furthermore, synthetic imagery has obvious consequences on all of us. The lines between real and synthetic are starting to blur and it becomes increasingly difficult to distinguish both. See the following quote with respect to <a href="#fig-genai-real-or-not-musk" class="quarto-xref">Figure&nbsp;28</a>.</p>
<blockquote class="blockquote">
<p>Zunächst hatte die Polizei … die Aktion habe nicht stattgefunden, das Bild davon sei ein »Fake«. … Auf Anfrage des SPIEGEL sagte ein Sprecher der Brandenburger Polizei: »Ob die Projektion tatsächlich so ablief, ist Teil dieser Ermittlungen. Derzeit können wir es jedenfalls nicht ausschließen.«</p>
</blockquote>
<div id="fig-genai-real-or-not-musk" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-genai-real-or-not-musk-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/genai/real_or_not_musk.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-genai-real-or-not-musk-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;28: “Real or Not?” <a href="https://www.spiegel.de/panorama/elon-musk-staatsschutz-ermittelt-nach-projektion-tesla-werk-in-gruenheide-a-8268048f-050b-4ae7-adf8-bb9cf572cf25?sara_ref=re-so-app-sh">Link</a>
</figcaption>
</figure>
</div>
<p>Furthermore, biases from the internet-scale data the models have been trained on seep into the models. An interesting article by the Washington post <a href="https://www.washingtonpost.com/technology/interactive/2024/ai-bias-beautiful-women-ugly-images/">Link</a> investigated this <a href="#fig-genai-ai-bias-wp-front" class="quarto-xref">Figure&nbsp;29</a>.</p>
<div id="fig-genai-ai-bias-wp-front" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-genai-ai-bias-wp-front-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/genai/ai_bias_wp_front.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-genai-ai-bias-wp-front-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29: AI bias: Example from a whitepaper front page. <a href="https://www.washingtonpost.com/technology/interactive/2024/ai-bias-beautiful-women-ugly-images/">Link</a>
</figcaption>
</figure>
</div>
<p>They found strong biases, for example, with respect to the depiction of <em>beautiful women</em>, see <a href="#fig-genai-ai-bias-wp-examples" class="quarto-xref">Figure&nbsp;30</a>.</p>
<div id="fig-genai-ai-bias-wp-examples" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-genai-ai-bias-wp-examples-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/genai/ai_bias_wp_examples.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-genai-ai-bias-wp-examples-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;30: AI bias: Additional examples. <a href="https://www.washingtonpost.com/technology/interactive/2024/ai-bias-beautiful-women-ugly-images/">Link</a>
</figcaption>
</figure>
</div>
</section>
<section id="references" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="references"><span class="header-section-number">5</span> References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-alemohammad_self-consuming_2023" class="csl-entry" role="listitem">
Alemohammad, Sina, Josue Casco-Rodriguez, Lorenzo Luzi, Ahmed Imtiaz Humayun, Hossein Babaei, Daniel LeJeune, Ali Siahkoohi, and Richard G. Baraniuk. 2023. <span>“Self-<span>Consuming</span> <span>Generative</span> <span>Models</span> <span>Go</span> <span>MAD</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2307.01850">https://doi.org/10.48550/arXiv.2307.01850</a>.
</div>
<div id="ref-bansal_cold_2022" class="csl-entry" role="listitem">
Bansal, Arpit, Eitan Borgnia, Hong-Min Chu, Jie S. Li, Hamid Kazemi, Furong Huang, Micah Goldblum, Jonas Geiping, and Tom Goldstein. 2022. <span>“Cold <span>Diffusion</span>: <span>Inverting</span> <span>Arbitrary</span> <span>Image</span> <span>Transforms</span> <span>Without</span> <span>Noise</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2208.09392">https://doi.org/10.48550/arXiv.2208.09392</a>.
</div>
<div id="ref-foster_generative_2023" class="csl-entry" role="listitem">
Foster, David. 2023. <em>Generative <span>Deep</span> <span>Learning</span></em>. 2nd ed. O’Reilly Media, Inc.
</div>
<div id="ref-goodfellow_generative_2014" class="csl-entry" role="listitem">
Goodfellow, Ian J., Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. <span>“Generative <span>Adversarial</span> <span>Networks</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.1406.2661">https://doi.org/10.48550/arXiv.1406.2661</a>.
</div>
<div id="ref-ho_denoising_2020" class="csl-entry" role="listitem">
Ho, Jonathan, Ajay Jain, and Pieter Abbeel. 2020. <span>“Denoising <span>Diffusion</span> <span>Probabilistic</span> <span>Models</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2006.11239">https://doi.org/10.48550/arXiv.2006.11239</a>.
</div>
<div id="ref-hu_lora_2021" class="csl-entry" role="listitem">
Hu, Edward J., Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. <span>“<span>LoRA</span>: <span>Low</span>-<span>Rank</span> <span>Adaptation</span> of <span>Large</span> <span>Language</span> <span>Models</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2106.09685">https://doi.org/10.48550/arXiv.2106.09685</a>.
</div>
<div id="ref-Kingma2014" class="csl-entry" role="listitem">
Kingma, Diederik P., and Max Welling. 2014. <span>“Auto-Encoding Variational Bayes.”</span> <em>2nd International Conference on Learning Representations, ICLR 2014 - Conference Track Proceedings</em>, no. Ml: 1–14.
</div>
<div id="ref-prince_understanding_2023" class="csl-entry" role="listitem">
Prince, Simon J. D. 2023. <em>Understanding <span>Deep</span> <span>Learning</span></em>. MIT Press. <a href="https://udlbook.github.io/udlbook/">https://udlbook.github.io/udlbook/</a>.
</div>
<div id="ref-rombach_high-resolution_2022" class="csl-entry" role="listitem">
Rombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. <span>“High-<span>Resolution</span> <span>Image</span> <span>Synthesis</span> with <span>Latent</span> <span>Diffusion</span> <span>Models</span>.”</span> arXiv. <a href="http://arxiv.org/abs/2112.10752">http://arxiv.org/abs/2112.10752</a>.
</div>
<div id="ref-sohl-dickstein_deep_2015" class="csl-entry" role="listitem">
Sohl-Dickstein, Jascha, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. 2015. <span>“Deep <span>Unsupervised</span> <span>Learning</span> Using <span>Nonequilibrium</span> <span>Thermodynamics</span>.”</span> arXiv. <a href="http://arxiv.org/abs/1503.03585">http://arxiv.org/abs/1503.03585</a>.
</div>
<div id="ref-torralba_foundations_2024" class="csl-entry" role="listitem">
Torralba, A., P. Isola, and W. T. Freeman. 2024. <em>Foundations of <span>Computer</span> <span>Vision</span></em>. Adaptive <span>Computation</span> and <span>Machine</span> <span>Learning</span> Series. MIT Press. <a href="https://mitpress.mit.edu/9780262048972/foundations-of-computer-vision/">https://mitpress.mit.edu/9780262048972/foundations-of-computer-vision/</a>.
</div>
</div>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/marco-willi\.github\.io\/cas-dl-compvis-lectures-hs2025\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../pages/lectures/intro_genai.html" class="pagination-link" aria-label="7 - Intro to Generative AI">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">7 - Intro to Generative AI</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../pages/slides/intro.html" class="pagination-link" aria-label="Introduction">
        <span class="nav-page-text">Introduction</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "8 - Generative Models "</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="an">params:</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co">   images_path: "/assets/images/genai/"</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>::: {.content-hidden}</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>{{&lt; include /assets/_macros.tex &gt;}}</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title="Learning Objectives"  collapse="true"}</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>After this lecture you should be able to:</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Explain the core mechanisms of major generative model families: autoregressive, diffusion, GANs, and VAEs</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Compare tradeoffs between these approaches (sampling speed, quality, coverage, likelihood computation)</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Understand how conditional generation extends models to accept additional inputs (text, images, class labels)</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Utilize pre-trained models (Stable Diffusion) and efficient fine-tuning techniques (LoRA) for practical applications</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title="TLDR Recap" collapse="true"}</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>Four major generative model families make different tradeoffs: **Autoregressive** (exact likelihood, slow sequential sampling), **Diffusion** (SOTA quality, iterative denoising), **GANs** (fast generation, adversarial training, mode collapse risk), **VAEs** (structured latent space, approximate likelihood). **Conditional generation** extends models to accept text, images, or labels as input. **Stable Diffusion** runs diffusion in VAE latent space for efficiency, enabling practical high-resolution generation. **LoRA** fine-tuning adapts pre-trained models with minimal compute. Societal considerations include synthetic data pollution, authenticity challenges, and embedded biases.</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a><span class="fu">## Modelling Approaches</span></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>Over the past decade, several families of generative models have emerged, each with distinct philosophies and trade-offs. We introduce a few of the most important ones in the following sections, however, note that this list is not exhaustive.</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a><span class="fu">### Autoregressive Models</span></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>Autoregressive models generate data sequentially, predicting one element at a time conditioned on all previous elements. This sequential factorization turns the complex problem of modeling high-dimensional distributions into a series of simpler conditional distributions.</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>::: {#fig-genai-autoregressive-overview}</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}autoregressive_overview.png)</span></span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>Autoregressive generation: Models predict each element sequentially, conditioning on all previously generated elements. Figure from @torralba_foundations_2024.</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>For an image with pixels $x_1, x_2, \ldots, x_n$, the joint distribution factors as:</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>p(x_1, x_2, \ldots, x_n) = p(x_1) \cdot p(x_2|x_1) \cdot p(x_3|x_1,x_2) \cdots p(x_n|x_1,\ldots,x_{n-1})</span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>Each conditional $p(x_i|x_1,\ldots,x_{i-1})$ is modeled by a neural network (typically a masked CNN or Transformer). During generation, we sample from these conditionals sequentially:</span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Sample $x_1 \sim p(x_1)$</span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Sample $x_2 \sim p(x_2|x_1)$</span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Sample $x_3 \sim p(x_3|x_1,x_2)$</span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Continue until all pixels generated</span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>**Key advantage**: Exact likelihood computation—we can evaluate $p(x)$ for any sample.</span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a>**Key limitation**: Sequential generation is slow—cannot parallelize pixel prediction.</span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a><span class="fu">### Diffusion Models</span></span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a>Diffusion models @ho_denoising_2020 @sohl-dickstein_deep_2015 have emerged as the state-of-the-art approach for high-quality image generation, powering systems like Stable Diffusion. The core insight is remarkably simple: learn to reverse a gradual noising process.</span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a>@fig-genai-diffusion-forward-fcv shows the **forward diffusion** process, whereby Gaussian noise is gradually added to an image (from left to right).</span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a>::: {#fig-genai-diffusion-forward-fcv}</span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}forward_diffusion_fcv.png)</span></span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a>Forward diffusion process. From @torralba_foundations_2024.</span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a>The Gaussian noise is added to data over $T$ steps (typically 1000), following a pre-specified noise schedule. This is a deterministic process, thus no neural network is involved.</span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a>The forward process at step $t$ is defined by:</span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb2-83"><a href="#cb2-83" aria-hidden="true" tabindex="-1"></a>z_t = \sqrt{1-\beta_t} z_{t-1} + \sqrt{\beta_t} \epsilon_t, \quad \epsilon_t \sim \mathcal{N}(0, I)</span>
<span id="cb2-84"><a href="#cb2-84" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb2-85"><a href="#cb2-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-86"><a href="#cb2-86" aria-hidden="true" tabindex="-1"></a>where $\beta_t$ controls the noise level (**the noise schedule**). This process gradually degrades the data $z_0 = x$ into pure noise $z_T \approx \mathcal{N}(0, I)$ over $T$ steps. Common noise schedules (linear, cosine) start with small $\beta_t$ values and gradually increase, ensuring smooth degradation from data to noise.</span>
<span id="cb2-87"><a href="#cb2-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-88"><a href="#cb2-88" aria-hidden="true" tabindex="-1"></a>A crucial mathematical property: we can **directly jump to any timestep** $z_t$ without iterating through all intermediate steps. This enables efficient training, where we randomly sample timesteps and train the denoiser on all noise levels simultaneously.</span>
<span id="cb2-89"><a href="#cb2-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-90"><a href="#cb2-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-91"><a href="#cb2-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-92"><a href="#cb2-92" aria-hidden="true" tabindex="-1"></a>The core idea is to train a neural network to model  the **reverse diffusion** process, thus going in the opposite direction and denoise an image. @fig-genai-diffusion-reverse-fcv shows the idea.</span>
<span id="cb2-93"><a href="#cb2-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-94"><a href="#cb2-94" aria-hidden="true" tabindex="-1"></a>::: {#fig-genai-diffusion-reverse-fcv}</span>
<span id="cb2-95"><a href="#cb2-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-96"><a href="#cb2-96" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}reverse_diffusion_fcv.png)</span></span>
<span id="cb2-97"><a href="#cb2-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-98"><a href="#cb2-98" aria-hidden="true" tabindex="-1"></a>Reverse diffusion process visualization. From @torralba_foundations_2024.</span>
<span id="cb2-99"><a href="#cb2-99" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-100"><a href="#cb2-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-101"><a href="#cb2-101" aria-hidden="true" tabindex="-1"></a>A neural network learns to gradually remove noise, step by step. Starting from pure noise $z_T \sim \mathcal{N}(0, I)$, the model denoises iteratively to produce realistic samples.</span>
<span id="cb2-102"><a href="#cb2-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-103"><a href="#cb2-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-104"><a href="#cb2-104" aria-hidden="true" tabindex="-1"></a>The neural network learns to predict the noise $\epsilon_t$ that was added at each timestep. Crucially, the network is **conditioned on the timestep** $t$, since the noise level varies dramatically from early (almost clean) to late (almost pure noise) steps. The training objective is simple:</span>
<span id="cb2-105"><a href="#cb2-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-106"><a href="#cb2-106" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb2-107"><a href="#cb2-107" aria-hidden="true" tabindex="-1"></a>\mathcal{L} = \mathbb{E}_{t, x, \epsilon} \| \epsilon - f_\theta(z_t, t) <span class="sc">\|</span>^2</span>
<span id="cb2-108"><a href="#cb2-108" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb2-109"><a href="#cb2-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-110"><a href="#cb2-110" aria-hidden="true" tabindex="-1"></a>Train the network to predict the noise $\epsilon$ that was added to create $z_t$ from $x$.</span>
<span id="cb2-111"><a href="#cb2-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-112"><a href="#cb2-112" aria-hidden="true" tabindex="-1"></a>::: {#fig-genai-diffusion-algorithm}</span>
<span id="cb2-113"><a href="#cb2-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-114"><a href="#cb2-114" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}diffusion_algorithm.png)</span></span>
<span id="cb2-115"><a href="#cb2-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-116"><a href="#cb2-116" aria-hidden="true" tabindex="-1"></a>Training and sampling algorithms: Training samples random timesteps and noise; sampling iteratively denoises from $T$ to 0. Guidance techniques (not shown) can further improve sample quality.</span>
<span id="cb2-117"><a href="#cb2-117" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-118"><a href="#cb2-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-119"><a href="#cb2-119" aria-hidden="true" tabindex="-1"></a>The denoising network typically uses a **U-Net architecture**—a convolutional encoder-decoder with skip connections. The U-Net processes noisy images at multiple resolutions, with timestep $t$ embedded and injected throughout the network.</span>
<span id="cb2-120"><a href="#cb2-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-121"><a href="#cb2-121" aria-hidden="true" tabindex="-1"></a>::: {#fig-genai-diffusion-unet}</span>
<span id="cb2-122"><a href="#cb2-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-123"><a href="#cb2-123" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}diffusion_unet.png)</span></span>
<span id="cb2-124"><a href="#cb2-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-125"><a href="#cb2-125" aria-hidden="true" tabindex="-1"></a>U-Net architecture for diffusion models: Downsampling path (left) captures context, upsampling path (right) enables precise localization, and skip connections (horizontal) preserve spatial details. Timestep embeddings condition the network on noise level. From @prince_understanding_2023.</span>
<span id="cb2-126"><a href="#cb2-126" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-127"><a href="#cb2-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-128"><a href="#cb2-128" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Pros and Cons of Diffusion Models"}</span>
<span id="cb2-129"><a href="#cb2-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-130"><a href="#cb2-130" aria-hidden="true" tabindex="-1"></a>**Pros**</span>
<span id="cb2-131"><a href="#cb2-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-132"><a href="#cb2-132" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>High-quality, diverse outputs</span>
<span id="cb2-133"><a href="#cb2-133" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Stable and reliable training</span>
<span id="cb2-134"><a href="#cb2-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-135"><a href="#cb2-135" aria-hidden="true" tabindex="-1"></a>**Cons**</span>
<span id="cb2-136"><a href="#cb2-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-137"><a href="#cb2-137" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Slow inference time</span>
<span id="cb2-138"><a href="#cb2-138" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>High compute (training + inference) cost</span>
<span id="cb2-139"><a href="#cb2-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-140"><a href="#cb2-140" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-141"><a href="#cb2-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-142"><a href="#cb2-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-143"><a href="#cb2-143" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Interesting Note on the Noise" collapse="true"}</span>
<span id="cb2-144"><a href="#cb2-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-145"><a href="#cb2-145" aria-hidden="true" tabindex="-1"></a>While the standard diffusion framework has proven highly successful, researchers have explored numerous extensions and modifications:</span>
<span id="cb2-146"><a href="#cb2-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-147"><a href="#cb2-147" aria-hidden="true" tabindex="-1"></a>**Cold Diffusion** <span class="co">[</span><span class="ot">@bansal_cold_2022</span><span class="co">]</span> demonstrates that the Gaussian noise assumption is not fundamental to diffusion models. The key insight: any degradation process that can be inverted works. Instead of adding Gaussian noise, Cold Diffusion applies transformations like blurring, pixelation, or masking—yet still learns to reverse them through the same denoising framework.</span>
<span id="cb2-148"><a href="#cb2-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-149"><a href="#cb2-149" aria-hidden="true" tabindex="-1"></a>::: {#fig-genai-cold-diffusion}</span>
<span id="cb2-150"><a href="#cb2-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-151"><a href="#cb2-151" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}cold_diffusion.png)</span></span>
<span id="cb2-152"><a href="#cb2-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-153"><a href="#cb2-153" aria-hidden="true" tabindex="-1"></a>Cold Diffusion: Various degradation operators (blur, downsampling, masking) can replace Gaussian noise in the diffusion framework. The model learns to reverse these transformations through the same training procedure. From @bansal_cold_2022.</span>
<span id="cb2-154"><a href="#cb2-154" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-155"><a href="#cb2-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-156"><a href="#cb2-156" aria-hidden="true" tabindex="-1"></a>This generalization reveals that diffusion's success stems from the gradual, multi-step inversion strategy rather than the specific choice of Gaussian noise. The framework adapts to diverse degradation operators, opening possibilities for task-specific degradations that may better suit particular domains or applications.</span>
<span id="cb2-157"><a href="#cb2-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-158"><a href="#cb2-158" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-159"><a href="#cb2-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-160"><a href="#cb2-160" aria-hidden="true" tabindex="-1"></a><span class="fu">### Generative Adversarial Networks</span></span>
<span id="cb2-161"><a href="#cb2-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-162"><a href="#cb2-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-163"><a href="#cb2-163" aria-hidden="true" tabindex="-1"></a>GANs, see @goodfellow_generative_2014, frame generative modeling as an **adversarial game** between two neural networks: a **generator** $g(\vect{z})$ that creates fake samples, and a **discriminator** $d(\vect{x})$ that tries to distinguish real data from generated fakes (see @fig-genai-gan-idea). This adversarial setup drives both networks to improve: the generator learns to create increasingly realistic samples, while the discriminator becomes better at spotting fakes.</span>
<span id="cb2-164"><a href="#cb2-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-165"><a href="#cb2-165" aria-hidden="true" tabindex="-1"></a>::: {#fig-genai-gan-idea}</span>
<span id="cb2-166"><a href="#cb2-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-167"><a href="#cb2-167" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}gan-architecture.jpg)</span></span>
<span id="cb2-168"><a href="#cb2-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-169"><a href="#cb2-169" aria-hidden="true" tabindex="-1"></a>GAN idea: The generator creates fake samples from random noise $z \sim \mathcal{N}(0, I)$, while the discriminator judges whether samples are real or fake. From <span class="co">[</span><span class="ot">Link</span><span class="co">](https://microsoft.github.io/GenStudio/)</span></span>
<span id="cb2-170"><a href="#cb2-170" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-171"><a href="#cb2-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-172"><a href="#cb2-172" aria-hidden="true" tabindex="-1"></a>The GAN training objective can be expressed as a minimax game:</span>
<span id="cb2-173"><a href="#cb2-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-174"><a href="#cb2-174" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-175"><a href="#cb2-175" aria-hidden="true" tabindex="-1"></a>\min_{g} \max_{d} \mathbb{E}_{\vect{x} \sim p_{data}}[\log d(\vect{x})] + \mathbb{E}_{\vect{z} \sim p_z}<span class="co">[</span><span class="ot">\log(1 - d(g(\vect{z})))</span><span class="co">]</span></span>
<span id="cb2-176"><a href="#cb2-176" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-177"><a href="#cb2-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-178"><a href="#cb2-178" aria-hidden="true" tabindex="-1"></a>The discriminator $d$ maximizes its ability to correctly classify real ($d(\vect{x}) \approx 1$) vs. fake ($d(g(\vect{z})) \approx 0$) samples. The generator $G$ minimizes the discriminator's success, trying to produce samples where $d(g(\vect{z})) \approx 1$ (fooling the discriminator). See @fig-genai-gan-learning for an illustration.</span>
<span id="cb2-179"><a href="#cb2-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-180"><a href="#cb2-180" aria-hidden="true" tabindex="-1"></a>In practice, training alternates between:</span>
<span id="cb2-181"><a href="#cb2-181" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Discriminator step**: Update $d$ to better distinguish real from fake</span>
<span id="cb2-182"><a href="#cb2-182" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Generator step**: Update $g$ to better fool the current discriminator</span>
<span id="cb2-183"><a href="#cb2-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-184"><a href="#cb2-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-185"><a href="#cb2-185" aria-hidden="true" tabindex="-1"></a>::: {#fig-genai-gan-learning}</span>
<span id="cb2-186"><a href="#cb2-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-187"><a href="#cb2-187" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}gan_learning.png)</span></span>
<span id="cb2-188"><a href="#cb2-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-189"><a href="#cb2-189" aria-hidden="true" tabindex="-1"></a>GAN learning process: As training progresses, the generator's distribution (green) moves closer to the real data distribution (black dots). The discriminator's decision boundary (blue) adapts accordingly. From @foster_generative_2023</span>
<span id="cb2-190"><a href="#cb2-190" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-191"><a href="#cb2-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-192"><a href="#cb2-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-193"><a href="#cb2-193" aria-hidden="true" tabindex="-1"></a><span class="fu">### Variational Autoencoders</span></span>
<span id="cb2-194"><a href="#cb2-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-195"><a href="#cb2-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-196"><a href="#cb2-196" aria-hidden="true" tabindex="-1"></a>Variational Autoencoders (VAEs) <span class="co">[</span><span class="ot">@Kingma2014</span><span class="co">]</span> take a different approach to generative modeling, building on the autoencoder architecture (see @fig-genai-vae-illustration) but with a crucial probabilistic twist.</span>
<span id="cb2-197"><a href="#cb2-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-198"><a href="#cb2-198" aria-hidden="true" tabindex="-1"></a>::: {#fig-genai-vae-illustration}</span>
<span id="cb2-199"><a href="#cb2-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-200"><a href="#cb2-200" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}vae_illustration.png)</span></span>
<span id="cb2-201"><a href="#cb2-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-202"><a href="#cb2-202" aria-hidden="true" tabindex="-1"></a>VAE architecture: The encoder maps inputs to latent distributions $q(z|x)$, while the decoder reconstructs from latent samples. From <span class="co">[</span><span class="ot">Source</span><span class="co">](https://lilianweng.github.io/posts/2018-08-12-vae/)</span>.</span>
<span id="cb2-203"><a href="#cb2-203" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-204"><a href="#cb2-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-205"><a href="#cb2-205" aria-hidden="true" tabindex="-1"></a>A VAE learns two networks that perform (approximate) inverse operations:</span>
<span id="cb2-206"><a href="#cb2-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-207"><a href="#cb2-207" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Encoder** $q_{\psi}(z|x)$: Maps data $x$ to a distribution over latent codes $z$</span>
<span id="cb2-208"><a href="#cb2-208" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Decoder** $p_{\theta}(x|z)$: Maps latent codes $z$ back to data $x$</span>
<span id="cb2-209"><a href="#cb2-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-210"><a href="#cb2-210" aria-hidden="true" tabindex="-1"></a>Unlike standard autoencoders, VAEs enforce structure on the latent space through probabilistic modeling (see @fig-genai-vae-illustration2), ensuring that randomly sampled latent codes produce meaningful outputs.</span>
<span id="cb2-211"><a href="#cb2-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-212"><a href="#cb2-212" aria-hidden="true" tabindex="-1"></a>::: {#fig-genai-vae-illustration2}</span>
<span id="cb2-213"><a href="#cb2-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-214"><a href="#cb2-214" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}vae_illustration2.png)</span></span>
<span id="cb2-215"><a href="#cb2-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-216"><a href="#cb2-216" aria-hidden="true" tabindex="-1"></a>VAE computational graph: The encoder outputs mean $\mu$ and variance $\sigma^2$ for each latent dimension. The **reparameterization trick** ($z = \mu + \epsilon \cdot \sigma$, where $\epsilon \sim \mathcal{N}(0, 1)$) enables gradient-based optimization. From <span class="co">[</span><span class="ot">Source</span><span class="co">](https://lilianweng.github.io/posts/2018-08-12-vae/)</span>.</span>
<span id="cb2-217"><a href="#cb2-217" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-218"><a href="#cb2-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-219"><a href="#cb2-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-220"><a href="#cb2-220" aria-hidden="true" tabindex="-1"></a>VAEs optimize the **Evidence Lower Bound (ELBO)**, which consists of two terms:</span>
<span id="cb2-221"><a href="#cb2-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-222"><a href="#cb2-222" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-223"><a href="#cb2-223" aria-hidden="true" tabindex="-1"></a>\text{ELBO} = \underbrace{\mathbb{E}_{z \sim q_{\psi}(z|x)}[\log p_{\theta}(x|z)]}_{\text{Reconstruction}} - \underbrace{\text{KL}(q_{\psi}(z|x) \| p(z))}_{\text{Regularization}}</span>
<span id="cb2-224"><a href="#cb2-224" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb2-225"><a href="#cb2-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-226"><a href="#cb2-226" aria-hidden="true" tabindex="-1"></a>The **reconstruction term** ensures the decoder can recover the input from the latent code. The **KL regularization term** (typically with prior $p(z) = \mathcal{N}(0, I)$) prevents the latent space from developing "gaps and tendrils"—ensuring that random samples from $\mathcal{N}(0, I)$ decode to meaningful outputs.</span>
<span id="cb2-227"><a href="#cb2-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-228"><a href="#cb2-228" aria-hidden="true" tabindex="-1"></a>This balance is key to VAE behavior: the reconstruction loss wants to use the latent space flexibly, while KL regularization pushes latent codes toward a standard normal distribution. The result is a **structured, continuous latent space** where interpolation and attribute manipulation work smoothly.</span>
<span id="cb2-229"><a href="#cb2-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-230"><a href="#cb2-230" aria-hidden="true" tabindex="-1"></a>::: {#fig-genai-vae-latent-space}</span>
<span id="cb2-231"><a href="#cb2-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-232"><a href="#cb2-232" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}vae_latent_space_face.png)</span></span>
<span id="cb2-233"><a href="#cb2-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-234"><a href="#cb2-234" aria-hidden="true" tabindex="-1"></a>VAE latent space: Traversing the latent space produces smooth interpolations between faces, demonstrating the structured representation learned by the model. From @prince_understanding_2023.</span>
<span id="cb2-235"><a href="#cb2-235" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-236"><a href="#cb2-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-237"><a href="#cb2-237" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Pros and Cons of VAEs"}</span>
<span id="cb2-238"><a href="#cb2-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-239"><a href="#cb2-239" aria-hidden="true" tabindex="-1"></a>**Pros**</span>
<span id="cb2-240"><a href="#cb2-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-241"><a href="#cb2-241" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Principled probabilistic foundation</span>
<span id="cb2-242"><a href="#cb2-242" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Latent space structure: can interpolate in latent space and interpret factors</span>
<span id="cb2-243"><a href="#cb2-243" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Stable training</span>
<span id="cb2-244"><a href="#cb2-244" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Can be used for representation learning</span>
<span id="cb2-245"><a href="#cb2-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-246"><a href="#cb2-246" aria-hidden="true" tabindex="-1"></a>**Cons**</span>
<span id="cb2-247"><a href="#cb2-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-248"><a href="#cb2-248" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Blurry outputs (more complex variants can reduce this)</span>
<span id="cb2-249"><a href="#cb2-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-250"><a href="#cb2-250" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-251"><a href="#cb2-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-252"><a href="#cb2-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-253"><a href="#cb2-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-254"><a href="#cb2-254" aria-hidden="true" tabindex="-1"></a><span class="fu">### Comparing GANs, VAEs, and Diffusion Models</span></span>
<span id="cb2-255"><a href="#cb2-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-256"><a href="#cb2-256" aria-hidden="true" tabindex="-1"></a>Having explored each model family, how do they actually behave differently? Let's examine their behavior on simple 1D and 2D distributions to build intuition about their strengths and failure modes.</span>
<span id="cb2-257"><a href="#cb2-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-258"><a href="#cb2-258" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 1D Distribution Comparison</span></span>
<span id="cb2-259"><a href="#cb2-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-260"><a href="#cb2-260" aria-hidden="true" tabindex="-1"></a>::: {#fig-genai-1d-true}</span>
<span id="cb2-261"><a href="#cb2-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-262"><a href="#cb2-262" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}1d_comparison/true_distribution.png)</span></span>
<span id="cb2-263"><a href="#cb2-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-264"><a href="#cb2-264" aria-hidden="true" tabindex="-1"></a>True 1D data distribution: A bimodal distribution that the generative models must learn to reproduce.</span>
<span id="cb2-265"><a href="#cb2-265" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-266"><a href="#cb2-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-267"><a href="#cb2-267" aria-hidden="true" tabindex="-1"></a>::: {.panel-tabset #fig-genai-1d-example}</span>
<span id="cb2-268"><a href="#cb2-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-269"><a href="#cb2-269" aria-hidden="true" tabindex="-1"></a><span class="fu">### GAN</span></span>
<span id="cb2-270"><a href="#cb2-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-271"><a href="#cb2-271" aria-hidden="true" tabindex="-1"></a>::: {#fig-genai-1d-comparison}</span>
<span id="cb2-272"><a href="#cb2-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-273"><a href="#cb2-273" aria-hidden="true" tabindex="-1"></a><span class="al">![GAN]({{&lt; meta params.images_path &gt;}}1d_comparison/gan_training_progress.png)</span></span>
<span id="cb2-274"><a href="#cb2-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-275"><a href="#cb2-275" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-276"><a href="#cb2-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-277"><a href="#cb2-277" aria-hidden="true" tabindex="-1"></a><span class="fu">### VAE</span></span>
<span id="cb2-278"><a href="#cb2-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-279"><a href="#cb2-279" aria-hidden="true" tabindex="-1"></a>::: {#fig-genai-1d-example-vae}</span>
<span id="cb2-280"><a href="#cb2-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-281"><a href="#cb2-281" aria-hidden="true" tabindex="-1"></a><span class="al">![VAE]({{&lt; meta params.images_path &gt;}}1d_comparison/vae_training_progress.png)</span></span>
<span id="cb2-282"><a href="#cb2-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-283"><a href="#cb2-283" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-284"><a href="#cb2-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-285"><a href="#cb2-285" aria-hidden="true" tabindex="-1"></a><span class="fu">### Diffusion</span></span>
<span id="cb2-286"><a href="#cb2-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-287"><a href="#cb2-287" aria-hidden="true" tabindex="-1"></a>::: {#fig-genai-1d-example-gan-diffusion}</span>
<span id="cb2-288"><a href="#cb2-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-289"><a href="#cb2-289" aria-hidden="true" tabindex="-1"></a><span class="al">![Diffusion]({{&lt; meta params.images_path &gt;}}1d_comparison/diffusion_training_progress.png)</span></span>
<span id="cb2-290"><a href="#cb2-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-291"><a href="#cb2-291" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-292"><a href="#cb2-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-293"><a href="#cb2-293" aria-hidden="true" tabindex="-1"></a>Training progression on 1D bimodal distribution. Each panel shows how the learned distribution (orange) approaches the true distribution (blue) over training iterations.</span>
<span id="cb2-294"><a href="#cb2-294" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-295"><a href="#cb2-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-296"><a href="#cb2-296" aria-hidden="true" tabindex="-1"></a>::: {#fig-genai-1d-final}</span>
<span id="cb2-297"><a href="#cb2-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-298"><a href="#cb2-298" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}1d_comparison/final_comparison.png)</span></span>
<span id="cb2-299"><a href="#cb2-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-300"><a href="#cb2-300" aria-hidden="true" tabindex="-1"></a>Final comparison: All three models after convergence, showing their different biases in capturing the bimodal distribution.</span>
<span id="cb2-301"><a href="#cb2-301" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-302"><a href="#cb2-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-303"><a href="#cb2-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-304"><a href="#cb2-304" aria-hidden="true" tabindex="-1"></a><span class="fu">#### 2D Distribution Comparison</span></span>
<span id="cb2-305"><a href="#cb2-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-306"><a href="#cb2-306" aria-hidden="true" tabindex="-1"></a>::: {#fig-genai-2d-true}</span>
<span id="cb2-307"><a href="#cb2-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-308"><a href="#cb2-308" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}2d_comparison/true_distribution.png)</span></span>
<span id="cb2-309"><a href="#cb2-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-310"><a href="#cb2-310" aria-hidden="true" tabindex="-1"></a>True 2D data distribution: A complex spiral pattern in 2D space.</span>
<span id="cb2-311"><a href="#cb2-311" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-312"><a href="#cb2-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-313"><a href="#cb2-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-314"><a href="#cb2-314" aria-hidden="true" tabindex="-1"></a>::: {.panel-tabset #fig-genai-2d-comparison}</span>
<span id="cb2-315"><a href="#cb2-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-316"><a href="#cb2-316" aria-hidden="true" tabindex="-1"></a><span class="fu">### GAN</span></span>
<span id="cb2-317"><a href="#cb2-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-318"><a href="#cb2-318" aria-hidden="true" tabindex="-1"></a>::: {#fig-genai-2d-example-gan}</span>
<span id="cb2-319"><a href="#cb2-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-320"><a href="#cb2-320" aria-hidden="true" tabindex="-1"></a><span class="al">![GAN]({{&lt; meta params.images_path &gt;}}2d_comparison/gan_training_progress.png)</span></span>
<span id="cb2-321"><a href="#cb2-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-322"><a href="#cb2-322" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-323"><a href="#cb2-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-324"><a href="#cb2-324" aria-hidden="true" tabindex="-1"></a><span class="fu">### VAE</span></span>
<span id="cb2-325"><a href="#cb2-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-326"><a href="#cb2-326" aria-hidden="true" tabindex="-1"></a>::: {#fig-genai-2d-example-vae}</span>
<span id="cb2-327"><a href="#cb2-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-328"><a href="#cb2-328" aria-hidden="true" tabindex="-1"></a><span class="al">![VAE]({{&lt; meta params.images_path &gt;}}2d_comparison/vae_training_progress.png)</span></span>
<span id="cb2-329"><a href="#cb2-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-330"><a href="#cb2-330" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-331"><a href="#cb2-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-332"><a href="#cb2-332" aria-hidden="true" tabindex="-1"></a><span class="fu">### Diffusion</span></span>
<span id="cb2-333"><a href="#cb2-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-334"><a href="#cb2-334" aria-hidden="true" tabindex="-1"></a>::: {#fig-genai-2d-example-gan-diffusion}</span>
<span id="cb2-335"><a href="#cb2-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-336"><a href="#cb2-336" aria-hidden="true" tabindex="-1"></a><span class="al">![Diffusion]({{&lt; meta params.images_path &gt;}}2d_comparison/diffusion_training_progress.png)</span></span>
<span id="cb2-337"><a href="#cb2-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-338"><a href="#cb2-338" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-339"><a href="#cb2-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-340"><a href="#cb2-340" aria-hidden="true" tabindex="-1"></a>Training progression on 2D spiral distribution. The models must learn a complex curved manifold.</span>
<span id="cb2-341"><a href="#cb2-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-342"><a href="#cb2-342" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-343"><a href="#cb2-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-344"><a href="#cb2-344" aria-hidden="true" tabindex="-1"></a>::: {#fig-genai-2d-final}</span>
<span id="cb2-345"><a href="#cb2-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-346"><a href="#cb2-346" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}2d_comparison/final_comparison.png)</span></span>
<span id="cb2-347"><a href="#cb2-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-348"><a href="#cb2-348" aria-hidden="true" tabindex="-1"></a>Final 2D comparison: Converged distributions overlaid on the true spiral. Notice how each model type handles the complex geometry differently.</span>
<span id="cb2-349"><a href="#cb2-349" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-350"><a href="#cb2-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-351"><a href="#cb2-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-352"><a href="#cb2-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-353"><a href="#cb2-353" aria-hidden="true" tabindex="-1"></a><span class="fu">## Conditional Generation</span></span>
<span id="cb2-354"><a href="#cb2-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-355"><a href="#cb2-355" aria-hidden="true" tabindex="-1"></a>So far, we have discussed **unconditional generation**: models that learn $p(\vect{x})$ or generate samples from the full training distribution. But many applications require **conditional generation**: producing outputs that match specific requirements or inputs.</span>
<span id="cb2-356"><a href="#cb2-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-357"><a href="#cb2-357" aria-hidden="true" tabindex="-1"></a>Conditional generation models learn $p(\vect{x}|\vect{c})$, where $\vect{c}$ is a conditioning signal:</span>
<span id="cb2-358"><a href="#cb2-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-359"><a href="#cb2-359" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Text prompt**: "A photo of a cat wearing sunglasses"</span>
<span id="cb2-360"><a href="#cb2-360" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Class label**: Generate a digit "7" or a face with "smiling" attribute</span>
<span id="cb2-361"><a href="#cb2-361" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Image**: Translate sketches to photos, colorize grayscale images</span>
<span id="cb2-362"><a href="#cb2-362" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Other modalities**: Depth maps, segmentation masks, audio, etc.</span>
<span id="cb2-363"><a href="#cb2-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-364"><a href="#cb2-364" aria-hidden="true" tabindex="-1"></a>The core modification is simple: provide the condition $\vect{c}$ as additional input to the model. However, the architectural integration varies by model family.</span>
<span id="cb2-365"><a href="#cb2-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-366"><a href="#cb2-366" aria-hidden="true" tabindex="-1"></a>One example are conditional GANs. As shown in @fig-genai-gan-conditional. In this case typically only the generator receives the conditioning information $g(\vect{z}, \vect{c})$ and $d(\vect{x})$. The discriminator does not require $\vect{c}$ in this case because input-output consistency is enforced with other losses that compare input and output (e.g. $L_2$ norm).</span>
<span id="cb2-367"><a href="#cb2-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-368"><a href="#cb2-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-369"><a href="#cb2-369" aria-hidden="true" tabindex="-1"></a>::: {#fig-genai-gan-conditional}</span>
<span id="cb2-370"><a href="#cb2-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-371"><a href="#cb2-371" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}gan_conditional_generation.png)</span></span>
<span id="cb2-372"><a href="#cb2-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-373"><a href="#cb2-373" aria-hidden="true" tabindex="-1"></a>Conditional GAN architecture: Generator receives the conditioning input, enabling controlled generation. From @prince_understanding_2023.</span>
<span id="cb2-374"><a href="#cb2-374" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-375"><a href="#cb2-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-376"><a href="#cb2-376" aria-hidden="true" tabindex="-1"></a>Often, the discriminator needs to be conditioned on $\vect{c}$ as well. The discriminator thus must judge whether $\vect{x}$ matches the condition $\vect{c}$, not just whether $\vect{x}$ looks realistic. This forces the generator to respect the conditioning signal to fool the discriminator.</span>
<span id="cb2-377"><a href="#cb2-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-378"><a href="#cb2-378" aria-hidden="true" tabindex="-1"></a>**Training objective**:</span>
<span id="cb2-379"><a href="#cb2-379" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb2-380"><a href="#cb2-380" aria-hidden="true" tabindex="-1"></a>\min_{g} \max_{d} \mathbb{E}_{x, c}[\log d(\vect{x}, \vect{c})] + \mathbb{E}_{z, c}<span class="co">[</span><span class="ot">\log(1 - d(g(\vect{z}, \vect{c}), \vect{c}))</span><span class="co">]</span></span>
<span id="cb2-381"><a href="#cb2-381" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb2-382"><a href="#cb2-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-383"><a href="#cb2-383" aria-hidden="true" tabindex="-1"></a>**Common conditioning mechanisms**:</span>
<span id="cb2-384"><a href="#cb2-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-385"><a href="#cb2-385" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Concatenation**: Concatenate $\vect{c}$ with $\vect{z}$ for generator, with $\vect{x}$ for discriminator</span>
<span id="cb2-386"><a href="#cb2-386" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Class embeddings**: For discrete labels, learn embedding vectors</span>
<span id="cb2-387"><a href="#cb2-387" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Cross-attention**: For complex conditions (text, images), use attention mechanisms</span>
<span id="cb2-388"><a href="#cb2-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-389"><a href="#cb2-389" aria-hidden="true" tabindex="-1"></a>Beyond GANs, other model families support conditioning:</span>
<span id="cb2-390"><a href="#cb2-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-391"><a href="#cb2-391" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Conditional VAEs**: Condition both encoder and decoder on $c$, useful for disentangling style from content</span>
<span id="cb2-392"><a href="#cb2-392" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Autoregressive models**: Condition each step on $c$, e.g., PixelCNN for class-conditional generation</span>
<span id="cb2-393"><a href="#cb2-393" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**ControlNet**: Add spatial conditioning (edges, depth, pose) to pre-trained diffusion models without retraining from scratch</span>
<span id="cb2-394"><a href="#cb2-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-395"><a href="#cb2-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-396"><a href="#cb2-396" aria-hidden="true" tabindex="-1"></a><span class="fu">## Pre-Trained Models</span></span>
<span id="cb2-397"><a href="#cb2-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-398"><a href="#cb2-398" aria-hidden="true" tabindex="-1"></a>Training generative models from scratch requires massive datasets (millions of images), significant compute resources (weeks on multiple GPUs), and careful hyperparameter tuning. For most practical applications, **pre-trained models** provide a much more accessible starting point.</span>
<span id="cb2-399"><a href="#cb2-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-400"><a href="#cb2-400" aria-hidden="true" tabindex="-1"></a>The landscape has been transformed by models like **Stable Diffusion**, which bring state-of-the-art generation capabilities to consumer hardware. These models are trained on billions of image-text pairs and can generate high-quality, diverse images from text prompts—with minimal customization required for many use cases.</span>
<span id="cb2-401"><a href="#cb2-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-402"><a href="#cb2-402" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Pros and Cons of Pre-Trained Models"}</span>
<span id="cb2-403"><a href="#cb2-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-404"><a href="#cb2-404" aria-hidden="true" tabindex="-1"></a>**Pros**</span>
<span id="cb2-405"><a href="#cb2-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-406"><a href="#cb2-406" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>High-quality image generation out-of-the-box</span>
<span id="cb2-407"><a href="#cb2-407" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Generate images via natural language prompts</span>
<span id="cb2-408"><a href="#cb2-408" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Easy to customize or fine-tune with minimal investment (LoRA, DreamBooth)</span>
<span id="cb2-409"><a href="#cb2-409" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Active community sharing models, techniques, and improvements</span>
<span id="cb2-410"><a href="#cb2-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-411"><a href="#cb2-411" aria-hidden="true" tabindex="-1"></a>**Cons**</span>
<span id="cb2-412"><a href="#cb2-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-413"><a href="#cb2-413" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Not suitable if your data is from a highly specific domain (e.g., medical imaging, satellite imagery)</span>
<span id="cb2-414"><a href="#cb2-414" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>May not work well for very specific output constraints (e.g., exact branding requirements)</span>
<span id="cb2-415"><a href="#cb2-415" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Biases from training data may propagate to outputs</span>
<span id="cb2-416"><a href="#cb2-416" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Licensing considerations for commercial use</span>
<span id="cb2-417"><a href="#cb2-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-418"><a href="#cb2-418" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-419"><a href="#cb2-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-420"><a href="#cb2-420" aria-hidden="true" tabindex="-1"></a><span class="fu">### Stable Diffusion: Practical State-of-the-Art</span></span>
<span id="cb2-421"><a href="#cb2-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-422"><a href="#cb2-422" aria-hidden="true" tabindex="-1"></a>**Stable Diffusion** <span class="co">[</span><span class="ot">@rombach_high-resolution_2022</span><span class="co">]</span> is an open-source text-to-image model that has become the de facto standard for accessible, high-quality image generation. See @fig-genai-stable-diffusion-xl-examples for some examples.</span>
<span id="cb2-423"><a href="#cb2-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-424"><a href="#cb2-424" aria-hidden="true" tabindex="-1"></a>::: {#fig-genai-stable-diffusion-xl-examples}</span>
<span id="cb2-425"><a href="#cb2-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-426"><a href="#cb2-426" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}pract_stable_diffusion_xl_examples.png)</span></span>
<span id="cb2-427"><a href="#cb2-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-428"><a href="#cb2-428" aria-hidden="true" tabindex="-1"></a>Stable Diffusion XL: Example generations showing diverse styles and subjects from text prompts. <span class="co">[</span><span class="ot">Source</span><span class="co">](https://huggingface.co/stabilityai/stable-diffusion-3.5-large)</span>.</span>
<span id="cb2-429"><a href="#cb2-429" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-430"><a href="#cb2-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-431"><a href="#cb2-431" aria-hidden="true" tabindex="-1"></a>Unlike pure pixel-space diffusion (which is computationally expensive), Stable Diffusion uses **latent diffusion**—running the diffusion process in a compressed latent space.</span>
<span id="cb2-432"><a href="#cb2-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-433"><a href="#cb2-433" aria-hidden="true" tabindex="-1"></a>::: {#fig-genai-diffusion-sd-architecture}</span>
<span id="cb2-434"><a href="#cb2-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-435"><a href="#cb2-435" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}diffusion_sd_architecture_genbook.png)</span></span>
<span id="cb2-436"><a href="#cb2-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-437"><a href="#cb2-437" aria-hidden="true" tabindex="-1"></a>Stable Diffusion pipeline: Text is encoded via CLIP, diffusion runs in VAE latent space (much smaller than pixel space), and the VAE decoder produces the final image. Adapted from GenBook.</span>
<span id="cb2-438"><a href="#cb2-438" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-439"><a href="#cb2-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-440"><a href="#cb2-440" aria-hidden="true" tabindex="-1"></a><span class="fu">### Why Latent Diffusion?</span></span>
<span id="cb2-441"><a href="#cb2-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-442"><a href="#cb2-442" aria-hidden="true" tabindex="-1"></a>Running diffusion directly on high-resolution images (e.g., 512×512×3) is computationally expensive—the U-Net must process millions of values at each denoising step. **Latent Diffusion** solves this by:</span>
<span id="cb2-443"><a href="#cb2-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-444"><a href="#cb2-444" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Pre-training a VAE** to compress images into a lower-dimensional latent space (e.g., 64×64×4)</span>
<span id="cb2-445"><a href="#cb2-445" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Running diffusion in the latent space** (much faster!)</span>
<span id="cb2-446"><a href="#cb2-446" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Decoding** the final latent representation back to pixel space</span>
<span id="cb2-447"><a href="#cb2-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-448"><a href="#cb2-448" aria-hidden="true" tabindex="-1"></a>This provides an **8× reduction** in memory and computation, making high-resolution generation practical on consumer GPUs.</span>
<span id="cb2-449"><a href="#cb2-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-450"><a href="#cb2-450" aria-hidden="true" tabindex="-1"></a><span class="fu">### Conditioning on Text (and More)</span></span>
<span id="cb2-451"><a href="#cb2-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-452"><a href="#cb2-452" aria-hidden="true" tabindex="-1"></a>Stable Diffusion supports **text-conditional generation** via cross-attention to CLIP text embeddings. The U-Net attends to text features at multiple scales, enabling precise control over generated content. Beyond text, the framework supports conditioning on:</span>
<span id="cb2-453"><a href="#cb2-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-454"><a href="#cb2-454" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Images**: For inpainting, outpainting, and image-to-image translation</span>
<span id="cb2-455"><a href="#cb2-455" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Depth maps**: For 3D-aware generation</span>
<span id="cb2-456"><a href="#cb2-456" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Segmentation masks**: For spatial control</span>
<span id="cb2-457"><a href="#cb2-457" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Style embeddings**: For artistic control</span>
<span id="cb2-458"><a href="#cb2-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-459"><a href="#cb2-459" aria-hidden="true" tabindex="-1"></a>::: {#fig-genai-diffusion-latent-diff-architecture}</span>
<span id="cb2-460"><a href="#cb2-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-461"><a href="#cb2-461" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}diffusion_latent_diff_architecture.png)</span></span>
<span id="cb2-462"><a href="#cb2-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-463"><a href="#cb2-463" aria-hidden="true" tabindex="-1"></a>Latent Diffusion architecture: The denoising U-Net operates in VAE latent space and uses cross-attention to condition on text (or other modalities). From @rombach_high-resolution_2022.</span>
<span id="cb2-464"><a href="#cb2-464" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-465"><a href="#cb2-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-466"><a href="#cb2-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-467"><a href="#cb2-467" aria-hidden="true" tabindex="-1"></a><span class="fu">### Fine-Tuning with LoRA</span></span>
<span id="cb2-468"><a href="#cb2-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-469"><a href="#cb2-469" aria-hidden="true" tabindex="-1"></a>While pre-trained models are powerful, you often want to adapt them to specific styles, subjects, or domains. Traditional fine-tuning updates all model parameters (billions of weights!), requiring massive memory and compute. **LoRA (Low-Rank Adaptation)**, see @fig-genai-lora-architecture, offers a far more efficient alternative.</span>
<span id="cb2-470"><a href="#cb2-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-471"><a href="#cb2-471" aria-hidden="true" tabindex="-1"></a>**Key idea**: Instead of updating the full weight matrix $W$, LoRA learns a low-rank update $\Delta W = BA$ where:</span>
<span id="cb2-472"><a href="#cb2-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-473"><a href="#cb2-473" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$B \in \mathbb{R}^{d \times r}$ and $A \in \mathbb{R}^{r \times k}$</span>
<span id="cb2-474"><a href="#cb2-474" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$r \ll \min(d, k)$ (typically $r = 4$ to $16$)</span>
<span id="cb2-475"><a href="#cb2-475" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Only $A$ and $B$ are trained; original weights $W$ remain frozen</span>
<span id="cb2-476"><a href="#cb2-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-477"><a href="#cb2-477" aria-hidden="true" tabindex="-1"></a>This reduces trainable parameters by **1000×** or more, enabling fine-tuning on a single consumer GPU in hours rather than days. LoRA adapters can be easily shared, combined, and switched, creating a thriving ecosystem of specialized models.</span>
<span id="cb2-478"><a href="#cb2-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-479"><a href="#cb2-479" aria-hidden="true" tabindex="-1"></a>**Popular use cases**:</span>
<span id="cb2-480"><a href="#cb2-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-481"><a href="#cb2-481" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Style transfer**: Train on artworks to mimic specific artistic styles</span>
<span id="cb2-482"><a href="#cb2-482" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Subject learning**: DreamBooth + LoRA to generate images of specific people/objects</span>
<span id="cb2-483"><a href="#cb2-483" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Domain adaptation**: Adapt to specialized domains (anime, architecture, product photography)</span>
<span id="cb2-484"><a href="#cb2-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-485"><a href="#cb2-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-486"><a href="#cb2-486" aria-hidden="true" tabindex="-1"></a>::: {#fig-genai-lora-architecture}</span>
<span id="cb2-487"><a href="#cb2-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-488"><a href="#cb2-488" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}lora.png)</span>{width=600}</span>
<span id="cb2-489"><a href="#cb2-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-490"><a href="#cb2-490" aria-hidden="true" tabindex="-1"></a>LoRA (Low-Rank Adaptation): Instead of fine-tuning all weights $W$, LoRA trains low-rank matrices $A$ and $B$ where $\Delta W = BA$. This drastically reduces trainable parameters. From @hu_lora_2021.</span>
<span id="cb2-491"><a href="#cb2-491" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-492"><a href="#cb2-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-493"><a href="#cb2-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-494"><a href="#cb2-494" aria-hidden="true" tabindex="-1"></a><span class="fu">### Hardware Requirements</span></span>
<span id="cb2-495"><a href="#cb2-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-496"><a href="#cb2-496" aria-hidden="true" tabindex="-1"></a>The computational demands vary significantly between inference and training:</span>
<span id="cb2-497"><a href="#cb2-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-498"><a href="#cb2-498" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Inference (Image Generation)</span></span>
<span id="cb2-499"><a href="#cb2-499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-500"><a href="#cb2-500" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Minimum**: 4-6 GB VRAM (GPU)</span>
<span id="cb2-501"><a href="#cb2-501" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Stable Diffusion 1.5 at 512×512 resolution</span>
<span id="cb2-502"><a href="#cb2-502" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Longer generation times, limited batch sizes</span>
<span id="cb2-503"><a href="#cb2-503" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Recommended**: 8-12 GB VRAM</span>
<span id="cb2-504"><a href="#cb2-504" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Stable Diffusion XL at 1024×1024</span>
<span id="cb2-505"><a href="#cb2-505" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Comfortable batch sizes, faster generation</span>
<span id="cb2-506"><a href="#cb2-506" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**High-end**: 16+ GB VRAM</span>
<span id="cb2-507"><a href="#cb2-507" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Multiple models loaded simultaneously</span>
<span id="cb2-508"><a href="#cb2-508" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Video generation, 3D-aware models</span>
<span id="cb2-509"><a href="#cb2-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-510"><a href="#cb2-510" aria-hidden="true" tabindex="-1"></a>**CPU-only** inference is possible but **extremely slow** (minutes per image vs. seconds). Many optimization techniques exist: quantization, attention slicing, model offloading.</span>
<span id="cb2-511"><a href="#cb2-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-512"><a href="#cb2-512" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Fine-Tuning (LoRA)</span></span>
<span id="cb2-513"><a href="#cb2-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-514"><a href="#cb2-514" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Minimum**: 12-16 GB VRAM</span>
<span id="cb2-515"><a href="#cb2-515" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>LoRA fine-tuning with small batch sizes</span>
<span id="cb2-516"><a href="#cb2-516" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Gradient checkpointing, mixed precision required</span>
<span id="cb2-517"><a href="#cb2-517" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Recommended**: 24 GB VRAM</span>
<span id="cb2-518"><a href="#cb2-518" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Comfortable batch sizes for faster convergence</span>
<span id="cb2-519"><a href="#cb2-519" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Multiple LoRA ranks simultaneously</span>
<span id="cb2-520"><a href="#cb2-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-521"><a href="#cb2-521" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Full Training (From Scratch)</span></span>
<span id="cb2-522"><a href="#cb2-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-523"><a href="#cb2-523" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Minimum**: Multiple A100 GPUs (40-80 GB each)</span>
<span id="cb2-524"><a href="#cb2-524" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Typical**: Clusters with 100+ GPUs</span>
<span id="cb2-525"><a href="#cb2-525" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Duration**: Days to weeks</span>
<span id="cb2-526"><a href="#cb2-526" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Cost**: $10,000s to $100,000s</span>
<span id="cb2-527"><a href="#cb2-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-528"><a href="#cb2-528" aria-hidden="true" tabindex="-1"></a>For most researchers and practitioners, pre-trained models + LoRA fine-tuning offer the best trade-off.</span>
<span id="cb2-529"><a href="#cb2-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-530"><a href="#cb2-530" aria-hidden="true" tabindex="-1"></a><span class="fu">### Open-Source Ecosystem</span></span>
<span id="cb2-531"><a href="#cb2-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-532"><a href="#cb2-532" aria-hidden="true" tabindex="-1"></a>The open-source community has created a rich ecosystem of tools, models, and resources:</span>
<span id="cb2-533"><a href="#cb2-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-534"><a href="#cb2-534" aria-hidden="true" tabindex="-1"></a><span class="fu">#### HuggingFace</span></span>
<span id="cb2-535"><a href="#cb2-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-536"><a href="#cb2-536" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">HuggingFace</span><span class="co">](https://huggingface.co/)</span> is the central hub for open-source generative models:</span>
<span id="cb2-537"><a href="#cb2-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-538"><a href="#cb2-538" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Diffusers library**: Unified Python API for diffusion models with extensive documentation</span>
<span id="cb2-539"><a href="#cb2-539" aria-hidden="true" tabindex="-1"></a>  <span class="in">```python</span></span>
<span id="cb2-540"><a href="#cb2-540" aria-hidden="true" tabindex="-1"></a>  <span class="im">from</span> diffusers <span class="im">import</span> StableDiffusionPipeline</span>
<span id="cb2-541"><a href="#cb2-541" aria-hidden="true" tabindex="-1"></a>  pipe <span class="op">=</span> StableDiffusionPipeline.from_pretrained(<span class="st">"stabilityai/stable-diffusion-2-1"</span>)</span>
<span id="cb2-542"><a href="#cb2-542" aria-hidden="true" tabindex="-1"></a>  image <span class="op">=</span> pipe(<span class="st">"A photo of a cat wearing sunglasses"</span>).images[<span class="dv">0</span>]</span>
<span id="cb2-543"><a href="#cb2-543" aria-hidden="true" tabindex="-1"></a>  <span class="in">```</span></span>
<span id="cb2-544"><a href="#cb2-544" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Model Hub**: 100,000+ models including base models, fine-tunes, and LoRA adapters</span>
<span id="cb2-545"><a href="#cb2-545" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Spaces**: Interactive demos for trying models before downloading</span>
<span id="cb2-546"><a href="#cb2-546" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Documentation**: Comprehensive guides for inference, fine-tuning, and training</span>
<span id="cb2-547"><a href="#cb2-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-548"><a href="#cb2-548" aria-hidden="true" tabindex="-1"></a>**Best for**: Programmatic access, research, production deployments, custom workflows</span>
<span id="cb2-549"><a href="#cb2-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-550"><a href="#cb2-550" aria-hidden="true" tabindex="-1"></a><span class="fu">#### CivitAI</span></span>
<span id="cb2-551"><a href="#cb2-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-552"><a href="#cb2-552" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">CivitAI</span><span class="co">](https://civitai.com/)</span> is a community-driven platform specializing in Stable Diffusion models:</span>
<span id="cb2-553"><a href="#cb2-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-554"><a href="#cb2-554" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Extensive model library**: Thousands of community-created models, LoRAs, and embeddings</span>
<span id="cb2-555"><a href="#cb2-555" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Style-focused**: Strong emphasis on artistic styles, anime, photorealism</span>
<span id="cb2-556"><a href="#cb2-556" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Preview images**: See example outputs before downloading</span>
<span id="cb2-557"><a href="#cb2-557" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Version control**: Track model updates and improvements</span>
<span id="cb2-558"><a href="#cb2-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-559"><a href="#cb2-559" aria-hidden="true" tabindex="-1"></a>**Best for**: Exploring diverse styles, finding specialized models, community engagement</span>
<span id="cb2-560"><a href="#cb2-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-561"><a href="#cb2-561" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Automatic1111 WebUI</span></span>
<span id="cb2-562"><a href="#cb2-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-563"><a href="#cb2-563" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Automatic1111</span><span class="co">](https://github.com/AUTOMATIC1111/stable-diffusion-webui)</span> provides a feature-rich web interface for local generation:</span>
<span id="cb2-564"><a href="#cb2-564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-565"><a href="#cb2-565" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Local execution**: Run models on your own hardware with full control</span>
<span id="cb2-566"><a href="#cb2-566" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Advanced features**: Inpainting, img2img, ControlNet, depth guidance</span>
<span id="cb2-567"><a href="#cb2-567" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Extensions ecosystem**: Hundreds of community plugins</span>
<span id="cb2-568"><a href="#cb2-568" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Batch processing**: Generate and iterate efficiently</span>
<span id="cb2-569"><a href="#cb2-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-570"><a href="#cb2-570" aria-hidden="true" tabindex="-1"></a>**Best for**: Interactive experimentation, full control over generation, no API costs</span>
<span id="cb2-571"><a href="#cb2-571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-572"><a href="#cb2-572" aria-hidden="true" tabindex="-1"></a><span class="fu">### Closed-Source Platforms</span></span>
<span id="cb2-573"><a href="#cb2-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-574"><a href="#cb2-574" aria-hidden="true" tabindex="-1"></a>Commercial platforms offer convenient APIs and frontier models:</span>
<span id="cb2-575"><a href="#cb2-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-576"><a href="#cb2-576" aria-hidden="true" tabindex="-1"></a><span class="fu">#### OpenAI</span></span>
<span id="cb2-577"><a href="#cb2-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-578"><a href="#cb2-578" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Latest model**: 4o Image with enhanced prompt following and coherence</span>
<span id="cb2-579"><a href="#cb2-579" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**API access**: Programmatic generation, inpainting, variations</span>
<span id="cb2-580"><a href="#cb2-580" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Pricing**: Pay per image generated</span>
<span id="cb2-581"><a href="#cb2-581" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Strengths**: Strong text understanding, safe outputs, reliable API</span>
<span id="cb2-582"><a href="#cb2-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-583"><a href="#cb2-583" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Google Gemini (Imagen)</span></span>
<span id="cb2-584"><a href="#cb2-584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-585"><a href="#cb2-585" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**High photorealism**: Particularly strong at realistic images</span>
<span id="cb2-586"><a href="#cb2-586" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Multimodal**: Integration with Gemini's broader capabilities</span>
<span id="cb2-587"><a href="#cb2-587" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Controlled access**: Available through Google Cloud</span>
<span id="cb2-588"><a href="#cb2-588" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Strengths**: Photo-quality outputs, factual accuracy</span>
<span id="cb2-589"><a href="#cb2-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-590"><a href="#cb2-590" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Replicate</span></span>
<span id="cb2-591"><a href="#cb2-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-592"><a href="#cb2-592" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Hybrid platform**: Both open- and closed-source models via unified API</span>
<span id="cb2-593"><a href="#cb2-593" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Pay-per-use**: No infrastructure management, pay only for inference</span>
<span id="cb2-594"><a href="#cb2-594" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Host custom models**: Deploy your own fine-tuned models</span>
<span id="cb2-595"><a href="#cb2-595" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Wide selection**: SDXL, Midjourney alternatives, domain-specific models</span>
<span id="cb2-596"><a href="#cb2-596" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-597"><a href="#cb2-597" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Midjourney</span></span>
<span id="cb2-598"><a href="#cb2-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-599"><a href="#cb2-599" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Artistic focus**: Highly curated, aesthetic, and stylized outputs</span>
<span id="cb2-600"><a href="#cb2-600" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Discord interface**: Community-driven prompt engineering</span>
<span id="cb2-601"><a href="#cb2-601" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**No API**: Interface only, no programmatic access</span>
<span id="cb2-602"><a href="#cb2-602" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Strengths**: Artistic coherence, aesthetic quality, strong defaults</span>
<span id="cb2-603"><a href="#cb2-603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-604"><a href="#cb2-604" aria-hidden="true" tabindex="-1"></a>**Choosing a platform**:</span>
<span id="cb2-605"><a href="#cb2-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-606"><a href="#cb2-606" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**For prototyping**: HuggingFace Spaces or Replicate (fast, no setup)</span>
<span id="cb2-607"><a href="#cb2-607" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**For production**: HuggingFace Diffusers or OpenAI API (reliable, scalable)</span>
<span id="cb2-608"><a href="#cb2-608" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**For experimentation**: Automatic1111 (full control, no API costs)</span>
<span id="cb2-609"><a href="#cb2-609" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**For art/design**: Midjourney or CivitAI models (aesthetic quality)</span>
<span id="cb2-610"><a href="#cb2-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-611"><a href="#cb2-611" aria-hidden="true" tabindex="-1"></a><span class="fu">## Conclusions &amp; Discussions</span></span>
<span id="cb2-612"><a href="#cb2-612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-613"><a href="#cb2-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-614"><a href="#cb2-614" aria-hidden="true" tabindex="-1"></a>An interesting thought: What happens if models are trained (inadvertedly) on synhetic data? @alemohammad_self-consuming_2023 studies this and found remarkable degradation, see @fig-genai-deterioration-on-pollution.</span>
<span id="cb2-615"><a href="#cb2-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-616"><a href="#cb2-616" aria-hidden="true" tabindex="-1"></a>::: {#fig-genai-deterioration-on-pollution}</span>
<span id="cb2-617"><a href="#cb2-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-618"><a href="#cb2-618" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}deterioration_on_pollution.png)</span></span>
<span id="cb2-619"><a href="#cb2-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-620"><a href="#cb2-620" aria-hidden="true" tabindex="-1"></a>Example: Deterioration of generated image quality under pollution or domain shift. From @alemohammad_self-consuming_2023.</span>
<span id="cb2-621"><a href="#cb2-621" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-622"><a href="#cb2-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-623"><a href="#cb2-623" aria-hidden="true" tabindex="-1"></a>Furthermore, synthetic imagery has obvious consequences on all of us. The lines between real and synthetic are starting to blur and it becomes increasingly difficult to distinguish both. See the following quote with respect to @fig-genai-real-or-not-musk.</span>
<span id="cb2-624"><a href="#cb2-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-625"><a href="#cb2-625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-626"><a href="#cb2-626" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Zunächst hatte die Polizei … die Aktion habe nicht stattgefunden, das Bild davon sei ein »Fake«. ... Auf Anfrage des SPIEGEL sagte ein Sprecher der Brandenburger Polizei: »Ob die Projektion tatsächlich so ablief, ist Teil dieser Ermittlungen. Derzeit können wir es jedenfalls nicht ausschließen.«</span></span>
<span id="cb2-627"><a href="#cb2-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-628"><a href="#cb2-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-629"><a href="#cb2-629" aria-hidden="true" tabindex="-1"></a>::: {#fig-genai-real-or-not-musk}</span>
<span id="cb2-630"><a href="#cb2-630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-631"><a href="#cb2-631" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}real_or_not_musk.png)</span></span>
<span id="cb2-632"><a href="#cb2-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-633"><a href="#cb2-633" aria-hidden="true" tabindex="-1"></a>"Real or Not?" <span class="co">[</span><span class="ot">Link</span><span class="co">](https://www.spiegel.de/panorama/elon-musk-staatsschutz-ermittelt-nach-projektion-tesla-werk-in-gruenheide-a-8268048f-050b-4ae7-adf8-bb9cf572cf25?sara_ref=re-so-app-sh)</span></span>
<span id="cb2-634"><a href="#cb2-634" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-635"><a href="#cb2-635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-636"><a href="#cb2-636" aria-hidden="true" tabindex="-1"></a>Furthermore, biases from the internet-scale data the models have been trained on seep into the models. An interesting article by the Washington post <span class="co">[</span><span class="ot">Link</span><span class="co">](https://www.washingtonpost.com/technology/interactive/2024/ai-bias-beautiful-women-ugly-images/)</span> investigated this @fig-genai-ai-bias-wp-front.</span>
<span id="cb2-637"><a href="#cb2-637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-638"><a href="#cb2-638" aria-hidden="true" tabindex="-1"></a>::: {#fig-genai-ai-bias-wp-front}</span>
<span id="cb2-639"><a href="#cb2-639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-640"><a href="#cb2-640" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}ai_bias_wp_front.png)</span></span>
<span id="cb2-641"><a href="#cb2-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-642"><a href="#cb2-642" aria-hidden="true" tabindex="-1"></a>AI bias: Example from a whitepaper front page. <span class="co">[</span><span class="ot">Link</span><span class="co">](https://www.washingtonpost.com/technology/interactive/2024/ai-bias-beautiful-women-ugly-images/)</span></span>
<span id="cb2-643"><a href="#cb2-643" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-644"><a href="#cb2-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-645"><a href="#cb2-645" aria-hidden="true" tabindex="-1"></a>They found strong biases, for example, with respect to the depiction of _beautiful women_, see @fig-genai-ai-bias-wp-examples.</span>
<span id="cb2-646"><a href="#cb2-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-647"><a href="#cb2-647" aria-hidden="true" tabindex="-1"></a>::: {#fig-genai-ai-bias-wp-examples}</span>
<span id="cb2-648"><a href="#cb2-648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-649"><a href="#cb2-649" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}ai_bias_wp_examples.png)</span></span>
<span id="cb2-650"><a href="#cb2-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-651"><a href="#cb2-651" aria-hidden="true" tabindex="-1"></a>AI bias: Additional examples. <span class="co">[</span><span class="ot">Link</span><span class="co">](https://www.washingtonpost.com/technology/interactive/2024/ai-bias-beautiful-women-ugly-images/)</span></span>
<span id="cb2-652"><a href="#cb2-652" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-653"><a href="#cb2-653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-654"><a href="#cb2-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-655"><a href="#cb2-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-656"><a href="#cb2-656" aria-hidden="true" tabindex="-1"></a><span class="fu">## References</span></span>
<span id="cb2-657"><a href="#cb2-657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-658"><a href="#cb2-658" aria-hidden="true" tabindex="-1"></a>::: {#refs}</span>
<span id="cb2-659"><a href="#cb2-659" aria-hidden="true" tabindex="-1"></a>:::</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© Copyright 2025, Marco Willi</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2025/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This page is built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>