<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Marco Willi">

<title>3 - Image Classification â€“ CAS Deep Learning - Computer Vision</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../pages/lectures/representation_learning.html" rel="next">
<link href="../../pages/lectures/cnns.html" rel="prev">
<link href="../..//assets/cv_logo_small.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-065a5179aebd64318d7ea99d77b64a9e.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-159896a7fbbc1299bfd73fe3af9cd852.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-97BR3FTSN1"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-97BR3FTSN1', { 'anonymize_ip': true});
</script>
<script src="https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js" integrity="sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script type="text/javascript">
window.PlotlyConfig = {MathJaxConfig: 'local'};
if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}
</script>
<script type="module">import "https://cdn.plot.ly/plotly-3.2.0.min"</script>


  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../pages/lectures/intro.html">Lectures</a></li><li class="breadcrumb-item"><a href="../../pages/lectures/classification.html">3 - Image Classification</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-center sidebar-header">
      <a href="../../index.html" class="sidebar-logo-link">
      <img src="../../assets/cv_logo_small.png" alt="" class="sidebar-logo light-content py-0 d-lg-inline d-none">
      <img src="../../assets/cv_logo_small.png" alt="" class="sidebar-logo dark-content py-0 d-lg-inline d-none">
      </a>
      <div class="sidebar-tools-main">
    <a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2025" title="GitHub organization" class="quarto-navigation-tool px-1" aria-label="GitHub organization"><i class="bi bi-github"></i></a>
    <a href="" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-code-square"></i></a>
</div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Background &amp; Preparation</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/background/machine_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1 - Machine Learning Basics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/background/neural_networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2 - Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/background/frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3 - Software &amp; Hardware for Deep Learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Lectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1 - Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/cnns.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2 - Convolutional Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/classification.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">3 - Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/representation_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4 - Representation Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/practical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5 - Practical</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/recent_advances.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6 - Recent Advances</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/intro_genai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">7 - Intro to Generative AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/generative_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">8 - Generative Models</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Slides</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/cnns.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Convolutional Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/representation_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Representation Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/practical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Practical Considerations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/recent_advances.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Recent Advances</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/intro_genai.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Intro to Generative AI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/generative_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Generative Models</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/quiz.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Quiz &amp; Exam Prep</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false">
 <span class="menu-text">Resources</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/literature.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Books</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/links.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Helpful Links &amp; Resources</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Mathematical Notation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/student_projects.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Project Ideas</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#parametric-approach" id="toc-parametric-approach" class="nav-link" data-scroll-target="#parametric-approach"><span class="header-section-number">2</span> Parametric Approach</a>
  <ul class="collapse">
  <li><a href="#softmax-classifier" id="toc-softmax-classifier" class="nav-link" data-scroll-target="#softmax-classifier"><span class="header-section-number">2.1</span> Softmax Classifier</a></li>
  </ul></li>
  <li><a href="#loss-function" id="toc-loss-function" class="nav-link" data-scroll-target="#loss-function"><span class="header-section-number">3</span> Loss Function</a>
  <ul class="collapse">
  <li><a href="#likelihood" id="toc-likelihood" class="nav-link" data-scroll-target="#likelihood"><span class="header-section-number">3.1</span> Likelihood</a></li>
  <li><a href="#maximum-likelihood" id="toc-maximum-likelihood" class="nav-link" data-scroll-target="#maximum-likelihood"><span class="header-section-number">3.2</span> Maximum Likelihood</a></li>
  <li><a href="#negative-log-likelihood" id="toc-negative-log-likelihood" class="nav-link" data-scroll-target="#negative-log-likelihood"><span class="header-section-number">3.3</span> Negative Log-Likelihood</a></li>
  <li><a href="#cross-entropy" id="toc-cross-entropy" class="nav-link" data-scroll-target="#cross-entropy"><span class="header-section-number">3.4</span> Cross-Entropy</a></li>
  <li><a href="#try-it-out-yourself" id="toc-try-it-out-yourself" class="nav-link" data-scroll-target="#try-it-out-yourself"><span class="header-section-number">3.5</span> Try it out yourself!</a></li>
  </ul></li>
  <li><a href="#architectures" id="toc-architectures" class="nav-link" data-scroll-target="#architectures"><span class="header-section-number">4</span> Architectures</a>
  <ul class="collapse">
  <li><a href="#alexnet" id="toc-alexnet" class="nav-link" data-scroll-target="#alexnet"><span class="header-section-number">4.1</span> AlexNet</a></li>
  <li><a href="#vgg" id="toc-vgg" class="nav-link" data-scroll-target="#vgg"><span class="header-section-number">4.2</span> VGG</a></li>
  <li><a href="#resnet" id="toc-resnet" class="nav-link" data-scroll-target="#resnet"><span class="header-section-number">4.3</span> ResNet</a></li>
  <li><a href="#convnext" id="toc-convnext" class="nav-link" data-scroll-target="#convnext"><span class="header-section-number">4.4</span> ConvNext</a></li>
  <li><a href="#imagenet-performance" id="toc-imagenet-performance" class="nav-link" data-scroll-target="#imagenet-performance"><span class="header-section-number">4.5</span> ImageNet Performance</a></li>
  <li><a href="#which-architecture" id="toc-which-architecture" class="nav-link" data-scroll-target="#which-architecture"><span class="header-section-number">4.6</span> Which Architecture?</a></li>
  </ul></li>
  <li><a href="#further-variations" id="toc-further-variations" class="nav-link" data-scroll-target="#further-variations"><span class="header-section-number">5</span> Further Variations</a>
  <ul class="collapse">
  <li><a href="#squeezeexcite-networks" id="toc-squeezeexcite-networks" class="nav-link" data-scroll-target="#squeezeexcite-networks"><span class="header-section-number">5.1</span> Squeeze/Excite Networks</a></li>
  <li><a href="#normalization-layers" id="toc-normalization-layers" class="nav-link" data-scroll-target="#normalization-layers"><span class="header-section-number">5.2</span> Normalization Layers</a></li>
  </ul></li>
  <li><a href="#pre-processing" id="toc-pre-processing" class="nav-link" data-scroll-target="#pre-processing"><span class="header-section-number">6</span> Pre-Processing</a></li>
  <li><a href="#transfer-learning" id="toc-transfer-learning" class="nav-link" data-scroll-target="#transfer-learning"><span class="header-section-number">7</span> Transfer Learning</a></li>
  <li><a href="#pytorch-example" id="toc-pytorch-example" class="nav-link" data-scroll-target="#pytorch-example"><span class="header-section-number">8</span> PyTorch Example</a>
  <ul class="collapse">
  <li><a href="#architecture" id="toc-architecture" class="nav-link" data-scroll-target="#architecture"><span class="header-section-number">8.1</span> Architecture</a></li>
  <li><a href="#loss-function-1" id="toc-loss-function-1" class="nav-link" data-scroll-target="#loss-function-1"><span class="header-section-number">8.2</span> Loss Function</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">9</span> References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2025/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../pages/lectures/intro.html">Lectures</a></li><li class="breadcrumb-item"><a href="../../pages/lectures/classification.html">3 - Image Classification</a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">3 - Image Classification</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Marco Willi </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Institute for Data Science I4DS, FHNW
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-tip callout-titled" title="Learning Objectives">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Learning Objectives
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li>Define and recognize classification problems.</li>
<li>Interpret logits, probabilities and the role of the softmax function.</li>
<li>Explain cross-entropy and maximum likelihood on a high-level.</li>
<li>Compare classic CNN architectures (AlexNetâ†’VGGâ†’ResNetâ†’ConvNeXt).</li>
<li>Apply transfer learning and explain when to freeze vs fine-tune layers.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="TLDR Recap">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>TLDR Recap
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Image Classification: Core Task</strong></p>
<ul>
<li>Assign images to predefined classes (multi-class: â‰¥2 classes, binary: 2 classes, multi-label: multiple class sets)</li>
<li>Use CNNs to learn hierarchical features automatically from raw pixels</li>
<li>Output: class probabilities or labels</li>
</ul>
<p><strong>Softmax: Logits â†’ Probabilities</strong></p>
<p><span class="math display">\[P(Y = k \mid X = \mathbf{x}) = \frac{e^{z_k}}{\sum_{j=1}^K e^{z_j}}\]</span></p>
<ul>
<li><strong>Logits</strong> <span class="math inline">\(\mathbf{z}\)</span>: Raw model outputs (unbounded)</li>
<li><strong>Softmax</strong> <span class="math inline">\(\sigma(\mathbf{z})\)</span>: Converts logits to valid probability distribution</li>
<li>Properties: Always sums to 1, outputs in [0,1], differentiable</li>
</ul>
<p><strong>Cross-Entropy Loss = Negative Log-Likelihood</strong></p>
<p>Maximum Likelihood approach:</p>
<p><span class="math display">\[L(\mathbf{X}, \mathbf{y}, \theta) = -\sum_{i=1}^N \sum_{j=1}^K y^{(i)}_j \log P(Y = j\mid X = \mathbf{x}^{(i)})\]</span></p>
<ul>
<li>Minimizing negative log-likelihood = maximizing likelihood of correct predictions</li>
<li>Equivalent to cross-entropy between true distribution <span class="math inline">\(\mathbf{y}\)</span> and predicted <span class="math inline">\(\hat{\mathbf{y}}\)</span></li>
<li>Penalizes confident wrong predictions heavily</li>
</ul>
<p><strong>Key Insight</strong>: Softmax + Cross-Entropy arise naturally from probabilistic modeling (categorical distribution + maximum likelihood estimation)</p>
<p><strong>Classic CNN Architectures Evolution</strong></p>
<ol type="1">
<li><strong>AlexNet (2012)</strong>: 5 conv + 3 FC layers, ReLU, dropout â†’ ImageNet breakthrough</li>
<li><strong>VGG (2014)</strong>: Deeper (16-19 layers), small 3Ã—3 kernels, simple design â†’ showed depth matters</li>
<li><strong>ResNet (2015)</strong>: Residual connections (skip connections) â†’ enabled 50-152 layers, solved vanishing gradients</li>
<li><strong>ConvNeXt (2022)</strong>: Modern CNN with Transformer-era tricks â†’ competes with Vision Transformers</li>
</ol>
<p><strong>Residual Connections: The Key Innovation</strong></p>
<p><span class="math display">\[\mathbf{x}_{l+1} = \mathbf{x}_l + F(\mathbf{x}_l)\]</span></p>
<ul>
<li>Network learns residual <span class="math inline">\(F(\mathbf{x})\)</span> instead of full transformation</li>
<li>Enables training very deep networks (&gt;100 layers)</li>
<li>Smoother loss landscape â†’ better optimization</li>
<li>Default behavior: pass activations unchanged (identity mapping)</li>
</ul>
<p><strong>Transfer Learning: Standing on Giantsâ€™ Shoulders</strong></p>
<ol type="1">
<li><strong>Pre-training</strong>: Train on large dataset (e.g., ImageNet: 1.2M images, 1000 classes)</li>
<li><strong>Transfer</strong>: Adapt to your specific task with much less data</li>
<li><strong>Strategies</strong>:
<ul>
<li><strong>Freeze early layers</strong> (extract features only) when data is very scarce</li>
<li><strong>Fine-tune all layers</strong> when you have moderate data</li>
<li><strong>Fine-tune only later layers</strong> as middle ground</li>
</ul></li>
</ol>
<p><strong>Why Transfer Learning Works</strong>: - Early layers learn general features (edges, textures) - Later layers learn task-specific features - Pre-trained features transfer well across vision tasks</p>
<p><strong>Architecture Selection Strategy</strong></p>
<ul>
<li>âœ… <strong>Donâ€™t be a hero</strong>: Use proven architectures first</li>
<li>âœ… <strong>ResNet-50/101</strong>: Excellent baseline for most tasks</li>
<li>âœ… <strong>EfficientNet</strong>: When model size/speed matters</li>
<li>âœ… <strong>ConvNeXt</strong>: State-of-the-art CNN performance</li>
</ul>
<p><strong>Consider</strong>: - Accuracy requirements - Inference speed (FLOPs) - Model size (memory) - Available training data</p>
<p><strong>Modern Components</strong></p>
<ul>
<li><strong>Normalization layers</strong> (BatchNorm, LayerNorm): Stabilize training, enable deeper networks</li>
<li><strong>Global Average Pooling</strong>: Replace large FC layers â†’ fewer parameters</li>
<li><strong>SE Blocks</strong>: Learn channel-wise attention â†’ better feature recalibration</li>
<li><strong>Depthwise Separable Convolutions</strong>: Reduce parameters while maintaining performance</li>
</ul>
<p><strong>Pre-processing Pipeline</strong></p>
<ol type="1">
<li>Resize/crop to fixed size (e.g., 224Ã—224)</li>
<li>Scale pixel values: [0, 255] â†’ [0, 1]</li>
<li>Normalize: Subtract mean, divide by std per channel</li>
<li>Data augmentation (training): flips, crops, color jitter</li>
</ol>
<p><strong>Best Practices</strong></p>
<ul>
<li>âœ… Start with pre-trained models (transfer learning)</li>
<li>âœ… Use proven architectures (ResNet, ConvNeXt)</li>
<li>âœ… Apply proper preprocessing (resize, normalize)</li>
<li>âœ… Use cross-entropy loss for classification</li>
<li>âœ… Monitor both training and validation metrics</li>
<li>âœ… Consider calibration (temperature scaling, label smoothing)</li>
</ul>
<p><strong>Key Takeaway</strong>: Classification success comes from combining strong architectures (ResNets), probabilistic foundations (softmax + cross-entropy), transfer learning, and proper training practices.</p>
</div>
</div>
</div>
<section id="introduction" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">1</span> Introduction</h2>
<p>Image classification is a core task of computer vision. In image classification, an image is assigned to a <strong>predefined set</strong> of classes. There are different variants of image classification:</p>
<ul>
<li>in <strong>multi-class classification</strong> there are <span class="math inline">\(&gt; 2\)</span> classes</li>
<li>in <strong>binary classification</strong> there are 2 classes</li>
<li>in <strong>multi-label classification</strong> there are several sets of classes.</li>
</ul>
<p><a href="#fig-classification-classification-illustration" class="quarto-xref">Figure&nbsp;1</a> illustrates a multi-class classification task.</p>
<div id="fig-classification-classification-illustration" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-classification-classification-illustration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/classification/classification_illustration.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classification-classification-illustration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Image Classification example.
</figcaption>
</figure>
</div>
<p><a href="#fig-image-classifier-example" class="quarto-xref">Figure&nbsp;2</a> shows an example from the paper by <span class="citation" data-cites="krizhevsky_imagenet_2012">Krizhevsky, Sutskever, and Hinton (<a href="#ref-krizhevsky_imagenet_2012" role="doc-biblioref">2012</a>)</span>, which achieved the best results in the 2012 ImageNet competition, demonstrating how well CNNs work. Note that each image had to be assigned to one of 1â€™000 classes.</p>
<div id="fig-image-classifier-example" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-image-classifier-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/classification/image_classification_example.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-image-classifier-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Image Classification example (from <span class="citation" data-cites="krizhevsky_imagenet_2012">Krizhevsky, Sutskever, and Hinton (<a href="#ref-krizhevsky_imagenet_2012" role="doc-biblioref">2012</a>)</span>).
</figcaption>
</figure>
</div>
<p><a href="#fig-classification-camera-traps" class="quarto-xref">Figure&nbsp;3</a> illustrates the challenge with images taken by camera traps, which need to be classified along animal species.</p>
<div id="fig-classification-camera-traps" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-classification-camera-traps-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/classification/camera_traps.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classification-camera-traps-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Example images from camera traps.
</figcaption>
</figure>
</div>
</section>
<section id="parametric-approach" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="parametric-approach"><span class="header-section-number">2</span> Parametric Approach</h2>
<p>In a parametric approach, we seek a model of the following form:</p>
<p><span class="math display">\[
\hat{y}^{(i)} = f(\theta, \mathbf{x}^{(i)})
\]</span></p>
<p>We want to find model parameters <span class="math inline">\(\theta\)</span> that output a score/prediction <span class="math inline">\(\hat{y}^{(i)}\)</span> for any data points <span class="math inline">\(\mathbf{x}^{(i)}\)</span> for each class <span class="math inline">\(k \in K\)</span>. We then want to assess how good this score is with the help of a loss function.</p>
<p>Note: The model parameters <span class="math inline">\(\theta\)</span> are all the learnable parameters of a model, e.g.&nbsp;the filters of the convolutional layers.</p>
<section id="softmax-classifier" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="softmax-classifier"><span class="header-section-number">2.1</span> Softmax Classifier</h3>
<p>With a softmax classifier, we interpret model predictions/scores as probabilities of class memberships: <span class="math inline">\(P(Y=\mathbf{y}^{(i)}| X = \mathbf{x}^{(i)})\)</span>. We interpret the output of a model as the parameters of a <a href="https://en.wikipedia.org/wiki/Categorical_distribution">Categorical Distribution</a> over all possible classes.</p>
<p>To obtain a valid probability distribution, the untransformed outputs <span class="math inline">\(\mathbf{z}\)</span>, also called <strong>logits</strong>, of a model are transformed with the softmax function <span class="math inline">\(\sigma(\mathbf{z})\)</span>:</p>
<p><span class="math display">\[
P(Y = k| X = \mathbf{x}^{(i)}) = \sigma(\mathbf{z})_k = \frac{e^{z_k}}{\sum_i^K e^{z_i}}
\]</span></p>
<p><a href="#fig-classification-logits-to-probs" class="quarto-xref">Figure&nbsp;4</a> shows an example of the effect of the softmax transformation.</p>
<div id="cell-fig-classification-logits-to-probs" class="cell" data-fig-width="600" data-execution_count="1">
<div class="cell-output cell-output-display">
<div id="fig-classification-logits-to-probs" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-classification-logits-to-probs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="classification_files/figure-html/fig-classification-logits-to-probs-output-1.png" width="854" height="278" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classification-logits-to-probs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Logits (left) to probabilities with the Softmax function (right).
</figcaption>
</figure>
</div>
</div>
</div>
<!-- {{< embed /demos/logits_to_probabilities.ipynb#fig-demo-logits-to-probs >}} -->
<div class="callout callout-style-default callout-tip callout-titled" title="ðŸŽ® Interactive Exploration">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>ðŸŽ® Interactive Exploration
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Try different logit distributions</strong> and see how softmax transforms them into probabilities!</p>
<div id="a5640596" class="cell" data-fig-width="800" data-execution_count="2">
<div class="cell-output cell-output-display">
<div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-3.2.0.min.js" integrity="sha256-iZ2u/oU2wf/vDbl/ChcX93WgbBRSBvUO6N413hDz7xM=" crossorigin="anonymous"></script>                <div id="afd0f605-2086-49a1-b56e-130ce0a8d72f" class="plotly-graph-div" style="height:380px; width:100%;"></div>            <script type="text/javascript">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById("afd0f605-2086-49a1-b56e-130ce0a8d72f")) {                    Plotly.newPlot(                        "afd0f605-2086-49a1-b56e-130ce0a8d72f",                        [{"marker":{"color":["steelblue","lightblue","lightblue","lightblue","lightblue"]},"name":"Logits","text":["5.00","1.00","0.50","0.00","-0.50"],"textposition":"outside","x":["Class 0","Class 1","Class 2","Class 3","Class 4"],"y":[5.0,1.0,0.5,0.0,-0.5],"type":"bar","xaxis":"x","yaxis":"y"},{"marker":{"color":["crimson","lightcoral","lightcoral","lightcoral","lightcoral"]},"name":"Probabilities","text":["0.961","0.018","0.011","0.006","0.004"],"textposition":"outside","x":["Class 0","Class 1","Class 2","Class 3","Class 4"],"y":{"dtype":"f8","bdata":"V\u002flS8QjD7j\u002faUL+8jweSP8b0AYf13oU\u002fRYC7deGHej+np8bfexdwPw=="},"type":"bar","xaxis":"x2","yaxis":"y2"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermap":[{"type":"scattermap","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"},"margin":{"b":0,"l":0,"r":0,"t":30}}},"xaxis":{"anchor":"y","domain":[0.0,0.44]},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"Logit Value"},"range":[-1.49,5.99],"automargin":true},"xaxis2":{"anchor":"y2","domain":[0.56,1.0]},"yaxis2":{"anchor":"x2","domain":[0.0,1.0],"title":{"text":"Probability"},"range":[0,1.1],"automargin":true},"annotations":[{"font":{"size":16},"showarrow":false,"text":"Logits (Raw Model Outputs)","x":0.22,"xanchor":"center","xref":"paper","y":1.0,"yanchor":"bottom","yref":"paper"},{"font":{"size":16},"showarrow":false,"text":"Probabilities (After Softmax)","x":0.78,"xanchor":"center","xref":"paper","y":1.0,"yanchor":"bottom","yref":"paper"},{"bgcolor":"lightyellow","bordercolor":"green","borderpad":4,"borderwidth":2,"font":{"color":"green","size":12},"showarrow":false,"text":"Sum = 1.000000","x":2,"xref":"x2","y":0.98,"yref":"y2"}],"margin":{"t":90,"r":40,"b":60,"l":60},"uniformtext":{"minsize":10,"mode":"hide"},"updatemenus":[{"bgcolor":"white","bordercolor":"#ddd","borderwidth":1,"buttons":[{"args":[{"y":[[1.0,1.0,1.0,1.0,1.0],[0.2,0.2,0.2,0.2,0.2]],"text":[["1.00","1.00","1.00","1.00","1.00"],["0.200","0.200","0.200","0.200","0.200"]],"marker.color":[["steelblue","lightblue","lightblue","lightblue","lightblue"],["crimson","lightcoral","lightcoral","lightcoral","lightcoral"]]},{"yaxis.range":[0.65,1.35],"yaxis2.range":[0,1.1],"annotations[2].text":"Sum = 1.000000"}],"label":"Uniform Logits (max prob=0.200, H=1.61)","method":"update"},{"args":[{"y":[[5.0,1.0,0.5,0.0,-0.5],[0.9613079751359618,0.01760696973345053,0.010679166967970121,0.00647724218626428,0.003928645976353374]],"text":[["5.00","1.00","0.50","0.00","-0.50"],["0.961","0.018","0.011","0.006","0.004"]],"marker.color":[["steelblue","lightblue","lightblue","lightblue","lightblue"],["crimson","lightcoral","lightcoral","lightcoral","lightcoral"]]},{"yaxis.range":[-1.49,5.99],"yaxis2.range":[0,1.1],"annotations[2].text":"Sum = 1.000000"}],"label":"Clear Winner (max prob=0.961, H=0.21)","method":"update"},{"args":[{"y":[[2.0,1.8,1.5,1.2,1.0],[0.30840687997286076,0.2525021970946111,0.1870582283698542,0.1385761439048298,0.11345655065784412]],"text":[["2.00","1.80","1.50","1.20","1.00"],["0.308","0.253","0.187","0.139","0.113"]],"marker.color":[["steelblue","lightblue","lightblue","lightblue","lightblue"],["crimson","lightcoral","lightcoral","lightcoral","lightcoral"]]},{"yaxis.range":[0.65,2.35],"yaxis2.range":[0,1.1],"annotations[2].text":"Sum = 1.000000"}],"label":"Close Competition (max prob=0.308, H=1.54)","method":"update"},{"args":[{"y":[[10.0,0.0,0.0,0.0,0.0],[0.9998184332534202,4.539168664494293e-05,4.539168664494293e-05,4.539168664494293e-05,4.539168664494293e-05]],"text":[["10.00","0.00","0.00","0.00","0.00"],["1.000","0.000","0.000","0.000","0.000"]],"marker.color":[["steelblue","lightblue","lightblue","lightblue","lightblue"],["crimson","lightcoral","lightcoral","lightcoral","lightcoral"]]},{"yaxis.range":[-1.7999999999999998,11.8],"yaxis2.range":[0,1.1],"annotations[2].text":"Sum = 1.000000"}],"label":"Very Confident (max prob=1.000, H=0.00)","method":"update"},{"args":[{"y":[[-0.5,-1.0,-1.5,-2.0,-2.5],[0.42865552877716695,0.2599927206586828,0.1576935563815933,0.09564597678455912,0.058012217397997876]],"text":[["-0.50","-1.00","-1.50","-2.00","-2.50"],["0.429","0.260","0.158","0.096","0.058"]],"marker.color":[["steelblue","lightblue","lightblue","lightblue","lightblue"],["crimson","lightcoral","lightcoral","lightcoral","lightcoral"]]},{"yaxis.range":[-2.86,-0.14],"yaxis2.range":[0,1.1],"annotations[2].text":"Sum = 1.000000"}],"label":"Negative Logits (max prob=0.429, H=1.39)","method":"update"},{"args":[{"y":[[3.0,0.0,-1.0,0.5,2.0],[0.6587323900276633,0.032796354538433686,0.01206510458005948,0.054072047328938305,0.24233410352490536]],"text":[["3.00","0.00","-1.00","0.50","2.00"],["0.659","0.033","0.012","0.054","0.242"]],"marker.color":[["steelblue","lightblue","lightblue","lightblue","lightblue"],["crimson","lightcoral","lightcoral","lightcoral","lightcoral"]]},{"yaxis.range":[-1.72,3.7199999999999998],"yaxis2.range":[0,1.1],"annotations[2].text":"Sum = 1.000000"}],"label":"Mixed Range (max prob=0.659, H=0.94)","method":"update"},{"args":[{"y":[[4.0,3.8,0.0,0.0,0.0],[0.5337097282781469,0.43696456775821213,0.009775234654546978,0.009775234654546978,0.009775234654546978]],"text":[["4.00","3.80","0.00","0.00","0.00"],["0.534","0.437","0.010","0.010","0.010"]],"marker.color":[["steelblue","lightblue","lightblue","lightblue","lightblue"],["crimson","lightcoral","lightcoral","lightcoral","lightcoral"]]},{"yaxis.range":[-0.72,4.72],"yaxis2.range":[0,1.1],"annotations[2].text":"Sum = 1.000000"}],"label":"Two Leaders (max prob=0.534, H=0.83)","method":"update"},{"args":[{"y":[[1.5,-0.3,2.1,0.8,-0.8],[0.2789974744369541,0.04611797234105903,0.5083665434330445,0.13854604560031086,0.027971964188631505]],"text":[["1.50","-0.30","2.10","0.80","-0.80"],["0.279","0.046","0.508","0.139","0.028"]],"marker.color":[["lightblue","lightblue","steelblue","lightblue","lightblue"],["lightcoral","lightcoral","crimson","lightcoral","lightcoral"]]},{"yaxis.range":[-1.322,2.622],"yaxis2.range":[0,1.1],"annotations[2].text":"Sum = 1.000000"}],"label":"Random-ish (max prob=0.508, H=1.22)","method":"update"}],"direction":"down","pad":{"b":10,"l":6,"r":6,"t":4},"type":"dropdown","x":0.5,"xanchor":"center","y":1.06,"yanchor":"bottom"}],"height":380,"showlegend":false},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('afd0f605-2086-49a1-b56e-130ce0a8d72f');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };            </script>        </div>
</div>
</div>
<p><strong>Key observations</strong>:</p>
<ul>
<li>Logits can be any real number (negative, zero, or positive)</li>
<li>Probabilities always sum to exactly 1.0 (see green box)</li>
<li><strong>Entropy</strong> measures uncertainty: lower = more confident, higher = more uncertain</li>
<li>Try â€œUniform Logitsâ€ vs â€œVery Confidentâ€ to see the extremes!</li>
</ul>
</div>
</div>
</div>
</section>
</section>
<section id="loss-function" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="loss-function"><span class="header-section-number">3</span> Loss Function</h2>
<p>Now we look at how to define the loss function <span class="math inline">\(L(\mathbf{X}, \mathbf{y}, \theta)\)</span> for a multi-class classification problem.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The loss function for a binary classification problem is identical. If we have a multi-label classification problem we have multiple outputs and thus multiple softmax functions. The principle is identical.</p>
</div>
</div>
<section id="likelihood" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="likelihood"><span class="header-section-number">3.1</span> Likelihood</h3>
<p>The likelihood of a data point <span class="math inline">\((\mathbf{x}^{(i)}, y^{(i)})\)</span> is the <strong>probability of observing a data point, given a specific model</strong>:</p>
<p><span class="math display">\[
P(Y=y^{(i)}| X = \mathbf{x}^{(i)}) = f(\theta, \mathbf{x}^{(i)})
\]</span></p>
<p>This means we formulate a model with a probabilistic interpretation of predictions: <span class="math inline">\(f(\theta, \mathbf{x}^{(i)}): \mathbb{R}^{n} \mapsto [0, 1]\)</span></p>
<p>For a multi-class classification, the label vector is one-hot encoded <span class="math inline">\(\mathbf{y}^{(i)} \in \{0, 1\}^K\)</span>, where the true class is coded with 1 and the rest with 0. The likelihood of a data point is defined as:</p>
<p><span class="math display">\[
\prod_{j=1}^K  P(Y = j| X = \mathbf{x}^{(i)})^{y^{(i)}_j}
\]</span></p>
<p>Since only one entry in <span class="math inline">\(\mathbf{y}^{(i)}\)</span> is 1, the likelihood is simply the prediction for the true class <span class="math inline">\(P(Y = y^{(i)}| X = \mathbf{x}^{(i)})\)</span>.</p>
<p><a href="http://www.awebb.info/probability/2017/05/18/cross-entropy-and-log-likelihood.html">More Info</a></p>
<div class="callout callout-style-simple callout-note callout-titled" title="ðŸ“ Quiz: Cat vs Dog Likelihood">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>ðŸ“ Quiz: Cat vs Dog Likelihood
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Scenario</strong>: Your CNN model classifies 5 images from our dataset. Letâ€™s explore how different prediction confidences affect likelihood!</p>
<p><strong>The Images</strong>: <img src="../../assets/images/classification/cats_and_dogs/cat_01.png" class="img-fluid" width="80" alt="Cat"> <img src="../../assets/images/classification/cats_and_dogs/dog_01.png" class="img-fluid" width="80" alt="Dog"> <img src="../../assets/images/classification/cats_and_dogs/cat_02.png" class="img-fluid" width="80" alt="Cat"> <img src="../../assets/images/classification/cats_and_dogs/dog_02.png" class="img-fluid" width="80" alt="Dog"> <img src="../../assets/images/classification/cats_and_dogs/dog_03.png" class="img-fluid" width="80" alt="Dog"></p>
<p><strong>True Labels</strong>: <span class="math inline">\(\mathbf{y} = [\text{Cat}, \text{Dog}, \text{Cat}, \text{Dog}, \text{Dog}]\)</span> or <span class="math inline">\([0, 1, 0, 1, 1]\)</span> in binary encoding.</p>
<p><strong>Question 1</strong>: Model A predicts: <span class="math inline">\(P(\text{Cat}) = [0.9, 0.2, 0.8, 0.3, 0.1]\)</span>. Whatâ€™s the likelihood?</p>
<p><strong>Think first</strong>: For each image, whatâ€™s the probability of the <em>correct</em> class?</p>
<details>
<summary>
Click for result
</summary>
<div id="301a18fc" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># True labels: [Cat=0, Dog=1, Cat=0, Dog=1, Dog=1]</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>y_true <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>])</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Model A predictions: P(Cat) for each image</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>p_cat <span class="op">=</span> np.array([<span class="fl">0.9</span>, <span class="fl">0.2</span>, <span class="fl">0.8</span>, <span class="fl">0.3</span>, <span class="fl">0.1</span>])</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>p_dog <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> p_cat  <span class="co"># P(Dog) = 1 - P(Cat)</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract probability of correct class for each image</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>p_correct <span class="op">=</span> np.where(y_true <span class="op">==</span> <span class="dv">0</span>, p_cat, p_dog)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"P(correct class): </span><span class="sc">{</span>p_correct<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Likelihood = product of correct probabilities</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>likelihood <span class="op">=</span> np.prod(p_correct)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Likelihood = </span><span class="sc">{</span>likelihood<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Individual contributions</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (true_label, prob) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(y_true, p_correct)):</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    label_name <span class="op">=</span> <span class="st">"Cat"</span> <span class="cf">if</span> true_label <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> <span class="st">"Dog"</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Image </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span>label_name<span class="sc">}</span><span class="ss">): P = </span><span class="sc">{</span>prob<span class="sc">:.1f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>P(correct class): [0.9 0.8 0.8 0.7 0.9]
Likelihood = 0.362880
Image 1 (Cat): P = 0.9
Image 2 (Dog): P = 0.8
Image 3 (Cat): P = 0.8
Image 4 (Dog): P = 0.7
Image 5 (Dog): P = 0.9</code></pre>
</div>
</div>
<p><strong>Interpretation</strong>: Model A is confident about the first cat (0.9) and the last dog (0.9), but uncertain about the middle images.</p>
</details>
<p><strong>Question 2</strong>: Model B predicts: <span class="math inline">\(P(\text{Cat}) = [0.8, 0.1, 0.9, 0.2, 0.1]\)</span>. Which model is better?</p>
<p><strong>Hint</strong>: Compare the likelihoods. Higher likelihood = better model!</p>
<details>
<summary>
Click for result
</summary>
<div id="4fb172ca" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Model B predictions</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>p_cat_B <span class="op">=</span> np.array([<span class="fl">0.8</span>, <span class="fl">0.1</span>, <span class="fl">0.9</span>, <span class="fl">0.2</span>, <span class="fl">0.1</span>])</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>p_dog_B <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> p_cat_B</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract probability of correct class</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>p_correct_B <span class="op">=</span> np.where(y_true <span class="op">==</span> <span class="dv">0</span>, p_cat_B, p_dog_B)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model B - P(correct class): </span><span class="sc">{</span>p_correct_B<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>likelihood_B <span class="op">=</span> np.prod(p_correct_B)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model B - Likelihood = </span><span class="sc">{</span>likelihood_B<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Comparison:"</span>)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model A: </span><span class="sc">{</span>likelihood<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model B: </span><span class="sc">{</span>likelihood_B<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model </span><span class="sc">{</span><span class="st">'B'</span> <span class="cf">if</span> likelihood_B <span class="op">&gt;</span> likelihood <span class="cf">else</span> <span class="st">'A'</span><span class="sc">}</span><span class="ss"> is better!"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model B - P(correct class): [0.8 0.9 0.9 0.8 0.9]
Model B - Likelihood = 0.466560

Comparison:
Model A: 0.362880
Model B: 0.466560
Model B is better!</code></pre>
</div>
</div>
<p><strong>Key Insight</strong>: Model B is more confident on the second cat (0.9 vs 0.8), leading to higher overall likelihood.</p>
</details>
<p><strong>Question 3</strong>: What if we have a poorly calibrated model? Model C: <span class="math inline">\(P(\text{Cat}) = [0.6, 0.4, 0.6, 0.4, 0.4]\)</span></p>
<p><strong>Consider</strong>: This model is not very confident about anything. How does this affect likelihood?</p>
<details>
<summary>
Click for result
</summary>
<div id="f98be48f" class="cell" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Model C: low confidence predictions</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>p_cat_C <span class="op">=</span> np.array([<span class="fl">0.6</span>, <span class="fl">0.4</span>, <span class="fl">0.6</span>, <span class="fl">0.4</span>, <span class="fl">0.4</span>])</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>p_dog_C <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> p_cat_C</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>p_correct_C <span class="op">=</span> np.where(y_true <span class="op">==</span> <span class="dv">0</span>, p_cat_C, p_dog_C)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model C - P(correct class): </span><span class="sc">{</span>p_correct_C<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>likelihood_C <span class="op">=</span> np.prod(p_correct_C)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model C - Likelihood = </span><span class="sc">{</span>likelihood_C<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">All Models Comparison:"</span>)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model A (mixed confidence): </span><span class="sc">{</span>likelihood<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model B (high confidence):  </span><span class="sc">{</span>likelihood_B<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model C (low confidence):   </span><span class="sc">{</span>likelihood_C<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Log-likelihoods (what we actually optimize)</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Negative Log-Likelihoods (Cross-Entropy Loss):"</span>)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model A: </span><span class="sc">{</span><span class="op">-</span>np<span class="sc">.</span>log(likelihood)<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model B: </span><span class="sc">{</span><span class="op">-</span>np<span class="sc">.</span>log(likelihood_B)<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model C: </span><span class="sc">{</span><span class="op">-</span>np<span class="sc">.</span>log(likelihood_C)<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Lower is better for loss!"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model C - P(correct class): [0.6 0.6 0.6 0.6 0.6]
Model C - Likelihood = 0.077760

All Models Comparison:
Model A (mixed confidence): 0.362880
Model B (high confidence):  0.466560
Model C (low confidence):   0.077760

Negative Log-Likelihoods (Cross-Entropy Loss):
Model A: 1.014
Model B: 0.762
Model C: 2.554
Lower is better for loss!</code></pre>
</div>
</div>
<p><strong>Critical Observation</strong>: - <strong>Confident correct predictions</strong> (Model B) â†’ High likelihood â†’ Low loss âœ… - <strong>Low confidence</strong> (Model C) â†’ Low likelihood â†’ High loss âŒ - <strong>This is why we train models to be confident when correct!</strong></p>
</details>
<p><strong>Question 4</strong>: What happens if we increase the dataset by ten fold? For example what happens with the likelihood of Model A if <span class="math inline">\(P(\text{Cat}) = [0.9, 0.2, 0.8, 0.3, 0.1]\)</span> is repeated 10 times?</p>
<details>
<summary>
Click for result
</summary>
<div id="5c03fd75" class="cell" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Repeat the dataset 10 times</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>p_cat_repeated <span class="op">=</span> np.tile(p_cat, <span class="dv">10</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>y_true_repeated <span class="op">=</span> np.tile(y_true, <span class="dv">10</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>p_dog_repeated <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> p_cat_repeated</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>p_correct_repeated <span class="op">=</span> np.where(y_true_repeated <span class="op">==</span> <span class="dv">0</span>, p_cat_repeated, p_dog_repeated)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Dataset repeated 10x (50 samples total)"</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"P(correct class) for first 5: </span><span class="sc">{</span>p_correct_repeated[:<span class="dv">5</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>likelihood_10x <span class="op">=</span> np.prod(p_correct_repeated)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Likelihood (10x dataset): </span><span class="sc">{</span>likelihood_10x<span class="sc">:.2e}</span><span class="ss">"</span>)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Original likelihood (1x):  </span><span class="sc">{</span>likelihood<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Ratio: </span><span class="sc">{</span>likelihood_10x <span class="op">/</span> likelihood<span class="sc">:.2e}</span><span class="ss">"</span>)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="co"># The key insight: likelihood shrinks exponentially!</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Numerical underflow risk!"</span>)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"This is why we use LOG-likelihood instead:"</span>)</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Log-likelihood (10x): </span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">sum</span>(np.log(p_correct_repeated))<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Log-likelihood (1x):  </span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">sum</span>(np.log(p_correct))<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The log version just scales linearly (10x) without numerical issues!"</span>)</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Lower is better for loss!"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Dataset repeated 10x (50 samples total)
P(correct class) for first 5: [0.9 0.8 0.8 0.7 0.9]

Likelihood (10x dataset): 3.96e-05
Original likelihood (1x):  0.362880

Ratio: 1.09e-04

Numerical underflow risk!
This is why we use LOG-likelihood instead:
Log-likelihood (10x): -10.137
Log-likelihood (1x):  -1.014
The log version just scales linearly (10x) without numerical issues!
Lower is better for loss!</code></pre>
</div>
</div>
<p><strong>Critical Observation</strong>:</p>
<ul>
<li>With 10x data, likelihood becomes TINY (underflow risk)</li>
<li>But log-likelihood scales linearly â†’ numerically stable!</li>
</ul>
</details>
<p><strong>Bonus Question</strong>: Whatâ€™s the perfect modelâ€™s likelihood?</p>
<details>
<summary>
Perfect Model Analysis
</summary>
<div id="9c22f056" class="cell" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Perfect model: 100% confident and always correct</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>p_cat_perfect <span class="op">=</span> np.array([<span class="fl">1.0</span>, <span class="fl">0.0</span>, <span class="fl">1.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>])  <span class="co"># Certain about each class</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>p_dog_perfect <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> p_cat_perfect</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>p_correct_perfect <span class="op">=</span> np.where(y_true <span class="op">==</span> <span class="dv">0</span>, p_cat_perfect, p_dog_perfect)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Perfect Model - P(correct class): </span><span class="sc">{</span>p_correct_perfect<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>likelihood_perfect <span class="op">=</span> np.prod(p_correct_perfect)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Perfect Model - Likelihood = </span><span class="sc">{</span>likelihood_perfect<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Perfect Model - Negative Log-Likelihood = </span><span class="sc">{</span><span class="op">-</span>np<span class="sc">.</span>log(likelihood_perfect)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">ðŸŽ¯ Maximum possible likelihood = 1.0"</span>)</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"ðŸŽ¯ Minimum possible loss = 0.0"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Perfect Model - P(correct class): [1. 1. 1. 1. 1.]
Perfect Model - Likelihood = 1.0
Perfect Model - Negative Log-Likelihood = -0.0

ðŸŽ¯ Maximum possible likelihood = 1.0
ðŸŽ¯ Minimum possible loss = 0.0</code></pre>
</div>
</div>
<p><strong>The Goal</strong>: Train your CNN to approach this perfect confidence on the training set (while generalizing to new data)!</p>
</details>
</div>
</div>
</div>
</section>
<section id="maximum-likelihood" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="maximum-likelihood"><span class="header-section-number">3.2</span> Maximum Likelihood</h3>
<p>The likelihood <span class="math inline">\(P(\mathbf{y} | \theta, \mathbf{X})\)</span> of observing our entire dataset <span class="math inline">\((\mathbf{X}, \mathbf{y})\)</span>, given the parameters <span class="math inline">\(\theta\)</span> and assuming that the data points <span class="math inline">\((\mathbf{x}^{(i)}, y^{(i)})\)</span> are independent and identically distributed, can be calculated as:</p>
<p><span class="math display">\[\begin{equation}
\prod_{i=1}^N \prod_{j=1}^K  P(Y = j| X = \mathbf{x}^{(i)})^{y^{(i)}_j}
\end{equation}\]</span></p>
<p>Under the Maximum Likelihood approach, we seek the parameters <span class="math inline">\(\theta\)</span> that maximize the likelihood of observing the dataset.</p>
</section>
<section id="negative-log-likelihood" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="negative-log-likelihood"><span class="header-section-number">3.3</span> Negative Log-Likelihood</h3>
<p>With Maximum Likelihood, we aim to choose the parameters <span class="math inline">\(\theta\)</span> such that <span class="math inline">\(p(\mathbf{y} | \theta, \mathbf{X})\)</span> is maximized. However, this function can be complex to handle, and we can use some mathematical tricks. We take the logarithm of the likelihood so that the product of probabilities becomes a sum. Since the logarithm is a monotonically increasing function, we can maximize its logarithm instead of the function <span class="math inline">\(p(\mathbf{y} | \theta, \mathbf{X})\)</span>. Finally, we take the negative of the function, allowing us to minimize it.</p>
<p><span class="math display">\[\begin{align}
L(\mathbf{X}, \mathbf{y}, \theta) &amp;= - \log \prod_{i=1}^N \prod_{j=1}^K P(Y = j| X = \mathbf{x}^{(i)})^{y^{(i)}_j} \\
&amp;= -\sum_{i=1}^N \sum_{j=1}^K y^{(i)}_j \log  P(Y = j| X = \mathbf{x}^{(i)})
\end{align}\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled" title="ðŸ’¡ Maximum Likelihood Intuition">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>ðŸ’¡ Maximum Likelihood Intuition
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Visualize the connection: Likelihood â†’ Log-Likelihood â†’ Cross-Entropy Loss</strong>:</p>
<div id="e464f057" class="cell" data-fig-width="800" data-execution_count="8">
<div class="cell-output cell-output-display">
<div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-3.2.0.min.js" integrity="sha256-iZ2u/oU2wf/vDbl/ChcX93WgbBRSBvUO6N413hDz7xM=" crossorigin="anonymous"></script>                <div id="21d8c068-1722-4587-99b6-374f2d42d05e" class="plotly-graph-div" style="height:850px; width:100%;"></div>            <script type="text/javascript">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById("21d8c068-1722-4587-99b6-374f2d42d05e")) {                    Plotly.newPlot(                        "21d8c068-1722-4587-99b6-374f2d42d05e",                        [{"fill":"tozeroy","hovertemplate":"Calibration: %{x:.3f}\u003cbr\u003eLikelihood: %{y:.6f}\u003cextra\u003e\u003c\u002fextra\u003e","line":{"color":"blue","width":3},"mode":"lines","name":"Likelihood","x":{"dtype":"f8","bdata":"AAAAAAAA4D+PtKCHP1DgPx5pQQ9\u002foOA\u002frR3ilr7w4D880oIe\u002fkDhP8uGI6Y9keE\u002fWjvELX3h4T\u002fo72S1vDHiP3ikBT38geI\u002fBlmmxDvS4j+VDUdMeyLjPyTC59O6cuM\u002fs3aIW\u002frC4z9CKynjORPkP9HfyWp5Y+Q\u002fYJRq8riz5D\u002fvSAt6+APlP379qwE4VOU\u002fDbJMiXek5T+cZu0Qt\u002fTlPysbjpj2ROY\u002fus8uIDaV5j9IhM+ndeXmP9g4cC+1Nec\u002fZu0Qt\u002fSF5z\u002f2obE+NNbnP4RWUsZzJug\u002fFAvzTbN26D+iv5PV8sboPzF0NF0yF+k\u002fwCjV5HFn6T9P3XVssbfpP96RFvTwB+o\u002fbUa3ezBY6j\u002f8+lcDcKjqP4uv+Iqv+Oo\u002fGmSZEu9I6z+oGDqaLpnrPzjN2iFu6es\u002fxoF7qa057D9WNhwx7YnsP+TqvLgs2uw\u002fdJ9dQGwq7T8CVP7Hq3rtP5EIn0\u002fryu0\u002fIL0\u002f1yob7j+vceBeamvuPz4mgeapu+4\u002fzdohbukL7z9cj8L1KFzvPw=="},"y":[0.03125,0.03443354766286089,0.03787142061220485,0.04157856137520928,0.045570486627498914,0.04986329801762563,0.05447369299154817,0.05941897561711188,0.06471706740852878,0.07038651815085659,0.07644651672447944,0.08291690192958687,0.08981817331065396,0.097171501980921,0.10499874144687331,0.11332243843272087,0.1221658437048782,0.13155292289644413,0.14150836733168146,0.1520576048504968,0.1632268106329203,0.17504291802358543,0.18753362935620865,0.20072742677806973,0.21465358307449006,0.22934217249331473,0.24482408156938934,0.26113101994904314,0.2782955312145653,0.2963510037086878,0.31533168135906303,0.3352726745027445,0.35620997071066646,0.3781804456121235,0.40122187371925055,0.4253729392515023,0.4506732469601336,0.4771633329526782,0.5048846755174302,0.5338797059479207,0.5641918193674025,0.5958653855533236,0.6289457597618137,0.6634792935521573,0.6995133456112795,0.7370962925782214,0.7762775398686221,0.8171075324991973,0.8596377659122201,0.9039207967999998],"type":"scatter","xaxis":"x","yaxis":"y"},{"fill":"tozeroy","hovertemplate":"Calibration: %{x:.3f}\u003cbr\u003eLog-Likelihood: %{y:.3f}\u003cextra\u003e\u003c\u002fextra\u003e","line":{"color":"green","width":3},"mode":"lines","name":"Log-Likelihood","x":{"dtype":"f8","bdata":"AAAAAAAA4D+PtKCHP1DgPx5pQQ9\u002foOA\u002frR3ilr7w4D880oIe\u002fkDhP8uGI6Y9keE\u002fWjvELX3h4T\u002fo72S1vDHiP3ikBT38geI\u002fBlmmxDvS4j+VDUdMeyLjPyTC59O6cuM\u002fs3aIW\u002frC4z9CKynjORPkP9HfyWp5Y+Q\u002fYJRq8riz5D\u002fvSAt6+APlP379qwE4VOU\u002fDbJMiXek5T+cZu0Qt\u002fTlPysbjpj2ROY\u002fus8uIDaV5j9IhM+ndeXmP9g4cC+1Nec\u002fZu0Qt\u002fSF5z\u002f2obE+NNbnP4RWUsZzJug\u002fFAvzTbN26D+iv5PV8sboPzF0NF0yF+k\u002fwCjV5HFn6T9P3XVssbfpP96RFvTwB+o\u002fbUa3ezBY6j\u002f8+lcDcKjqP4uv+Iqv+Oo\u002fGmSZEu9I6z+oGDqaLpnrPzjN2iFu6es\u002fxoF7qa057D9WNhwx7YnsP+TqvLgs2uw\u002fdJ9dQGwq7T8CVP7Hq3rtP5EIn0\u002fryu0\u002fIL0\u002f1yob7j+vceBeamvuPz4mgeapu+4\u002fzdohbukL7z9cj8L1KFzvPw=="},"y":[-3.4657359027997265,-3.3687239670659745,-3.2735585249068304,-3.180170596111644,-3.0884949950822365,-2.998470057514165,-2.9100373912516186,-2.8231416487948353,-2.7377303192404607,-2.6537535376962533,-2.5711639104380253,-2.4899163542738187,-2.409967948752084,-2.3312778000008842,-2.2538069151167033,-2.1775180861369585,-2.102375782731947,-2.028346052841538,-1.955396430561049,-1.883495850650785,-1.8126145691057773,-1.7427240892774107,-1.6737970930876913,-1.6058073769206063,-1.5387297918140836,-1.4725401876109037,-1.4072153607582394,-1.3427330054734203,-1.2790716680187924,-1.2162107038510803,-1.1541302374311302,-1.092811124498236,-1.0322349166298899,-0.9723838279228045,-0.9132407036446394,-0.8547889907181977,-0.7970127099110345,-0.7398964296135804,-0.6834252410981251,-0.6275847351594339,-0.5723609800454106,-0.5177405005932779,-0.4637102584930737,-0.4102576336061852,-0.35737040627192346,-0.3050367405401191,-0.2532451682721586,-0.2019845740570511,-0.15124418089289385,-0.10101353658759733],"type":"scatter","xaxis":"x2","yaxis":"y2"},{"fill":"tozeroy","hovertemplate":"Calibration: %{x:.3f}\u003cbr\u003eNLL Loss: %{y:.3f}\u003cextra\u003e\u003c\u002fextra\u003e","line":{"color":"red","width":3},"mode":"lines","name":"Negative Log-Likelihood","x":{"dtype":"f8","bdata":"AAAAAAAA4D+PtKCHP1DgPx5pQQ9\u002foOA\u002frR3ilr7w4D880oIe\u002fkDhP8uGI6Y9keE\u002fWjvELX3h4T\u002fo72S1vDHiP3ikBT38geI\u002fBlmmxDvS4j+VDUdMeyLjPyTC59O6cuM\u002fs3aIW\u002frC4z9CKynjORPkP9HfyWp5Y+Q\u002fYJRq8riz5D\u002fvSAt6+APlP379qwE4VOU\u002fDbJMiXek5T+cZu0Qt\u002fTlPysbjpj2ROY\u002fus8uIDaV5j9IhM+ndeXmP9g4cC+1Nec\u002fZu0Qt\u002fSF5z\u002f2obE+NNbnP4RWUsZzJug\u002fFAvzTbN26D+iv5PV8sboPzF0NF0yF+k\u002fwCjV5HFn6T9P3XVssbfpP96RFvTwB+o\u002fbUa3ezBY6j\u002f8+lcDcKjqP4uv+Iqv+Oo\u002fGmSZEu9I6z+oGDqaLpnrPzjN2iFu6es\u002fxoF7qa057D9WNhwx7YnsP+TqvLgs2uw\u002fdJ9dQGwq7T8CVP7Hq3rtP5EIn0\u002fryu0\u002fIL0\u002f1yob7j+vceBeamvuPz4mgeapu+4\u002fzdohbukL7z9cj8L1KFzvPw=="},"y":[3.4657359027997265,3.3687239670659745,3.2735585249068304,3.180170596111644,3.0884949950822365,2.998470057514165,2.9100373912516186,2.8231416487948353,2.7377303192404607,2.6537535376962533,2.5711639104380253,2.4899163542738187,2.409967948752084,2.3312778000008842,2.2538069151167033,2.1775180861369585,2.102375782731947,2.028346052841538,1.955396430561049,1.883495850650785,1.8126145691057773,1.7427240892774107,1.6737970930876913,1.6058073769206063,1.5387297918140836,1.4725401876109037,1.4072153607582394,1.3427330054734203,1.2790716680187924,1.2162107038510803,1.1541302374311302,1.092811124498236,1.0322349166298899,0.9723838279228045,0.9132407036446394,0.8547889907181977,0.7970127099110345,0.7398964296135804,0.6834252410981251,0.6275847351594339,0.5723609800454106,0.5177405005932779,0.4637102584930737,0.4102576336061852,0.35737040627192346,0.3050367405401191,0.2532451682721586,0.2019845740570511,0.15124418089289385,0.10101353658759733],"type":"scatter","xaxis":"x3","yaxis":"y3"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermap":[{"type":"scattermap","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"},"margin":{"b":0,"l":0,"r":0,"t":30}}},"xaxis":{"anchor":"y","domain":[0.0,1.0]},"yaxis":{"anchor":"x","domain":[0.7666666666666666,0.9999999999999999],"title":{"text":"Likelihood"}},"xaxis2":{"anchor":"y2","domain":[0.0,1.0]},"yaxis2":{"anchor":"x2","domain":[0.3833333333333333,0.6166666666666666],"title":{"text":"Log-Likelihood"}},"xaxis3":{"anchor":"y3","domain":[0.0,1.0],"title":{"text":"Model Calibration (probability assigned to correct class)"}},"yaxis3":{"anchor":"x3","domain":[0.0,0.2333333333333333],"title":{"text":"Loss"}},"annotations":[{"font":{"size":16},"showarrow":false,"text":"Likelihood: L(\u03b8) = \u220f\u1d62 p(y\u1d62|x\u1d62,\u03b8)  [Product of probabilities]","x":0.5,"xanchor":"center","xref":"paper","y":0.9999999999999999,"yanchor":"bottom","yref":"paper"},{"font":{"size":16},"showarrow":false,"text":"Log-Likelihood: log L(\u03b8) = \u03a3\u1d62 log p(y\u1d62|x\u1d62,\u03b8)  [Sum of log-probabilities]","x":0.5,"xanchor":"center","xref":"paper","y":0.6166666666666666,"yanchor":"bottom","yref":"paper"},{"font":{"size":16},"showarrow":false,"text":"Negative Log-Likelihood: -log L(\u03b8) = Cross-Entropy Loss","x":0.5,"xanchor":"center","xref":"paper","y":0.2333333333333333,"yanchor":"bottom","yref":"paper"},{"arrowhead":2,"ax":-50,"ay":0,"font":{"color":"darkblue","size":11},"showarrow":true,"text":"MAXIMIZE\u003cbr\u003eTHIS\u003cbr\u003e(Training Goal)","x":0.95,"xref":"x","y":0.4519603983999999,"yref":"y"},{"arrowhead":2,"ax":-50,"ay":0,"font":{"color":"darkgreen","size":11},"showarrow":true,"text":"MAXIMIZE\u003cbr\u003eTHIS\u003cbr\u003e(Equivalent Goal)","x":0.95,"xref":"x2","y":-0.050506768293798666,"yref":"y2"},{"arrowhead":2,"ax":-50,"ay":30,"font":{"color":"darkred","size":11},"showarrow":true,"text":"MINIMIZE\u003cbr\u003eTHIS\u003cbr\u003e(Actual Training Loss)","x":0.95,"xref":"x3","y":0.151520304881396,"yref":"y3"}],"title":{"text":"Maximum Likelihood \u2194 Cross-Entropy Connection\u003cbr\u003e\u003csub\u003eDataset: 5 samples (3 Dogs, 2 Cats)\u003c\u002fsub\u003e","y":1.0,"x":0.5,"xanchor":"center","yanchor":"top"},"height":850,"showlegend":false},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('21d8c068-1722-4587-99b6-374f2d42d05e');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };            </script>        </div>
</div>
</div>
<p><strong>Critical Insights</strong>:</p>
<ul>
<li><strong>X-axis</strong>: Model calibration = probability assigned to the correct class (ranges from 0.5=random guessing to 0.98=well-calibrated)</li>
<li><strong>Top panel</strong>: Likelihood shrinks exponentially with dataset size â†’ numerical underflow risk!</li>
<li><strong>Middle panel</strong>: Log-likelihood converts products to sums â†’ numerically stable, same maximum</li>
<li><strong>Bottom panel</strong>: Negative log-likelihood (NLL) = Cross-Entropy Loss used in training</li>
<li><strong>Key Connection</strong>: max L(Î¸) âŸº max log L(Î¸) âŸº min -log L(Î¸) âŸº min Cross-Entropy</li>
</ul>
<p>The visualization shows: as model calibration improves (x increases), likelihood increases, log-likelihood increases, and loss (NLL) decreases.</p>
</div>
</div>
</div>
</section>
<section id="cross-entropy" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="cross-entropy"><span class="header-section-number">3.4</span> Cross-Entropy</h3>
<p>The loss function, derived with Maximum Likelihood, can also be viewed through the lens of cross-entropy between two discrete probability functions. Specifically, we can calculate and minimize the cross-entropy between the true distribution <span class="math inline">\(\mathbf{y}^{(i)}\)</span> and the predicted <span class="math inline">\(\mathbf{\hat{y}}^{(i)}\)</span>. Cross-entropy comes from information theory and measures how many bits/nats on average are needed to describe an event of a probability distribution <span class="math inline">\(p(x)\)</span> when using the approximation <span class="math inline">\(q(x)\)</span>.</p>
<p><span class="math display">\[\begin{equation}
CE = - \sum_{x \in X} p(x) \log q(x) \\
CE = - \sum_{i=1}^N \sum_{j=1}^K y_j^{(i)} \log \hat{y}_j^{(i)}
\end{equation}\]</span></p>
<p>It is evident that cross-entropy is identical to the negative log-likelihood.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="ðŸ“Š Cross-Entropy Loss Landscape">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>ðŸ“Š Cross-Entropy Loss Landscape
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Visualize how cross-entropy penalizes predictions</strong>:</p>
<div id="d8c7075f" class="cell" data-fig-width="800" data-execution_count="9">
<div class="cell-output cell-output-display">
<div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-3.2.0.min.js" integrity="sha256-iZ2u/oU2wf/vDbl/ChcX93WgbBRSBvUO6N413hDz7xM=" crossorigin="anonymous"></script>                <div id="5244ad7c-987a-423b-8e89-f81b61518b72" class="plotly-graph-div" style="height:500px; width:100%;"></div>            <script type="text/javascript">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById("5244ad7c-987a-423b-8e89-f81b61518b72")) {                    Plotly.newPlot(                        "5244ad7c-987a-423b-8e89-f81b61518b72",                        [{"line":{"color":"blue","width":3},"mode":"lines","name":"True class = 1","x":{"dtype":"f8","bdata":"exSuR+F6hD8Lv1ubZmCUP9hz4JJcg54\u002fU5QyRSlTpD+67vRApGSpPyFJtzwfdq4\u002fw9E8HM3DsT\u002f3\u002fh2aiky0Pyos\u002fxdI1bY\u002fXVnglQVeuT+RhsETw+a7P8SzopGAb74\u002ffPDBBx98wD8Wh7LGfcDBP7Ado4XcBMM\u002fSbSTRDtJxD\u002fjSoQDmo3FP33hdML40cY\u002fFnhlgVcWyD+wDlZAtlrJP0qlRv8Un8o\u002f4zs3vnPjyz990id90ifNPxdpGDwxbM4\u002fsP8I+4+wzz8ly\u002fxcd3rQP3IWdbymHNE\u002fv2HtG9a+0T8MrWV7BWHSP1j43do0A9M\u002fpUNWOmSl0z\u002fyjs6Zk0fUPz\u002faRvnC6dQ\u002fjCW\u002fWPKL1T\u002fZcDe4IS7WPya8rxdR0NY\u002fcgcod4By1z+\u002fUqDWrxTYPwyeGDbfttg\u002fWemQlQ5Z2T+mNAn1PfvZP\u002fN\u002fgVRtndo\u002fP8v5s5w\u002f2z+MFnITzOHbP9lh6nL7g9w\u002fJq1i0iom3T9z+NoxWsjdP8BDU5GJat4\u002fDI\u002fL8LgM3z9Z2kNQ6K7fP9MS3teLKOA\u002fejiah6N54D8gXlY3u8rgP8aDEufSG+E\u002fbanOlups4T8Tz4pGAr7hP7r0RvYZD+I\u002fYBoDpjFg4j8GQL9VSbHiP61lewVhAuM\u002fU4s3tXhT4z\u002f6sPNkkKTjP6DWrxSo9eM\u002fR\u002fxrxL9G5D\u002ftISh015fkP5NH5CPv6OQ\u002fOm2g0wY65T\u002fgklyDHovlP4e4GDM23OU\u002fLd7U4k0t5j\u002fUA5GSZX7mP3opTUJ9z+Y\u002fIE8J8pQg5z\u002fHdMWhrHHnP22agVHEwuc\u002fFMA9AdwT6D+65fmw82ToP2ALtmALtug\u002fBzFyECMH6T+tVi7AOljpP1R86m9Sqek\u002f+qGmH2r66T+hx2LPgUvqP0ftHn+ZnOo\u002f7RLbLrHt6j+UOJfeyD7rPzpeU47gj+s\u002f4YMPPvjg6z+HqcvtDzLsPy7Ph50ng+w\u002f1PRDTT\u002fU7D96GgD9ViXtPyFAvKxudu0\u002fx2V4XIbH7T9uizQMnhjuPxSx8Lu1ae4\u002futasa8267j9h\u002fGgb5QvvPwciJcv8XO8\u002frkfhehSu7z8="},"y":{"dtype":"f8","bdata":"FVW1u7FrEkD\u002fnI5YMVYPQPbKEbpEGwxAN9L699TPCUBj8RNd4AcIQPsFAVstkwZAd9FZLvlXBUAvDWyt30YEQFUU4ajxVQNAazfnyGV+AkAtj59jZLsBQH8emdxZCQFAT1ZYso5lAEC+xjZMzJv\u002fP5x\u002fn\u002fFmgf4\u002f9t3T8zl5\u002fT9HHPS9D4H8P55RR78Tl\u002fs\u002f+FTWbL25+j8zwHOmwOf5P6Fx\u002fuoBIPk\u002ffjpwS41h+D+ddCRhj6v3P7ZzsMNP\u002ffY\u002ffYdNoCxW9j83bqcul7X1P6Sp59AQG\u002fU\u002fhoWcuSiG9D+AUkr7efbzP\u002fXfJuypa\u002fM\u002fzA5vzWbl8j+tAXWpZmPyP3XxS15m5fE\u002fnm4TzShr8T\u002f9J4MndvTwP2D+mVYbgfA\u002fmFNOdukQ8D\u002fRI8bFakfvP+zNQqqucu4\u002fvu5nFlWj7T+f5Ij4GdnsP4R\u002fmiq+E+w\u002ftz+l+wZT6z\u002f+N9jGvZbqP\u002fgnZpav3uk\u002f0cSe0Kwq6T\u002fvE\u002fLtiHroPwuQvTYazuc\u002ft23shzkl5z8DN5cdwn\u002fmP6lR7GKR3eU\u002fM8\u002fDxoY+5T9m61WUg6LkPwN9nc9qCeQ\u002fgt3+FCFz4z+Q2dh7jN\u002fiP3hwsHuUTuI\u002f8c2x0yHA4T8KPUl0HjThPwoInmp1quA\u002f8Xm\u002fzRIj4D\u002fBTbNaxzvfP4W+mAOsNd4\u002fea0FO7Ez3T\u002fcU9w+tjXcP8w7EM2bO9s\u002fIIWMDERF2j\u002fmM9B3klLZP2YqGslrY9g\u002fGU4C6LV31z9vw2DYV4\u002fWP8o3Z6o5qtU\u002fauXSa0TI1D8SZyAaYunTPwCYrJV9DdM\u002fuKmvlYI00j+iUgGdXV7RP7yDlu\u002f7itA\u002fDu9UEZd0zz8wVwwjdtjNPz\u002fstLFzQcw\u002fQdlDlW+vyj\u002f7dQHRSiLJPxG9MoXnmcc\u002fQfCb4SgWxj+uLc0Y85bEPz30KlQrHMM\u002f7rOlqLelwT+vjBQMfzPAP0qVUJbSir0\u002fbhHZ\u002f722uj9uTZsRk+q3P+\u002frNQAmJrU\u002fd0f6Z0xpsj\u002fm\u002f2V7umevP1sobYBhC6o\u002f6Kxy1UC9pD8320ufG\u002fqeP3fIshQAlZQ\u002fvNRvPkWVhD8="},"type":"scatter"},{"line":{"color":"red","width":3},"mode":"lines","name":"True class = 0","x":{"dtype":"f8","bdata":"exSuR+F6hD8Lv1ubZmCUP9hz4JJcg54\u002fU5QyRSlTpD+67vRApGSpPyFJtzwfdq4\u002fw9E8HM3DsT\u002f3\u002fh2aiky0Pyos\u002fxdI1bY\u002fXVnglQVeuT+RhsETw+a7P8SzopGAb74\u002ffPDBBx98wD8Wh7LGfcDBP7Ado4XcBMM\u002fSbSTRDtJxD\u002fjSoQDmo3FP33hdML40cY\u002fFnhlgVcWyD+wDlZAtlrJP0qlRv8Un8o\u002f4zs3vnPjyz990id90ifNPxdpGDwxbM4\u002fsP8I+4+wzz8ly\u002fxcd3rQP3IWdbymHNE\u002fv2HtG9a+0T8MrWV7BWHSP1j43do0A9M\u002fpUNWOmSl0z\u002fyjs6Zk0fUPz\u002faRvnC6dQ\u002fjCW\u002fWPKL1T\u002fZcDe4IS7WPya8rxdR0NY\u002fcgcod4By1z+\u002fUqDWrxTYPwyeGDbfttg\u002fWemQlQ5Z2T+mNAn1PfvZP\u002fN\u002fgVRtndo\u002fP8v5s5w\u002f2z+MFnITzOHbP9lh6nL7g9w\u002fJq1i0iom3T9z+NoxWsjdP8BDU5GJat4\u002fDI\u002fL8LgM3z9Z2kNQ6K7fP9MS3teLKOA\u002fejiah6N54D8gXlY3u8rgP8aDEufSG+E\u002fbanOlups4T8Tz4pGAr7hP7r0RvYZD+I\u002fYBoDpjFg4j8GQL9VSbHiP61lewVhAuM\u002fU4s3tXhT4z\u002f6sPNkkKTjP6DWrxSo9eM\u002fR\u002fxrxL9G5D\u002ftISh015fkP5NH5CPv6OQ\u002fOm2g0wY65T\u002fgklyDHovlP4e4GDM23OU\u002fLd7U4k0t5j\u002fUA5GSZX7mP3opTUJ9z+Y\u002fIE8J8pQg5z\u002fHdMWhrHHnP22agVHEwuc\u002fFMA9AdwT6D+65fmw82ToP2ALtmALtug\u002fBzFyECMH6T+tVi7AOljpP1R86m9Sqek\u002f+qGmH2r66T+hx2LPgUvqP0ftHn+ZnOo\u002f7RLbLrHt6j+UOJfeyD7rPzpeU47gj+s\u002f4YMPPvjg6z+HqcvtDzLsPy7Ph50ng+w\u002f1PRDTT\u002fU7D96GgD9ViXtPyFAvKxudu0\u002fx2V4XIbH7T9uizQMnhjuPxSx8Lu1ae4\u002futasa8267j9h\u002fGgb5QvvPwciJcv8XO8\u002frkfhehSu7z8="},"y":{"dtype":"f8","bdata":"vNRvPkWVhD9WyLIUAJWUPzfbS58b+p4\u002f16xy1UC9pD9bKG2AYQuqP+b\u002fZXu6Z68\u002fb0f6Z0xpsj\u002fv6zUAJia1P2VNmxGT6rc\u002fbhHZ\u002f722uj9KlVCW0oq9P6uMFAx\u002fM8A\u002f7rOlqLelwT899CpUKxzDP64tzRjzlsQ\u002fPPCb4SgWxj8RvTKF55nHP\u002ft1AdFKIsk\u002fQdlDlW+vyj8\u002f7LSxc0HMPytXDCN22M0\u002fDu9UEZd0zz+6g5bv+4rQP6JSAZ1dXtE\u002fuKmvlYI00j\u002f9l6yVfQ3TPxJnIBpi6dM\u002fauXSa0TI1D\u002fKN2eqOarVP2\u002fDYNhXj9Y\u002fF04C6LV31z9mKhrJa2PYP+Yz0HeSUtk\u002fIIWMDERF2j\u002fJOxDNmzvbP9xT3D62Ndw\u002fea0FO7Ez3T+FvpgDrDXeP8FNs1rHO98\u002f8Hm\u002fzRIj4D8KCJ5qdargPwo9SXQeNOE\u002f8c2x0yHA4T94cLB7lE7iP47Z2HuM3+I\u002fgt3+FCFz4z8DfZ3PagnkP2brVZSDouQ\u002fM8\u002fDxoY+5T+nUexikd3lPwI3lx3Cf+Y\u002ft23shzkl5z8LkL02Gs7nP+4T8u2Ieug\u002f0cSe0Kwq6T\u002f3J2aWr97pP\u002f432Ma9luo\u002ftj+l+wZT6z+Df5oqvhPsP5\u002fkiPgZ2ew\u002fve5nFlWj7T\u002fszUKqrnLuP88jxsVqR+8\u002fmFNOdukQ8D9g\u002fplWG4HwP\u002fwngyd29PA\u002fnm4TzShr8T908UteZuXxP60BdalmY\u002fI\u002fyw5vzWbl8j\u002f13ybsqWvzP4BSSvt59vM\u002fhYWcuSiG9D+kqefQEBv1PzZupy6XtfU\u002ffYdNoCxW9j+2c7DDT\u002f32P5t0JGGPq\u002fc\u002ffTpwS41h+D+gcf7qASD5PzPAc6bA5\u002fk\u002f91TWbL25+j+fUUe\u002fE5f7P0cc9L0Pgfw\u002f893T8zl5\u002fT+cf5\u002fxZoH+P7zGNkzMm\u002f8\u002fT1ZYso5lAEB+HpncWQkBQC6Pn2NkuwFAajfnyGV+AkBTFOGo8VUDQC8NbK3fRgRAddFZLvlXBUD7BQFbLZMGQGHxE13gBwhAMtL699TPCUD0yhG6RBsMQPecjlgxVg9AFVW1u7FrEkA="},"type":"scatter"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermap":[{"type":"scattermap","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"},"margin":{"b":0,"l":0,"r":0,"t":30}}},"title":{"text":"Cross-Entropy Loss: How wrong predictions are penalized"},"xaxis":{"title":{"text":"Predicted Probability for Class 1"}},"yaxis":{"title":{"text":"Cross-Entropy Loss"}},"hovermode":"x unified","height":500,"annotations":[{"arrowhead":2,"ax":50,"ay":-50,"showarrow":true,"text":"High penalty for\u003cbr\u003econfident wrong prediction!","x":0.1,"y":2.3},{"arrowhead":2,"ax":-50,"ay":-50,"showarrow":true,"text":"High penalty for\u003cbr\u003econfident wrong prediction!","x":0.9,"y":2.3}]},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('5244ad7c-987a-423b-8e89-f81b61518b72');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };            </script>        </div>
</div>
</div>
<p><strong>Key Insights</strong>:</p>
<ul>
<li>Loss approaches <strong>âˆž</strong> when model is confidently wrong (pâ†’0 for true class)</li>
<li>Loss approaches <strong>0</strong> when model is confidently correct (pâ†’1 for true class)</li>
<li><strong>Asymmetric penalty</strong>: Being wrong and confident is much worse than being uncertain</li>
</ul>
</div>
</div>
</div>
<div id="cell-fig-classification-cross-entropy" class="cell" data-fig-width="600" data-execution_count="10">
<div class="cell-output cell-output-display">
<div id="fig-classification-cross-entropy" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-classification-cross-entropy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="classification_files/figure-html/fig-classification-cross-entropy-output-1.png" width="854" height="278" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classification-cross-entropy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: True Distribution (left) and Predicted Distribution (right).
</figcaption>
</figure>
</div>
</div>
</div>
<p><a href="#fig-classification-cross-entropy" class="quarto-xref">Figure&nbsp;5</a> shows an example with a cross-entropy value of: 0.266.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="ðŸŽ¯ Interactive Multi-Class Cross-Entropy">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>ðŸŽ¯ Interactive Multi-Class Cross-Entropy
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>See how different predictions map to loss values on the curve</strong>:</p>
<div id="525ea1e0" class="cell" data-fig-width="800" data-execution_count="11">
<div class="cell-output cell-output-display">
<div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-3.2.0.min.js" integrity="sha256-iZ2u/oU2wf/vDbl/ChcX93WgbBRSBvUO6N413hDz7xM=" crossorigin="anonymous"></script>                <div id="b9d63290-471a-4726-a19e-6d156f290ac3" class="plotly-graph-div" style="height:700px; width:100%;"></div>            <script type="text/javascript">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById("b9d63290-471a-4726-a19e-6d156f290ac3")) {                    Plotly.newPlot(                        "b9d63290-471a-4726-a19e-6d156f290ac3",                        [{"marker":{"color":["lightcoral","lightgreen","lightcoral","lightcoral","lightcoral"]},"name":"Probabilities","text":["0.05","0.85","0.05","0.03","0.02"],"textposition":"outside","x":["Class 0","Class 1","Class 2","Class 3","Class 4"],"y":[0.05,0.85,0.05,0.03,0.02],"type":"bar","xaxis":"x","yaxis":"y"},{"line":{"color":"blue","width":3},"mode":"lines","name":"CE Loss Curve","showlegend":false,"x":{"dtype":"f8","bdata":"exSuR+F6hD8Lv1ubZmCUP9hz4JJcg54\u002fU5QyRSlTpD+67vRApGSpPyFJtzwfdq4\u002fw9E8HM3DsT\u002f3\u002fh2aiky0Pyos\u002fxdI1bY\u002fXVnglQVeuT+RhsETw+a7P8SzopGAb74\u002ffPDBBx98wD8Wh7LGfcDBP7Ado4XcBMM\u002fSbSTRDtJxD\u002fjSoQDmo3FP33hdML40cY\u002fFnhlgVcWyD+wDlZAtlrJP0qlRv8Un8o\u002f4zs3vnPjyz990id90ifNPxdpGDwxbM4\u002fsP8I+4+wzz8ly\u002fxcd3rQP3IWdbymHNE\u002fv2HtG9a+0T8MrWV7BWHSP1j43do0A9M\u002fpUNWOmSl0z\u002fyjs6Zk0fUPz\u002faRvnC6dQ\u002fjCW\u002fWPKL1T\u002fZcDe4IS7WPya8rxdR0NY\u002fcgcod4By1z+\u002fUqDWrxTYPwyeGDbfttg\u002fWemQlQ5Z2T+mNAn1PfvZP\u002fN\u002fgVRtndo\u002fP8v5s5w\u002f2z+MFnITzOHbP9lh6nL7g9w\u002fJq1i0iom3T9z+NoxWsjdP8BDU5GJat4\u002fDI\u002fL8LgM3z9Z2kNQ6K7fP9MS3teLKOA\u002fejiah6N54D8gXlY3u8rgP8aDEufSG+E\u002fbanOlups4T8Tz4pGAr7hP7r0RvYZD+I\u002fYBoDpjFg4j8GQL9VSbHiP61lewVhAuM\u002fU4s3tXhT4z\u002f6sPNkkKTjP6DWrxSo9eM\u002fR\u002fxrxL9G5D\u002ftISh015fkP5NH5CPv6OQ\u002fOm2g0wY65T\u002fgklyDHovlP4e4GDM23OU\u002fLd7U4k0t5j\u002fUA5GSZX7mP3opTUJ9z+Y\u002fIE8J8pQg5z\u002fHdMWhrHHnP22agVHEwuc\u002fFMA9AdwT6D+65fmw82ToP2ALtmALtug\u002fBzFyECMH6T+tVi7AOljpP1R86m9Sqek\u002f+qGmH2r66T+hx2LPgUvqP0ftHn+ZnOo\u002f7RLbLrHt6j+UOJfeyD7rPzpeU47gj+s\u002f4YMPPvjg6z+HqcvtDzLsPy7Ph50ng+w\u002f1PRDTT\u002fU7D96GgD9ViXtPyFAvKxudu0\u002fx2V4XIbH7T9uizQMnhjuPxSx8Lu1ae4\u002futasa8267j9h\u002fGgb5QvvPwciJcv8XO8\u002frkfhehSu7z8="},"y":{"dtype":"f8","bdata":"FVW1u7FrEkD\u002fnI5YMVYPQPbKEbpEGwxAN9L699TPCUBj8RNd4AcIQPsFAVstkwZAd9FZLvlXBUAvDWyt30YEQFUU4ajxVQNAazfnyGV+AkAtj59jZLsBQH8emdxZCQFAT1ZYso5lAEC+xjZMzJv\u002fP5x\u002fn\u002fFmgf4\u002f9t3T8zl5\u002fT9HHPS9D4H8P55RR78Tl\u002fs\u002f+FTWbL25+j8zwHOmwOf5P6Fx\u002fuoBIPk\u002ffjpwS41h+D+ddCRhj6v3P7ZzsMNP\u002ffY\u002ffYdNoCxW9j83bqcul7X1P6Sp59AQG\u002fU\u002fhoWcuSiG9D+AUkr7efbzP\u002fXfJuypa\u002fM\u002fzA5vzWbl8j+tAXWpZmPyP3XxS15m5fE\u002fnm4TzShr8T\u002f9J4MndvTwP2D+mVYbgfA\u002fmFNOdukQ8D\u002fRI8bFakfvP+zNQqqucu4\u002fvu5nFlWj7T+f5Ij4GdnsP4R\u002fmiq+E+w\u002ftz+l+wZT6z\u002f+N9jGvZbqP\u002fgnZpav3uk\u002f0cSe0Kwq6T\u002fvE\u002fLtiHroPwuQvTYazuc\u002ft23shzkl5z8DN5cdwn\u002fmP6lR7GKR3eU\u002fM8\u002fDxoY+5T9m61WUg6LkPwN9nc9qCeQ\u002fgt3+FCFz4z+Q2dh7jN\u002fiP3hwsHuUTuI\u002f8c2x0yHA4T8KPUl0HjThPwoInmp1quA\u002f8Xm\u002fzRIj4D\u002fBTbNaxzvfP4W+mAOsNd4\u002fea0FO7Ez3T\u002fcU9w+tjXcP8w7EM2bO9s\u002fIIWMDERF2j\u002fmM9B3klLZP2YqGslrY9g\u002fGU4C6LV31z9vw2DYV4\u002fWP8o3Z6o5qtU\u002fauXSa0TI1D8SZyAaYunTPwCYrJV9DdM\u002fuKmvlYI00j+iUgGdXV7RP7yDlu\u002f7itA\u002fDu9UEZd0zz8wVwwjdtjNPz\u002fstLFzQcw\u002fQdlDlW+vyj\u002f7dQHRSiLJPxG9MoXnmcc\u002fQfCb4SgWxj+uLc0Y85bEPz30KlQrHMM\u002f7rOlqLelwT+vjBQMfzPAP0qVUJbSir0\u002fbhHZ\u002f722uj9uTZsRk+q3P+\u002frNQAmJrU\u002fd0f6Z0xpsj\u002fm\u002f2V7umevP1sobYBhC6o\u002f6Kxy1UC9pD8320ufG\u002fqeP3fIshQAlZQ\u002fvNRvPkWVhD8="},"type":"scatter","xaxis":"x2","yaxis":"y2"},{"hovertemplate":"%{text}\u003cextra\u003e\u003c\u002fextra\u003e","marker":{"color":"red","line":{"color":"darkred","width":2},"size":15,"symbol":"circle"},"mode":"markers","name":"Confident Correct","showlegend":false,"text":["Confident Correct\u003cbr\u003ep=0.850\u003cbr\u003eLoss=0.163"],"x":[0.85],"y":[0.16251892938012788],"type":"scatter","xaxis":"x2","yaxis":"y2"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermap":[{"type":"scattermap","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"},"margin":{"b":0,"l":0,"r":0,"t":30}}},"xaxis":{"anchor":"y","domain":[0.0,1.0]},"yaxis":{"anchor":"x","domain":[0.6175,1.0],"range":[0,1.1],"title":{"text":"Probability"},"automargin":true},"xaxis2":{"anchor":"y2","domain":[0.0,1.0],"title":{"text":"Predicted Probability for True Class (Class 1)"}},"yaxis2":{"anchor":"x2","domain":[0.0,0.4675],"title":{"text":"Cross-Entropy Loss","standoff":20},"range":[0,5],"automargin":true},"annotations":[{"font":{"size":16},"showarrow":false,"text":"Predicted Probabilities","x":0.5,"xanchor":"center","xref":"paper","y":1.0,"yanchor":"bottom","yref":"paper","yshift":-2},{"font":{"size":16},"showarrow":false,"text":"Cross-Entropy Loss Landscape - Red Dot Shows Current Loss","x":0.5,"xanchor":"center","xref":"paper","y":1.0,"yanchor":"bottom","yref":"paper","yshift":-10},{"font":{"color":"green","family":"Arial Black","size":12},"showarrow":false,"text":"\u2190 TRUE LABEL","x":"Class 1","xref":"x","y":1.05,"yref":"y"}],"margin":{"t":120,"r":40,"b":60,"l":70},"title":{"pad":{"t":6,"b":6},"text":"Scenario: Confident Correct | Probability for True Class = 0.850 | Loss = 0.163","y":0.985,"yanchor":"top","x":0.5,"xanchor":"center"},"updatemenus":[{"bgcolor":"white","bordercolor":"#ddd","borderwidth":1,"buttons":[{"args":[{"y":[[0.0,1.0,0.0,0.0,0.0],[4.605170185988091,3.9170863073846927,3.5133146797828614,3.22648042425342,3.003845908328161,2.8218638524710946,2.667955743900332,2.534606318340273,2.4169648355187214,2.3117175765573186,2.2165000708645466,2.1295659288624047,2.049588578530155,1.9755366303547102,1.9065923155212436,1.8420962833896488,1.7815091533718872,1.724384066755284,1.6703466655210928,1.619080209926335,1.5703143290948918,1.523816389733525,1.4793847841859027,1.4368436473953188,1.3960386525945687,1.356833631746566,1.31910783389558,1.2827536821790502,1.2476749244716814,1.2137850975686864,1.1810062432058883,1.1492678279320032,1.1185058291876804,1.0886619578149417,1.0596829932642315,1.0315202124474112,1.004128896839779,0.9774679053090215,0.9514993024262686,0.9261880338303496,0.9015016416734801,0.8774100143527765,0.8538851656867256,0.8309010394748382,0.8084333360177416,0.7864593577015971,0.7649578711867643,0.7439089841032013,0.723294034457857,0.7030954912131339,0.683296864709111,0.6638826257826395,0.6448381325893535,0.6261495642646938,0.6078038606710126,0.5897886675727921,0.5720922866635325,0.5547036299380449,0.5376121779644631,0.5208079416627254,0.5042814272417769,0.4880236039873261,0.4720258746265034,0.45628004802593797,0.44077831400619316,0.42551322007869596,0.41047764993170865,0.3956648035098723,0.38106817854774133,0.3666815534317905,0.3524989712778312,0.3385147251218511,0.3247233441321301,0.31111958075926893,0.2976983987485937,0.28445496194640496,0.2713846238378129,0.2584829177595187,0.24574554773595775,0.23316837989175943,0.22074743439756792,0.20847887790996292,0.1963590164695431,0.1843842888242553,0.17255126014777214,0.16085661612519914,0.1492971573806318,0.1378697942231208,0.1265715416894628,0.11539951486391434,0.10435092445647395,0.09342307262277469,0.08261334900991789,0.0719192270137493,0.061338260234151495,0.0508680791159157,0.04050638776365095,0.030250960920024297,0.020099641097381906,0.01005033585350145],[-1.000000082690371e-10]],"text":[["0.00","1.00","0.00","0.00","0.00"],null,["Perfect Prediction\u003cbr\u003ep=1.000\u003cbr\u003eLoss=-0.000"]],"x":[["Class 0","Class 1","Class 2","Class 3","Class 4"],[0.01,0.0198989898989899,0.029797979797979796,0.039696969696969696,0.049595959595959596,0.059494949494949496,0.06939393939393938,0.07929292929292929,0.08919191919191918,0.09909090909090908,0.10898989898989898,0.11888888888888888,0.12878787878787878,0.1386868686868687,0.1485858585858586,0.15848484848484848,0.16838383838383839,0.1782828282828283,0.18818181818181817,0.19808080808080808,0.207979797979798,0.21787878787878787,0.22777777777777777,0.23767676767676768,0.24757575757575756,0.25747474747474747,0.2673737373737374,0.2772727272727273,0.2871717171717172,0.29707070707070704,0.30696969696969695,0.31686868686868686,0.32676767676767676,0.33666666666666667,0.3465656565656566,0.3564646464646465,0.36636363636363634,0.37626262626262624,0.38616161616161615,0.39606060606060606,0.40595959595959596,0.41585858585858587,0.4257575757575757,0.43565656565656563,0.44555555555555554,0.45545454545454545,0.46535353535353535,0.47525252525252526,0.4851515151515151,0.495050505050505,0.5049494949494949,0.5148484848484849,0.5247474747474747,0.5346464646464646,0.5445454545454546,0.5544444444444444,0.5643434343434344,0.5742424242424242,0.5841414141414141,0.594040404040404,0.6039393939393939,0.6138383838383838,0.6237373737373737,0.6336363636363637,0.6435353535353535,0.6534343434343434,0.6633333333333333,0.6732323232323232,0.6831313131313131,0.693030303030303,0.702929292929293,0.7128282828282828,0.7227272727272727,0.7326262626262626,0.7425252525252525,0.7524242424242424,0.7623232323232323,0.7722222222222221,0.7821212121212121,0.792020202020202,0.8019191919191919,0.8118181818181818,0.8217171717171717,0.8316161616161616,0.8415151515151514,0.8514141414141414,0.8613131313131313,0.8712121212121212,0.8811111111111111,0.891010101010101,0.9009090909090909,0.9108080808080807,0.9207070707070707,0.9306060606060605,0.9405050505050505,0.9504040404040404,0.9603030303030302,0.9702020202020202,0.98010101010101,0.99],[1.0]]},{"title.text":"Scenario: Perfect Prediction | Probability for True Class = 1.000 | Loss = -0.000"}],"label":"Perfect Prediction","method":"update"},{"args":[{"y":[[0.05,0.85,0.05,0.03,0.02],[4.605170185988091,3.9170863073846927,3.5133146797828614,3.22648042425342,3.003845908328161,2.8218638524710946,2.667955743900332,2.534606318340273,2.4169648355187214,2.3117175765573186,2.2165000708645466,2.1295659288624047,2.049588578530155,1.9755366303547102,1.9065923155212436,1.8420962833896488,1.7815091533718872,1.724384066755284,1.6703466655210928,1.619080209926335,1.5703143290948918,1.523816389733525,1.4793847841859027,1.4368436473953188,1.3960386525945687,1.356833631746566,1.31910783389558,1.2827536821790502,1.2476749244716814,1.2137850975686864,1.1810062432058883,1.1492678279320032,1.1185058291876804,1.0886619578149417,1.0596829932642315,1.0315202124474112,1.004128896839779,0.9774679053090215,0.9514993024262686,0.9261880338303496,0.9015016416734801,0.8774100143527765,0.8538851656867256,0.8309010394748382,0.8084333360177416,0.7864593577015971,0.7649578711867643,0.7439089841032013,0.723294034457857,0.7030954912131339,0.683296864709111,0.6638826257826395,0.6448381325893535,0.6261495642646938,0.6078038606710126,0.5897886675727921,0.5720922866635325,0.5547036299380449,0.5376121779644631,0.5208079416627254,0.5042814272417769,0.4880236039873261,0.4720258746265034,0.45628004802593797,0.44077831400619316,0.42551322007869596,0.41047764993170865,0.3956648035098723,0.38106817854774133,0.3666815534317905,0.3524989712778312,0.3385147251218511,0.3247233441321301,0.31111958075926893,0.2976983987485937,0.28445496194640496,0.2713846238378129,0.2584829177595187,0.24574554773595775,0.23316837989175943,0.22074743439756792,0.20847887790996292,0.1963590164695431,0.1843842888242553,0.17255126014777214,0.16085661612519914,0.1492971573806318,0.1378697942231208,0.1265715416894628,0.11539951486391434,0.10435092445647395,0.09342307262277469,0.08261334900991789,0.0719192270137493,0.061338260234151495,0.0508680791159157,0.04050638776365095,0.030250960920024297,0.020099641097381906,0.01005033585350145],[0.16251892938012788]],"text":[["0.05","0.85","0.05","0.03","0.02"],null,["Confident Correct\u003cbr\u003ep=0.850\u003cbr\u003eLoss=0.163"]],"x":[["Class 0","Class 1","Class 2","Class 3","Class 4"],[0.01,0.0198989898989899,0.029797979797979796,0.039696969696969696,0.049595959595959596,0.059494949494949496,0.06939393939393938,0.07929292929292929,0.08919191919191918,0.09909090909090908,0.10898989898989898,0.11888888888888888,0.12878787878787878,0.1386868686868687,0.1485858585858586,0.15848484848484848,0.16838383838383839,0.1782828282828283,0.18818181818181817,0.19808080808080808,0.207979797979798,0.21787878787878787,0.22777777777777777,0.23767676767676768,0.24757575757575756,0.25747474747474747,0.2673737373737374,0.2772727272727273,0.2871717171717172,0.29707070707070704,0.30696969696969695,0.31686868686868686,0.32676767676767676,0.33666666666666667,0.3465656565656566,0.3564646464646465,0.36636363636363634,0.37626262626262624,0.38616161616161615,0.39606060606060606,0.40595959595959596,0.41585858585858587,0.4257575757575757,0.43565656565656563,0.44555555555555554,0.45545454545454545,0.46535353535353535,0.47525252525252526,0.4851515151515151,0.495050505050505,0.5049494949494949,0.5148484848484849,0.5247474747474747,0.5346464646464646,0.5445454545454546,0.5544444444444444,0.5643434343434344,0.5742424242424242,0.5841414141414141,0.594040404040404,0.6039393939393939,0.6138383838383838,0.6237373737373737,0.6336363636363637,0.6435353535353535,0.6534343434343434,0.6633333333333333,0.6732323232323232,0.6831313131313131,0.693030303030303,0.702929292929293,0.7128282828282828,0.7227272727272727,0.7326262626262626,0.7425252525252525,0.7524242424242424,0.7623232323232323,0.7722222222222221,0.7821212121212121,0.792020202020202,0.8019191919191919,0.8118181818181818,0.8217171717171717,0.8316161616161616,0.8415151515151514,0.8514141414141414,0.8613131313131313,0.8712121212121212,0.8811111111111111,0.891010101010101,0.9009090909090909,0.9108080808080807,0.9207070707070707,0.9306060606060605,0.9405050505050505,0.9504040404040404,0.9603030303030302,0.9702020202020202,0.98010101010101,0.99],[0.85]]},{"title.text":"Scenario: Confident Correct | Probability for True Class = 0.850 | Loss = 0.163"}],"label":"Confident Correct","method":"update"},{"args":[{"y":[[0.15,0.35,0.2,0.15,0.15],[4.605170185988091,3.9170863073846927,3.5133146797828614,3.22648042425342,3.003845908328161,2.8218638524710946,2.667955743900332,2.534606318340273,2.4169648355187214,2.3117175765573186,2.2165000708645466,2.1295659288624047,2.049588578530155,1.9755366303547102,1.9065923155212436,1.8420962833896488,1.7815091533718872,1.724384066755284,1.6703466655210928,1.619080209926335,1.5703143290948918,1.523816389733525,1.4793847841859027,1.4368436473953188,1.3960386525945687,1.356833631746566,1.31910783389558,1.2827536821790502,1.2476749244716814,1.2137850975686864,1.1810062432058883,1.1492678279320032,1.1185058291876804,1.0886619578149417,1.0596829932642315,1.0315202124474112,1.004128896839779,0.9774679053090215,0.9514993024262686,0.9261880338303496,0.9015016416734801,0.8774100143527765,0.8538851656867256,0.8309010394748382,0.8084333360177416,0.7864593577015971,0.7649578711867643,0.7439089841032013,0.723294034457857,0.7030954912131339,0.683296864709111,0.6638826257826395,0.6448381325893535,0.6261495642646938,0.6078038606710126,0.5897886675727921,0.5720922866635325,0.5547036299380449,0.5376121779644631,0.5208079416627254,0.5042814272417769,0.4880236039873261,0.4720258746265034,0.45628004802593797,0.44077831400619316,0.42551322007869596,0.41047764993170865,0.3956648035098723,0.38106817854774133,0.3666815534317905,0.3524989712778312,0.3385147251218511,0.3247233441321301,0.31111958075926893,0.2976983987485937,0.28445496194640496,0.2713846238378129,0.2584829177595187,0.24574554773595775,0.23316837989175943,0.22074743439756792,0.20847887790996292,0.1963590164695431,0.1843842888242553,0.17255126014777214,0.16085661612519914,0.1492971573806318,0.1378697942231208,0.1265715416894628,0.11539951486391434,0.10435092445647395,0.09342307262277469,0.08261334900991789,0.0719192270137493,0.061338260234151495,0.0508680791159157,0.04050638776365095,0.030250960920024297,0.020099641097381906,0.01005033585350145],[1.0498221242129635]],"text":[["0.15","0.35","0.20","0.15","0.15"],null,["Uncertain Correct\u003cbr\u003ep=0.350\u003cbr\u003eLoss=1.050"]],"x":[["Class 0","Class 1","Class 2","Class 3","Class 4"],[0.01,0.0198989898989899,0.029797979797979796,0.039696969696969696,0.049595959595959596,0.059494949494949496,0.06939393939393938,0.07929292929292929,0.08919191919191918,0.09909090909090908,0.10898989898989898,0.11888888888888888,0.12878787878787878,0.1386868686868687,0.1485858585858586,0.15848484848484848,0.16838383838383839,0.1782828282828283,0.18818181818181817,0.19808080808080808,0.207979797979798,0.21787878787878787,0.22777777777777777,0.23767676767676768,0.24757575757575756,0.25747474747474747,0.2673737373737374,0.2772727272727273,0.2871717171717172,0.29707070707070704,0.30696969696969695,0.31686868686868686,0.32676767676767676,0.33666666666666667,0.3465656565656566,0.3564646464646465,0.36636363636363634,0.37626262626262624,0.38616161616161615,0.39606060606060606,0.40595959595959596,0.41585858585858587,0.4257575757575757,0.43565656565656563,0.44555555555555554,0.45545454545454545,0.46535353535353535,0.47525252525252526,0.4851515151515151,0.495050505050505,0.5049494949494949,0.5148484848484849,0.5247474747474747,0.5346464646464646,0.5445454545454546,0.5544444444444444,0.5643434343434344,0.5742424242424242,0.5841414141414141,0.594040404040404,0.6039393939393939,0.6138383838383838,0.6237373737373737,0.6336363636363637,0.6435353535353535,0.6534343434343434,0.6633333333333333,0.6732323232323232,0.6831313131313131,0.693030303030303,0.702929292929293,0.7128282828282828,0.7227272727272727,0.7326262626262626,0.7425252525252525,0.7524242424242424,0.7623232323232323,0.7722222222222221,0.7821212121212121,0.792020202020202,0.8019191919191919,0.8118181818181818,0.8217171717171717,0.8316161616161616,0.8415151515151514,0.8514141414141414,0.8613131313131313,0.8712121212121212,0.8811111111111111,0.891010101010101,0.9009090909090909,0.9108080808080807,0.9207070707070707,0.9306060606060605,0.9405050505050505,0.9504040404040404,0.9603030303030302,0.9702020202020202,0.98010101010101,0.99],[0.35]]},{"title.text":"Scenario: Uncertain Correct | Probability for True Class = 0.350 | Loss = 1.050"}],"label":"Uncertain Correct","method":"update"},{"args":[{"y":[[0.85,0.05,0.05,0.03,0.02],[4.605170185988091,3.9170863073846927,3.5133146797828614,3.22648042425342,3.003845908328161,2.8218638524710946,2.667955743900332,2.534606318340273,2.4169648355187214,2.3117175765573186,2.2165000708645466,2.1295659288624047,2.049588578530155,1.9755366303547102,1.9065923155212436,1.8420962833896488,1.7815091533718872,1.724384066755284,1.6703466655210928,1.619080209926335,1.5703143290948918,1.523816389733525,1.4793847841859027,1.4368436473953188,1.3960386525945687,1.356833631746566,1.31910783389558,1.2827536821790502,1.2476749244716814,1.2137850975686864,1.1810062432058883,1.1492678279320032,1.1185058291876804,1.0886619578149417,1.0596829932642315,1.0315202124474112,1.004128896839779,0.9774679053090215,0.9514993024262686,0.9261880338303496,0.9015016416734801,0.8774100143527765,0.8538851656867256,0.8309010394748382,0.8084333360177416,0.7864593577015971,0.7649578711867643,0.7439089841032013,0.723294034457857,0.7030954912131339,0.683296864709111,0.6638826257826395,0.6448381325893535,0.6261495642646938,0.6078038606710126,0.5897886675727921,0.5720922866635325,0.5547036299380449,0.5376121779644631,0.5208079416627254,0.5042814272417769,0.4880236039873261,0.4720258746265034,0.45628004802593797,0.44077831400619316,0.42551322007869596,0.41047764993170865,0.3956648035098723,0.38106817854774133,0.3666815534317905,0.3524989712778312,0.3385147251218511,0.3247233441321301,0.31111958075926893,0.2976983987485937,0.28445496194640496,0.2713846238378129,0.2584829177595187,0.24574554773595775,0.23316837989175943,0.22074743439756792,0.20847887790996292,0.1963590164695431,0.1843842888242553,0.17255126014777214,0.16085661612519914,0.1492971573806318,0.1378697942231208,0.1265715416894628,0.11539951486391434,0.10435092445647395,0.09342307262277469,0.08261334900991789,0.0719192270137493,0.061338260234151495,0.0508680791159157,0.04050638776365095,0.030250960920024297,0.020099641097381906,0.01005033585350145],[2.995732271553991]],"text":[["0.85","0.05","0.05","0.03","0.02"],null,["Confident Wrong\u003cbr\u003ep=0.050\u003cbr\u003eLoss=2.996"]],"x":[["Class 0","Class 1","Class 2","Class 3","Class 4"],[0.01,0.0198989898989899,0.029797979797979796,0.039696969696969696,0.049595959595959596,0.059494949494949496,0.06939393939393938,0.07929292929292929,0.08919191919191918,0.09909090909090908,0.10898989898989898,0.11888888888888888,0.12878787878787878,0.1386868686868687,0.1485858585858586,0.15848484848484848,0.16838383838383839,0.1782828282828283,0.18818181818181817,0.19808080808080808,0.207979797979798,0.21787878787878787,0.22777777777777777,0.23767676767676768,0.24757575757575756,0.25747474747474747,0.2673737373737374,0.2772727272727273,0.2871717171717172,0.29707070707070704,0.30696969696969695,0.31686868686868686,0.32676767676767676,0.33666666666666667,0.3465656565656566,0.3564646464646465,0.36636363636363634,0.37626262626262624,0.38616161616161615,0.39606060606060606,0.40595959595959596,0.41585858585858587,0.4257575757575757,0.43565656565656563,0.44555555555555554,0.45545454545454545,0.46535353535353535,0.47525252525252526,0.4851515151515151,0.495050505050505,0.5049494949494949,0.5148484848484849,0.5247474747474747,0.5346464646464646,0.5445454545454546,0.5544444444444444,0.5643434343434344,0.5742424242424242,0.5841414141414141,0.594040404040404,0.6039393939393939,0.6138383838383838,0.6237373737373737,0.6336363636363637,0.6435353535353535,0.6534343434343434,0.6633333333333333,0.6732323232323232,0.6831313131313131,0.693030303030303,0.702929292929293,0.7128282828282828,0.7227272727272727,0.7326262626262626,0.7425252525252525,0.7524242424242424,0.7623232323232323,0.7722222222222221,0.7821212121212121,0.792020202020202,0.8019191919191919,0.8118181818181818,0.8217171717171717,0.8316161616161616,0.8415151515151514,0.8514141414141414,0.8613131313131313,0.8712121212121212,0.8811111111111111,0.891010101010101,0.9009090909090909,0.9108080808080807,0.9207070707070707,0.9306060606060605,0.9405050505050505,0.9504040404040404,0.9603030303030302,0.9702020202020202,0.98010101010101,0.99],[0.05]]},{"title.text":"Scenario: Confident Wrong | Probability for True Class = 0.050 | Loss = 2.996"}],"label":"Confident Wrong","method":"update"},{"args":[{"y":[[0.2,0.2,0.2,0.2,0.2],[4.605170185988091,3.9170863073846927,3.5133146797828614,3.22648042425342,3.003845908328161,2.8218638524710946,2.667955743900332,2.534606318340273,2.4169648355187214,2.3117175765573186,2.2165000708645466,2.1295659288624047,2.049588578530155,1.9755366303547102,1.9065923155212436,1.8420962833896488,1.7815091533718872,1.724384066755284,1.6703466655210928,1.619080209926335,1.5703143290948918,1.523816389733525,1.4793847841859027,1.4368436473953188,1.3960386525945687,1.356833631746566,1.31910783389558,1.2827536821790502,1.2476749244716814,1.2137850975686864,1.1810062432058883,1.1492678279320032,1.1185058291876804,1.0886619578149417,1.0596829932642315,1.0315202124474112,1.004128896839779,0.9774679053090215,0.9514993024262686,0.9261880338303496,0.9015016416734801,0.8774100143527765,0.8538851656867256,0.8309010394748382,0.8084333360177416,0.7864593577015971,0.7649578711867643,0.7439089841032013,0.723294034457857,0.7030954912131339,0.683296864709111,0.6638826257826395,0.6448381325893535,0.6261495642646938,0.6078038606710126,0.5897886675727921,0.5720922866635325,0.5547036299380449,0.5376121779644631,0.5208079416627254,0.5042814272417769,0.4880236039873261,0.4720258746265034,0.45628004802593797,0.44077831400619316,0.42551322007869596,0.41047764993170865,0.3956648035098723,0.38106817854774133,0.3666815534317905,0.3524989712778312,0.3385147251218511,0.3247233441321301,0.31111958075926893,0.2976983987485937,0.28445496194640496,0.2713846238378129,0.2584829177595187,0.24574554773595775,0.23316837989175943,0.22074743439756792,0.20847887790996292,0.1963590164695431,0.1843842888242553,0.17255126014777214,0.16085661612519914,0.1492971573806318,0.1378697942231208,0.1265715416894628,0.11539951486391434,0.10435092445647395,0.09342307262277469,0.08261334900991789,0.0719192270137493,0.061338260234151495,0.0508680791159157,0.04050638776365095,0.030250960920024297,0.020099641097381906,0.01005033585350145],[1.6094379119341002]],"text":[["0.20","0.20","0.20","0.20","0.20"],null,["Uniform (Random)\u003cbr\u003ep=0.200\u003cbr\u003eLoss=1.609"]],"x":[["Class 0","Class 1","Class 2","Class 3","Class 4"],[0.01,0.0198989898989899,0.029797979797979796,0.039696969696969696,0.049595959595959596,0.059494949494949496,0.06939393939393938,0.07929292929292929,0.08919191919191918,0.09909090909090908,0.10898989898989898,0.11888888888888888,0.12878787878787878,0.1386868686868687,0.1485858585858586,0.15848484848484848,0.16838383838383839,0.1782828282828283,0.18818181818181817,0.19808080808080808,0.207979797979798,0.21787878787878787,0.22777777777777777,0.23767676767676768,0.24757575757575756,0.25747474747474747,0.2673737373737374,0.2772727272727273,0.2871717171717172,0.29707070707070704,0.30696969696969695,0.31686868686868686,0.32676767676767676,0.33666666666666667,0.3465656565656566,0.3564646464646465,0.36636363636363634,0.37626262626262624,0.38616161616161615,0.39606060606060606,0.40595959595959596,0.41585858585858587,0.4257575757575757,0.43565656565656563,0.44555555555555554,0.45545454545454545,0.46535353535353535,0.47525252525252526,0.4851515151515151,0.495050505050505,0.5049494949494949,0.5148484848484849,0.5247474747474747,0.5346464646464646,0.5445454545454546,0.5544444444444444,0.5643434343434344,0.5742424242424242,0.5841414141414141,0.594040404040404,0.6039393939393939,0.6138383838383838,0.6237373737373737,0.6336363636363637,0.6435353535353535,0.6534343434343434,0.6633333333333333,0.6732323232323232,0.6831313131313131,0.693030303030303,0.702929292929293,0.7128282828282828,0.7227272727272727,0.7326262626262626,0.7425252525252525,0.7524242424242424,0.7623232323232323,0.7722222222222221,0.7821212121212121,0.792020202020202,0.8019191919191919,0.8118181818181818,0.8217171717171717,0.8316161616161616,0.8415151515151514,0.8514141414141414,0.8613131313131313,0.8712121212121212,0.8811111111111111,0.891010101010101,0.9009090909090909,0.9108080808080807,0.9207070707070707,0.9306060606060605,0.9405050505050505,0.9504040404040404,0.9603030303030302,0.9702020202020202,0.98010101010101,0.99],[0.2]]},{"title.text":"Scenario: Uniform (Random) | Probability for True Class = 0.200 | Loss = 1.609"}],"label":"Uniform (Random)","method":"update"}],"direction":"down","pad":{"b":10,"l":6,"r":6,"t":4},"type":"dropdown","x":0.5,"xanchor":"center","y":1.2,"yanchor":"top"}],"height":700,"showlegend":false},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('b9d63290-471a-4726-a19e-6d156f290ac3');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };            </script>        </div>
</div>
</div>
<p><strong>Key Takeaways</strong>:</p>
<ul>
<li><strong>Green bar</strong> = True class; <strong>Red dot</strong> = Current position on loss curve</li>
<li>Watch how the red dot moves along the curve as predictions change</li>
<li>Perfect prediction (p=1.0) â†’ Dot at left, Loss â‰ˆ 0</li>
<li>Confident wrong (p=0.05) â†’ Dot at right, Loss â‰ˆ 3.0</li>
</ul>
</div>
</div>
</div>
</section>
<section id="try-it-out-yourself" class="level3" data-number="3.5">
<h3 data-number="3.5" class="anchored" data-anchor-id="try-it-out-yourself"><span class="header-section-number">3.5</span> Try it out yourself!</h3>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Train a CNN for image classification in your browser!
</div>
</div>
<div class="callout-body-container callout-body">
<p><a href="https://poloclub.github.io/cnn-explainer/">CNN Explainer</a></p>
</div>
</div>
</section>
</section>
<section id="architectures" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="architectures"><span class="header-section-number">4</span> Architectures</h2>
<p>Architecture refers to the complete definition of an untrained (deep-learning) model, i.e., the sequence and configuration of layers. The following are some important architectures that have been successfully used for image classification.</p>
<!-- TODO: add a simple visualization that makes clear at the end is a vector of size k with the class logits -->
<section id="alexnet" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="alexnet"><span class="header-section-number">4.1</span> AlexNet</h3>
<p>CNNs became extremely popular after winning the <a href="https://www.image-net.org/">ImageNet</a> Competition. <span class="citation" data-cites="krizhevsky_imagenet_2012">Krizhevsky, Sutskever, and Hinton (<a href="#ref-krizhevsky_imagenet_2012" role="doc-biblioref">2012</a>)</span> implemented a CNN with multiple layers, known as the AlexNet architecture, as shown in <a href="#fig-classification-alexnet" class="quarto-xref">Figure&nbsp;6</a>. ImageNet is a large, hierarchical image dataset <span class="citation" data-cites="deng_imagenet_2009">Deng et al. (<a href="#ref-deng_imagenet_2009" role="doc-biblioref">2009</a>)</span>, which enabled efficient training of CNNs for the first time.</p>
<p>AlexNet consists of 5 convolutional layers and 3 fully-connected layers. The last layer is a 1000-way softmax output to model the classes in ImageNet.</p>
<p>The model was trained with two GPUs (GTX 580) with 3GB memory each. Since 3GB was insufficient to train the model, the architecture was split across the GPUs. Some layers were split between the GPUs, allowing a larger network to be trained.</p>
<p><a href="#fig-classification-alexnet" class="quarto-xref">Figure&nbsp;6</a> shows the detailed architecture, including kernel sizes, the number of filters per layer, activation map dimensions, and pooling layers. ReLU was used as the activation function. This representation is from the original paper and shows how the model was split across two GPUs.</p>
<div id="fig-classification-alexnet" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-classification-alexnet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/classification/alexnet.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classification-alexnet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: AlexNet <span class="citation" data-cites="krizhevsky_imagenet_2012">Krizhevsky, Sutskever, and Hinton (<a href="#ref-krizhevsky_imagenet_2012" role="doc-biblioref">2012</a>)</span>.
</figcaption>
</figure>
</div>
<p>Since the split across two GPUs is no longer necessary, the architecture is somewhat simplified. Modern implementations are shown in <a href="#fig-classification-alexnet2" class="quarto-xref">Figure&nbsp;7</a>.</p>
<div id="fig-classification-alexnet2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-classification-alexnet2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/classification/alexnet_illustration.jpg" class="img-fluid figure-img" width="400">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classification-alexnet2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: AlexNet <span class="citation" data-cites="prince_understanding_2023">Prince (<a href="#ref-prince_understanding_2023" role="doc-biblioref">2023</a>)</span>
</figcaption>
</figure>
</div>
<p><a href="#fig-classification-alexnet-table" class="quarto-xref">Figure&nbsp;8</a> presents the operations in AlexNet in tabular form.</p>
<div id="fig-classification-alexnet-table" class="quarto-float quarto-figure quarto-figure-center anchored" width="600">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-classification-alexnet-table-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/classification/alexnet_table.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classification-alexnet-table-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Source: <span class="citation" data-cites="johnson_eecs_2019">Johnson (<a href="#ref-johnson_eecs_2019" role="doc-biblioref">2019</a>)</span>.
</figcaption>
</figure>
</div>
<p>We can also easily load AlexNet via torchvision.</p>
<div id="6bd190ec" class="cell" data-execution_count="12">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.models <span class="im">as</span> models</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchinfo</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>alexnet <span class="op">=</span> models.alexnet()</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.zeros(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">224</span>, <span class="dv">224</span>, dtype<span class="op">=</span>torch.<span class="bu">float</span>, requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>yhat <span class="op">=</span> alexnet(x)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(torchinfo.summary(alexnet, input_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">224</span>, <span class="dv">224</span>)))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
AlexNet                                  [1, 1000]                 --
â”œâ”€Sequential: 1-1                        [1, 256, 6, 6]            --
â”‚    â””â”€Conv2d: 2-1                       [1, 64, 55, 55]           23,296
â”‚    â””â”€ReLU: 2-2                         [1, 64, 55, 55]           --
â”‚    â””â”€MaxPool2d: 2-3                    [1, 64, 27, 27]           --
â”‚    â””â”€Conv2d: 2-4                       [1, 192, 27, 27]          307,392
â”‚    â””â”€ReLU: 2-5                         [1, 192, 27, 27]          --
â”‚    â””â”€MaxPool2d: 2-6                    [1, 192, 13, 13]          --
â”‚    â””â”€Conv2d: 2-7                       [1, 384, 13, 13]          663,936
â”‚    â””â”€ReLU: 2-8                         [1, 384, 13, 13]          --
â”‚    â””â”€Conv2d: 2-9                       [1, 256, 13, 13]          884,992
â”‚    â””â”€ReLU: 2-10                        [1, 256, 13, 13]          --
â”‚    â””â”€Conv2d: 2-11                      [1, 256, 13, 13]          590,080
â”‚    â””â”€ReLU: 2-12                        [1, 256, 13, 13]          --
â”‚    â””â”€MaxPool2d: 2-13                   [1, 256, 6, 6]            --
â”œâ”€AdaptiveAvgPool2d: 1-2                 [1, 256, 6, 6]            --
â”œâ”€Sequential: 1-3                        [1, 1000]                 --
â”‚    â””â”€Dropout: 2-14                     [1, 9216]                 --
â”‚    â””â”€Linear: 2-15                      [1, 4096]                 37,752,832
â”‚    â””â”€ReLU: 2-16                        [1, 4096]                 --
â”‚    â””â”€Dropout: 2-17                     [1, 4096]                 --
â”‚    â””â”€Linear: 2-18                      [1, 4096]                 16,781,312
â”‚    â””â”€ReLU: 2-19                        [1, 4096]                 --
â”‚    â””â”€Linear: 2-20                      [1, 1000]                 4,097,000
==========================================================================================
Total params: 61,100,840
Trainable params: 61,100,840
Non-trainable params: 0
Total mult-adds (Units.MEGABYTES): 714.68
==========================================================================================
Input size (MB): 0.60
Forward/backward pass size (MB): 3.95
Params size (MB): 244.40
Estimated Total Size (MB): 248.96
==========================================================================================</code></pre>
</div>
</div>
</section>
<section id="vgg" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="vgg"><span class="header-section-number">4.2</span> VGG</h3>
<p><span class="citation" data-cites="simonyan_very_2015">Simonyan and Zisserman (<a href="#ref-simonyan_very_2015" role="doc-biblioref">2015</a>)</span> won the ImageNet Challenge in 2014 with their VGG architecture. They showed that smaller 3x3 kernels work significantly better and that deeper networks with 16-19 layers can be trained. <a href="#fig-classification-vgg" class="quarto-xref">Figure&nbsp;9</a> shows the architecture as presented in the original paper. <a href="#fig-classification-vgg-arch" class="quarto-xref">Figure&nbsp;10</a> visualizes the architecture.</p>
<div id="fig-classification-vgg" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-classification-vgg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/classification/vgg.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classification-vgg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: VGG <span class="citation" data-cites="simonyan_very_2015">Simonyan and Zisserman (<a href="#ref-simonyan_very_2015" role="doc-biblioref">2015</a>)</span>.
</figcaption>
</figure>
</div>
<div id="fig-classification-vgg-arch" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-classification-vgg-arch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/classification/vgg_illustration.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classification-vgg-arch-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: VGG <span class="citation" data-cites="prince_understanding_2023">Prince (<a href="#ref-prince_understanding_2023" role="doc-biblioref">2023</a>)</span>
</figcaption>
</figure>
</div>
<p>VGG introduced a popular design element: A layer has the same number of filters as the previous layer unless the activation map dimensions are halved, in which case the number of filters is doubled (see also <a href="#fig-classification-vgg-arch" class="quarto-xref">Figure&nbsp;10</a>). This was done to maintain the time complexity of the layers. VGG does not use normalization layers.</p>
<p><a href="#fig-classification-vgg-design" class="quarto-xref">Figure&nbsp;11</a> compares VGG with AlexNet.</p>
<div id="fig-classification-vgg-design" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-classification-vgg-design-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/classification/vgg_design.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classification-vgg-design-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: Source: <span class="citation" data-cites="johnson_eecs_2019">Johnson (<a href="#ref-johnson_eecs_2019" role="doc-biblioref">2019</a>)</span>.
</figcaption>
</figure>
</div>
</section>
<section id="resnet" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="resnet"><span class="header-section-number">4.3</span> ResNet</h3>
<p><span class="citation" data-cites="He2016">He et al. (<a href="#ref-He2016" role="doc-biblioref">2016</a>)</span> wondered whether CNNs could be improved simply by making them deeper, i.e., adding more layers. Their experiments showed that adding layers eventually stops being beneficial, and performance saturates and then rapidly degrades (see <a href="#fig-classification-resnet-motivation1" class="quarto-xref">Figure&nbsp;12</a>). They noticed that the performance did not degrade due to overfitting (see <a href="#fig-classification-resnet-motivation2" class="quarto-xref">Figure&nbsp;13</a>, which shows that training error is also poor), but because back-propagation becomes less effective and the weights do not optimize well. <span class="citation" data-cites="He2016">He et al. (<a href="#ref-He2016" role="doc-biblioref">2016</a>)</span> hypothesized that deeper networks could not perform worse than shallower ones because the additional layers could simply pass on activations unchanged.</p>
<div id="fig-classification-resnet-motivation1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-classification-resnet-motivation1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/classification/resnet_test_error.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classification-resnet-motivation1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: Source: <span class="citation" data-cites="He2016">He et al. (<a href="#ref-He2016" role="doc-biblioref">2016</a>)</span>
</figcaption>
</figure>
</div>
<div id="fig-classification-resnet-motivation2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-classification-resnet-motivation2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/classification/resnet_train_error.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classification-resnet-motivation2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13: Source: <span class="citation" data-cites="He2016">He et al. (<a href="#ref-He2016" role="doc-biblioref">2016</a>)</span>
</figcaption>
</figure>
</div>
<p>They then hypothesized that passing on activations unchanged as a default behavior could avoid this problem. They introduced identity mappings. <a href="#fig-classification-residual-connection" class="quarto-xref">Figure&nbsp;14</a> shows such a connection. These connections are also called residual connections because the network only needs to learn the change in activations from layer <span class="math inline">\(i\)</span> to layer <span class="math inline">\(i+1\)</span>, the residue.</p>
<div id="fig-classification-residual-connection" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-classification-residual-connection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/classification/residual_connection.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classification-residual-connection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14: ResNet <span class="citation" data-cites="He2016">He et al. (<a href="#ref-He2016" role="doc-biblioref">2016</a>)</span> (Graphic from <span class="citation" data-cites="johnson_eecs_2019">Johnson (<a href="#ref-johnson_eecs_2019" role="doc-biblioref">2019</a>)</span>.)
</figcaption>
</figure>
</div>
<p>ResNet was otherwise inspired by VGG16. The convolutional layers consist of 3x3 kernels (except the first one), and the number of filters is doubled when the activation map dimensions are halved. ResNet uses convolutions with stride 2 for down-sampling and no max pooling. At the end, ResNet uses a global average pooling layer followed by a fully-connected layer with the number of classes. Variants of ResNet have 18, 34, 50, and over 100 layers. ResNet also uses batch normalization. <a href="#fig-classification-resnet-struk" class="quarto-xref">Figure&nbsp;15</a> shows the architecture.</p>
<div id="fig-classification-resnet-struk" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-classification-resnet-struk-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/classification/resnet_arch_horiz.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classification-resnet-struk-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15: Source: <span class="citation" data-cites="He2016">He et al. (<a href="#ref-He2016" role="doc-biblioref">2016</a>)</span>.
</figcaption>
</figure>
</div>
<p>ResNet is very popular and is still widely used today (there are now more modern variants). Recent studies of identity mappings (or more generally skip connections) have shown that the gradient surface of the loss function becomes smoother, allowing the network to be optimized better. This is impressively illustrated in <a href="#fig-classification-skip-loss-landscape" class="quarto-xref">Figure&nbsp;16</a>.</p>
<div id="fig-classification-skip-loss-landscape" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-classification-skip-loss-landscape-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/classification/loss_landscape_skip.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classification-skip-loss-landscape-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16: Source: <span class="citation" data-cites="li_visualizing_2018">Li et al. (<a href="#ref-li_visualizing_2018" role="doc-biblioref">2018</a>)</span>.
</figcaption>
</figure>
</div>
</section>
<section id="convnext" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="convnext"><span class="header-section-number">4.4</span> ConvNext</h3>
<p>One of the most modern CNN architectures was described in <span class="citation" data-cites="liu_convnet_2022">Liu et al. (<a href="#ref-liu_convnet_2022" role="doc-biblioref">2022</a>)</span>. This architecture uses tricks and implementation ideas accumulated over decades from various architectures. <a href="#fig-classification-convnext" class="quarto-xref">Figure&nbsp;17</a> shows, starting from a modern version of ResNet, what has been adjusted to define this state-of-the-art architecture. Examples include: larger kernels, different activation functions, layer normalization instead of batch normalization, and depthwise separable convolutions.</p>
<div id="fig-classification-convnext" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-classification-convnext-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/classification/convnext.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classification-convnext-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17: Convnext <span class="citation" data-cites="liu_convnet_2022">Liu et al. (<a href="#ref-liu_convnet_2022" role="doc-biblioref">2022</a>)</span>.
</figcaption>
</figure>
</div>
<p>There is already a new version of this architecture <span class="citation" data-cites="woo_convnext_2023">Woo et al. (<a href="#ref-woo_convnext_2023" role="doc-biblioref">2023</a>)</span>.</p>
</section>
<section id="imagenet-performance" class="level3" data-number="4.5">
<h3 data-number="4.5" class="anchored" data-anchor-id="imagenet-performance"><span class="header-section-number">4.5</span> ImageNet Performance</h3>
<p><a href="#fig-classification-image-net-performance" class="quarto-xref">Figure&nbsp;18</a> shows the development of ImageNet performance. As of 2023, we are much closer to 100%, see <a href="https://paperswithcode.com/sota/image-classification-on-imagenet?metric=Top%205%20Accuracy">Link</a>.</p>
<div id="fig-classification-image-net-performance" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-classification-image-net-performance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/classification/image_net_performance.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classification-image-net-performance-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;18: Graphic from <span class="citation" data-cites="johnson_eecs_2019">Johnson (<a href="#ref-johnson_eecs_2019" role="doc-biblioref">2019</a>)</span>
</figcaption>
</figure>
</div>
</section>
<section id="which-architecture" class="level3" data-number="4.6">
<h3 data-number="4.6" class="anchored" data-anchor-id="which-architecture"><span class="header-section-number">4.6</span> Which Architecture?</h3>
<p>Which architecture should be chosen for a specific problem? A common tip is: <em>Donâ€™t be a hero.</em></p>
<p>One should rely on off-the-shelf architectures and not implement their own without a good reason.</p>
<p>Typically, ResNet-50 or ResNet-101 are good choices. However, there are also models that require significantly fewer parameters, such as Efficient Nets.</p>
<p>Important considerations are also the requirements regarding accuracy, performance (FLOPs), and model size (memory).</p>
</section>
</section>
<section id="further-variations" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="further-variations"><span class="header-section-number">5</span> Further Variations</h2>
<p>There are countless other architectures and especially specific layers or layer blocks that can be used in various architectures. The following are Squeeze/Excite Networks and the important class of normalization layers, which are often used and can significantly improve models or accelerate the optimization process.</p>
<section id="squeezeexcite-networks" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="squeezeexcite-networks"><span class="header-section-number">5.1</span> Squeeze/Excite Networks</h3>
<p>Squeeze-and-Excite Networks (SE-Networks) were introduced in 2019 <span class="citation" data-cites="hu_squeeze-and-excitation_2019">Hu et al. (<a href="#ref-hu_squeeze-and-excitation_2019" role="doc-biblioref">2019</a>)</span>. These include so-called Squeeze and Excite blocks (SE blocks), which allow the scaling of activation maps of a layer. This scaling is learnable through a few additional parameters. In practice, significant performance gains have been observed. <a href="#fig-classification-se-networks" class="quarto-xref">Figure&nbsp;19</a> shows an illustration.</p>
<div id="fig-classification-se-networks" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-classification-se-networks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/classification/se_networks.png" class="img-fluid figure-img" width="800">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classification-se-networks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;19: Source: <span class="citation" data-cites="hu_squeeze-and-excitation_2019">Hu et al. (<a href="#ref-hu_squeeze-and-excitation_2019" role="doc-biblioref">2019</a>)</span>.
</figcaption>
</figure>
</div>
<p>These SE blocks can be easily applied to arbitrary activation maps. <a href="#fig-classification-se-networks" class="quarto-xref">Figure&nbsp;19</a> shows an input feature map <span class="math inline">\(\mathbf{\mathsf{X}} \in \mathbb{R}^{H' \times W' \times C'}\)</span> that is transformed with <span class="math inline">\(F_{tr}\)</span> (e.g., with a convolutional layer). This results in the activation maps <span class="math inline">\(\mathbf{\mathsf{U}} \in \mathbb{R}^{H \times W \times C}\)</span>.</p>
<p>The transformation <span class="math inline">\(F_{sq}(\cdot)\)</span> applies the squeeze operation, a global average pooling, to generate a description of each channel by aggregating spatial information of the activation map <span class="math inline">\(\mathbf{\mathsf{U}}\)</span>. This results in a vector <span class="math inline">\(z \in \mathbb{R}^{1 \times 1 \times C}\)</span>.</p>
<p>The excitation operation <span class="math inline">\(F_{ex}(\cdot, W)\)</span> uses a gating mechanism with parameters <span class="math inline">\(W\)</span>, implemented with two fully-connected layers and activation functions. The result is <span class="math inline">\(s \in \mathbb{R}^{1 \times 1 \times C}\)</span>, the channel weights. It is called gating because the weights range from <span class="math inline">\([0, 1]\)</span> and thus control how much information of a channel flows through (gatekeeping).</p>
<p>Finally, <span class="math inline">\(F_{scale}(\cdot)\)</span> scales the activation maps <span class="math inline">\(\mathbf{\mathsf{U}}\)</span> with the channel weights.</p>
<p>The operation can be described as follows:</p>
<p><span class="math display">\[\begin{align}
U &amp;= F_{tr}(X) \\
z &amp;= F_{sq}(U) = \text{GlobalAvgPool}(U) \\
s &amp;= F_{ex}(z, W) = \sigma(\mathbf{W}_2 g(\mathbf{W}_1 z)) \\
\hat{X} &amp;= F_{scale}(U, s) = U \odot s
\end{align}\]</span></p>
<p>where <span class="math inline">\(g()\)</span> represents the ReLU function, and <span class="math inline">\(\sigma\)</span> represents the sigmoid function.</p>
</section>
<section id="normalization-layers" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="normalization-layers"><span class="header-section-number">5.2</span> Normalization Layers</h3>
<p>Normalization layers normalize activation maps to improve parameter learning. There are many variations, as shown in <a href="#fig-classification-normalization" class="quarto-xref">Figure&nbsp;20</a>. In modern architectures, normalization layers are typically used. Popular is, for example, layer normalization (see <span class="citation" data-cites="ba_layer_2016">Ba, Kiros, and Hinton (<a href="#ref-ba_layer_2016" role="doc-biblioref">2016</a>)</span>). The general form of normalization is given in equation <a href="#eq-classification-normalization" class="quarto-xref">Equation&nbsp;1</a>. The parameters <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\beta\)</span> are learned, while the means <span class="math inline">\(E[x]\)</span> and variances <span class="math inline">\(\sigma^2[x]\)</span> are estimated from the activations. Why normalization layers work and which ones to prefer are still subjects of research and are often empirically tested in practice (considered as hyper-parameters). It is believed that the cost function becomes smoother overall, allowing the network to train faster and better <span class="citation" data-cites="santurkar_how_2019">Santurkar et al. (<a href="#ref-santurkar_how_2019" role="doc-biblioref">2019</a>)</span>.</p>
<p><span id="eq-classification-normalization"><span class="math display">\[
y = \frac{x - E[x]}{\sqrt{\sigma^2[x] + \epsilon}} * \gamma + \beta
\tag{1}\]</span></span></p>
<div id="fig-classification-normalization" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-classification-normalization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/classification/normalization.png" class="img-fluid figure-img" width="300">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-classification-normalization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;20: Source: <span class="citation" data-cites="qiao_micro-batch_2020">Qiao et al. (<a href="#ref-qiao_micro-batch_2020" role="doc-biblioref">2020</a>)</span>.
</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Normalization layers are very common and used in almost any architecture. While batch normalization has traditionally been used in CNNs, newer models rely on layer normalization.</p>
</div>
</div>
</section>
</section>
<section id="pre-processing" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="pre-processing"><span class="header-section-number">6</span> Pre-Processing</h2>
<p>Typically, images are pre-processed before being processed by the CNN. This is done to make the optimization process converge faster. Common steps include:</p>
<ul>
<li>Resizing/cropping to a fixed size, e.g., 224x224x3. This is necessary for the images to be processed in a batch (they must have the same dimensions).</li>
<li>Scaling: Images are scaled from the interval [0, 255] to the interval [0, 1].</li>
<li>Normalization: Normalization is often done along the color channels so that the mean value of the pixel values is 0.</li>
</ul>
<p>In <code>torchvision</code>, the following pre-processing steps are used: <a href="https://github.com/pytorch/vision/blob/main/torchvision/transforms/_presets.py">Link</a></p>
</section>
<section id="transfer-learning" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="transfer-learning"><span class="header-section-number">7</span> Transfer Learning</h2>
<p>Transfer Learning refers to the process of adapting a trained model for Task A to Task B. Adapting pre-trained models often leads to better results and also reduces the number of training iterations. Deep learning frameworks often provide easy access to such models (also known as Model Zoo). For example, from PyTorch: <a href="https://pytorch.org/vision/stable/models.html">Link</a>.</p>
<div id="5f08c5e4" class="cell" data-execution_count="13">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.models <span class="im">import</span> resnet50, ResNet50_Weights</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>resnet50(weights<span class="op">=</span>ResNet50_Weights.IMAGENET1K_V2)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="pytorch-example" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="pytorch-example"><span class="header-section-number">8</span> PyTorch Example</h2>
<section id="architecture" class="level3" data-number="8.1">
<h3 data-number="8.1" class="anchored" data-anchor-id="architecture"><span class="header-section-number">8.1</span> Architecture</h3>
<p>Example of a CNN architecture.</p>
<div id="90b65961" class="cell" data-execution_count="14">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Net(nn.Module):</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(<span class="dv">3</span>, <span class="dv">6</span>, <span class="dv">5</span>)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pool <span class="op">=</span> nn.MaxPool2d(<span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(<span class="dv">6</span>, <span class="dv">16</span>, <span class="dv">5</span>)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">16</span> <span class="op">*</span> <span class="dv">5</span> <span class="op">*</span> <span class="dv">5</span>, <span class="dv">120</span>)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">120</span>, <span class="dv">84</span>)</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc3 <span class="op">=</span> nn.Linear(<span class="dv">84</span>, <span class="dv">10</span>)</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(F.relu(<span class="va">self</span>.conv1(x)))</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(F.relu(<span class="va">self</span>.conv2(x)))</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.flatten(x, <span class="dv">1</span>)</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc2(x))</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc3(x)</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>net <span class="op">=</span> Net()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="loss-function-1" class="level3" data-number="8.2">
<h3 data-number="8.2" class="anchored" data-anchor-id="loss-function-1"><span class="header-section-number">8.2</span> Loss Function</h3>
<div id="2aed2bb6" class="cell" data-execution_count="15">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(net.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>, momentum<span class="op">=</span><span class="fl">0.9</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
</section>
<section id="references" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="references"><span class="header-section-number">9</span> References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-ba_layer_2016" class="csl-entry" role="listitem">
Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. <span>â€œLayer <span>Normalization</span>.â€</span> arXiv. <a href="http://arxiv.org/abs/1607.06450">http://arxiv.org/abs/1607.06450</a>.
</div>
<div id="ref-deng_imagenet_2009" class="csl-entry" role="listitem">
Deng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. <span>â€œ<span>ImageNet</span>: <span>A</span> Large-Scale Hierarchical Image Database.â€</span> In <em>2009 <span>IEEE</span> <span>Conference</span> on <span>Computer</span> <span>Vision</span> and <span>Pattern</span> <span>Recognition</span></em>, 248â€“55. Miami, FL: IEEE. <a href="https://doi.org/10.1109/CVPR.2009.5206848">https://doi.org/10.1109/CVPR.2009.5206848</a>.
</div>
<div id="ref-He2016" class="csl-entry" role="listitem">
He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. <span>â€œDeep <span>Residual</span> <span>Learning</span> for <span>Image</span> <span>Recognition</span>.â€</span> In <em>2016 <span>IEEE</span> <span>Conference</span> on <span>Computer</span> <span>Vision</span> and <span>Pattern</span> <span>Recognition</span> (<span>CVPR</span>)</em>, 770â€“78. IEEE. <a href="https://doi.org/10.1109/CVPR.2016.90">https://doi.org/10.1109/CVPR.2016.90</a>.
</div>
<div id="ref-hu_squeeze-and-excitation_2019" class="csl-entry" role="listitem">
Hu, Jie, Li Shen, Samuel Albanie, Gang Sun, and Enhua Wu. 2019. <span>â€œSqueeze-and-<span>Excitation</span> <span>Networks</span>.â€</span> arXiv. <a href="http://arxiv.org/abs/1709.01507">http://arxiv.org/abs/1709.01507</a>.
</div>
<div id="ref-johnson_eecs_2019" class="csl-entry" role="listitem">
Johnson, Justin. 2019. <span>â€œ<span>EECS</span> 498-007 / 598-005: <span>Deep</span> <span>Learning</span> for <span>Computer</span> <span>Vision</span>.â€</span> Lecture {Notes} / {Slides}. <a href="https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2019/">https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2019/</a>.
</div>
<div id="ref-krizhevsky_imagenet_2012" class="csl-entry" role="listitem">
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. <span>â€œ<span>ImageNet</span> <span>Classification</span> with <span>Deep</span> <span>Convolutional</span> <span>Neural</span> <span>Networks</span>.â€</span> In <em>Advances in <span>Neural</span> <span>Information</span> <span>Processing</span> <span>Systems</span></em>, edited by F. Pereira, C. J. Burges, L. Bottou, and K. Q. Weinberger. Vol. 25. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf</a>.
</div>
<div id="ref-li_visualizing_2018" class="csl-entry" role="listitem">
Li, Hao, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. 2018. <span>â€œVisualizing the <span>Loss</span> <span>Landscape</span> of <span>Neural</span> <span>Nets</span>.â€</span> arXiv. <a href="http://arxiv.org/abs/1712.09913">http://arxiv.org/abs/1712.09913</a>.
</div>
<div id="ref-liu_convnet_2022" class="csl-entry" role="listitem">
Liu, Zhuang, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. 2022. <span>â€œA <span>ConvNet</span> for the 2020s.â€</span> arXiv. <a href="http://arxiv.org/abs/2201.03545">http://arxiv.org/abs/2201.03545</a>.
</div>
<div id="ref-prince_understanding_2023" class="csl-entry" role="listitem">
Prince, Simon J. D. 2023. <em>Understanding <span>Deep</span> <span>Learning</span></em>. MIT Press. <a href="https://udlbook.github.io/udlbook/">https://udlbook.github.io/udlbook/</a>.
</div>
<div id="ref-qiao_micro-batch_2020" class="csl-entry" role="listitem">
Qiao, Siyuan, Huiyu Wang, Chenxi Liu, Wei Shen, and Alan Yuille. 2020. <span>â€œMicro-<span>Batch</span> <span>Training</span> with <span>Batch</span>-<span>Channel</span> <span>Normalization</span> and <span>Weight</span> <span>Standardization</span>.â€</span> arXiv. <a href="http://arxiv.org/abs/1903.10520">http://arxiv.org/abs/1903.10520</a>.
</div>
<div id="ref-santurkar_how_2019" class="csl-entry" role="listitem">
Santurkar, Shibani, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. 2019. <span>â€œHow <span>Does</span> <span>Batch</span> <span>Normalization</span> <span>Help</span> <span>Optimization</span>?â€</span> arXiv. <a href="http://arxiv.org/abs/1805.11604">http://arxiv.org/abs/1805.11604</a>.
</div>
<div id="ref-simonyan_very_2015" class="csl-entry" role="listitem">
Simonyan, Karen, and Andrew Zisserman. 2015. <span>â€œVery <span>Deep</span> <span>Convolutional</span> <span>Networks</span> for <span>Large</span>-<span>Scale</span> <span>Image</span> <span>Recognition</span>.â€</span> arXiv. <a href="http://arxiv.org/abs/1409.1556">http://arxiv.org/abs/1409.1556</a>.
</div>
<div id="ref-woo_convnext_2023" class="csl-entry" role="listitem">
Woo, Sanghyun, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, and Saining Xie. 2023. <span>â€œ<span>ConvNeXt</span> <span>V2</span>: <span>Co</span>-Designing and <span>Scaling</span> <span>ConvNets</span> with <span>Masked</span> <span>Autoencoders</span>.â€</span> arXiv. <a href="http://arxiv.org/abs/2301.00808">http://arxiv.org/abs/2301.00808</a>.
</div>
</div>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "î§‹";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/marco-willi\.github\.io\/cas-dl-compvis-lectures-hs2025\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../pages/lectures/cnns.html" class="pagination-link" aria-label="2 - Convolutional Neural Networks">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">2 - Convolutional Neural Networks</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../pages/lectures/representation_learning.html" class="pagination-link" aria-label="4 - Representation Learning">
        <span class="nav-page-text">4 - Representation Learning</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb16" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "3 - Image Classification"</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="an">params:</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="co">   images_path: "/assets/images/classification/"</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="an">execute:</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="co">  cache: false</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="co">  freeze: false</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>::: {.content-hidden}</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>{{&lt; include /assets/_macros.tex &gt;}}</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title="Learning Objectives" collapse="true"}</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Define and recognize classification problems.</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Interpret logits, probabilities and the role of the softmax function.</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Explain cross-entropy and maximum likelihood on a high-level.</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Compare classic CNN architectures (AlexNetâ†’VGGâ†’ResNetâ†’ConvNeXt).</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Apply transfer learning and explain when to freeze vs fine-tune layers.</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title="TLDR Recap" collapse="true"}</span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>**Image Classification: Core Task**</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Assign images to predefined classes (multi-class: â‰¥2 classes, binary: 2 classes, multi-label: multiple class sets)</span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Use CNNs to learn hierarchical features automatically from raw pixels</span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Output: class probabilities or labels</span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a>**Softmax: Logits â†’ Probabilities**</span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a>$$P(Y = k \mid X = \vect{x}) = \frac{e^{z_k}}{\sum_{j=1}^K e^{z_j}}$$</span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Logits** $\vect{z}$: Raw model outputs (unbounded)</span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Softmax** $\sigma(\vect{z})$: Converts logits to valid probability distribution</span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Properties: Always sums to 1, outputs in <span class="co">[</span><span class="ot">0,1</span><span class="co">]</span>, differentiable</span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a>**Cross-Entropy Loss = Negative Log-Likelihood**</span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a>Maximum Likelihood approach:</span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-45"><a href="#cb16-45" aria-hidden="true" tabindex="-1"></a>$$L(\m{X}, \vect{y}, \theta) = -\sum_{i=1}^N \sum_{j=1}^K y^{(i)}_j \log P(Y = j\mid X = \vect{x}^{(i)})$$</span>
<span id="cb16-46"><a href="#cb16-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-47"><a href="#cb16-47" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Minimizing negative log-likelihood = maximizing likelihood of correct predictions</span>
<span id="cb16-48"><a href="#cb16-48" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Equivalent to cross-entropy between true distribution $\vect{y}$ and predicted $\hat{\vect{y}}$</span>
<span id="cb16-49"><a href="#cb16-49" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Penalizes confident wrong predictions heavily</span>
<span id="cb16-50"><a href="#cb16-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-51"><a href="#cb16-51" aria-hidden="true" tabindex="-1"></a>**Key Insight**: Softmax + Cross-Entropy arise naturally from probabilistic modeling (categorical distribution + maximum likelihood estimation)</span>
<span id="cb16-52"><a href="#cb16-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-53"><a href="#cb16-53" aria-hidden="true" tabindex="-1"></a>**Classic CNN Architectures Evolution**</span>
<span id="cb16-54"><a href="#cb16-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-55"><a href="#cb16-55" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**AlexNet (2012)**: 5 conv + 3 FC layers, ReLU, dropout â†’ ImageNet breakthrough</span>
<span id="cb16-56"><a href="#cb16-56" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**VGG (2014)**: Deeper (16-19 layers), small 3Ã—3 kernels, simple design â†’ showed depth matters</span>
<span id="cb16-57"><a href="#cb16-57" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**ResNet (2015)**: Residual connections (skip connections) â†’ enabled 50-152 layers, solved vanishing gradients</span>
<span id="cb16-58"><a href="#cb16-58" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**ConvNeXt (2022)**: Modern CNN with Transformer-era tricks â†’ competes with Vision Transformers</span>
<span id="cb16-59"><a href="#cb16-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-60"><a href="#cb16-60" aria-hidden="true" tabindex="-1"></a>**Residual Connections: The Key Innovation**</span>
<span id="cb16-61"><a href="#cb16-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-62"><a href="#cb16-62" aria-hidden="true" tabindex="-1"></a>$$\vect{x}_{l+1} = \vect{x}_l + F(\vect{x}_l)$$</span>
<span id="cb16-63"><a href="#cb16-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-64"><a href="#cb16-64" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Network learns residual $F(\vect{x})$ instead of full transformation</span>
<span id="cb16-65"><a href="#cb16-65" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Enables training very deep networks (&gt;100 layers)</span>
<span id="cb16-66"><a href="#cb16-66" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Smoother loss landscape â†’ better optimization</span>
<span id="cb16-67"><a href="#cb16-67" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Default behavior: pass activations unchanged (identity mapping)</span>
<span id="cb16-68"><a href="#cb16-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-69"><a href="#cb16-69" aria-hidden="true" tabindex="-1"></a>**Transfer Learning: Standing on Giants' Shoulders**</span>
<span id="cb16-70"><a href="#cb16-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-71"><a href="#cb16-71" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Pre-training**: Train on large dataset (e.g., ImageNet: 1.2M images, 1000 classes)</span>
<span id="cb16-72"><a href="#cb16-72" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Transfer**: Adapt to your specific task with much less data</span>
<span id="cb16-73"><a href="#cb16-73" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Strategies**:</span>
<span id="cb16-74"><a href="#cb16-74" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Freeze early layers** (extract features only) when data is very scarce</span>
<span id="cb16-75"><a href="#cb16-75" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Fine-tune all layers** when you have moderate data</span>
<span id="cb16-76"><a href="#cb16-76" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>**Fine-tune only later layers** as middle ground</span>
<span id="cb16-77"><a href="#cb16-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-78"><a href="#cb16-78" aria-hidden="true" tabindex="-1"></a>**Why Transfer Learning Works**:</span>
<span id="cb16-79"><a href="#cb16-79" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Early layers learn general features (edges, textures)</span>
<span id="cb16-80"><a href="#cb16-80" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Later layers learn task-specific features</span>
<span id="cb16-81"><a href="#cb16-81" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Pre-trained features transfer well across vision tasks</span>
<span id="cb16-82"><a href="#cb16-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-83"><a href="#cb16-83" aria-hidden="true" tabindex="-1"></a>**Architecture Selection Strategy**</span>
<span id="cb16-84"><a href="#cb16-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-85"><a href="#cb16-85" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>âœ… **Don't be a hero**: Use proven architectures first</span>
<span id="cb16-86"><a href="#cb16-86" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>âœ… **ResNet-50/101**: Excellent baseline for most tasks</span>
<span id="cb16-87"><a href="#cb16-87" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>âœ… **EfficientNet**: When model size/speed matters</span>
<span id="cb16-88"><a href="#cb16-88" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>âœ… **ConvNeXt**: State-of-the-art CNN performance</span>
<span id="cb16-89"><a href="#cb16-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-90"><a href="#cb16-90" aria-hidden="true" tabindex="-1"></a>**Consider**:</span>
<span id="cb16-91"><a href="#cb16-91" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Accuracy requirements</span>
<span id="cb16-92"><a href="#cb16-92" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Inference speed (FLOPs)</span>
<span id="cb16-93"><a href="#cb16-93" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Model size (memory)</span>
<span id="cb16-94"><a href="#cb16-94" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Available training data</span>
<span id="cb16-95"><a href="#cb16-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-96"><a href="#cb16-96" aria-hidden="true" tabindex="-1"></a>**Modern Components**</span>
<span id="cb16-97"><a href="#cb16-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-98"><a href="#cb16-98" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Normalization layers** (BatchNorm, LayerNorm): Stabilize training, enable deeper networks</span>
<span id="cb16-99"><a href="#cb16-99" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Global Average Pooling**: Replace large FC layers â†’ fewer parameters</span>
<span id="cb16-100"><a href="#cb16-100" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**SE Blocks**: Learn channel-wise attention â†’ better feature recalibration</span>
<span id="cb16-101"><a href="#cb16-101" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Depthwise Separable Convolutions**: Reduce parameters while maintaining performance</span>
<span id="cb16-102"><a href="#cb16-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-103"><a href="#cb16-103" aria-hidden="true" tabindex="-1"></a>**Pre-processing Pipeline**</span>
<span id="cb16-104"><a href="#cb16-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-105"><a href="#cb16-105" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Resize/crop to fixed size (e.g., 224Ã—224)</span>
<span id="cb16-106"><a href="#cb16-106" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Scale pixel values: <span class="co">[</span><span class="ot">0, 255</span><span class="co">]</span> â†’ <span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span></span>
<span id="cb16-107"><a href="#cb16-107" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Normalize: Subtract mean, divide by std per channel</span>
<span id="cb16-108"><a href="#cb16-108" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Data augmentation (training): flips, crops, color jitter</span>
<span id="cb16-109"><a href="#cb16-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-110"><a href="#cb16-110" aria-hidden="true" tabindex="-1"></a>**Best Practices**</span>
<span id="cb16-111"><a href="#cb16-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-112"><a href="#cb16-112" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>âœ… Start with pre-trained models (transfer learning)</span>
<span id="cb16-113"><a href="#cb16-113" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>âœ… Use proven architectures (ResNet, ConvNeXt)</span>
<span id="cb16-114"><a href="#cb16-114" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>âœ… Apply proper preprocessing (resize, normalize)</span>
<span id="cb16-115"><a href="#cb16-115" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>âœ… Use cross-entropy loss for classification</span>
<span id="cb16-116"><a href="#cb16-116" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>âœ… Monitor both training and validation metrics</span>
<span id="cb16-117"><a href="#cb16-117" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>âœ… Consider calibration (temperature scaling, label smoothing)</span>
<span id="cb16-118"><a href="#cb16-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-119"><a href="#cb16-119" aria-hidden="true" tabindex="-1"></a>**Key Takeaway**: Classification success comes from combining strong architectures (ResNets), probabilistic foundations (softmax + cross-entropy), transfer learning, and proper training practices.</span>
<span id="cb16-120"><a href="#cb16-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-121"><a href="#cb16-121" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-122"><a href="#cb16-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-123"><a href="#cb16-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-124"><a href="#cb16-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-125"><a href="#cb16-125" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction</span></span>
<span id="cb16-126"><a href="#cb16-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-127"><a href="#cb16-127" aria-hidden="true" tabindex="-1"></a>Image classification is a core task of computer vision. In image classification, an image is assigned to a **predefined set** of classes. There are different variants of image classification:</span>
<span id="cb16-128"><a href="#cb16-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-129"><a href="#cb16-129" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>in **multi-class classification** there are $&gt; 2$ classes</span>
<span id="cb16-130"><a href="#cb16-130" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>in **binary classification** there are 2 classes</span>
<span id="cb16-131"><a href="#cb16-131" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>in **multi-label classification** there are several sets of classes.</span>
<span id="cb16-132"><a href="#cb16-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-133"><a href="#cb16-133" aria-hidden="true" tabindex="-1"></a>@fig-classification-classification-illustration illustrates a multi-class classification task.</span>
<span id="cb16-134"><a href="#cb16-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-135"><a href="#cb16-135" aria-hidden="true" tabindex="-1"></a>::: {#fig-classification-classification-illustration}</span>
<span id="cb16-136"><a href="#cb16-136" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}classification_illustration.jpg)</span></span>
<span id="cb16-137"><a href="#cb16-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-138"><a href="#cb16-138" aria-hidden="true" tabindex="-1"></a>Image Classification example.</span>
<span id="cb16-139"><a href="#cb16-139" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-140"><a href="#cb16-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-141"><a href="#cb16-141" aria-hidden="true" tabindex="-1"></a>@fig-image-classifier-example shows an example from the paper by @krizhevsky_imagenet_2012, which achieved the best results in the 2012 ImageNet competition, demonstrating how well CNNs work. Note that each image had to be assigned to one of 1'000 classes.</span>
<span id="cb16-142"><a href="#cb16-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-143"><a href="#cb16-143" aria-hidden="true" tabindex="-1"></a>::: {#fig-image-classifier-example}</span>
<span id="cb16-144"><a href="#cb16-144" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}image_classification_example.png)</span></span>
<span id="cb16-145"><a href="#cb16-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-146"><a href="#cb16-146" aria-hidden="true" tabindex="-1"></a>Image Classification example (from @krizhevsky_imagenet_2012).</span>
<span id="cb16-147"><a href="#cb16-147" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-148"><a href="#cb16-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-149"><a href="#cb16-149" aria-hidden="true" tabindex="-1"></a>@fig-classification-camera-traps illustrates the challenge with images taken by camera traps, which need to be classified along animal species.</span>
<span id="cb16-150"><a href="#cb16-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-151"><a href="#cb16-151" aria-hidden="true" tabindex="-1"></a>::: {#fig-classification-camera-traps}</span>
<span id="cb16-152"><a href="#cb16-152" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}camera_traps.png)</span></span>
<span id="cb16-153"><a href="#cb16-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-154"><a href="#cb16-154" aria-hidden="true" tabindex="-1"></a>Example images from camera traps.</span>
<span id="cb16-155"><a href="#cb16-155" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-156"><a href="#cb16-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-157"><a href="#cb16-157" aria-hidden="true" tabindex="-1"></a><span class="fu">## Parametric Approach</span></span>
<span id="cb16-158"><a href="#cb16-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-159"><a href="#cb16-159" aria-hidden="true" tabindex="-1"></a>In a parametric approach, we seek a model of the following form:</span>
<span id="cb16-160"><a href="#cb16-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-161"><a href="#cb16-161" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-162"><a href="#cb16-162" aria-hidden="true" tabindex="-1"></a>\hat{y}^{(i)} = f(\theta, \vect{x}^{(i)})</span>
<span id="cb16-163"><a href="#cb16-163" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-164"><a href="#cb16-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-165"><a href="#cb16-165" aria-hidden="true" tabindex="-1"></a>We want to find model parameters $\theta$ that output a score/prediction $\hat{y}^{(i)}$ for any data points $\vect{x}^{(i)}$ for each class $k \in K$. We then want to assess how good this score is with the help of a loss function.</span>
<span id="cb16-166"><a href="#cb16-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-167"><a href="#cb16-167" aria-hidden="true" tabindex="-1"></a>Note: The model parameters $\theta$ are all the learnable parameters of a model, e.g. the filters of the convolutional layers.</span>
<span id="cb16-168"><a href="#cb16-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-169"><a href="#cb16-169" aria-hidden="true" tabindex="-1"></a><span class="fu">### Softmax Classifier</span></span>
<span id="cb16-170"><a href="#cb16-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-171"><a href="#cb16-171" aria-hidden="true" tabindex="-1"></a>With a softmax classifier, we interpret model predictions/scores as probabilities of class memberships: $P(Y=\vect{y}^{(i)}| X = \vect{x}^{(i)})$. We interpret the output of a model as the parameters of a <span class="co">[</span><span class="ot">Categorical Distribution</span><span class="co">](https://en.wikipedia.org/wiki/Categorical_distribution)</span> over all possible classes.</span>
<span id="cb16-172"><a href="#cb16-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-173"><a href="#cb16-173" aria-hidden="true" tabindex="-1"></a>To obtain a valid probability distribution, the untransformed outputs $\vect{z}$, also called **logits**, of a model are transformed with the softmax function $\sigma(\vect{z})$:</span>
<span id="cb16-174"><a href="#cb16-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-175"><a href="#cb16-175" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-176"><a href="#cb16-176" aria-hidden="true" tabindex="-1"></a>P(Y = k| X = \vect{x}^{(i)}) = \sigma(\vect{z})_k = \frac{e^{z_k}}{\sum_i^K e^{z_i}}</span>
<span id="cb16-177"><a href="#cb16-177" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-178"><a href="#cb16-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-179"><a href="#cb16-179" aria-hidden="true" tabindex="-1"></a>@fig-classification-logits-to-probs shows an example of the effect of the softmax transformation.</span>
<span id="cb16-180"><a href="#cb16-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-183"><a href="#cb16-183" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-184"><a href="#cb16-184" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb16-185"><a href="#cb16-185" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb16-186"><a href="#cb16-186" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 600</span></span>
<span id="cb16-187"><a href="#cb16-187" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-classification-logits-to-probs</span></span>
<span id="cb16-188"><a href="#cb16-188" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap:</span></span>
<span id="cb16-189"><a href="#cb16-189" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - "Logits (left) to probabilities with the Softmax function (right)."</span></span>
<span id="cb16-190"><a href="#cb16-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-191"><a href="#cb16-191" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb16-192"><a href="#cb16-192" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb16-193"><a href="#cb16-193" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb16-194"><a href="#cb16-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-195"><a href="#cb16-195" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">123</span>)</span>
<span id="cb16-196"><a href="#cb16-196" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> np.random.normal(size<span class="op">=</span>(<span class="dv">10</span>, )) <span class="op">*</span> <span class="dv">2</span></span>
<span id="cb16-197"><a href="#cb16-197" aria-hidden="true" tabindex="-1"></a>softmax_output <span class="op">=</span> np.exp(logits) <span class="op">/</span> np.exp(logits).<span class="bu">sum</span>()</span>
<span id="cb16-198"><a href="#cb16-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-199"><a href="#cb16-199" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(ncols<span class="op">=</span><span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">9</span>, <span class="dv">3</span>))</span>
<span id="cb16-200"><a href="#cb16-200" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> sns.barplot(x<span class="op">=</span>[i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">10</span>)], y<span class="op">=</span>logits, ax<span class="op">=</span>ax[<span class="dv">0</span>]).<span class="bu">set</span>(</span>
<span id="cb16-201"><a href="#cb16-201" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">"Logits"</span>)</span>
<span id="cb16-202"><a href="#cb16-202" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> sns.barplot(x<span class="op">=</span>[i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">10</span>)], y<span class="op">=</span>softmax_output, ax<span class="op">=</span>ax[<span class="dv">1</span>]).<span class="bu">set</span>(</span>
<span id="cb16-203"><a href="#cb16-203" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">"Softmax"</span>, ylim<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb16-204"><a href="#cb16-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-205"><a href="#cb16-205" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb16-206"><a href="#cb16-206" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb16-207"><a href="#cb16-207" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-208"><a href="#cb16-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-209"><a href="#cb16-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-210"><a href="#cb16-210" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- {{&lt; embed /demos/logits_to_probabilities.ipynb#fig-demo-logits-to-probs &gt;}} --&gt;</span></span>
<span id="cb16-211"><a href="#cb16-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-212"><a href="#cb16-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-213"><a href="#cb16-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-214"><a href="#cb16-214" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title="ðŸŽ® Interactive Exploration" collapse="true"}</span>
<span id="cb16-215"><a href="#cb16-215" aria-hidden="true" tabindex="-1"></a>**Try different logit distributions** and see how softmax transforms them into probabilities!</span>
<span id="cb16-216"><a href="#cb16-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-219"><a href="#cb16-219" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-220"><a href="#cb16-220" aria-hidden="true" tabindex="-1"></a><span class="co"># | eval: true</span></span>
<span id="cb16-221"><a href="#cb16-221" aria-hidden="true" tabindex="-1"></a><span class="co"># | echo: false</span></span>
<span id="cb16-222"><a href="#cb16-222" aria-hidden="true" tabindex="-1"></a><span class="co"># | fig-width: 800</span></span>
<span id="cb16-223"><a href="#cb16-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-224"><a href="#cb16-224" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb16-225"><a href="#cb16-225" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.graph_objects <span class="im">as</span> go</span>
<span id="cb16-226"><a href="#cb16-226" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> plotly.subplots <span class="im">import</span> make_subplots</span>
<span id="cb16-227"><a href="#cb16-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-228"><a href="#cb16-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-229"><a href="#cb16-229" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> softmax(logits):</span>
<span id="cb16-230"><a href="#cb16-230" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Compute softmax values for logits."""</span></span>
<span id="cb16-231"><a href="#cb16-231" aria-hidden="true" tabindex="-1"></a>    exp_logits <span class="op">=</span> np.exp(logits <span class="op">-</span> np.<span class="bu">max</span>(logits))  <span class="co"># numerical stability</span></span>
<span id="cb16-232"><a href="#cb16-232" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> exp_logits <span class="op">/</span> np.<span class="bu">sum</span>(exp_logits)</span>
<span id="cb16-233"><a href="#cb16-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-234"><a href="#cb16-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-235"><a href="#cb16-235" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _padded_range(vals, frac<span class="op">=</span><span class="fl">0.18</span>, min_abs_pad<span class="op">=</span><span class="fl">0.35</span>):</span>
<span id="cb16-236"><a href="#cb16-236" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Return a y-range padded enough so 'textposition=outside' is always visible."""</span></span>
<span id="cb16-237"><a href="#cb16-237" aria-hidden="true" tabindex="-1"></a>    vmin, vmax <span class="op">=</span> <span class="bu">float</span>(np.<span class="bu">min</span>(vals)), <span class="bu">float</span>(np.<span class="bu">max</span>(vals))</span>
<span id="cb16-238"><a href="#cb16-238" aria-hidden="true" tabindex="-1"></a>    span <span class="op">=</span> <span class="bu">max</span>(<span class="fl">1e-9</span>, vmax <span class="op">-</span> vmin)</span>
<span id="cb16-239"><a href="#cb16-239" aria-hidden="true" tabindex="-1"></a>    pad <span class="op">=</span> <span class="bu">max</span>(min_abs_pad, span <span class="op">*</span> frac)</span>
<span id="cb16-240"><a href="#cb16-240" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [vmin <span class="op">-</span> pad, vmax <span class="op">+</span> pad]</span>
<span id="cb16-241"><a href="#cb16-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-242"><a href="#cb16-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-243"><a href="#cb16-243" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_interactive_logits_explorer():</span>
<span id="cb16-244"><a href="#cb16-244" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Interactive exploration of logits â†’ probabilities."""</span></span>
<span id="cb16-245"><a href="#cb16-245" aria-hidden="true" tabindex="-1"></a>    n_classes <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb16-246"><a href="#cb16-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-247"><a href="#cb16-247" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Scenarios</span></span>
<span id="cb16-248"><a href="#cb16-248" aria-hidden="true" tabindex="-1"></a>    scenarios <span class="op">=</span> {</span>
<span id="cb16-249"><a href="#cb16-249" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Uniform Logits"</span>: [<span class="fl">1.0</span>, <span class="fl">1.0</span>, <span class="fl">1.0</span>, <span class="fl">1.0</span>, <span class="fl">1.0</span>],</span>
<span id="cb16-250"><a href="#cb16-250" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Clear Winner"</span>: [<span class="fl">5.0</span>, <span class="fl">1.0</span>, <span class="fl">0.5</span>, <span class="fl">0.0</span>, <span class="op">-</span><span class="fl">0.5</span>],</span>
<span id="cb16-251"><a href="#cb16-251" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Close Competition"</span>: [<span class="fl">2.0</span>, <span class="fl">1.8</span>, <span class="fl">1.5</span>, <span class="fl">1.2</span>, <span class="fl">1.0</span>],</span>
<span id="cb16-252"><a href="#cb16-252" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Very Confident"</span>: [<span class="fl">10.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>],</span>
<span id="cb16-253"><a href="#cb16-253" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Negative Logits"</span>: [<span class="op">-</span><span class="fl">0.5</span>, <span class="op">-</span><span class="fl">1.0</span>, <span class="op">-</span><span class="fl">1.5</span>, <span class="op">-</span><span class="fl">2.0</span>, <span class="op">-</span><span class="fl">2.5</span>],</span>
<span id="cb16-254"><a href="#cb16-254" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Mixed Range"</span>: [<span class="fl">3.0</span>, <span class="fl">0.0</span>, <span class="op">-</span><span class="fl">1.0</span>, <span class="fl">0.5</span>, <span class="fl">2.0</span>],</span>
<span id="cb16-255"><a href="#cb16-255" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Two Leaders"</span>: [<span class="fl">4.0</span>, <span class="fl">3.8</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>],</span>
<span id="cb16-256"><a href="#cb16-256" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Random-ish"</span>: [<span class="fl">1.5</span>, <span class="op">-</span><span class="fl">0.3</span>, <span class="fl">2.1</span>, <span class="fl">0.8</span>, <span class="op">-</span><span class="fl">0.8</span>],</span>
<span id="cb16-257"><a href="#cb16-257" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb16-258"><a href="#cb16-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-259"><a href="#cb16-259" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initial scenario</span></span>
<span id="cb16-260"><a href="#cb16-260" aria-hidden="true" tabindex="-1"></a>    initial_name <span class="op">=</span> <span class="st">"Clear Winner"</span></span>
<span id="cb16-261"><a href="#cb16-261" aria-hidden="true" tabindex="-1"></a>    initial_logits <span class="op">=</span> scenarios[initial_name]</span>
<span id="cb16-262"><a href="#cb16-262" aria-hidden="true" tabindex="-1"></a>    initial_probs <span class="op">=</span> softmax(initial_logits)</span>
<span id="cb16-263"><a href="#cb16-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-264"><a href="#cb16-264" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Figure and subplots</span></span>
<span id="cb16-265"><a href="#cb16-265" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> make_subplots(</span>
<span id="cb16-266"><a href="#cb16-266" aria-hidden="true" tabindex="-1"></a>        rows<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb16-267"><a href="#cb16-267" aria-hidden="true" tabindex="-1"></a>        cols<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb16-268"><a href="#cb16-268" aria-hidden="true" tabindex="-1"></a>        subplot_titles<span class="op">=</span>(<span class="st">"Logits (Raw Model Outputs)"</span>, <span class="st">"Probabilities (After Softmax)"</span>),</span>
<span id="cb16-269"><a href="#cb16-269" aria-hidden="true" tabindex="-1"></a>        horizontal_spacing<span class="op">=</span><span class="fl">0.12</span>,</span>
<span id="cb16-270"><a href="#cb16-270" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb16-271"><a href="#cb16-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-272"><a href="#cb16-272" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Nudge subplot titles slightly down to leave a lane for the dropdown above them</span></span>
<span id="cb16-273"><a href="#cb16-273" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> ann <span class="kw">in</span> fig.layout.annotations:</span>
<span id="cb16-274"><a href="#cb16-274" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> ann.text <span class="kw">in</span> (<span class="st">"Logits (Raw Model Outputs)"</span>, <span class="st">"Probabilities (After Softmax)"</span>):</span>
<span id="cb16-275"><a href="#cb16-275" aria-hidden="true" tabindex="-1"></a>            ann.y <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb16-276"><a href="#cb16-276" aria-hidden="true" tabindex="-1"></a>            ann.yanchor <span class="op">=</span> <span class="st">"bottom"</span></span>
<span id="cb16-277"><a href="#cb16-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-278"><a href="#cb16-278" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Colors for initial state</span></span>
<span id="cb16-279"><a href="#cb16-279" aria-hidden="true" tabindex="-1"></a>    max_logit_idx <span class="op">=</span> <span class="bu">int</span>(np.argmax(initial_logits))</span>
<span id="cb16-280"><a href="#cb16-280" aria-hidden="true" tabindex="-1"></a>    logit_colors <span class="op">=</span> [</span>
<span id="cb16-281"><a href="#cb16-281" aria-hidden="true" tabindex="-1"></a>        <span class="st">"lightblue"</span> <span class="cf">if</span> i <span class="op">!=</span> max_logit_idx <span class="cf">else</span> <span class="st">"steelblue"</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_classes)</span>
<span id="cb16-282"><a href="#cb16-282" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb16-283"><a href="#cb16-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-284"><a href="#cb16-284" aria-hidden="true" tabindex="-1"></a>    max_prob_idx <span class="op">=</span> <span class="bu">int</span>(np.argmax(initial_probs))</span>
<span id="cb16-285"><a href="#cb16-285" aria-hidden="true" tabindex="-1"></a>    prob_colors <span class="op">=</span> [</span>
<span id="cb16-286"><a href="#cb16-286" aria-hidden="true" tabindex="-1"></a>        <span class="st">"lightcoral"</span> <span class="cf">if</span> i <span class="op">!=</span> max_prob_idx <span class="cf">else</span> <span class="st">"crimson"</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_classes)</span>
<span id="cb16-287"><a href="#cb16-287" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb16-288"><a href="#cb16-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-289"><a href="#cb16-289" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Left: logits bars</span></span>
<span id="cb16-290"><a href="#cb16-290" aria-hidden="true" tabindex="-1"></a>    fig.add_trace(</span>
<span id="cb16-291"><a href="#cb16-291" aria-hidden="true" tabindex="-1"></a>        go.Bar(</span>
<span id="cb16-292"><a href="#cb16-292" aria-hidden="true" tabindex="-1"></a>            x<span class="op">=</span>[<span class="ss">f"Class </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_classes)],</span>
<span id="cb16-293"><a href="#cb16-293" aria-hidden="true" tabindex="-1"></a>            y<span class="op">=</span>initial_logits,</span>
<span id="cb16-294"><a href="#cb16-294" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"Logits"</span>,</span>
<span id="cb16-295"><a href="#cb16-295" aria-hidden="true" tabindex="-1"></a>            marker_color<span class="op">=</span>logit_colors,</span>
<span id="cb16-296"><a href="#cb16-296" aria-hidden="true" tabindex="-1"></a>            text<span class="op">=</span>[<span class="ss">f"</span><span class="sc">{</span>z<span class="sc">:.2f}</span><span class="ss">"</span> <span class="cf">for</span> z <span class="kw">in</span> initial_logits],</span>
<span id="cb16-297"><a href="#cb16-297" aria-hidden="true" tabindex="-1"></a>            textposition<span class="op">=</span><span class="st">"outside"</span>,</span>
<span id="cb16-298"><a href="#cb16-298" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb16-299"><a href="#cb16-299" aria-hidden="true" tabindex="-1"></a>        row<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb16-300"><a href="#cb16-300" aria-hidden="true" tabindex="-1"></a>        col<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb16-301"><a href="#cb16-301" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb16-302"><a href="#cb16-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-303"><a href="#cb16-303" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Right: probability bars</span></span>
<span id="cb16-304"><a href="#cb16-304" aria-hidden="true" tabindex="-1"></a>    fig.add_trace(</span>
<span id="cb16-305"><a href="#cb16-305" aria-hidden="true" tabindex="-1"></a>        go.Bar(</span>
<span id="cb16-306"><a href="#cb16-306" aria-hidden="true" tabindex="-1"></a>            x<span class="op">=</span>[<span class="ss">f"Class </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_classes)],</span>
<span id="cb16-307"><a href="#cb16-307" aria-hidden="true" tabindex="-1"></a>            y<span class="op">=</span>initial_probs,</span>
<span id="cb16-308"><a href="#cb16-308" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"Probabilities"</span>,</span>
<span id="cb16-309"><a href="#cb16-309" aria-hidden="true" tabindex="-1"></a>            marker_color<span class="op">=</span>prob_colors,</span>
<span id="cb16-310"><a href="#cb16-310" aria-hidden="true" tabindex="-1"></a>            text<span class="op">=</span>[<span class="ss">f"</span><span class="sc">{</span>p<span class="sc">:.3f}</span><span class="ss">"</span> <span class="cf">for</span> p <span class="kw">in</span> initial_probs],</span>
<span id="cb16-311"><a href="#cb16-311" aria-hidden="true" tabindex="-1"></a>            textposition<span class="op">=</span><span class="st">"outside"</span>,</span>
<span id="cb16-312"><a href="#cb16-312" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb16-313"><a href="#cb16-313" aria-hidden="true" tabindex="-1"></a>        row<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb16-314"><a href="#cb16-314" aria-hidden="true" tabindex="-1"></a>        col<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb16-315"><a href="#cb16-315" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb16-316"><a href="#cb16-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-317"><a href="#cb16-317" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Dropdown buttons</span></span>
<span id="cb16-318"><a href="#cb16-318" aria-hidden="true" tabindex="-1"></a>    buttons <span class="op">=</span> []</span>
<span id="cb16-319"><a href="#cb16-319" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> name, logits <span class="kw">in</span> scenarios.items():</span>
<span id="cb16-320"><a href="#cb16-320" aria-hidden="true" tabindex="-1"></a>        probs <span class="op">=</span> softmax(logits)</span>
<span id="cb16-321"><a href="#cb16-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-322"><a href="#cb16-322" aria-hidden="true" tabindex="-1"></a>        max_logit_idx <span class="op">=</span> <span class="bu">int</span>(np.argmax(logits))</span>
<span id="cb16-323"><a href="#cb16-323" aria-hidden="true" tabindex="-1"></a>        logit_colors <span class="op">=</span> [</span>
<span id="cb16-324"><a href="#cb16-324" aria-hidden="true" tabindex="-1"></a>            <span class="st">"lightblue"</span> <span class="cf">if</span> i <span class="op">!=</span> max_logit_idx <span class="cf">else</span> <span class="st">"steelblue"</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_classes)</span>
<span id="cb16-325"><a href="#cb16-325" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb16-326"><a href="#cb16-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-327"><a href="#cb16-327" aria-hidden="true" tabindex="-1"></a>        max_prob_idx <span class="op">=</span> <span class="bu">int</span>(np.argmax(probs))</span>
<span id="cb16-328"><a href="#cb16-328" aria-hidden="true" tabindex="-1"></a>        prob_colors <span class="op">=</span> [</span>
<span id="cb16-329"><a href="#cb16-329" aria-hidden="true" tabindex="-1"></a>            <span class="st">"lightcoral"</span> <span class="cf">if</span> i <span class="op">!=</span> max_prob_idx <span class="cf">else</span> <span class="st">"crimson"</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_classes)</span>
<span id="cb16-330"><a href="#cb16-330" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb16-331"><a href="#cb16-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-332"><a href="#cb16-332" aria-hidden="true" tabindex="-1"></a>        entropy <span class="op">=</span> <span class="op">-</span>np.<span class="bu">sum</span>(probs <span class="op">*</span> np.log(probs <span class="op">+</span> <span class="fl">1e-10</span>))</span>
<span id="cb16-333"><a href="#cb16-333" aria-hidden="true" tabindex="-1"></a>        max_prob <span class="op">=</span> <span class="bu">float</span>(np.<span class="bu">max</span>(probs))</span>
<span id="cb16-334"><a href="#cb16-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-335"><a href="#cb16-335" aria-hidden="true" tabindex="-1"></a>        y1_range <span class="op">=</span> _padded_range(logits)</span>
<span id="cb16-336"><a href="#cb16-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-337"><a href="#cb16-337" aria-hidden="true" tabindex="-1"></a>        buttons.append(</span>
<span id="cb16-338"><a href="#cb16-338" aria-hidden="true" tabindex="-1"></a>            <span class="bu">dict</span>(</span>
<span id="cb16-339"><a href="#cb16-339" aria-hidden="true" tabindex="-1"></a>                label<span class="op">=</span><span class="ss">f"</span><span class="sc">{</span>name<span class="sc">}</span><span class="ss"> (max prob=</span><span class="sc">{</span>max_prob<span class="sc">:.3f}</span><span class="ss">, H=</span><span class="sc">{</span>entropy<span class="sc">:.2f}</span><span class="ss">)"</span>,</span>
<span id="cb16-340"><a href="#cb16-340" aria-hidden="true" tabindex="-1"></a>                method<span class="op">=</span><span class="st">"update"</span>,</span>
<span id="cb16-341"><a href="#cb16-341" aria-hidden="true" tabindex="-1"></a>                args<span class="op">=</span>[</span>
<span id="cb16-342"><a href="#cb16-342" aria-hidden="true" tabindex="-1"></a>                    {</span>
<span id="cb16-343"><a href="#cb16-343" aria-hidden="true" tabindex="-1"></a>                        <span class="st">"y"</span>: [logits, probs],</span>
<span id="cb16-344"><a href="#cb16-344" aria-hidden="true" tabindex="-1"></a>                        <span class="st">"text"</span>: [</span>
<span id="cb16-345"><a href="#cb16-345" aria-hidden="true" tabindex="-1"></a>                            [<span class="ss">f"</span><span class="sc">{</span>z<span class="sc">:.2f}</span><span class="ss">"</span> <span class="cf">for</span> z <span class="kw">in</span> logits],</span>
<span id="cb16-346"><a href="#cb16-346" aria-hidden="true" tabindex="-1"></a>                            [<span class="ss">f"</span><span class="sc">{</span>p<span class="sc">:.3f}</span><span class="ss">"</span> <span class="cf">for</span> p <span class="kw">in</span> probs],</span>
<span id="cb16-347"><a href="#cb16-347" aria-hidden="true" tabindex="-1"></a>                        ],</span>
<span id="cb16-348"><a href="#cb16-348" aria-hidden="true" tabindex="-1"></a>                        <span class="st">"marker.color"</span>: [logit_colors, prob_colors],</span>
<span id="cb16-349"><a href="#cb16-349" aria-hidden="true" tabindex="-1"></a>                    },</span>
<span id="cb16-350"><a href="#cb16-350" aria-hidden="true" tabindex="-1"></a>                    {</span>
<span id="cb16-351"><a href="#cb16-351" aria-hidden="true" tabindex="-1"></a>                        <span class="st">"yaxis.range"</span>: y1_range,  <span class="co"># left subplot (logits)</span></span>
<span id="cb16-352"><a href="#cb16-352" aria-hidden="true" tabindex="-1"></a>                        <span class="st">"yaxis2.range"</span>: [<span class="dv">0</span>, <span class="fl">1.1</span>],  <span class="co"># right subplot fixed</span></span>
<span id="cb16-353"><a href="#cb16-353" aria-hidden="true" tabindex="-1"></a>                        <span class="st">"annotations[2].text"</span>: <span class="ss">f"Sum = </span><span class="sc">{</span><span class="bu">sum</span>(probs)<span class="sc">:.6f}</span><span class="ss">"</span>,  <span class="co"># keep "Sum=" annotation current</span></span>
<span id="cb16-354"><a href="#cb16-354" aria-hidden="true" tabindex="-1"></a>                    },</span>
<span id="cb16-355"><a href="#cb16-355" aria-hidden="true" tabindex="-1"></a>                ],</span>
<span id="cb16-356"><a href="#cb16-356" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb16-357"><a href="#cb16-357" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb16-358"><a href="#cb16-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-359"><a href="#cb16-359" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Layout: compact head space; dropdown above titles; opens upward</span></span>
<span id="cb16-360"><a href="#cb16-360" aria-hidden="true" tabindex="-1"></a>    fig.update_layout(</span>
<span id="cb16-361"><a href="#cb16-361" aria-hidden="true" tabindex="-1"></a>        updatemenus<span class="op">=</span>[</span>
<span id="cb16-362"><a href="#cb16-362" aria-hidden="true" tabindex="-1"></a>            <span class="bu">dict</span>(</span>
<span id="cb16-363"><a href="#cb16-363" aria-hidden="true" tabindex="-1"></a>                <span class="bu">type</span><span class="op">=</span><span class="st">"dropdown"</span>,</span>
<span id="cb16-364"><a href="#cb16-364" aria-hidden="true" tabindex="-1"></a>                direction<span class="op">=</span><span class="st">"down"</span>,</span>
<span id="cb16-365"><a href="#cb16-365" aria-hidden="true" tabindex="-1"></a>                x<span class="op">=</span><span class="fl">0.5</span>,</span>
<span id="cb16-366"><a href="#cb16-366" aria-hidden="true" tabindex="-1"></a>                xanchor<span class="op">=</span><span class="st">"center"</span>,</span>
<span id="cb16-367"><a href="#cb16-367" aria-hidden="true" tabindex="-1"></a>                y<span class="op">=</span><span class="fl">1.06</span>,</span>
<span id="cb16-368"><a href="#cb16-368" aria-hidden="true" tabindex="-1"></a>                yanchor<span class="op">=</span><span class="st">"bottom"</span>,  <span class="co"># sits just above titles</span></span>
<span id="cb16-369"><a href="#cb16-369" aria-hidden="true" tabindex="-1"></a>                buttons<span class="op">=</span>buttons,</span>
<span id="cb16-370"><a href="#cb16-370" aria-hidden="true" tabindex="-1"></a>                pad<span class="op">=</span><span class="bu">dict</span>(t<span class="op">=</span><span class="dv">4</span>, r<span class="op">=</span><span class="dv">6</span>, l<span class="op">=</span><span class="dv">6</span>, b<span class="op">=</span><span class="dv">10</span>),</span>
<span id="cb16-371"><a href="#cb16-371" aria-hidden="true" tabindex="-1"></a>                bgcolor<span class="op">=</span><span class="st">"white"</span>,</span>
<span id="cb16-372"><a href="#cb16-372" aria-hidden="true" tabindex="-1"></a>                bordercolor<span class="op">=</span><span class="st">"#ddd"</span>,</span>
<span id="cb16-373"><a href="#cb16-373" aria-hidden="true" tabindex="-1"></a>                borderwidth<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb16-374"><a href="#cb16-374" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb16-375"><a href="#cb16-375" aria-hidden="true" tabindex="-1"></a>        ],</span>
<span id="cb16-376"><a href="#cb16-376" aria-hidden="true" tabindex="-1"></a>        height<span class="op">=</span><span class="dv">380</span>,  <span class="co"># compact but comfortable</span></span>
<span id="cb16-377"><a href="#cb16-377" aria-hidden="true" tabindex="-1"></a>        margin<span class="op">=</span><span class="bu">dict</span>(t<span class="op">=</span><span class="dv">90</span>, r<span class="op">=</span><span class="dv">40</span>, b<span class="op">=</span><span class="dv">60</span>, l<span class="op">=</span><span class="dv">60</span>),</span>
<span id="cb16-378"><a href="#cb16-378" aria-hidden="true" tabindex="-1"></a>        showlegend<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb16-379"><a href="#cb16-379" aria-hidden="true" tabindex="-1"></a>        uniformtext_minsize<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb16-380"><a href="#cb16-380" aria-hidden="true" tabindex="-1"></a>        uniformtext_mode<span class="op">=</span><span class="st">"hide"</span>,</span>
<span id="cb16-381"><a href="#cb16-381" aria-hidden="true" tabindex="-1"></a>        yaxis1<span class="op">=</span><span class="bu">dict</span>(</span>
<span id="cb16-382"><a href="#cb16-382" aria-hidden="true" tabindex="-1"></a>            title<span class="op">=</span><span class="st">"Logit Value"</span>, <span class="bu">range</span><span class="op">=</span>_padded_range(initial_logits), automargin<span class="op">=</span><span class="va">True</span></span>
<span id="cb16-383"><a href="#cb16-383" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb16-384"><a href="#cb16-384" aria-hidden="true" tabindex="-1"></a>        yaxis2<span class="op">=</span><span class="bu">dict</span>(title<span class="op">=</span><span class="st">"Probability"</span>, <span class="bu">range</span><span class="op">=</span>[<span class="dv">0</span>, <span class="fl">1.1</span>], automargin<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb16-385"><a href="#cb16-385" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb16-386"><a href="#cb16-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-387"><a href="#cb16-387" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sum annotation for probabilities (right subplot)</span></span>
<span id="cb16-388"><a href="#cb16-388" aria-hidden="true" tabindex="-1"></a>    fig.add_annotation(</span>
<span id="cb16-389"><a href="#cb16-389" aria-hidden="true" tabindex="-1"></a>        x<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb16-390"><a href="#cb16-390" aria-hidden="true" tabindex="-1"></a>        y<span class="op">=</span><span class="fl">0.98</span>,</span>
<span id="cb16-391"><a href="#cb16-391" aria-hidden="true" tabindex="-1"></a>        text<span class="op">=</span><span class="ss">f"Sum = </span><span class="sc">{</span><span class="bu">sum</span>(initial_probs)<span class="sc">:.6f}</span><span class="ss">"</span>,</span>
<span id="cb16-392"><a href="#cb16-392" aria-hidden="true" tabindex="-1"></a>        showarrow<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb16-393"><a href="#cb16-393" aria-hidden="true" tabindex="-1"></a>        xref<span class="op">=</span><span class="st">"x2"</span>,</span>
<span id="cb16-394"><a href="#cb16-394" aria-hidden="true" tabindex="-1"></a>        yref<span class="op">=</span><span class="st">"y2"</span>,</span>
<span id="cb16-395"><a href="#cb16-395" aria-hidden="true" tabindex="-1"></a>        font<span class="op">=</span><span class="bu">dict</span>(size<span class="op">=</span><span class="dv">12</span>, color<span class="op">=</span><span class="st">"green"</span>),</span>
<span id="cb16-396"><a href="#cb16-396" aria-hidden="true" tabindex="-1"></a>        bgcolor<span class="op">=</span><span class="st">"lightyellow"</span>,</span>
<span id="cb16-397"><a href="#cb16-397" aria-hidden="true" tabindex="-1"></a>        bordercolor<span class="op">=</span><span class="st">"green"</span>,</span>
<span id="cb16-398"><a href="#cb16-398" aria-hidden="true" tabindex="-1"></a>        borderwidth<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb16-399"><a href="#cb16-399" aria-hidden="true" tabindex="-1"></a>        borderpad<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb16-400"><a href="#cb16-400" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb16-401"><a href="#cb16-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-402"><a href="#cb16-402" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> fig</span>
<span id="cb16-403"><a href="#cb16-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-404"><a href="#cb16-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-405"><a href="#cb16-405" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> create_interactive_logits_explorer()</span>
<span id="cb16-406"><a href="#cb16-406" aria-hidden="true" tabindex="-1"></a>fig.show()</span>
<span id="cb16-407"><a href="#cb16-407" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-408"><a href="#cb16-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-409"><a href="#cb16-409" aria-hidden="true" tabindex="-1"></a>**Key observations**:</span>
<span id="cb16-410"><a href="#cb16-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-411"><a href="#cb16-411" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Logits can be any real number (negative, zero, or positive)</span>
<span id="cb16-412"><a href="#cb16-412" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Probabilities always sum to exactly 1.0 (see green box)</span>
<span id="cb16-413"><a href="#cb16-413" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Entropy** measures uncertainty: lower = more confident, higher = more uncertain</span>
<span id="cb16-414"><a href="#cb16-414" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Try "Uniform Logits" vs "Very Confident" to see the extremes!</span>
<span id="cb16-415"><a href="#cb16-415" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-416"><a href="#cb16-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-417"><a href="#cb16-417" aria-hidden="true" tabindex="-1"></a><span class="fu">## Loss Function</span></span>
<span id="cb16-418"><a href="#cb16-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-419"><a href="#cb16-419" aria-hidden="true" tabindex="-1"></a>Now we look at how to define the loss function $L(\m{X}, \vect{y}, \theta)$ for a multi-class classification problem.</span>
<span id="cb16-420"><a href="#cb16-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-421"><a href="#cb16-421" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb16-422"><a href="#cb16-422" aria-hidden="true" tabindex="-1"></a>The loss function for a binary classification problem is identical. If we have a multi-label classification problem we have multiple outputs and thus multiple softmax functions. The principle is identical.</span>
<span id="cb16-423"><a href="#cb16-423" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-424"><a href="#cb16-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-425"><a href="#cb16-425" aria-hidden="true" tabindex="-1"></a><span class="fu">### Likelihood</span></span>
<span id="cb16-426"><a href="#cb16-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-427"><a href="#cb16-427" aria-hidden="true" tabindex="-1"></a>The likelihood of a data point $(\vect{x}^{(i)}, y^{(i)})$ is the **probability of observing a data point, given a specific model**:</span>
<span id="cb16-428"><a href="#cb16-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-429"><a href="#cb16-429" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-430"><a href="#cb16-430" aria-hidden="true" tabindex="-1"></a>P(Y=y^{(i)}| X = \vect{x}^{(i)}) = f(\theta, \vect{x}^{(i)})</span>
<span id="cb16-431"><a href="#cb16-431" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-432"><a href="#cb16-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-433"><a href="#cb16-433" aria-hidden="true" tabindex="-1"></a>This means we formulate a model with a probabilistic interpretation of predictions: $f(\theta, \vect{x}^{(i)}): \mathbb{R}^{n} \mapsto <span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>$</span>
<span id="cb16-434"><a href="#cb16-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-435"><a href="#cb16-435" aria-hidden="true" tabindex="-1"></a>For a multi-class classification, the label vector is one-hot encoded $\vect{y}^{(i)} \in <span class="sc">\{</span>0, 1<span class="sc">\}</span>^K$, where the true class is coded with 1 and the rest with 0. The likelihood of a data point is defined as:</span>
<span id="cb16-436"><a href="#cb16-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-437"><a href="#cb16-437" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-438"><a href="#cb16-438" aria-hidden="true" tabindex="-1"></a>\prod_{j=1}^K  P(Y = j| X = \vect{x}^{(i)})^{y^{(i)}_j}</span>
<span id="cb16-439"><a href="#cb16-439" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-440"><a href="#cb16-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-441"><a href="#cb16-441" aria-hidden="true" tabindex="-1"></a>Since only one entry in $\vect{y}^{(i)}$ is 1, the likelihood is simply the prediction for the true class $P(Y = y^{(i)}| X = \vect{x}^{(i)})$.</span>
<span id="cb16-442"><a href="#cb16-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-443"><a href="#cb16-443" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">More Info</span><span class="co">](http://www.awebb.info/probability/2017/05/18/cross-entropy-and-log-likelihood.html)</span></span>
<span id="cb16-444"><a href="#cb16-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-445"><a href="#cb16-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-446"><a href="#cb16-446" aria-hidden="true" tabindex="-1"></a>::: {.callout-note appearance="simple" title="ðŸ“ Quiz: Cat vs Dog Likelihood" collapse="true"}</span>
<span id="cb16-447"><a href="#cb16-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-448"><a href="#cb16-448" aria-hidden="true" tabindex="-1"></a>**Scenario**: Your CNN model classifies 5 images from our dataset. Let's explore how different prediction confidences affect likelihood!</span>
<span id="cb16-449"><a href="#cb16-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-450"><a href="#cb16-450" aria-hidden="true" tabindex="-1"></a>**The Images**:</span>
<span id="cb16-451"><a href="#cb16-451" aria-hidden="true" tabindex="-1"></a><span class="al">![Cat]({{&lt; meta params.images_path &gt;}}cats_and_dogs/cat_01.png)</span>{width=80}</span>
<span id="cb16-452"><a href="#cb16-452" aria-hidden="true" tabindex="-1"></a><span class="al">![Dog]({{&lt; meta params.images_path &gt;}}cats_and_dogs/dog_01.png)</span>{width=80}</span>
<span id="cb16-453"><a href="#cb16-453" aria-hidden="true" tabindex="-1"></a><span class="al">![Cat]({{&lt; meta params.images_path &gt;}}cats_and_dogs/cat_02.png)</span>{width=80}</span>
<span id="cb16-454"><a href="#cb16-454" aria-hidden="true" tabindex="-1"></a><span class="al">![Dog]({{&lt; meta params.images_path &gt;}}cats_and_dogs/dog_02.png)</span>{width=80}</span>
<span id="cb16-455"><a href="#cb16-455" aria-hidden="true" tabindex="-1"></a><span class="al">![Dog]({{&lt; meta params.images_path &gt;}}cats_and_dogs/dog_03.png)</span>{width=80}</span>
<span id="cb16-456"><a href="#cb16-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-457"><a href="#cb16-457" aria-hidden="true" tabindex="-1"></a>**True Labels**: $\vect{y} = <span class="co">[</span><span class="ot">\text{Cat}, \text{Dog}, \text{Cat}, \text{Dog}, \text{Dog}</span><span class="co">]</span>$ or $<span class="co">[</span><span class="ot">0, 1, 0, 1, 1</span><span class="co">]</span>$ in binary encoding.</span>
<span id="cb16-458"><a href="#cb16-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-459"><a href="#cb16-459" aria-hidden="true" tabindex="-1"></a>**Question 1**: Model A predicts: $P(\text{Cat}) = <span class="co">[</span><span class="ot">0.9, 0.2, 0.8, 0.3, 0.1</span><span class="co">]</span>$. What's the likelihood?</span>
<span id="cb16-460"><a href="#cb16-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-461"><a href="#cb16-461" aria-hidden="true" tabindex="-1"></a>**Think first**: For each image, what's the probability of the *correct* class?</span>
<span id="cb16-462"><a href="#cb16-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-463"><a href="#cb16-463" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb16-464"><a href="#cb16-464" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Click for result<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb16-465"><a href="#cb16-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-468"><a href="#cb16-468" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-469"><a href="#cb16-469" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb16-470"><a href="#cb16-470" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb16-471"><a href="#cb16-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-472"><a href="#cb16-472" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb16-473"><a href="#cb16-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-474"><a href="#cb16-474" aria-hidden="true" tabindex="-1"></a><span class="co"># True labels: [Cat=0, Dog=1, Cat=0, Dog=1, Dog=1]</span></span>
<span id="cb16-475"><a href="#cb16-475" aria-hidden="true" tabindex="-1"></a>y_true <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>])</span>
<span id="cb16-476"><a href="#cb16-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-477"><a href="#cb16-477" aria-hidden="true" tabindex="-1"></a><span class="co"># Model A predictions: P(Cat) for each image</span></span>
<span id="cb16-478"><a href="#cb16-478" aria-hidden="true" tabindex="-1"></a>p_cat <span class="op">=</span> np.array([<span class="fl">0.9</span>, <span class="fl">0.2</span>, <span class="fl">0.8</span>, <span class="fl">0.3</span>, <span class="fl">0.1</span>])</span>
<span id="cb16-479"><a href="#cb16-479" aria-hidden="true" tabindex="-1"></a>p_dog <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> p_cat  <span class="co"># P(Dog) = 1 - P(Cat)</span></span>
<span id="cb16-480"><a href="#cb16-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-481"><a href="#cb16-481" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract probability of correct class for each image</span></span>
<span id="cb16-482"><a href="#cb16-482" aria-hidden="true" tabindex="-1"></a>p_correct <span class="op">=</span> np.where(y_true <span class="op">==</span> <span class="dv">0</span>, p_cat, p_dog)</span>
<span id="cb16-483"><a href="#cb16-483" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"P(correct class): </span><span class="sc">{</span>p_correct<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-484"><a href="#cb16-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-485"><a href="#cb16-485" aria-hidden="true" tabindex="-1"></a><span class="co"># Likelihood = product of correct probabilities</span></span>
<span id="cb16-486"><a href="#cb16-486" aria-hidden="true" tabindex="-1"></a>likelihood <span class="op">=</span> np.prod(p_correct)</span>
<span id="cb16-487"><a href="#cb16-487" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Likelihood = </span><span class="sc">{</span>likelihood<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb16-488"><a href="#cb16-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-489"><a href="#cb16-489" aria-hidden="true" tabindex="-1"></a><span class="co"># Individual contributions</span></span>
<span id="cb16-490"><a href="#cb16-490" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (true_label, prob) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(y_true, p_correct)):</span>
<span id="cb16-491"><a href="#cb16-491" aria-hidden="true" tabindex="-1"></a>    label_name <span class="op">=</span> <span class="st">"Cat"</span> <span class="cf">if</span> true_label <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> <span class="st">"Dog"</span></span>
<span id="cb16-492"><a href="#cb16-492" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Image </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span>label_name<span class="sc">}</span><span class="ss">): P = </span><span class="sc">{</span>prob<span class="sc">:.1f}</span><span class="ss">"</span>)</span>
<span id="cb16-493"><a href="#cb16-493" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-494"><a href="#cb16-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-495"><a href="#cb16-495" aria-hidden="true" tabindex="-1"></a>**Interpretation**: Model A is confident about the first cat (0.9) and the last dog (0.9), but uncertain about the middle images.</span>
<span id="cb16-496"><a href="#cb16-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-497"><a href="#cb16-497" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb16-498"><a href="#cb16-498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-499"><a href="#cb16-499" aria-hidden="true" tabindex="-1"></a>**Question 2**: Model B predicts: $P(\text{Cat}) = <span class="co">[</span><span class="ot">0.8, 0.1, 0.9, 0.2, 0.1</span><span class="co">]</span>$. Which model is better?</span>
<span id="cb16-500"><a href="#cb16-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-501"><a href="#cb16-501" aria-hidden="true" tabindex="-1"></a>**Hint**: Compare the likelihoods. Higher likelihood = better model!</span>
<span id="cb16-502"><a href="#cb16-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-503"><a href="#cb16-503" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb16-504"><a href="#cb16-504" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Click for result<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb16-505"><a href="#cb16-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-508"><a href="#cb16-508" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-509"><a href="#cb16-509" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb16-510"><a href="#cb16-510" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb16-511"><a href="#cb16-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-512"><a href="#cb16-512" aria-hidden="true" tabindex="-1"></a><span class="co"># Model B predictions</span></span>
<span id="cb16-513"><a href="#cb16-513" aria-hidden="true" tabindex="-1"></a>p_cat_B <span class="op">=</span> np.array([<span class="fl">0.8</span>, <span class="fl">0.1</span>, <span class="fl">0.9</span>, <span class="fl">0.2</span>, <span class="fl">0.1</span>])</span>
<span id="cb16-514"><a href="#cb16-514" aria-hidden="true" tabindex="-1"></a>p_dog_B <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> p_cat_B</span>
<span id="cb16-515"><a href="#cb16-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-516"><a href="#cb16-516" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract probability of correct class</span></span>
<span id="cb16-517"><a href="#cb16-517" aria-hidden="true" tabindex="-1"></a>p_correct_B <span class="op">=</span> np.where(y_true <span class="op">==</span> <span class="dv">0</span>, p_cat_B, p_dog_B)</span>
<span id="cb16-518"><a href="#cb16-518" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model B - P(correct class): </span><span class="sc">{</span>p_correct_B<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-519"><a href="#cb16-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-520"><a href="#cb16-520" aria-hidden="true" tabindex="-1"></a>likelihood_B <span class="op">=</span> np.prod(p_correct_B)</span>
<span id="cb16-521"><a href="#cb16-521" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model B - Likelihood = </span><span class="sc">{</span>likelihood_B<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb16-522"><a href="#cb16-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-523"><a href="#cb16-523" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Comparison:"</span>)</span>
<span id="cb16-524"><a href="#cb16-524" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model A: </span><span class="sc">{</span>likelihood<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb16-525"><a href="#cb16-525" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model B: </span><span class="sc">{</span>likelihood_B<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb16-526"><a href="#cb16-526" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model </span><span class="sc">{</span><span class="st">'B'</span> <span class="cf">if</span> likelihood_B <span class="op">&gt;</span> likelihood <span class="cf">else</span> <span class="st">'A'</span><span class="sc">}</span><span class="ss"> is better!"</span>)</span>
<span id="cb16-527"><a href="#cb16-527" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-528"><a href="#cb16-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-529"><a href="#cb16-529" aria-hidden="true" tabindex="-1"></a>**Key Insight**: Model B is more confident on the second cat (0.9 vs 0.8), leading to higher overall likelihood.</span>
<span id="cb16-530"><a href="#cb16-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-531"><a href="#cb16-531" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb16-532"><a href="#cb16-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-533"><a href="#cb16-533" aria-hidden="true" tabindex="-1"></a>**Question 3**: What if we have a poorly calibrated model? Model C: $P(\text{Cat}) = <span class="co">[</span><span class="ot">0.6, 0.4, 0.6, 0.4, 0.4</span><span class="co">]</span>$</span>
<span id="cb16-534"><a href="#cb16-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-535"><a href="#cb16-535" aria-hidden="true" tabindex="-1"></a>**Consider**: This model is not very confident about anything. How does this affect likelihood?</span>
<span id="cb16-536"><a href="#cb16-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-537"><a href="#cb16-537" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb16-538"><a href="#cb16-538" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Click for result<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb16-539"><a href="#cb16-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-542"><a href="#cb16-542" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-543"><a href="#cb16-543" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb16-544"><a href="#cb16-544" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb16-545"><a href="#cb16-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-546"><a href="#cb16-546" aria-hidden="true" tabindex="-1"></a><span class="co"># Model C: low confidence predictions</span></span>
<span id="cb16-547"><a href="#cb16-547" aria-hidden="true" tabindex="-1"></a>p_cat_C <span class="op">=</span> np.array([<span class="fl">0.6</span>, <span class="fl">0.4</span>, <span class="fl">0.6</span>, <span class="fl">0.4</span>, <span class="fl">0.4</span>])</span>
<span id="cb16-548"><a href="#cb16-548" aria-hidden="true" tabindex="-1"></a>p_dog_C <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> p_cat_C</span>
<span id="cb16-549"><a href="#cb16-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-550"><a href="#cb16-550" aria-hidden="true" tabindex="-1"></a>p_correct_C <span class="op">=</span> np.where(y_true <span class="op">==</span> <span class="dv">0</span>, p_cat_C, p_dog_C)</span>
<span id="cb16-551"><a href="#cb16-551" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model C - P(correct class): </span><span class="sc">{</span>p_correct_C<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-552"><a href="#cb16-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-553"><a href="#cb16-553" aria-hidden="true" tabindex="-1"></a>likelihood_C <span class="op">=</span> np.prod(p_correct_C)</span>
<span id="cb16-554"><a href="#cb16-554" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model C - Likelihood = </span><span class="sc">{</span>likelihood_C<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb16-555"><a href="#cb16-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-556"><a href="#cb16-556" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">All Models Comparison:"</span>)</span>
<span id="cb16-557"><a href="#cb16-557" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model A (mixed confidence): </span><span class="sc">{</span>likelihood<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb16-558"><a href="#cb16-558" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model B (high confidence):  </span><span class="sc">{</span>likelihood_B<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb16-559"><a href="#cb16-559" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model C (low confidence):   </span><span class="sc">{</span>likelihood_C<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb16-560"><a href="#cb16-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-561"><a href="#cb16-561" aria-hidden="true" tabindex="-1"></a><span class="co"># Log-likelihoods (what we actually optimize)</span></span>
<span id="cb16-562"><a href="#cb16-562" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Negative Log-Likelihoods (Cross-Entropy Loss):"</span>)</span>
<span id="cb16-563"><a href="#cb16-563" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model A: </span><span class="sc">{</span><span class="op">-</span>np<span class="sc">.</span>log(likelihood)<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb16-564"><a href="#cb16-564" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model B: </span><span class="sc">{</span><span class="op">-</span>np<span class="sc">.</span>log(likelihood_B)<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb16-565"><a href="#cb16-565" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model C: </span><span class="sc">{</span><span class="op">-</span>np<span class="sc">.</span>log(likelihood_C)<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb16-566"><a href="#cb16-566" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Lower is better for loss!"</span>)</span>
<span id="cb16-567"><a href="#cb16-567" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-568"><a href="#cb16-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-569"><a href="#cb16-569" aria-hidden="true" tabindex="-1"></a>**Critical Observation**:</span>
<span id="cb16-570"><a href="#cb16-570" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Confident correct predictions** (Model B) â†’ High likelihood â†’ Low loss âœ…</span>
<span id="cb16-571"><a href="#cb16-571" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Low confidence** (Model C) â†’ Low likelihood â†’ High loss âŒ</span>
<span id="cb16-572"><a href="#cb16-572" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**This is why we train models to be confident when correct!**</span>
<span id="cb16-573"><a href="#cb16-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-574"><a href="#cb16-574" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb16-575"><a href="#cb16-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-576"><a href="#cb16-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-577"><a href="#cb16-577" aria-hidden="true" tabindex="-1"></a>**Question 4**: What happens if we increase the dataset by ten fold? For example what happens with the likelihood of Model A if $P(\text{Cat}) = <span class="co">[</span><span class="ot">0.9, 0.2, 0.8, 0.3, 0.1</span><span class="co">]</span>$ is repeated 10 times?</span>
<span id="cb16-578"><a href="#cb16-578" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-579"><a href="#cb16-579" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb16-580"><a href="#cb16-580" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Click for result<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb16-581"><a href="#cb16-581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-584"><a href="#cb16-584" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-585"><a href="#cb16-585" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb16-586"><a href="#cb16-586" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb16-587"><a href="#cb16-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-588"><a href="#cb16-588" aria-hidden="true" tabindex="-1"></a><span class="co"># Repeat the dataset 10 times</span></span>
<span id="cb16-589"><a href="#cb16-589" aria-hidden="true" tabindex="-1"></a>p_cat_repeated <span class="op">=</span> np.tile(p_cat, <span class="dv">10</span>)</span>
<span id="cb16-590"><a href="#cb16-590" aria-hidden="true" tabindex="-1"></a>y_true_repeated <span class="op">=</span> np.tile(y_true, <span class="dv">10</span>)</span>
<span id="cb16-591"><a href="#cb16-591" aria-hidden="true" tabindex="-1"></a>p_dog_repeated <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> p_cat_repeated</span>
<span id="cb16-592"><a href="#cb16-592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-593"><a href="#cb16-593" aria-hidden="true" tabindex="-1"></a>p_correct_repeated <span class="op">=</span> np.where(y_true_repeated <span class="op">==</span> <span class="dv">0</span>, p_cat_repeated, p_dog_repeated)</span>
<span id="cb16-594"><a href="#cb16-594" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Dataset repeated 10x (50 samples total)"</span>)</span>
<span id="cb16-595"><a href="#cb16-595" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"P(correct class) for first 5: </span><span class="sc">{</span>p_correct_repeated[:<span class="dv">5</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-596"><a href="#cb16-596" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-597"><a href="#cb16-597" aria-hidden="true" tabindex="-1"></a>likelihood_10x <span class="op">=</span> np.prod(p_correct_repeated)</span>
<span id="cb16-598"><a href="#cb16-598" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Likelihood (10x dataset): </span><span class="sc">{</span>likelihood_10x<span class="sc">:.2e}</span><span class="ss">"</span>)</span>
<span id="cb16-599"><a href="#cb16-599" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Original likelihood (1x):  </span><span class="sc">{</span>likelihood<span class="sc">:.6f}</span><span class="ss">"</span>)</span>
<span id="cb16-600"><a href="#cb16-600" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Ratio: </span><span class="sc">{</span>likelihood_10x <span class="op">/</span> likelihood<span class="sc">:.2e}</span><span class="ss">"</span>)</span>
<span id="cb16-601"><a href="#cb16-601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-602"><a href="#cb16-602" aria-hidden="true" tabindex="-1"></a><span class="co"># The key insight: likelihood shrinks exponentially!</span></span>
<span id="cb16-603"><a href="#cb16-603" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Numerical underflow risk!"</span>)</span>
<span id="cb16-604"><a href="#cb16-604" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"This is why we use LOG-likelihood instead:"</span>)</span>
<span id="cb16-605"><a href="#cb16-605" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Log-likelihood (10x): </span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">sum</span>(np.log(p_correct_repeated))<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb16-606"><a href="#cb16-606" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Log-likelihood (1x):  </span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">sum</span>(np.log(p_correct))<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb16-607"><a href="#cb16-607" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The log version just scales linearly (10x) without numerical issues!"</span>)</span>
<span id="cb16-608"><a href="#cb16-608" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Lower is better for loss!"</span>)</span>
<span id="cb16-609"><a href="#cb16-609" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-610"><a href="#cb16-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-611"><a href="#cb16-611" aria-hidden="true" tabindex="-1"></a>**Critical Observation**:</span>
<span id="cb16-612"><a href="#cb16-612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-613"><a href="#cb16-613" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>With 10x data, likelihood becomes TINY (underflow risk)</span>
<span id="cb16-614"><a href="#cb16-614" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>But log-likelihood scales linearly â†’ numerically stable!</span>
<span id="cb16-615"><a href="#cb16-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-616"><a href="#cb16-616" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb16-617"><a href="#cb16-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-618"><a href="#cb16-618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-619"><a href="#cb16-619" aria-hidden="true" tabindex="-1"></a>**Bonus Question**: What's the perfect model's likelihood?</span>
<span id="cb16-620"><a href="#cb16-620" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-621"><a href="#cb16-621" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb16-622"><a href="#cb16-622" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Perfect Model Analysis<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb16-623"><a href="#cb16-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-626"><a href="#cb16-626" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-627"><a href="#cb16-627" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb16-628"><a href="#cb16-628" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb16-629"><a href="#cb16-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-630"><a href="#cb16-630" aria-hidden="true" tabindex="-1"></a><span class="co"># Perfect model: 100% confident and always correct</span></span>
<span id="cb16-631"><a href="#cb16-631" aria-hidden="true" tabindex="-1"></a>p_cat_perfect <span class="op">=</span> np.array([<span class="fl">1.0</span>, <span class="fl">0.0</span>, <span class="fl">1.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>])  <span class="co"># Certain about each class</span></span>
<span id="cb16-632"><a href="#cb16-632" aria-hidden="true" tabindex="-1"></a>p_dog_perfect <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> p_cat_perfect</span>
<span id="cb16-633"><a href="#cb16-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-634"><a href="#cb16-634" aria-hidden="true" tabindex="-1"></a>p_correct_perfect <span class="op">=</span> np.where(y_true <span class="op">==</span> <span class="dv">0</span>, p_cat_perfect, p_dog_perfect)</span>
<span id="cb16-635"><a href="#cb16-635" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Perfect Model - P(correct class): </span><span class="sc">{</span>p_correct_perfect<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-636"><a href="#cb16-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-637"><a href="#cb16-637" aria-hidden="true" tabindex="-1"></a>likelihood_perfect <span class="op">=</span> np.prod(p_correct_perfect)</span>
<span id="cb16-638"><a href="#cb16-638" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Perfect Model - Likelihood = </span><span class="sc">{</span>likelihood_perfect<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-639"><a href="#cb16-639" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Perfect Model - Negative Log-Likelihood = </span><span class="sc">{</span><span class="op">-</span>np<span class="sc">.</span>log(likelihood_perfect)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-640"><a href="#cb16-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-641"><a href="#cb16-641" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">ðŸŽ¯ Maximum possible likelihood = 1.0"</span>)</span>
<span id="cb16-642"><a href="#cb16-642" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"ðŸŽ¯ Minimum possible loss = 0.0"</span>)</span>
<span id="cb16-643"><a href="#cb16-643" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-644"><a href="#cb16-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-645"><a href="#cb16-645" aria-hidden="true" tabindex="-1"></a>**The Goal**: Train your CNN to approach this perfect confidence on the training set (while generalizing to new data)!</span>
<span id="cb16-646"><a href="#cb16-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-647"><a href="#cb16-647" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb16-648"><a href="#cb16-648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-649"><a href="#cb16-649" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-650"><a href="#cb16-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-651"><a href="#cb16-651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-652"><a href="#cb16-652" aria-hidden="true" tabindex="-1"></a><span class="fu">### Maximum Likelihood</span></span>
<span id="cb16-653"><a href="#cb16-653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-654"><a href="#cb16-654" aria-hidden="true" tabindex="-1"></a>The likelihood $P(\vect{y} | \theta, \m{X})$ of observing our entire dataset $(\m{X}, \vect{y})$, given the parameters $\theta$ and assuming that the data points $(\vect{x}^{(i)}, y^{(i)})$ are independent and identically distributed, can be calculated as:</span>
<span id="cb16-655"><a href="#cb16-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-656"><a href="#cb16-656" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb16-657"><a href="#cb16-657" aria-hidden="true" tabindex="-1"></a>\prod_{i=1}^N \prod_{j=1}^K  P(Y = j| X = \vect{x}^{(i)})^{y^{(i)}_j}</span>
<span id="cb16-658"><a href="#cb16-658" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb16-659"><a href="#cb16-659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-660"><a href="#cb16-660" aria-hidden="true" tabindex="-1"></a>Under the Maximum Likelihood approach, we seek the parameters $\theta$ that maximize the likelihood of observing the dataset.</span>
<span id="cb16-661"><a href="#cb16-661" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-662"><a href="#cb16-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-663"><a href="#cb16-663" aria-hidden="true" tabindex="-1"></a><span class="fu">### Negative Log-Likelihood</span></span>
<span id="cb16-664"><a href="#cb16-664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-665"><a href="#cb16-665" aria-hidden="true" tabindex="-1"></a>With Maximum Likelihood, we aim to choose the parameters $\theta$ such that $p(\vect{y} | \theta, \m{X})$ is maximized. However, this function can be complex to handle, and we can use some mathematical tricks. We take the logarithm of the likelihood so that the product of probabilities becomes a sum. Since the logarithm is a monotonically increasing function, we can maximize its logarithm instead of the function $p(\vect{y} | \theta, \m{X})$. Finally, we take the negative of the function, allowing us to minimize it.</span>
<span id="cb16-666"><a href="#cb16-666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-667"><a href="#cb16-667" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb16-668"><a href="#cb16-668" aria-hidden="true" tabindex="-1"></a>L(\m{X}, \vect{y}, \theta) &amp;= - \log \prod_{i=1}^N \prod_{j=1}^K P(Y = j| X = \vect{x}^{(i)})^{y^{(i)}_j} <span class="sc">\\</span></span>
<span id="cb16-669"><a href="#cb16-669" aria-hidden="true" tabindex="-1"></a>&amp;= -\sum_{i=1}^N \sum_{j=1}^K y^{(i)}_j \log  P(Y = j| X = \vect{x}^{(i)})</span>
<span id="cb16-670"><a href="#cb16-670" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb16-671"><a href="#cb16-671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-672"><a href="#cb16-672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-673"><a href="#cb16-673" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title="ðŸ’¡ Maximum Likelihood Intuition" collapse="true"}</span>
<span id="cb16-674"><a href="#cb16-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-675"><a href="#cb16-675" aria-hidden="true" tabindex="-1"></a>**Visualize the connection: Likelihood â†’ Log-Likelihood â†’ Cross-Entropy Loss**:</span>
<span id="cb16-676"><a href="#cb16-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-679"><a href="#cb16-679" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-680"><a href="#cb16-680" aria-hidden="true" tabindex="-1"></a><span class="co"># | eval: true</span></span>
<span id="cb16-681"><a href="#cb16-681" aria-hidden="true" tabindex="-1"></a><span class="co"># | echo: false</span></span>
<span id="cb16-682"><a href="#cb16-682" aria-hidden="true" tabindex="-1"></a><span class="co"># | fig-width: 800</span></span>
<span id="cb16-683"><a href="#cb16-683" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-684"><a href="#cb16-684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-685"><a href="#cb16-685" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_likelihood_viz():</span>
<span id="cb16-686"><a href="#cb16-686" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Visualize how likelihood changes with model quality (calibration)."""</span></span>
<span id="cb16-687"><a href="#cb16-687" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-688"><a href="#cb16-688" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Fixed dataset: 5 samples with true labels</span></span>
<span id="cb16-689"><a href="#cb16-689" aria-hidden="true" tabindex="-1"></a>    <span class="co"># True labels: [Cat, Dog, Cat, Dog, Dog]</span></span>
<span id="cb16-690"><a href="#cb16-690" aria-hidden="true" tabindex="-1"></a>    n_samples <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb16-691"><a href="#cb16-691" aria-hidden="true" tabindex="-1"></a>    y_true_binary <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>])  <span class="co"># 0=Cat, 1=Dog</span></span>
<span id="cb16-692"><a href="#cb16-692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-693"><a href="#cb16-693" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Model calibration parameter: varies from 0.5 (random) to 0.98 (well-calibrated)</span></span>
<span id="cb16-694"><a href="#cb16-694" aria-hidden="true" tabindex="-1"></a>    <span class="co"># This represents how well the model can predict the correct class</span></span>
<span id="cb16-695"><a href="#cb16-695" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Think of it as: p(correct class) = calibration, p(wrong class) = 1 - calibration</span></span>
<span id="cb16-696"><a href="#cb16-696" aria-hidden="true" tabindex="-1"></a>    calibration_values <span class="op">=</span> np.linspace(<span class="fl">0.5</span>, <span class="fl">0.98</span>, <span class="dv">50</span>)</span>
<span id="cb16-697"><a href="#cb16-697" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-698"><a href="#cb16-698" aria-hidden="true" tabindex="-1"></a>    likelihoods <span class="op">=</span> []</span>
<span id="cb16-699"><a href="#cb16-699" aria-hidden="true" tabindex="-1"></a>    log_likelihoods <span class="op">=</span> []</span>
<span id="cb16-700"><a href="#cb16-700" aria-hidden="true" tabindex="-1"></a>    neg_log_likelihoods <span class="op">=</span> []</span>
<span id="cb16-701"><a href="#cb16-701" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-702"><a href="#cb16-702" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> calib <span class="kw">in</span> calibration_values:</span>
<span id="cb16-703"><a href="#cb16-703" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Model predictions: always assign 'calib' probability to the TRUE class</span></span>
<span id="cb16-704"><a href="#cb16-704" aria-hidden="true" tabindex="-1"></a>        <span class="co"># For a binary problem: p(true_class) = calib, p(other_class) = 1 - calib</span></span>
<span id="cb16-705"><a href="#cb16-705" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Example: if calib=0.8, true=Dog â†’ predict p(Dog)=0.8, p(Cat)=0.2</span></span>
<span id="cb16-706"><a href="#cb16-706" aria-hidden="true" tabindex="-1"></a>        <span class="co">#          if calib=0.8, true=Cat â†’ predict p(Cat)=0.8, p(Dog)=0.2</span></span>
<span id="cb16-707"><a href="#cb16-707" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-708"><a href="#cb16-708" aria-hidden="true" tabindex="-1"></a>        <span class="co"># For likelihood, we only care about p(true_class) for each sample</span></span>
<span id="cb16-709"><a href="#cb16-709" aria-hidden="true" tabindex="-1"></a>        <span class="co"># All samples get probability = calib (since we're evaluating the true class)</span></span>
<span id="cb16-710"><a href="#cb16-710" aria-hidden="true" tabindex="-1"></a>        y_pred_probs <span class="op">=</span> np.full(n_samples, calib)  <span class="co"># Each sample: p(true_class) = calib</span></span>
<span id="cb16-711"><a href="#cb16-711" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-712"><a href="#cb16-712" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Likelihood = product of all predicted probabilities for true classes</span></span>
<span id="cb16-713"><a href="#cb16-713" aria-hidden="true" tabindex="-1"></a>        <span class="co"># L(Î¸) = p(yâ‚|xâ‚,Î¸) Ã— p(yâ‚‚|xâ‚‚,Î¸) Ã— ... Ã— p(yâ‚…|xâ‚…,Î¸)</span></span>
<span id="cb16-714"><a href="#cb16-714" aria-hidden="true" tabindex="-1"></a>        <span class="co"># With 5 samples: L = calib^5</span></span>
<span id="cb16-715"><a href="#cb16-715" aria-hidden="true" tabindex="-1"></a>        likelihood <span class="op">=</span> np.prod(y_pred_probs)</span>
<span id="cb16-716"><a href="#cb16-716" aria-hidden="true" tabindex="-1"></a>        likelihoods.append(likelihood)</span>
<span id="cb16-717"><a href="#cb16-717" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-718"><a href="#cb16-718" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Log-likelihood = sum of log-probabilities (numerically stable)</span></span>
<span id="cb16-719"><a href="#cb16-719" aria-hidden="true" tabindex="-1"></a>        <span class="co"># log L(Î¸) = log p(yâ‚|xâ‚,Î¸) + log p(yâ‚‚|xâ‚‚,Î¸) + ... + log p(yâ‚…|xâ‚…,Î¸)</span></span>
<span id="cb16-720"><a href="#cb16-720" aria-hidden="true" tabindex="-1"></a>        log_likelihood <span class="op">=</span> np.<span class="bu">sum</span>(np.log(y_pred_probs))</span>
<span id="cb16-721"><a href="#cb16-721" aria-hidden="true" tabindex="-1"></a>        log_likelihoods.append(log_likelihood)</span>
<span id="cb16-722"><a href="#cb16-722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-723"><a href="#cb16-723" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Negative log-likelihood = what we minimize during training</span></span>
<span id="cb16-724"><a href="#cb16-724" aria-hidden="true" tabindex="-1"></a>        <span class="co"># -log L(Î¸) = Cross-Entropy Loss</span></span>
<span id="cb16-725"><a href="#cb16-725" aria-hidden="true" tabindex="-1"></a>        neg_log_likelihoods.append(<span class="op">-</span>log_likelihood)</span>
<span id="cb16-726"><a href="#cb16-726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-727"><a href="#cb16-727" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create subplots with more space for title</span></span>
<span id="cb16-728"><a href="#cb16-728" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> make_subplots(</span>
<span id="cb16-729"><a href="#cb16-729" aria-hidden="true" tabindex="-1"></a>        rows<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb16-730"><a href="#cb16-730" aria-hidden="true" tabindex="-1"></a>        cols<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb16-731"><a href="#cb16-731" aria-hidden="true" tabindex="-1"></a>        subplot_titles<span class="op">=</span>(</span>
<span id="cb16-732"><a href="#cb16-732" aria-hidden="true" tabindex="-1"></a>            <span class="st">"Likelihood: L(Î¸) = âˆáµ¢ p(yáµ¢|xáµ¢,Î¸)  [Product of probabilities]"</span>,</span>
<span id="cb16-733"><a href="#cb16-733" aria-hidden="true" tabindex="-1"></a>            <span class="st">"Log-Likelihood: log L(Î¸) = Î£áµ¢ log p(yáµ¢|xáµ¢,Î¸)  [Sum of log-probabilities]"</span>,</span>
<span id="cb16-734"><a href="#cb16-734" aria-hidden="true" tabindex="-1"></a>            <span class="st">"Negative Log-Likelihood: -log L(Î¸) = Cross-Entropy Loss"</span>,</span>
<span id="cb16-735"><a href="#cb16-735" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb16-736"><a href="#cb16-736" aria-hidden="true" tabindex="-1"></a>        vertical_spacing<span class="op">=</span><span class="fl">0.15</span>,</span>
<span id="cb16-737"><a href="#cb16-737" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb16-738"><a href="#cb16-738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-739"><a href="#cb16-739" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Panel 1: Likelihood (gets very small!)</span></span>
<span id="cb16-740"><a href="#cb16-740" aria-hidden="true" tabindex="-1"></a>    fig.add_trace(</span>
<span id="cb16-741"><a href="#cb16-741" aria-hidden="true" tabindex="-1"></a>        go.Scatter(</span>
<span id="cb16-742"><a href="#cb16-742" aria-hidden="true" tabindex="-1"></a>            x<span class="op">=</span>calibration_values,</span>
<span id="cb16-743"><a href="#cb16-743" aria-hidden="true" tabindex="-1"></a>            y<span class="op">=</span>likelihoods,</span>
<span id="cb16-744"><a href="#cb16-744" aria-hidden="true" tabindex="-1"></a>            mode<span class="op">=</span><span class="st">"lines"</span>,</span>
<span id="cb16-745"><a href="#cb16-745" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"Likelihood"</span>,</span>
<span id="cb16-746"><a href="#cb16-746" aria-hidden="true" tabindex="-1"></a>            line<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span><span class="st">"blue"</span>, width<span class="op">=</span><span class="dv">3</span>),</span>
<span id="cb16-747"><a href="#cb16-747" aria-hidden="true" tabindex="-1"></a>            fill<span class="op">=</span><span class="st">"tozeroy"</span>,</span>
<span id="cb16-748"><a href="#cb16-748" aria-hidden="true" tabindex="-1"></a>            hovertemplate<span class="op">=</span><span class="st">"Calibration: %</span><span class="sc">{x:.3f}</span><span class="st">&lt;br&gt;Likelihood: %</span><span class="sc">{y:.6f}</span><span class="st">&lt;extra&gt;&lt;/extra&gt;"</span>,</span>
<span id="cb16-749"><a href="#cb16-749" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb16-750"><a href="#cb16-750" aria-hidden="true" tabindex="-1"></a>        row<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb16-751"><a href="#cb16-751" aria-hidden="true" tabindex="-1"></a>        col<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb16-752"><a href="#cb16-752" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb16-753"><a href="#cb16-753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-754"><a href="#cb16-754" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Panel 2: Log-likelihood (numerically stable)</span></span>
<span id="cb16-755"><a href="#cb16-755" aria-hidden="true" tabindex="-1"></a>    fig.add_trace(</span>
<span id="cb16-756"><a href="#cb16-756" aria-hidden="true" tabindex="-1"></a>        go.Scatter(</span>
<span id="cb16-757"><a href="#cb16-757" aria-hidden="true" tabindex="-1"></a>            x<span class="op">=</span>calibration_values,</span>
<span id="cb16-758"><a href="#cb16-758" aria-hidden="true" tabindex="-1"></a>            y<span class="op">=</span>log_likelihoods,</span>
<span id="cb16-759"><a href="#cb16-759" aria-hidden="true" tabindex="-1"></a>            mode<span class="op">=</span><span class="st">"lines"</span>,</span>
<span id="cb16-760"><a href="#cb16-760" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"Log-Likelihood"</span>,</span>
<span id="cb16-761"><a href="#cb16-761" aria-hidden="true" tabindex="-1"></a>            line<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span><span class="st">"green"</span>, width<span class="op">=</span><span class="dv">3</span>),</span>
<span id="cb16-762"><a href="#cb16-762" aria-hidden="true" tabindex="-1"></a>            fill<span class="op">=</span><span class="st">"tozeroy"</span>,</span>
<span id="cb16-763"><a href="#cb16-763" aria-hidden="true" tabindex="-1"></a>            hovertemplate<span class="op">=</span><span class="st">"Calibration: %</span><span class="sc">{x:.3f}</span><span class="st">&lt;br&gt;Log-Likelihood: %</span><span class="sc">{y:.3f}</span><span class="st">&lt;extra&gt;&lt;/extra&gt;"</span>,</span>
<span id="cb16-764"><a href="#cb16-764" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb16-765"><a href="#cb16-765" aria-hidden="true" tabindex="-1"></a>        row<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb16-766"><a href="#cb16-766" aria-hidden="true" tabindex="-1"></a>        col<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb16-767"><a href="#cb16-767" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb16-768"><a href="#cb16-768" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-769"><a href="#cb16-769" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Panel 3: Negative log-likelihood (this is the LOSS we minimize)</span></span>
<span id="cb16-770"><a href="#cb16-770" aria-hidden="true" tabindex="-1"></a>    fig.add_trace(</span>
<span id="cb16-771"><a href="#cb16-771" aria-hidden="true" tabindex="-1"></a>        go.Scatter(</span>
<span id="cb16-772"><a href="#cb16-772" aria-hidden="true" tabindex="-1"></a>            x<span class="op">=</span>calibration_values,</span>
<span id="cb16-773"><a href="#cb16-773" aria-hidden="true" tabindex="-1"></a>            y<span class="op">=</span>neg_log_likelihoods,</span>
<span id="cb16-774"><a href="#cb16-774" aria-hidden="true" tabindex="-1"></a>            mode<span class="op">=</span><span class="st">"lines"</span>,</span>
<span id="cb16-775"><a href="#cb16-775" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"Negative Log-Likelihood"</span>,</span>
<span id="cb16-776"><a href="#cb16-776" aria-hidden="true" tabindex="-1"></a>            line<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span><span class="st">"red"</span>, width<span class="op">=</span><span class="dv">3</span>),</span>
<span id="cb16-777"><a href="#cb16-777" aria-hidden="true" tabindex="-1"></a>            fill<span class="op">=</span><span class="st">"tozeroy"</span>,</span>
<span id="cb16-778"><a href="#cb16-778" aria-hidden="true" tabindex="-1"></a>            hovertemplate<span class="op">=</span><span class="st">"Calibration: %</span><span class="sc">{x:.3f}</span><span class="st">&lt;br&gt;NLL Loss: %</span><span class="sc">{y:.3f}</span><span class="st">&lt;extra&gt;&lt;/extra&gt;"</span>,</span>
<span id="cb16-779"><a href="#cb16-779" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb16-780"><a href="#cb16-780" aria-hidden="true" tabindex="-1"></a>        row<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb16-781"><a href="#cb16-781" aria-hidden="true" tabindex="-1"></a>        col<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb16-782"><a href="#cb16-782" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb16-783"><a href="#cb16-783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-784"><a href="#cb16-784" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update axes labels</span></span>
<span id="cb16-785"><a href="#cb16-785" aria-hidden="true" tabindex="-1"></a>    fig.update_xaxes(</span>
<span id="cb16-786"><a href="#cb16-786" aria-hidden="true" tabindex="-1"></a>        title_text<span class="op">=</span><span class="st">"Model Calibration (probability assigned to correct class)"</span>,</span>
<span id="cb16-787"><a href="#cb16-787" aria-hidden="true" tabindex="-1"></a>        row<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb16-788"><a href="#cb16-788" aria-hidden="true" tabindex="-1"></a>        col<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb16-789"><a href="#cb16-789" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb16-790"><a href="#cb16-790" aria-hidden="true" tabindex="-1"></a>    fig.update_yaxes(title_text<span class="op">=</span><span class="st">"Likelihood"</span>, row<span class="op">=</span><span class="dv">1</span>, col<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb16-791"><a href="#cb16-791" aria-hidden="true" tabindex="-1"></a>    fig.update_yaxes(title_text<span class="op">=</span><span class="st">"Log-Likelihood"</span>, row<span class="op">=</span><span class="dv">2</span>, col<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb16-792"><a href="#cb16-792" aria-hidden="true" tabindex="-1"></a>    fig.update_yaxes(title_text<span class="op">=</span><span class="st">"Loss"</span>, row<span class="op">=</span><span class="dv">3</span>, col<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb16-793"><a href="#cb16-793" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-794"><a href="#cb16-794" aria-hidden="true" tabindex="-1"></a>    fig.update_layout(</span>
<span id="cb16-795"><a href="#cb16-795" aria-hidden="true" tabindex="-1"></a>        height<span class="op">=</span><span class="dv">850</span>,</span>
<span id="cb16-796"><a href="#cb16-796" aria-hidden="true" tabindex="-1"></a>        showlegend<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb16-797"><a href="#cb16-797" aria-hidden="true" tabindex="-1"></a>        title<span class="op">=</span>{</span>
<span id="cb16-798"><a href="#cb16-798" aria-hidden="true" tabindex="-1"></a>            <span class="st">"text"</span>: <span class="st">"Maximum Likelihood â†” Cross-Entropy Connection&lt;br&gt;&lt;sub&gt;Dataset: 5 samples (3 Dogs, 2 Cats)&lt;/sub&gt;"</span>,</span>
<span id="cb16-799"><a href="#cb16-799" aria-hidden="true" tabindex="-1"></a>            <span class="st">"y"</span>: <span class="fl">1.0</span>,</span>
<span id="cb16-800"><a href="#cb16-800" aria-hidden="true" tabindex="-1"></a>            <span class="st">"x"</span>: <span class="fl">0.5</span>,</span>
<span id="cb16-801"><a href="#cb16-801" aria-hidden="true" tabindex="-1"></a>            <span class="st">"xanchor"</span>: <span class="st">"center"</span>,</span>
<span id="cb16-802"><a href="#cb16-802" aria-hidden="true" tabindex="-1"></a>            <span class="st">"yanchor"</span>: <span class="st">"top"</span>,</span>
<span id="cb16-803"><a href="#cb16-803" aria-hidden="true" tabindex="-1"></a>        },</span>
<span id="cb16-804"><a href="#cb16-804" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb16-805"><a href="#cb16-805" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-806"><a href="#cb16-806" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add explanatory annotations</span></span>
<span id="cb16-807"><a href="#cb16-807" aria-hidden="true" tabindex="-1"></a>    fig.add_annotation(</span>
<span id="cb16-808"><a href="#cb16-808" aria-hidden="true" tabindex="-1"></a>        x<span class="op">=</span><span class="fl">0.95</span>,</span>
<span id="cb16-809"><a href="#cb16-809" aria-hidden="true" tabindex="-1"></a>        y<span class="op">=</span><span class="bu">max</span>(likelihoods) <span class="op">*</span> <span class="fl">0.5</span>,</span>
<span id="cb16-810"><a href="#cb16-810" aria-hidden="true" tabindex="-1"></a>        text<span class="op">=</span><span class="st">"MAXIMIZE&lt;br&gt;THIS&lt;br&gt;(Training Goal)"</span>,</span>
<span id="cb16-811"><a href="#cb16-811" aria-hidden="true" tabindex="-1"></a>        showarrow<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb16-812"><a href="#cb16-812" aria-hidden="true" tabindex="-1"></a>        arrowhead<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb16-813"><a href="#cb16-813" aria-hidden="true" tabindex="-1"></a>        ax<span class="op">=-</span><span class="dv">50</span>,</span>
<span id="cb16-814"><a href="#cb16-814" aria-hidden="true" tabindex="-1"></a>        ay<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb16-815"><a href="#cb16-815" aria-hidden="true" tabindex="-1"></a>        row<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb16-816"><a href="#cb16-816" aria-hidden="true" tabindex="-1"></a>        col<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb16-817"><a href="#cb16-817" aria-hidden="true" tabindex="-1"></a>        font<span class="op">=</span><span class="bu">dict</span>(size<span class="op">=</span><span class="dv">11</span>, color<span class="op">=</span><span class="st">"darkblue"</span>),</span>
<span id="cb16-818"><a href="#cb16-818" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb16-819"><a href="#cb16-819" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-820"><a href="#cb16-820" aria-hidden="true" tabindex="-1"></a>    fig.add_annotation(</span>
<span id="cb16-821"><a href="#cb16-821" aria-hidden="true" tabindex="-1"></a>        x<span class="op">=</span><span class="fl">0.95</span>,</span>
<span id="cb16-822"><a href="#cb16-822" aria-hidden="true" tabindex="-1"></a>        y<span class="op">=</span><span class="bu">max</span>(log_likelihoods) <span class="op">*</span> <span class="fl">0.5</span>,</span>
<span id="cb16-823"><a href="#cb16-823" aria-hidden="true" tabindex="-1"></a>        text<span class="op">=</span><span class="st">"MAXIMIZE&lt;br&gt;THIS&lt;br&gt;(Equivalent Goal)"</span>,</span>
<span id="cb16-824"><a href="#cb16-824" aria-hidden="true" tabindex="-1"></a>        showarrow<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb16-825"><a href="#cb16-825" aria-hidden="true" tabindex="-1"></a>        arrowhead<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb16-826"><a href="#cb16-826" aria-hidden="true" tabindex="-1"></a>        ax<span class="op">=-</span><span class="dv">50</span>,</span>
<span id="cb16-827"><a href="#cb16-827" aria-hidden="true" tabindex="-1"></a>        ay<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb16-828"><a href="#cb16-828" aria-hidden="true" tabindex="-1"></a>        row<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb16-829"><a href="#cb16-829" aria-hidden="true" tabindex="-1"></a>        col<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb16-830"><a href="#cb16-830" aria-hidden="true" tabindex="-1"></a>        font<span class="op">=</span><span class="bu">dict</span>(size<span class="op">=</span><span class="dv">11</span>, color<span class="op">=</span><span class="st">"darkgreen"</span>),</span>
<span id="cb16-831"><a href="#cb16-831" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb16-832"><a href="#cb16-832" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-833"><a href="#cb16-833" aria-hidden="true" tabindex="-1"></a>    fig.add_annotation(</span>
<span id="cb16-834"><a href="#cb16-834" aria-hidden="true" tabindex="-1"></a>        x<span class="op">=</span><span class="fl">0.95</span>,</span>
<span id="cb16-835"><a href="#cb16-835" aria-hidden="true" tabindex="-1"></a>        y<span class="op">=</span><span class="bu">min</span>(neg_log_likelihoods) <span class="op">*</span> <span class="fl">1.5</span>,</span>
<span id="cb16-836"><a href="#cb16-836" aria-hidden="true" tabindex="-1"></a>        text<span class="op">=</span><span class="st">"MINIMIZE&lt;br&gt;THIS&lt;br&gt;(Actual Training Loss)"</span>,</span>
<span id="cb16-837"><a href="#cb16-837" aria-hidden="true" tabindex="-1"></a>        showarrow<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb16-838"><a href="#cb16-838" aria-hidden="true" tabindex="-1"></a>        arrowhead<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb16-839"><a href="#cb16-839" aria-hidden="true" tabindex="-1"></a>        ax<span class="op">=-</span><span class="dv">50</span>,</span>
<span id="cb16-840"><a href="#cb16-840" aria-hidden="true" tabindex="-1"></a>        ay<span class="op">=</span><span class="dv">30</span>,</span>
<span id="cb16-841"><a href="#cb16-841" aria-hidden="true" tabindex="-1"></a>        row<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb16-842"><a href="#cb16-842" aria-hidden="true" tabindex="-1"></a>        col<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb16-843"><a href="#cb16-843" aria-hidden="true" tabindex="-1"></a>        font<span class="op">=</span><span class="bu">dict</span>(size<span class="op">=</span><span class="dv">11</span>, color<span class="op">=</span><span class="st">"darkred"</span>),</span>
<span id="cb16-844"><a href="#cb16-844" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb16-845"><a href="#cb16-845" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-846"><a href="#cb16-846" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> fig</span>
<span id="cb16-847"><a href="#cb16-847" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-848"><a href="#cb16-848" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-849"><a href="#cb16-849" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> create_likelihood_viz()</span>
<span id="cb16-850"><a href="#cb16-850" aria-hidden="true" tabindex="-1"></a>fig.show()</span>
<span id="cb16-851"><a href="#cb16-851" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-852"><a href="#cb16-852" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-853"><a href="#cb16-853" aria-hidden="true" tabindex="-1"></a>**Critical Insights**:</span>
<span id="cb16-854"><a href="#cb16-854" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-855"><a href="#cb16-855" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**X-axis**: Model calibration = probability assigned to the correct class (ranges from 0.5=random guessing to 0.98=well-calibrated)</span>
<span id="cb16-856"><a href="#cb16-856" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Top panel**: Likelihood shrinks exponentially with dataset size â†’ numerical underflow risk!</span>
<span id="cb16-857"><a href="#cb16-857" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Middle panel**: Log-likelihood converts products to sums â†’ numerically stable, same maximum</span>
<span id="cb16-858"><a href="#cb16-858" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Bottom panel**: Negative log-likelihood (NLL) = Cross-Entropy Loss used in training</span>
<span id="cb16-859"><a href="#cb16-859" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Key Connection**: max L(Î¸) âŸº max log L(Î¸) âŸº min -log L(Î¸) âŸº min Cross-Entropy</span>
<span id="cb16-860"><a href="#cb16-860" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-861"><a href="#cb16-861" aria-hidden="true" tabindex="-1"></a>The visualization shows: as model calibration improves (x increases), likelihood increases, log-likelihood increases, and loss (NLL) decreases.</span>
<span id="cb16-862"><a href="#cb16-862" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-863"><a href="#cb16-863" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-864"><a href="#cb16-864" aria-hidden="true" tabindex="-1"></a><span class="fu">### Cross-Entropy</span></span>
<span id="cb16-865"><a href="#cb16-865" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-866"><a href="#cb16-866" aria-hidden="true" tabindex="-1"></a>The loss function, derived with Maximum Likelihood, can also be viewed through the lens of cross-entropy between two discrete probability functions. Specifically, we can calculate and minimize the cross-entropy between the true distribution $\vect{y}^{(i)}$ and the predicted $\vect{\hat{y}}^{(i)}$. Cross-entropy comes from information theory and measures how many bits/nats on average are needed to describe an event of a probability distribution $p(x)$ when using the approximation $q(x)$.</span>
<span id="cb16-867"><a href="#cb16-867" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-868"><a href="#cb16-868" aria-hidden="true" tabindex="-1"></a>\begin{equation}</span>
<span id="cb16-869"><a href="#cb16-869" aria-hidden="true" tabindex="-1"></a>CE = - \sum_{x \in X} p(x) \log q(x) <span class="sc">\\</span></span>
<span id="cb16-870"><a href="#cb16-870" aria-hidden="true" tabindex="-1"></a>CE = - \sum_{i=1}^N \sum_{j=1}^K y_j^{(i)} \log \hat{y}_j^{(i)}</span>
<span id="cb16-871"><a href="#cb16-871" aria-hidden="true" tabindex="-1"></a>\end{equation}</span>
<span id="cb16-872"><a href="#cb16-872" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-873"><a href="#cb16-873" aria-hidden="true" tabindex="-1"></a>It is evident that cross-entropy is identical to the negative log-likelihood.</span>
<span id="cb16-874"><a href="#cb16-874" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-875"><a href="#cb16-875" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title="ðŸ“Š Cross-Entropy Loss Landscape" collapse="true"}</span>
<span id="cb16-876"><a href="#cb16-876" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-877"><a href="#cb16-877" aria-hidden="true" tabindex="-1"></a>**Visualize how cross-entropy penalizes predictions**:</span>
<span id="cb16-878"><a href="#cb16-878" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-881"><a href="#cb16-881" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-882"><a href="#cb16-882" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb16-883"><a href="#cb16-883" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb16-884"><a href="#cb16-884" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 800</span></span>
<span id="cb16-885"><a href="#cb16-885" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-886"><a href="#cb16-886" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_ce_landscape():</span>
<span id="cb16-887"><a href="#cb16-887" aria-hidden="true" tabindex="-1"></a>    <span class="co"># For true label = 1</span></span>
<span id="cb16-888"><a href="#cb16-888" aria-hidden="true" tabindex="-1"></a>    p_pred <span class="op">=</span> np.linspace(<span class="fl">0.01</span>, <span class="fl">0.99</span>, <span class="dv">100</span>)</span>
<span id="cb16-889"><a href="#cb16-889" aria-hidden="true" tabindex="-1"></a>    loss_true_1 <span class="op">=</span> <span class="op">-</span>np.log(p_pred)</span>
<span id="cb16-890"><a href="#cb16-890" aria-hidden="true" tabindex="-1"></a>    loss_true_0 <span class="op">=</span> <span class="op">-</span>np.log(<span class="dv">1</span> <span class="op">-</span> p_pred)</span>
<span id="cb16-891"><a href="#cb16-891" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-892"><a href="#cb16-892" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> go.Figure()</span>
<span id="cb16-893"><a href="#cb16-893" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-894"><a href="#cb16-894" aria-hidden="true" tabindex="-1"></a>    fig.add_trace(go.Scatter(</span>
<span id="cb16-895"><a href="#cb16-895" aria-hidden="true" tabindex="-1"></a>        x<span class="op">=</span>p_pred,</span>
<span id="cb16-896"><a href="#cb16-896" aria-hidden="true" tabindex="-1"></a>        y<span class="op">=</span>loss_true_1,</span>
<span id="cb16-897"><a href="#cb16-897" aria-hidden="true" tabindex="-1"></a>        mode<span class="op">=</span><span class="st">'lines'</span>,</span>
<span id="cb16-898"><a href="#cb16-898" aria-hidden="true" tabindex="-1"></a>        name<span class="op">=</span><span class="st">'True class = 1'</span>,</span>
<span id="cb16-899"><a href="#cb16-899" aria-hidden="true" tabindex="-1"></a>        line<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span><span class="st">'blue'</span>, width<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb16-900"><a href="#cb16-900" aria-hidden="true" tabindex="-1"></a>    ))</span>
<span id="cb16-901"><a href="#cb16-901" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-902"><a href="#cb16-902" aria-hidden="true" tabindex="-1"></a>    fig.add_trace(go.Scatter(</span>
<span id="cb16-903"><a href="#cb16-903" aria-hidden="true" tabindex="-1"></a>        x<span class="op">=</span>p_pred,</span>
<span id="cb16-904"><a href="#cb16-904" aria-hidden="true" tabindex="-1"></a>        y<span class="op">=</span>loss_true_0,</span>
<span id="cb16-905"><a href="#cb16-905" aria-hidden="true" tabindex="-1"></a>        mode<span class="op">=</span><span class="st">'lines'</span>,</span>
<span id="cb16-906"><a href="#cb16-906" aria-hidden="true" tabindex="-1"></a>        name<span class="op">=</span><span class="st">'True class = 0'</span>,</span>
<span id="cb16-907"><a href="#cb16-907" aria-hidden="true" tabindex="-1"></a>        line<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span><span class="st">'red'</span>, width<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb16-908"><a href="#cb16-908" aria-hidden="true" tabindex="-1"></a>    ))</span>
<span id="cb16-909"><a href="#cb16-909" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-910"><a href="#cb16-910" aria-hidden="true" tabindex="-1"></a>    fig.update_layout(</span>
<span id="cb16-911"><a href="#cb16-911" aria-hidden="true" tabindex="-1"></a>        title<span class="op">=</span><span class="st">"Cross-Entropy Loss: How wrong predictions are penalized"</span>,</span>
<span id="cb16-912"><a href="#cb16-912" aria-hidden="true" tabindex="-1"></a>        xaxis_title<span class="op">=</span><span class="st">"Predicted Probability for Class 1"</span>,</span>
<span id="cb16-913"><a href="#cb16-913" aria-hidden="true" tabindex="-1"></a>        yaxis_title<span class="op">=</span><span class="st">"Cross-Entropy Loss"</span>,</span>
<span id="cb16-914"><a href="#cb16-914" aria-hidden="true" tabindex="-1"></a>        hovermode<span class="op">=</span><span class="st">'x unified'</span>,</span>
<span id="cb16-915"><a href="#cb16-915" aria-hidden="true" tabindex="-1"></a>        height<span class="op">=</span><span class="dv">500</span></span>
<span id="cb16-916"><a href="#cb16-916" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb16-917"><a href="#cb16-917" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-918"><a href="#cb16-918" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add annotations</span></span>
<span id="cb16-919"><a href="#cb16-919" aria-hidden="true" tabindex="-1"></a>    fig.add_annotation(</span>
<span id="cb16-920"><a href="#cb16-920" aria-hidden="true" tabindex="-1"></a>        x<span class="op">=</span><span class="fl">0.1</span>, y<span class="op">=</span><span class="fl">2.3</span>,</span>
<span id="cb16-921"><a href="#cb16-921" aria-hidden="true" tabindex="-1"></a>        text<span class="op">=</span><span class="st">"High penalty for&lt;br&gt;confident wrong prediction!"</span>,</span>
<span id="cb16-922"><a href="#cb16-922" aria-hidden="true" tabindex="-1"></a>        showarrow<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb16-923"><a href="#cb16-923" aria-hidden="true" tabindex="-1"></a>        arrowhead<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb16-924"><a href="#cb16-924" aria-hidden="true" tabindex="-1"></a>        ax<span class="op">=</span><span class="dv">50</span>, ay<span class="op">=-</span><span class="dv">50</span></span>
<span id="cb16-925"><a href="#cb16-925" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb16-926"><a href="#cb16-926" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-927"><a href="#cb16-927" aria-hidden="true" tabindex="-1"></a>    fig.add_annotation(</span>
<span id="cb16-928"><a href="#cb16-928" aria-hidden="true" tabindex="-1"></a>        x<span class="op">=</span><span class="fl">0.9</span>, y<span class="op">=</span><span class="fl">2.3</span>,</span>
<span id="cb16-929"><a href="#cb16-929" aria-hidden="true" tabindex="-1"></a>        text<span class="op">=</span><span class="st">"High penalty for&lt;br&gt;confident wrong prediction!"</span>,</span>
<span id="cb16-930"><a href="#cb16-930" aria-hidden="true" tabindex="-1"></a>        showarrow<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb16-931"><a href="#cb16-931" aria-hidden="true" tabindex="-1"></a>        arrowhead<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb16-932"><a href="#cb16-932" aria-hidden="true" tabindex="-1"></a>        ax<span class="op">=-</span><span class="dv">50</span>, ay<span class="op">=-</span><span class="dv">50</span></span>
<span id="cb16-933"><a href="#cb16-933" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb16-934"><a href="#cb16-934" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-935"><a href="#cb16-935" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> fig</span>
<span id="cb16-936"><a href="#cb16-936" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-937"><a href="#cb16-937" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> create_ce_landscape()</span>
<span id="cb16-938"><a href="#cb16-938" aria-hidden="true" tabindex="-1"></a>fig.show()</span>
<span id="cb16-939"><a href="#cb16-939" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-940"><a href="#cb16-940" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-941"><a href="#cb16-941" aria-hidden="true" tabindex="-1"></a>**Key Insights**:</span>
<span id="cb16-942"><a href="#cb16-942" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-943"><a href="#cb16-943" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Loss approaches **âˆž** when model is confidently wrong (pâ†’0 for true class)</span>
<span id="cb16-944"><a href="#cb16-944" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Loss approaches **0** when model is confidently correct (pâ†’1 for true class)</span>
<span id="cb16-945"><a href="#cb16-945" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Asymmetric penalty**: Being wrong and confident is much worse than being uncertain</span>
<span id="cb16-946"><a href="#cb16-946" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-947"><a href="#cb16-947" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-950"><a href="#cb16-950" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-951"><a href="#cb16-951" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb16-952"><a href="#cb16-952" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb16-953"><a href="#cb16-953" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 600</span></span>
<span id="cb16-954"><a href="#cb16-954" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-classification-cross-entropy</span></span>
<span id="cb16-955"><a href="#cb16-955" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap:</span></span>
<span id="cb16-956"><a href="#cb16-956" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - "True Distribution (left) and Predicted Distribution (right)."</span></span>
<span id="cb16-957"><a href="#cb16-957" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-958"><a href="#cb16-958" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb16-959"><a href="#cb16-959" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb16-960"><a href="#cb16-960" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb16-961"><a href="#cb16-961" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-962"><a href="#cb16-962" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">123</span>)</span>
<span id="cb16-963"><a href="#cb16-963" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb16-964"><a href="#cb16-964" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> np.random.normal(size<span class="op">=</span>(<span class="dv">5</span>, )) <span class="op">*</span> <span class="dv">2</span></span>
<span id="cb16-965"><a href="#cb16-965" aria-hidden="true" tabindex="-1"></a>y_hat <span class="op">=</span> np.exp(logits) <span class="op">/</span> np.exp(logits).<span class="bu">sum</span>()</span>
<span id="cb16-966"><a href="#cb16-966" aria-hidden="true" tabindex="-1"></a>y_true <span class="op">=</span> np.zeros_like(logits)</span>
<span id="cb16-967"><a href="#cb16-967" aria-hidden="true" tabindex="-1"></a>y_true[<span class="dv">1</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb16-968"><a href="#cb16-968" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-969"><a href="#cb16-969" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(ncols<span class="op">=</span><span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">9</span>, <span class="dv">3</span>))</span>
<span id="cb16-970"><a href="#cb16-970" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> sns.barplot(x<span class="op">=</span>[i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, k)], y<span class="op">=</span>y_true, ax<span class="op">=</span>ax[<span class="dv">0</span>]).<span class="bu">set</span>(</span>
<span id="cb16-971"><a href="#cb16-971" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">"True Distribution"</span>)</span>
<span id="cb16-972"><a href="#cb16-972" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> sns.barplot(x<span class="op">=</span>[i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, k)], y<span class="op">=</span>y_hat, ax<span class="op">=</span>ax[<span class="dv">1</span>]).<span class="bu">set</span>(</span>
<span id="cb16-973"><a href="#cb16-973" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">"Predicted Distribution"</span>, ylim<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb16-974"><a href="#cb16-974" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-975"><a href="#cb16-975" aria-hidden="true" tabindex="-1"></a>cross_entropy <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>(<span class="op">-</span> y_true <span class="op">*</span> np.log(y_hat))<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">:.3f}</span><span class="ss">"</span></span>
<span id="cb16-976"><a href="#cb16-976" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-977"><a href="#cb16-977" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb16-978"><a href="#cb16-978" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb16-979"><a href="#cb16-979" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-980"><a href="#cb16-980" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-981"><a href="#cb16-981" aria-hidden="true" tabindex="-1"></a>@fig-classification-cross-entropy shows an example with a cross-entropy value of: <span class="in">`{python} cross_entropy`</span>.</span>
<span id="cb16-982"><a href="#cb16-982" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-983"><a href="#cb16-983" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title="ðŸŽ¯ Interactive Multi-Class Cross-Entropy" collapse="true"}</span>
<span id="cb16-984"><a href="#cb16-984" aria-hidden="true" tabindex="-1"></a>**See how different predictions map to loss values on the curve**:</span>
<span id="cb16-985"><a href="#cb16-985" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-988"><a href="#cb16-988" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-989"><a href="#cb16-989" aria-hidden="true" tabindex="-1"></a><span class="co"># | eval: true</span></span>
<span id="cb16-990"><a href="#cb16-990" aria-hidden="true" tabindex="-1"></a><span class="co"># | echo: false</span></span>
<span id="cb16-991"><a href="#cb16-991" aria-hidden="true" tabindex="-1"></a><span class="co"># | fig-width: 800</span></span>
<span id="cb16-992"><a href="#cb16-992" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-993"><a href="#cb16-993" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-994"><a href="#cb16-994" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_multiclass_ce_demo():</span>
<span id="cb16-995"><a href="#cb16-995" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Interactive multi-class cross-entropy demonstration."""</span></span>
<span id="cb16-996"><a href="#cb16-996" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-997"><a href="#cb16-997" aria-hidden="true" tabindex="-1"></a>    <span class="co"># True label (one-hot)</span></span>
<span id="cb16-998"><a href="#cb16-998" aria-hidden="true" tabindex="-1"></a>    true_class <span class="op">=</span> <span class="dv">1</span>  <span class="co"># Class 1 (0-indexed)</span></span>
<span id="cb16-999"><a href="#cb16-999" aria-hidden="true" tabindex="-1"></a>    n_classes <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb16-1000"><a href="#cb16-1000" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1001"><a href="#cb16-1001" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create scenarios with different predictions</span></span>
<span id="cb16-1002"><a href="#cb16-1002" aria-hidden="true" tabindex="-1"></a>    scenarios <span class="op">=</span> {</span>
<span id="cb16-1003"><a href="#cb16-1003" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Perfect Prediction"</span>: [<span class="fl">0.0</span>, <span class="fl">1.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>],</span>
<span id="cb16-1004"><a href="#cb16-1004" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Confident Correct"</span>: [<span class="fl">0.05</span>, <span class="fl">0.85</span>, <span class="fl">0.05</span>, <span class="fl">0.03</span>, <span class="fl">0.02</span>],</span>
<span id="cb16-1005"><a href="#cb16-1005" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Uncertain Correct"</span>: [<span class="fl">0.15</span>, <span class="fl">0.35</span>, <span class="fl">0.2</span>, <span class="fl">0.15</span>, <span class="fl">0.15</span>],</span>
<span id="cb16-1006"><a href="#cb16-1006" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Confident Wrong"</span>: [<span class="fl">0.85</span>, <span class="fl">0.05</span>, <span class="fl">0.05</span>, <span class="fl">0.03</span>, <span class="fl">0.02</span>],</span>
<span id="cb16-1007"><a href="#cb16-1007" aria-hidden="true" tabindex="-1"></a>        <span class="st">"Uniform (Random)"</span>: [<span class="fl">0.2</span>, <span class="fl">0.2</span>, <span class="fl">0.2</span>, <span class="fl">0.2</span>, <span class="fl">0.2</span>],</span>
<span id="cb16-1008"><a href="#cb16-1008" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb16-1009"><a href="#cb16-1009" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1010"><a href="#cb16-1010" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate cross-entropy for each scenario</span></span>
<span id="cb16-1011"><a href="#cb16-1011" aria-hidden="true" tabindex="-1"></a>    ce_losses <span class="op">=</span> {}</span>
<span id="cb16-1012"><a href="#cb16-1012" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> name, probs <span class="kw">in</span> scenarios.items():</span>
<span id="cb16-1013"><a href="#cb16-1013" aria-hidden="true" tabindex="-1"></a>        ce <span class="op">=</span> <span class="op">-</span>np.log(probs[true_class] <span class="op">+</span> <span class="fl">1e-10</span>)</span>
<span id="cb16-1014"><a href="#cb16-1014" aria-hidden="true" tabindex="-1"></a>        ce_losses[name] <span class="op">=</span> ce</span>
<span id="cb16-1015"><a href="#cb16-1015" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1016"><a href="#cb16-1016" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create visualization with 2 rows</span></span>
<span id="cb16-1017"><a href="#cb16-1017" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> make_subplots(</span>
<span id="cb16-1018"><a href="#cb16-1018" aria-hidden="true" tabindex="-1"></a>        rows<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb16-1019"><a href="#cb16-1019" aria-hidden="true" tabindex="-1"></a>        cols<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb16-1020"><a href="#cb16-1020" aria-hidden="true" tabindex="-1"></a>        subplot_titles<span class="op">=</span>(</span>
<span id="cb16-1021"><a href="#cb16-1021" aria-hidden="true" tabindex="-1"></a>            <span class="st">"Predicted Probabilities"</span>,</span>
<span id="cb16-1022"><a href="#cb16-1022" aria-hidden="true" tabindex="-1"></a>            <span class="st">"Cross-Entropy Loss Landscape - Red Dot Shows Current Loss"</span>,</span>
<span id="cb16-1023"><a href="#cb16-1023" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb16-1024"><a href="#cb16-1024" aria-hidden="true" tabindex="-1"></a>        row_heights<span class="op">=</span>[<span class="fl">0.45</span>, <span class="fl">0.55</span>],</span>
<span id="cb16-1025"><a href="#cb16-1025" aria-hidden="true" tabindex="-1"></a>        vertical_spacing<span class="op">=</span><span class="fl">0.15</span>,</span>
<span id="cb16-1026"><a href="#cb16-1026" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb16-1027"><a href="#cb16-1027" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1028"><a href="#cb16-1028" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initial scenario</span></span>
<span id="cb16-1029"><a href="#cb16-1029" aria-hidden="true" tabindex="-1"></a>    initial_name <span class="op">=</span> <span class="st">"Confident Correct"</span></span>
<span id="cb16-1030"><a href="#cb16-1030" aria-hidden="true" tabindex="-1"></a>    initial_probs <span class="op">=</span> scenarios[initial_name]</span>
<span id="cb16-1031"><a href="#cb16-1031" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1032"><a href="#cb16-1032" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Row 1: Add probability bars</span></span>
<span id="cb16-1033"><a href="#cb16-1033" aria-hidden="true" tabindex="-1"></a>    colors <span class="op">=</span> [</span>
<span id="cb16-1034"><a href="#cb16-1034" aria-hidden="true" tabindex="-1"></a>        <span class="st">"lightcoral"</span> <span class="cf">if</span> i <span class="op">!=</span> true_class <span class="cf">else</span> <span class="st">"lightgreen"</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_classes)</span>
<span id="cb16-1035"><a href="#cb16-1035" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb16-1036"><a href="#cb16-1036" aria-hidden="true" tabindex="-1"></a>    fig.add_trace(</span>
<span id="cb16-1037"><a href="#cb16-1037" aria-hidden="true" tabindex="-1"></a>        go.Bar(</span>
<span id="cb16-1038"><a href="#cb16-1038" aria-hidden="true" tabindex="-1"></a>            x<span class="op">=</span>[<span class="ss">f"Class </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_classes)],</span>
<span id="cb16-1039"><a href="#cb16-1039" aria-hidden="true" tabindex="-1"></a>            y<span class="op">=</span>initial_probs,</span>
<span id="cb16-1040"><a href="#cb16-1040" aria-hidden="true" tabindex="-1"></a>            marker_color<span class="op">=</span>colors,</span>
<span id="cb16-1041"><a href="#cb16-1041" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"Probabilities"</span>,</span>
<span id="cb16-1042"><a href="#cb16-1042" aria-hidden="true" tabindex="-1"></a>            text<span class="op">=</span>[<span class="ss">f"</span><span class="sc">{</span>p<span class="sc">:.2f}</span><span class="ss">"</span> <span class="cf">for</span> p <span class="kw">in</span> initial_probs],</span>
<span id="cb16-1043"><a href="#cb16-1043" aria-hidden="true" tabindex="-1"></a>            textposition<span class="op">=</span><span class="st">"outside"</span>,</span>
<span id="cb16-1044"><a href="#cb16-1044" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb16-1045"><a href="#cb16-1045" aria-hidden="true" tabindex="-1"></a>        row<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb16-1046"><a href="#cb16-1046" aria-hidden="true" tabindex="-1"></a>        col<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb16-1047"><a href="#cb16-1047" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb16-1048"><a href="#cb16-1048" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1049"><a href="#cb16-1049" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Row 2: Add CE loss landscape</span></span>
<span id="cb16-1050"><a href="#cb16-1050" aria-hidden="true" tabindex="-1"></a>    p_pred <span class="op">=</span> np.linspace(<span class="fl">0.01</span>, <span class="fl">0.99</span>, <span class="dv">100</span>)</span>
<span id="cb16-1051"><a href="#cb16-1051" aria-hidden="true" tabindex="-1"></a>    loss_curve <span class="op">=</span> <span class="op">-</span>np.log(p_pred)</span>
<span id="cb16-1052"><a href="#cb16-1052" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1053"><a href="#cb16-1053" aria-hidden="true" tabindex="-1"></a>    fig.add_trace(</span>
<span id="cb16-1054"><a href="#cb16-1054" aria-hidden="true" tabindex="-1"></a>        go.Scatter(</span>
<span id="cb16-1055"><a href="#cb16-1055" aria-hidden="true" tabindex="-1"></a>            x<span class="op">=</span>p_pred,</span>
<span id="cb16-1056"><a href="#cb16-1056" aria-hidden="true" tabindex="-1"></a>            y<span class="op">=</span>loss_curve,</span>
<span id="cb16-1057"><a href="#cb16-1057" aria-hidden="true" tabindex="-1"></a>            mode<span class="op">=</span><span class="st">"lines"</span>,</span>
<span id="cb16-1058"><a href="#cb16-1058" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">"CE Loss Curve"</span>,</span>
<span id="cb16-1059"><a href="#cb16-1059" aria-hidden="true" tabindex="-1"></a>            line<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span><span class="st">"blue"</span>, width<span class="op">=</span><span class="dv">3</span>),</span>
<span id="cb16-1060"><a href="#cb16-1060" aria-hidden="true" tabindex="-1"></a>            showlegend<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb16-1061"><a href="#cb16-1061" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb16-1062"><a href="#cb16-1062" aria-hidden="true" tabindex="-1"></a>        row<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb16-1063"><a href="#cb16-1063" aria-hidden="true" tabindex="-1"></a>        col<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb16-1064"><a href="#cb16-1064" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb16-1065"><a href="#cb16-1065" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1066"><a href="#cb16-1066" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add point for initial scenario on the loss landscape</span></span>
<span id="cb16-1067"><a href="#cb16-1067" aria-hidden="true" tabindex="-1"></a>    initial_pred <span class="op">=</span> initial_probs[true_class]</span>
<span id="cb16-1068"><a href="#cb16-1068" aria-hidden="true" tabindex="-1"></a>    initial_loss <span class="op">=</span> ce_losses[initial_name]</span>
<span id="cb16-1069"><a href="#cb16-1069" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1070"><a href="#cb16-1070" aria-hidden="true" tabindex="-1"></a>    fig.add_trace(</span>
<span id="cb16-1071"><a href="#cb16-1071" aria-hidden="true" tabindex="-1"></a>        go.Scatter(</span>
<span id="cb16-1072"><a href="#cb16-1072" aria-hidden="true" tabindex="-1"></a>            x<span class="op">=</span>[initial_pred],</span>
<span id="cb16-1073"><a href="#cb16-1073" aria-hidden="true" tabindex="-1"></a>            y<span class="op">=</span>[initial_loss],</span>
<span id="cb16-1074"><a href="#cb16-1074" aria-hidden="true" tabindex="-1"></a>            mode<span class="op">=</span><span class="st">"markers"</span>,</span>
<span id="cb16-1075"><a href="#cb16-1075" aria-hidden="true" tabindex="-1"></a>            marker<span class="op">=</span><span class="bu">dict</span>(</span>
<span id="cb16-1076"><a href="#cb16-1076" aria-hidden="true" tabindex="-1"></a>                size<span class="op">=</span><span class="dv">15</span>,</span>
<span id="cb16-1077"><a href="#cb16-1077" aria-hidden="true" tabindex="-1"></a>                color<span class="op">=</span><span class="st">"red"</span>,</span>
<span id="cb16-1078"><a href="#cb16-1078" aria-hidden="true" tabindex="-1"></a>                symbol<span class="op">=</span><span class="st">"circle"</span>,</span>
<span id="cb16-1079"><a href="#cb16-1079" aria-hidden="true" tabindex="-1"></a>                line<span class="op">=</span><span class="bu">dict</span>(width<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span><span class="st">"darkred"</span>),</span>
<span id="cb16-1080"><a href="#cb16-1080" aria-hidden="true" tabindex="-1"></a>            ),</span>
<span id="cb16-1081"><a href="#cb16-1081" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span>initial_name,</span>
<span id="cb16-1082"><a href="#cb16-1082" aria-hidden="true" tabindex="-1"></a>            text<span class="op">=</span>[<span class="ss">f"</span><span class="sc">{</span>initial_name<span class="sc">}</span><span class="ss">&lt;br&gt;p=</span><span class="sc">{</span>initial_pred<span class="sc">:.3f}</span><span class="ss">&lt;br&gt;Loss=</span><span class="sc">{</span>initial_loss<span class="sc">:.3f}</span><span class="ss">"</span>],</span>
<span id="cb16-1083"><a href="#cb16-1083" aria-hidden="true" tabindex="-1"></a>            hovertemplate<span class="op">=</span><span class="st">"%</span><span class="sc">{text}</span><span class="st">&lt;extra&gt;&lt;/extra&gt;"</span>,</span>
<span id="cb16-1084"><a href="#cb16-1084" aria-hidden="true" tabindex="-1"></a>            showlegend<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb16-1085"><a href="#cb16-1085" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb16-1086"><a href="#cb16-1086" aria-hidden="true" tabindex="-1"></a>        row<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb16-1087"><a href="#cb16-1087" aria-hidden="true" tabindex="-1"></a>        col<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb16-1088"><a href="#cb16-1088" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb16-1089"><a href="#cb16-1089" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1090"><a href="#cb16-1090" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add true label indicator</span></span>
<span id="cb16-1091"><a href="#cb16-1091" aria-hidden="true" tabindex="-1"></a>    fig.add_annotation(</span>
<span id="cb16-1092"><a href="#cb16-1092" aria-hidden="true" tabindex="-1"></a>        x<span class="op">=</span><span class="ss">f"Class </span><span class="sc">{</span>true_class<span class="sc">}</span><span class="ss">"</span>,</span>
<span id="cb16-1093"><a href="#cb16-1093" aria-hidden="true" tabindex="-1"></a>        y<span class="op">=</span><span class="fl">1.05</span>,</span>
<span id="cb16-1094"><a href="#cb16-1094" aria-hidden="true" tabindex="-1"></a>        text<span class="op">=</span><span class="st">"â† TRUE LABEL"</span>,</span>
<span id="cb16-1095"><a href="#cb16-1095" aria-hidden="true" tabindex="-1"></a>        showarrow<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb16-1096"><a href="#cb16-1096" aria-hidden="true" tabindex="-1"></a>        yref<span class="op">=</span><span class="st">"y1"</span>,</span>
<span id="cb16-1097"><a href="#cb16-1097" aria-hidden="true" tabindex="-1"></a>        xref<span class="op">=</span><span class="st">"x1"</span>,</span>
<span id="cb16-1098"><a href="#cb16-1098" aria-hidden="true" tabindex="-1"></a>        font<span class="op">=</span><span class="bu">dict</span>(size<span class="op">=</span><span class="dv">12</span>, color<span class="op">=</span><span class="st">"green"</span>, family<span class="op">=</span><span class="st">"Arial Black"</span>),</span>
<span id="cb16-1099"><a href="#cb16-1099" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb16-1100"><a href="#cb16-1100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1101"><a href="#cb16-1101" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Build scenario buttons</span></span>
<span id="cb16-1102"><a href="#cb16-1102" aria-hidden="true" tabindex="-1"></a>    buttons <span class="op">=</span> []</span>
<span id="cb16-1103"><a href="#cb16-1103" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> name, probs <span class="kw">in</span> scenarios.items():</span>
<span id="cb16-1104"><a href="#cb16-1104" aria-hidden="true" tabindex="-1"></a>        colors <span class="op">=</span> [</span>
<span id="cb16-1105"><a href="#cb16-1105" aria-hidden="true" tabindex="-1"></a>            <span class="st">"lightcoral"</span> <span class="cf">if</span> i <span class="op">!=</span> true_class <span class="cf">else</span> <span class="st">"lightgreen"</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_classes)</span>
<span id="cb16-1106"><a href="#cb16-1106" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb16-1107"><a href="#cb16-1107" aria-hidden="true" tabindex="-1"></a>        pred_for_true <span class="op">=</span> probs[true_class]</span>
<span id="cb16-1108"><a href="#cb16-1108" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> ce_losses[name]</span>
<span id="cb16-1109"><a href="#cb16-1109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1110"><a href="#cb16-1110" aria-hidden="true" tabindex="-1"></a>        buttons.append(</span>
<span id="cb16-1111"><a href="#cb16-1111" aria-hidden="true" tabindex="-1"></a>            <span class="bu">dict</span>(</span>
<span id="cb16-1112"><a href="#cb16-1112" aria-hidden="true" tabindex="-1"></a>                label<span class="op">=</span>name,</span>
<span id="cb16-1113"><a href="#cb16-1113" aria-hidden="true" tabindex="-1"></a>                method<span class="op">=</span><span class="st">"update"</span>,</span>
<span id="cb16-1114"><a href="#cb16-1114" aria-hidden="true" tabindex="-1"></a>                args<span class="op">=</span>[</span>
<span id="cb16-1115"><a href="#cb16-1115" aria-hidden="true" tabindex="-1"></a>                    {</span>
<span id="cb16-1116"><a href="#cb16-1116" aria-hidden="true" tabindex="-1"></a>                        <span class="st">"y"</span>: [</span>
<span id="cb16-1117"><a href="#cb16-1117" aria-hidden="true" tabindex="-1"></a>                            probs,  <span class="co"># bars (row 1)</span></span>
<span id="cb16-1118"><a href="#cb16-1118" aria-hidden="true" tabindex="-1"></a>                            loss_curve,  <span class="co"># CE curve (row 2)</span></span>
<span id="cb16-1119"><a href="#cb16-1119" aria-hidden="true" tabindex="-1"></a>                            [loss],  <span class="co"># red dot y (row 2)</span></span>
<span id="cb16-1120"><a href="#cb16-1120" aria-hidden="true" tabindex="-1"></a>                        ],</span>
<span id="cb16-1121"><a href="#cb16-1121" aria-hidden="true" tabindex="-1"></a>                        <span class="st">"text"</span>: [</span>
<span id="cb16-1122"><a href="#cb16-1122" aria-hidden="true" tabindex="-1"></a>                            [<span class="ss">f"</span><span class="sc">{</span>p<span class="sc">:.2f}</span><span class="ss">"</span> <span class="cf">for</span> p <span class="kw">in</span> probs],</span>
<span id="cb16-1123"><a href="#cb16-1123" aria-hidden="true" tabindex="-1"></a>                            <span class="va">None</span>,</span>
<span id="cb16-1124"><a href="#cb16-1124" aria-hidden="true" tabindex="-1"></a>                            [<span class="ss">f"</span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">&lt;br&gt;p=</span><span class="sc">{</span>pred_for_true<span class="sc">:.3f}</span><span class="ss">&lt;br&gt;Loss=</span><span class="sc">{</span>loss<span class="sc">:.3f}</span><span class="ss">"</span>],</span>
<span id="cb16-1125"><a href="#cb16-1125" aria-hidden="true" tabindex="-1"></a>                        ],</span>
<span id="cb16-1126"><a href="#cb16-1126" aria-hidden="true" tabindex="-1"></a>                        <span class="st">"x"</span>: [</span>
<span id="cb16-1127"><a href="#cb16-1127" aria-hidden="true" tabindex="-1"></a>                            [<span class="ss">f"Class </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_classes)],</span>
<span id="cb16-1128"><a href="#cb16-1128" aria-hidden="true" tabindex="-1"></a>                            p_pred,</span>
<span id="cb16-1129"><a href="#cb16-1129" aria-hidden="true" tabindex="-1"></a>                            [pred_for_true],</span>
<span id="cb16-1130"><a href="#cb16-1130" aria-hidden="true" tabindex="-1"></a>                        ],</span>
<span id="cb16-1131"><a href="#cb16-1131" aria-hidden="true" tabindex="-1"></a>                    },</span>
<span id="cb16-1132"><a href="#cb16-1132" aria-hidden="true" tabindex="-1"></a>                    {</span>
<span id="cb16-1133"><a href="#cb16-1133" aria-hidden="true" tabindex="-1"></a>                        <span class="st">"title.text"</span>: <span class="ss">f"Scenario: </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss"> | Probability for True Class = </span><span class="sc">{</span>pred_for_true<span class="sc">:.3f}</span><span class="ss"> | Loss = </span><span class="sc">{</span>loss<span class="sc">:.3f}</span><span class="ss">"</span></span>
<span id="cb16-1134"><a href="#cb16-1134" aria-hidden="true" tabindex="-1"></a>                    },</span>
<span id="cb16-1135"><a href="#cb16-1135" aria-hidden="true" tabindex="-1"></a>                ],</span>
<span id="cb16-1136"><a href="#cb16-1136" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb16-1137"><a href="#cb16-1137" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb16-1138"><a href="#cb16-1138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1139"><a href="#cb16-1139" aria-hidden="true" tabindex="-1"></a>    <span class="co"># --- Fix overlaps ---</span></span>
<span id="cb16-1140"><a href="#cb16-1140" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1) Move subplot titles slightly *down*; give row-2 (bottom) title extra downward shift</span></span>
<span id="cb16-1141"><a href="#cb16-1141" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> ann <span class="kw">in</span> fig.layout.annotations:</span>
<span id="cb16-1142"><a href="#cb16-1142" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> ann.text <span class="op">==</span> <span class="st">"Predicted Probabilities"</span>:</span>
<span id="cb16-1143"><a href="#cb16-1143" aria-hidden="true" tabindex="-1"></a>            ann.y <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb16-1144"><a href="#cb16-1144" aria-hidden="true" tabindex="-1"></a>            ann.yanchor <span class="op">=</span> <span class="st">"bottom"</span></span>
<span id="cb16-1145"><a href="#cb16-1145" aria-hidden="true" tabindex="-1"></a>            ann.yshift <span class="op">=</span> <span class="op">-</span><span class="dv">2</span></span>
<span id="cb16-1146"><a href="#cb16-1146" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> ann.text <span class="op">==</span> <span class="st">"Cross-Entropy Loss Landscape - Red Dot Shows Current Loss"</span>:</span>
<span id="cb16-1147"><a href="#cb16-1147" aria-hidden="true" tabindex="-1"></a>            ann.y <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb16-1148"><a href="#cb16-1148" aria-hidden="true" tabindex="-1"></a>            ann.yanchor <span class="op">=</span> <span class="st">"bottom"</span></span>
<span id="cb16-1149"><a href="#cb16-1149" aria-hidden="true" tabindex="-1"></a>            ann.yshift <span class="op">=</span> <span class="op">-</span><span class="dv">10</span>  <span class="co"># more space so it doesn't collide with yaxis2 title</span></span>
<span id="cb16-1150"><a href="#cb16-1150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1151"><a href="#cb16-1151" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2) Increase standoff for yaxis2 title to keep it clear of the plot area and subtitles</span></span>
<span id="cb16-1152"><a href="#cb16-1152" aria-hidden="true" tabindex="-1"></a>    <span class="co">#    (this adds horizontal spacing between axis title and ticks/plot body)</span></span>
<span id="cb16-1153"><a href="#cb16-1153" aria-hidden="true" tabindex="-1"></a>    yaxis2_layout <span class="op">=</span> <span class="bu">dict</span>(</span>
<span id="cb16-1154"><a href="#cb16-1154" aria-hidden="true" tabindex="-1"></a>        title<span class="op">=</span><span class="st">"Cross-Entropy Loss"</span>, <span class="bu">range</span><span class="op">=</span>[<span class="dv">0</span>, <span class="dv">5</span>], automargin<span class="op">=</span><span class="va">True</span>, title_standoff<span class="op">=</span><span class="dv">20</span></span>
<span id="cb16-1155"><a href="#cb16-1155" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb16-1156"><a href="#cb16-1156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1157"><a href="#cb16-1157" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3) Place dropdown *below* the overall title, clearly separated (opens downward)</span></span>
<span id="cb16-1158"><a href="#cb16-1158" aria-hidden="true" tabindex="-1"></a>    <span class="co">#    Adjust top margin to reserve space for title + menu without overlap.</span></span>
<span id="cb16-1159"><a href="#cb16-1159" aria-hidden="true" tabindex="-1"></a>    fig.update_layout(</span>
<span id="cb16-1160"><a href="#cb16-1160" aria-hidden="true" tabindex="-1"></a>        updatemenus<span class="op">=</span>[</span>
<span id="cb16-1161"><a href="#cb16-1161" aria-hidden="true" tabindex="-1"></a>            <span class="bu">dict</span>(</span>
<span id="cb16-1162"><a href="#cb16-1162" aria-hidden="true" tabindex="-1"></a>                <span class="bu">type</span><span class="op">=</span><span class="st">"dropdown"</span>,</span>
<span id="cb16-1163"><a href="#cb16-1163" aria-hidden="true" tabindex="-1"></a>                direction<span class="op">=</span><span class="st">"down"</span>,  <span class="co"># opens downward</span></span>
<span id="cb16-1164"><a href="#cb16-1164" aria-hidden="true" tabindex="-1"></a>                x<span class="op">=</span><span class="fl">0.5</span>,</span>
<span id="cb16-1165"><a href="#cb16-1165" aria-hidden="true" tabindex="-1"></a>                xanchor<span class="op">=</span><span class="st">"center"</span>,</span>
<span id="cb16-1166"><a href="#cb16-1166" aria-hidden="true" tabindex="-1"></a>                y<span class="op">=</span><span class="fl">1.2</span>,</span>
<span id="cb16-1167"><a href="#cb16-1167" aria-hidden="true" tabindex="-1"></a>                yanchor<span class="op">=</span><span class="st">"top"</span>,  <span class="co"># just below the figure title</span></span>
<span id="cb16-1168"><a href="#cb16-1168" aria-hidden="true" tabindex="-1"></a>                buttons<span class="op">=</span>buttons,</span>
<span id="cb16-1169"><a href="#cb16-1169" aria-hidden="true" tabindex="-1"></a>                pad<span class="op">=</span><span class="bu">dict</span>(t<span class="op">=</span><span class="dv">4</span>, r<span class="op">=</span><span class="dv">6</span>, l<span class="op">=</span><span class="dv">6</span>, b<span class="op">=</span><span class="dv">10</span>),</span>
<span id="cb16-1170"><a href="#cb16-1170" aria-hidden="true" tabindex="-1"></a>                bgcolor<span class="op">=</span><span class="st">"white"</span>,</span>
<span id="cb16-1171"><a href="#cb16-1171" aria-hidden="true" tabindex="-1"></a>                bordercolor<span class="op">=</span><span class="st">"#ddd"</span>,</span>
<span id="cb16-1172"><a href="#cb16-1172" aria-hidden="true" tabindex="-1"></a>                borderwidth<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb16-1173"><a href="#cb16-1173" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb16-1174"><a href="#cb16-1174" aria-hidden="true" tabindex="-1"></a>        ],</span>
<span id="cb16-1175"><a href="#cb16-1175" aria-hidden="true" tabindex="-1"></a>        height<span class="op">=</span><span class="dv">700</span>,</span>
<span id="cb16-1176"><a href="#cb16-1176" aria-hidden="true" tabindex="-1"></a>        margin<span class="op">=</span><span class="bu">dict</span>(t<span class="op">=</span><span class="dv">120</span>, r<span class="op">=</span><span class="dv">40</span>, b<span class="op">=</span><span class="dv">60</span>, l<span class="op">=</span><span class="dv">70</span>),  <span class="co"># a bit more left margin for yaxis title</span></span>
<span id="cb16-1177"><a href="#cb16-1177" aria-hidden="true" tabindex="-1"></a>        showlegend<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb16-1178"><a href="#cb16-1178" aria-hidden="true" tabindex="-1"></a>        yaxis1<span class="op">=</span><span class="bu">dict</span>(<span class="bu">range</span><span class="op">=</span>[<span class="dv">0</span>, <span class="fl">1.1</span>], title<span class="op">=</span><span class="st">"Probability"</span>, automargin<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb16-1179"><a href="#cb16-1179" aria-hidden="true" tabindex="-1"></a>        yaxis2<span class="op">=</span>yaxis2_layout,</span>
<span id="cb16-1180"><a href="#cb16-1180" aria-hidden="true" tabindex="-1"></a>        xaxis2<span class="op">=</span><span class="bu">dict</span>(title<span class="op">=</span><span class="st">"Predicted Probability for True Class (Class 1)"</span>),</span>
<span id="cb16-1181"><a href="#cb16-1181" aria-hidden="true" tabindex="-1"></a>        title<span class="op">=</span><span class="bu">dict</span>(</span>
<span id="cb16-1182"><a href="#cb16-1182" aria-hidden="true" tabindex="-1"></a>            text<span class="op">=</span><span class="ss">f"Scenario: </span><span class="sc">{</span>initial_name<span class="sc">}</span><span class="ss"> | Probability for True Class = </span><span class="sc">{</span>initial_pred<span class="sc">:.3f}</span><span class="ss"> | Loss = </span><span class="sc">{</span>initial_loss<span class="sc">:.3f}</span><span class="ss">"</span>,</span>
<span id="cb16-1183"><a href="#cb16-1183" aria-hidden="true" tabindex="-1"></a>            y<span class="op">=</span><span class="fl">0.985</span>,  <span class="co"># overall title near top</span></span>
<span id="cb16-1184"><a href="#cb16-1184" aria-hidden="true" tabindex="-1"></a>            yanchor<span class="op">=</span><span class="st">"top"</span>,</span>
<span id="cb16-1185"><a href="#cb16-1185" aria-hidden="true" tabindex="-1"></a>            x<span class="op">=</span><span class="fl">0.5</span>,</span>
<span id="cb16-1186"><a href="#cb16-1186" aria-hidden="true" tabindex="-1"></a>            xanchor<span class="op">=</span><span class="st">"center"</span>,</span>
<span id="cb16-1187"><a href="#cb16-1187" aria-hidden="true" tabindex="-1"></a>            pad<span class="op">=</span><span class="bu">dict</span>(t<span class="op">=</span><span class="dv">6</span>, b<span class="op">=</span><span class="dv">6</span>),</span>
<span id="cb16-1188"><a href="#cb16-1188" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb16-1189"><a href="#cb16-1189" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb16-1190"><a href="#cb16-1190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1191"><a href="#cb16-1191" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> fig</span>
<span id="cb16-1192"><a href="#cb16-1192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1193"><a href="#cb16-1193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1194"><a href="#cb16-1194" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> create_multiclass_ce_demo()</span>
<span id="cb16-1195"><a href="#cb16-1195" aria-hidden="true" tabindex="-1"></a>fig.show()</span>
<span id="cb16-1196"><a href="#cb16-1196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1197"><a href="#cb16-1197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1198"><a href="#cb16-1198" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-1199"><a href="#cb16-1199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1200"><a href="#cb16-1200" aria-hidden="true" tabindex="-1"></a>**Key Takeaways**:</span>
<span id="cb16-1201"><a href="#cb16-1201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1202"><a href="#cb16-1202" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Green bar** = True class; **Red dot** = Current position on loss curve</span>
<span id="cb16-1203"><a href="#cb16-1203" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Watch how the red dot moves along the curve as predictions change</span>
<span id="cb16-1204"><a href="#cb16-1204" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Perfect prediction (p=1.0) â†’ Dot at left, Loss â‰ˆ 0</span>
<span id="cb16-1205"><a href="#cb16-1205" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Confident wrong (p=0.05) â†’ Dot at right, Loss â‰ˆ 3.0</span>
<span id="cb16-1206"><a href="#cb16-1206" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-1207"><a href="#cb16-1207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1208"><a href="#cb16-1208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1209"><a href="#cb16-1209" aria-hidden="true" tabindex="-1"></a><span class="fu">### Try it out yourself!</span></span>
<span id="cb16-1210"><a href="#cb16-1210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1211"><a href="#cb16-1211" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb16-1212"><a href="#cb16-1212" aria-hidden="true" tabindex="-1"></a><span class="fu">## Train a CNN for image classification in your browser!</span></span>
<span id="cb16-1213"><a href="#cb16-1213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1214"><a href="#cb16-1214" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">CNN Explainer</span><span class="co">](https://poloclub.github.io/cnn-explainer/)</span></span>
<span id="cb16-1215"><a href="#cb16-1215" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-1216"><a href="#cb16-1216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1217"><a href="#cb16-1217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1218"><a href="#cb16-1218" aria-hidden="true" tabindex="-1"></a><span class="fu">## Architectures</span></span>
<span id="cb16-1219"><a href="#cb16-1219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1220"><a href="#cb16-1220" aria-hidden="true" tabindex="-1"></a>Architecture refers to the complete definition of an untrained (deep-learning) model, i.e., the sequence and configuration of layers. The following are some important architectures that have been successfully used for image classification.</span>
<span id="cb16-1221"><a href="#cb16-1221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1222"><a href="#cb16-1222" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- </span><span class="al">TODO</span><span class="co">: add a simple visualization that makes clear at the end is a vector of size k with the class logits --&gt;</span></span>
<span id="cb16-1223"><a href="#cb16-1223" aria-hidden="true" tabindex="-1"></a><span class="fu">### AlexNet</span></span>
<span id="cb16-1224"><a href="#cb16-1224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1225"><a href="#cb16-1225" aria-hidden="true" tabindex="-1"></a>CNNs became extremely popular after winning the <span class="co">[</span><span class="ot">ImageNet</span><span class="co">](https://www.image-net.org/)</span> Competition. @krizhevsky_imagenet_2012 implemented a CNN with multiple layers, known as the AlexNet architecture, as shown in @fig-classification-alexnet. ImageNet is a large, hierarchical image dataset @deng_imagenet_2009, which enabled efficient training of CNNs for the first time.</span>
<span id="cb16-1226"><a href="#cb16-1226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1227"><a href="#cb16-1227" aria-hidden="true" tabindex="-1"></a>AlexNet consists of 5 convolutional layers and 3 fully-connected layers. The last layer is a 1000-way softmax output to model the classes in ImageNet.</span>
<span id="cb16-1228"><a href="#cb16-1228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1229"><a href="#cb16-1229" aria-hidden="true" tabindex="-1"></a>The model was trained with two GPUs (GTX 580) with 3GB memory each. Since 3GB was insufficient to train the model, the architecture was split across the GPUs. Some layers were split between the GPUs, allowing a larger network to be trained.</span>
<span id="cb16-1230"><a href="#cb16-1230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1231"><a href="#cb16-1231" aria-hidden="true" tabindex="-1"></a>@fig-classification-alexnet shows the detailed architecture, including kernel sizes, the number of filters per layer, activation map dimensions, and pooling layers. ReLU was used as the activation function. This representation is from the original paper and shows how the model was split across two GPUs.</span>
<span id="cb16-1232"><a href="#cb16-1232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1233"><a href="#cb16-1233" aria-hidden="true" tabindex="-1"></a>::: {#fig-classification-alexnet}</span>
<span id="cb16-1234"><a href="#cb16-1234" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}alexnet.png)</span>{width=600}</span>
<span id="cb16-1235"><a href="#cb16-1235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1236"><a href="#cb16-1236" aria-hidden="true" tabindex="-1"></a>AlexNet @krizhevsky_imagenet_2012.</span>
<span id="cb16-1237"><a href="#cb16-1237" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-1238"><a href="#cb16-1238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1239"><a href="#cb16-1239" aria-hidden="true" tabindex="-1"></a>Since the split across two GPUs is no longer necessary, the architecture is somewhat simplified. Modern implementations are shown in @fig-classification-alexnet2.</span>
<span id="cb16-1240"><a href="#cb16-1240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1241"><a href="#cb16-1241" aria-hidden="true" tabindex="-1"></a>::: {#fig-classification-alexnet2}</span>
<span id="cb16-1242"><a href="#cb16-1242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1243"><a href="#cb16-1243" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}alexnet_illustration.jpg)</span>{width=400}</span>
<span id="cb16-1244"><a href="#cb16-1244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1245"><a href="#cb16-1245" aria-hidden="true" tabindex="-1"></a>AlexNet @prince_understanding_2023</span>
<span id="cb16-1246"><a href="#cb16-1246" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-1247"><a href="#cb16-1247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1248"><a href="#cb16-1248" aria-hidden="true" tabindex="-1"></a>@fig-classification-alexnet-table presents the operations in AlexNet in tabular form.</span>
<span id="cb16-1249"><a href="#cb16-1249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1250"><a href="#cb16-1250" aria-hidden="true" tabindex="-1"></a>::: {#fig-classification-alexnet-table width=600}</span>
<span id="cb16-1251"><a href="#cb16-1251" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}alexnet_table.jpg)</span></span>
<span id="cb16-1252"><a href="#cb16-1252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1253"><a href="#cb16-1253" aria-hidden="true" tabindex="-1"></a>Source: @johnson_eecs_2019.</span>
<span id="cb16-1254"><a href="#cb16-1254" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-1255"><a href="#cb16-1255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1256"><a href="#cb16-1256" aria-hidden="true" tabindex="-1"></a>We can also easily load AlexNet via torchvision.</span>
<span id="cb16-1257"><a href="#cb16-1257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1260"><a href="#cb16-1260" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-1261"><a href="#cb16-1261" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb16-1262"><a href="#cb16-1262" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb16-1263"><a href="#cb16-1263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1264"><a href="#cb16-1264" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb16-1265"><a href="#cb16-1265" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.models <span class="im">as</span> models</span>
<span id="cb16-1266"><a href="#cb16-1266" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchinfo</span>
<span id="cb16-1267"><a href="#cb16-1267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1268"><a href="#cb16-1268" aria-hidden="true" tabindex="-1"></a>alexnet <span class="op">=</span> models.alexnet()</span>
<span id="cb16-1269"><a href="#cb16-1269" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.zeros(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">224</span>, <span class="dv">224</span>, dtype<span class="op">=</span>torch.<span class="bu">float</span>, requires_grad<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb16-1270"><a href="#cb16-1270" aria-hidden="true" tabindex="-1"></a>yhat <span class="op">=</span> alexnet(x)</span>
<span id="cb16-1271"><a href="#cb16-1271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1272"><a href="#cb16-1272" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(torchinfo.summary(alexnet, input_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">224</span>, <span class="dv">224</span>)))</span>
<span id="cb16-1273"><a href="#cb16-1273" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-1274"><a href="#cb16-1274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1275"><a href="#cb16-1275" aria-hidden="true" tabindex="-1"></a><span class="fu">### VGG</span></span>
<span id="cb16-1276"><a href="#cb16-1276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1277"><a href="#cb16-1277" aria-hidden="true" tabindex="-1"></a>@simonyan_very_2015 won the ImageNet Challenge in 2014 with their VGG architecture. They showed that smaller 3x3 kernels work significantly better and that deeper networks with 16-19 layers can be trained. @fig-classification-vgg shows the architecture as presented in the original paper. @fig-classification-vgg-arch visualizes the architecture.</span>
<span id="cb16-1278"><a href="#cb16-1278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1279"><a href="#cb16-1279" aria-hidden="true" tabindex="-1"></a>::: {#fig-classification-vgg}</span>
<span id="cb16-1280"><a href="#cb16-1280" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}vgg.png)</span>{width=600}</span>
<span id="cb16-1281"><a href="#cb16-1281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1282"><a href="#cb16-1282" aria-hidden="true" tabindex="-1"></a>VGG @simonyan_very_2015.</span>
<span id="cb16-1283"><a href="#cb16-1283" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-1284"><a href="#cb16-1284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1285"><a href="#cb16-1285" aria-hidden="true" tabindex="-1"></a>::: {#fig-classification-vgg-arch}</span>
<span id="cb16-1286"><a href="#cb16-1286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1287"><a href="#cb16-1287" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}vgg_illustration.jpg)</span></span>
<span id="cb16-1288"><a href="#cb16-1288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1289"><a href="#cb16-1289" aria-hidden="true" tabindex="-1"></a>VGG @prince_understanding_2023</span>
<span id="cb16-1290"><a href="#cb16-1290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1291"><a href="#cb16-1291" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-1292"><a href="#cb16-1292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1293"><a href="#cb16-1293" aria-hidden="true" tabindex="-1"></a>VGG introduced a popular design element: A layer has the same number of filters as the previous layer unless the activation map dimensions are halved, in which case the number of filters is doubled (see also @fig-classification-vgg-arch). This was done to maintain the time complexity of the layers. VGG does not use normalization layers.</span>
<span id="cb16-1294"><a href="#cb16-1294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1295"><a href="#cb16-1295" aria-hidden="true" tabindex="-1"></a>@fig-classification-vgg-design compares VGG with AlexNet.</span>
<span id="cb16-1296"><a href="#cb16-1296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1297"><a href="#cb16-1297" aria-hidden="true" tabindex="-1"></a>::: {#fig-classification-vgg-design}</span>
<span id="cb16-1298"><a href="#cb16-1298" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}vgg_design.jpg)</span>{width=600}</span>
<span id="cb16-1299"><a href="#cb16-1299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1300"><a href="#cb16-1300" aria-hidden="true" tabindex="-1"></a>Source: @johnson_eecs_2019.</span>
<span id="cb16-1301"><a href="#cb16-1301" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-1302"><a href="#cb16-1302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1303"><a href="#cb16-1303" aria-hidden="true" tabindex="-1"></a><span class="fu">### ResNet</span></span>
<span id="cb16-1304"><a href="#cb16-1304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1305"><a href="#cb16-1305" aria-hidden="true" tabindex="-1"></a>@He2016 wondered whether CNNs could be improved simply by making them deeper, i.e., adding more layers. Their experiments showed that adding layers eventually stops being beneficial, and performance saturates and then rapidly degrades (see @fig-classification-resnet-motivation1). They noticed that the performance did not degrade due to overfitting (see @fig-classification-resnet-motivation2, which shows that training error is also poor), but because back-propagation becomes less effective and the weights do not optimize well. @He2016 hypothesized that deeper networks could not perform worse than shallower ones because the additional layers could simply pass on activations unchanged.</span>
<span id="cb16-1306"><a href="#cb16-1306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1307"><a href="#cb16-1307" aria-hidden="true" tabindex="-1"></a>::: {#fig-classification-resnet-motivation1}</span>
<span id="cb16-1308"><a href="#cb16-1308" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}resnet_test_error.jpg)</span>{width=600}</span>
<span id="cb16-1309"><a href="#cb16-1309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1310"><a href="#cb16-1310" aria-hidden="true" tabindex="-1"></a>Source: @He2016</span>
<span id="cb16-1311"><a href="#cb16-1311" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-1312"><a href="#cb16-1312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1313"><a href="#cb16-1313" aria-hidden="true" tabindex="-1"></a>::: {#fig-classification-resnet-motivation2}</span>
<span id="cb16-1314"><a href="#cb16-1314" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}resnet_train_error.jpg)</span>{width=600}</span>
<span id="cb16-1315"><a href="#cb16-1315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1316"><a href="#cb16-1316" aria-hidden="true" tabindex="-1"></a>Source: @He2016</span>
<span id="cb16-1317"><a href="#cb16-1317" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-1318"><a href="#cb16-1318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1319"><a href="#cb16-1319" aria-hidden="true" tabindex="-1"></a>They then hypothesized that passing on activations unchanged as a default behavior could avoid this problem. They introduced identity mappings. @fig-classification-residual-connection shows such a connection. These connections are also called residual connections because the network only needs to learn the change in activations from layer $i$ to layer $i+1$, the residue.</span>
<span id="cb16-1320"><a href="#cb16-1320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1321"><a href="#cb16-1321" aria-hidden="true" tabindex="-1"></a>::: {#fig-classification-residual-connection}</span>
<span id="cb16-1322"><a href="#cb16-1322" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}residual_connection.jpg)</span>{width=600}</span>
<span id="cb16-1323"><a href="#cb16-1323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1324"><a href="#cb16-1324" aria-hidden="true" tabindex="-1"></a>ResNet @He2016 (Graphic from @johnson_eecs_2019.)</span>
<span id="cb16-1325"><a href="#cb16-1325" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-1326"><a href="#cb16-1326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1327"><a href="#cb16-1327" aria-hidden="true" tabindex="-1"></a>ResNet was otherwise inspired by VGG16. The convolutional layers consist of 3x3 kernels (except the first one), and the number of filters is doubled when the activation map dimensions are halved. ResNet uses convolutions with stride 2 for down-sampling and no max pooling. At the end, ResNet uses a global average pooling layer followed by a fully-connected layer with the number of classes. Variants of ResNet have 18, 34, 50, and over 100 layers. ResNet also uses batch normalization. @fig-classification-resnet-struk shows the architecture.</span>
<span id="cb16-1328"><a href="#cb16-1328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1329"><a href="#cb16-1329" aria-hidden="true" tabindex="-1"></a>::: {#fig-classification-resnet-struk}</span>
<span id="cb16-1330"><a href="#cb16-1330" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}resnet_arch_horiz.png)</span>{width=600}</span>
<span id="cb16-1331"><a href="#cb16-1331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1332"><a href="#cb16-1332" aria-hidden="true" tabindex="-1"></a>Source: @He2016.</span>
<span id="cb16-1333"><a href="#cb16-1333" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-1334"><a href="#cb16-1334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1335"><a href="#cb16-1335" aria-hidden="true" tabindex="-1"></a>ResNet is very popular and is still widely used today (there are now more modern variants). Recent studies of identity mappings (or more generally skip connections) have shown that the gradient surface of the loss function becomes smoother, allowing the network to be optimized better. This is impressively illustrated in @fig-classification-skip-loss-landscape.</span>
<span id="cb16-1336"><a href="#cb16-1336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1337"><a href="#cb16-1337" aria-hidden="true" tabindex="-1"></a>::: {#fig-classification-skip-loss-landscape}</span>
<span id="cb16-1338"><a href="#cb16-1338" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}loss_landscape_skip.png)</span>{width=600}</span>
<span id="cb16-1339"><a href="#cb16-1339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1340"><a href="#cb16-1340" aria-hidden="true" tabindex="-1"></a>Source: @li_visualizing_2018.</span>
<span id="cb16-1341"><a href="#cb16-1341" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-1342"><a href="#cb16-1342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1343"><a href="#cb16-1343" aria-hidden="true" tabindex="-1"></a><span class="fu">### ConvNext</span></span>
<span id="cb16-1344"><a href="#cb16-1344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1345"><a href="#cb16-1345" aria-hidden="true" tabindex="-1"></a>One of the most modern CNN architectures was described in @liu_convnet_2022. This architecture uses tricks and implementation ideas accumulated over decades from various architectures. @fig-classification-convnext shows, starting from a modern version of ResNet, what has been adjusted to define this state-of-the-art architecture. Examples include: larger kernels, different activation functions, layer normalization instead of batch normalization, and depthwise separable convolutions.</span>
<span id="cb16-1346"><a href="#cb16-1346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1347"><a href="#cb16-1347" aria-hidden="true" tabindex="-1"></a>::: {#fig-classification-convnext}</span>
<span id="cb16-1348"><a href="#cb16-1348" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}convnext.png)</span>{width=600}</span>
<span id="cb16-1349"><a href="#cb16-1349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1350"><a href="#cb16-1350" aria-hidden="true" tabindex="-1"></a>Convnext @liu_convnet_2022.</span>
<span id="cb16-1351"><a href="#cb16-1351" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-1352"><a href="#cb16-1352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1353"><a href="#cb16-1353" aria-hidden="true" tabindex="-1"></a>There is already a new version of this architecture @woo_convnext_2023.</span>
<span id="cb16-1354"><a href="#cb16-1354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1355"><a href="#cb16-1355" aria-hidden="true" tabindex="-1"></a><span class="fu">### ImageNet Performance</span></span>
<span id="cb16-1356"><a href="#cb16-1356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1357"><a href="#cb16-1357" aria-hidden="true" tabindex="-1"></a>@fig-classification-image-net-performance shows the development of ImageNet performance. As of 2023, we are much closer to 100%, see <span class="co">[</span><span class="ot">Link</span><span class="co">](https://paperswithcode.com/sota/image-classification-on-imagenet?metric=Top%205%20Accuracy)</span>.</span>
<span id="cb16-1358"><a href="#cb16-1358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1359"><a href="#cb16-1359" aria-hidden="true" tabindex="-1"></a>::: {#fig-classification-image-net-performance}</span>
<span id="cb16-1360"><a href="#cb16-1360" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}image_net_performance.jpg)</span>{width=600}</span>
<span id="cb16-1361"><a href="#cb16-1361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1362"><a href="#cb16-1362" aria-hidden="true" tabindex="-1"></a>Graphic from @johnson_eecs_2019</span>
<span id="cb16-1363"><a href="#cb16-1363" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-1364"><a href="#cb16-1364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1365"><a href="#cb16-1365" aria-hidden="true" tabindex="-1"></a><span class="fu">### Which Architecture?</span></span>
<span id="cb16-1366"><a href="#cb16-1366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1367"><a href="#cb16-1367" aria-hidden="true" tabindex="-1"></a>Which architecture should be chosen for a specific problem? A common tip is: *Don't be a hero.*</span>
<span id="cb16-1368"><a href="#cb16-1368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1369"><a href="#cb16-1369" aria-hidden="true" tabindex="-1"></a>One should rely on off-the-shelf architectures and not implement their own without a good reason.</span>
<span id="cb16-1370"><a href="#cb16-1370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1371"><a href="#cb16-1371" aria-hidden="true" tabindex="-1"></a>Typically, ResNet-50 or ResNet-101 are good choices. However, there are also models that require significantly fewer parameters, such as Efficient Nets.</span>
<span id="cb16-1372"><a href="#cb16-1372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1373"><a href="#cb16-1373" aria-hidden="true" tabindex="-1"></a>Important considerations are also the requirements regarding accuracy, performance (FLOPs), and model size (memory).</span>
<span id="cb16-1374"><a href="#cb16-1374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1375"><a href="#cb16-1375" aria-hidden="true" tabindex="-1"></a><span class="fu">## Further Variations</span></span>
<span id="cb16-1376"><a href="#cb16-1376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1377"><a href="#cb16-1377" aria-hidden="true" tabindex="-1"></a>There are countless other architectures and especially specific layers or layer blocks that can be used in various architectures. The following are Squeeze/Excite Networks and the important class of normalization layers, which are often used and can significantly improve models or accelerate the optimization process.</span>
<span id="cb16-1378"><a href="#cb16-1378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1379"><a href="#cb16-1379" aria-hidden="true" tabindex="-1"></a><span class="fu">### Squeeze/Excite Networks</span></span>
<span id="cb16-1380"><a href="#cb16-1380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1381"><a href="#cb16-1381" aria-hidden="true" tabindex="-1"></a>Squeeze-and-Excite Networks (SE-Networks) were introduced in 2019 @hu_squeeze-and-excitation_2019. These include so-called Squeeze and Excite blocks (SE blocks), which allow the scaling of activation maps of a layer. This scaling is learnable through a few additional parameters. In practice, significant performance gains have been observed. @fig-classification-se-networks shows an illustration.</span>
<span id="cb16-1382"><a href="#cb16-1382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1383"><a href="#cb16-1383" aria-hidden="true" tabindex="-1"></a>::: {#fig-classification-se-networks}</span>
<span id="cb16-1384"><a href="#cb16-1384" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}se_networks.png)</span>{width=800}</span>
<span id="cb16-1385"><a href="#cb16-1385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1386"><a href="#cb16-1386" aria-hidden="true" tabindex="-1"></a>Source: @hu_squeeze-and-excitation_2019.</span>
<span id="cb16-1387"><a href="#cb16-1387" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-1388"><a href="#cb16-1388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1389"><a href="#cb16-1389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1390"><a href="#cb16-1390" aria-hidden="true" tabindex="-1"></a>These SE blocks can be easily applied to arbitrary activation maps. @fig-classification-se-networks shows an input feature map $\tensor{X} \in \mathbb{R}^{H' \times W' \times C'}$ that is transformed with $F_{tr}$ (e.g., with a convolutional layer). This results in the activation maps $\tensor{U} \in \mathbb{R}^{H \times W \times C}$.</span>
<span id="cb16-1391"><a href="#cb16-1391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1392"><a href="#cb16-1392" aria-hidden="true" tabindex="-1"></a>The transformation $F_{sq}(\cdot)$ applies the squeeze operation, a global average pooling, to generate a description of each channel by aggregating spatial information of the activation map $\tensor{U}$. This results in a vector $z \in \mathbb{R}^{1 \times 1 \times C}$.</span>
<span id="cb16-1393"><a href="#cb16-1393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1394"><a href="#cb16-1394" aria-hidden="true" tabindex="-1"></a>The excitation operation $F_{ex}(\cdot, W)$ uses a gating mechanism with parameters $W$, implemented with two fully-connected layers and activation functions. The result is $s \in \mathbb{R}^{1 \times 1 \times C}$, the channel weights. It is called gating because the weights range from $<span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>$ and thus control how much information of a channel flows through (gatekeeping).</span>
<span id="cb16-1395"><a href="#cb16-1395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1396"><a href="#cb16-1396" aria-hidden="true" tabindex="-1"></a>Finally, $F_{scale}(\cdot)$ scales the activation maps $\tensor{U}$ with the channel weights.</span>
<span id="cb16-1397"><a href="#cb16-1397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1398"><a href="#cb16-1398" aria-hidden="true" tabindex="-1"></a>The operation can be described as follows:</span>
<span id="cb16-1399"><a href="#cb16-1399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1400"><a href="#cb16-1400" aria-hidden="true" tabindex="-1"></a>\begin{align}</span>
<span id="cb16-1401"><a href="#cb16-1401" aria-hidden="true" tabindex="-1"></a>U &amp;= F_{tr}(X) <span class="sc">\\</span></span>
<span id="cb16-1402"><a href="#cb16-1402" aria-hidden="true" tabindex="-1"></a>z &amp;= F_{sq}(U) = \text{GlobalAvgPool}(U) <span class="sc">\\</span></span>
<span id="cb16-1403"><a href="#cb16-1403" aria-hidden="true" tabindex="-1"></a>s &amp;= F_{ex}(z, W) = \sigma(\mathbf{W}_2 g(\mathbf{W}_1 z)) <span class="sc">\\</span></span>
<span id="cb16-1404"><a href="#cb16-1404" aria-hidden="true" tabindex="-1"></a>\hat{X} &amp;= F_{scale}(U, s) = U \odot s</span>
<span id="cb16-1405"><a href="#cb16-1405" aria-hidden="true" tabindex="-1"></a>\end{align}</span>
<span id="cb16-1406"><a href="#cb16-1406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1407"><a href="#cb16-1407" aria-hidden="true" tabindex="-1"></a>where $g()$ represents the ReLU function, and $\sigma$ represents the sigmoid function.</span>
<span id="cb16-1408"><a href="#cb16-1408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1409"><a href="#cb16-1409" aria-hidden="true" tabindex="-1"></a><span class="fu">### Normalization Layers</span></span>
<span id="cb16-1410"><a href="#cb16-1410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1411"><a href="#cb16-1411" aria-hidden="true" tabindex="-1"></a>Normalization layers normalize activation maps to improve parameter learning. There are many variations, as shown in @fig-classification-normalization. In modern architectures, normalization layers are typically used. Popular is, for example, layer normalization (see @ba_layer_2016). The general form of normalization is given in equation @eq-classification-normalization. The parameters $\gamma$ and $\beta$ are learned, while the means $E<span class="co">[</span><span class="ot">x</span><span class="co">]</span>$ and variances $\sigma^2<span class="co">[</span><span class="ot">x</span><span class="co">]</span>$ are estimated from the activations. Why normalization layers work and which ones to prefer are still subjects of research and are often empirically tested in practice (considered as hyper-parameters). It is believed that the cost function becomes smoother overall, allowing the network to train faster and better @santurkar_how_2019.</span>
<span id="cb16-1412"><a href="#cb16-1412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1413"><a href="#cb16-1413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1414"><a href="#cb16-1414" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb16-1415"><a href="#cb16-1415" aria-hidden="true" tabindex="-1"></a>y = \frac{x - E<span class="co">[</span><span class="ot">x</span><span class="co">]</span>}{\sqrt{\sigma^2<span class="co">[</span><span class="ot">x</span><span class="co">]</span> + \epsilon}} * \gamma + \beta</span>
<span id="cb16-1416"><a href="#cb16-1416" aria-hidden="true" tabindex="-1"></a>$$ {#eq-classification-normalization}</span>
<span id="cb16-1417"><a href="#cb16-1417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1418"><a href="#cb16-1418" aria-hidden="true" tabindex="-1"></a>::: {#fig-classification-normalization}</span>
<span id="cb16-1419"><a href="#cb16-1419" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}normalization.png)</span>{width=300}</span>
<span id="cb16-1420"><a href="#cb16-1420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1421"><a href="#cb16-1421" aria-hidden="true" tabindex="-1"></a>Source: @qiao_micro-batch_2020.</span>
<span id="cb16-1422"><a href="#cb16-1422" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-1423"><a href="#cb16-1423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1424"><a href="#cb16-1424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1425"><a href="#cb16-1425" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb16-1426"><a href="#cb16-1426" aria-hidden="true" tabindex="-1"></a>Normalization layers are very common and used in almost any architecture. While batch normalization has traditionally been used in CNNs, newer models rely on layer normalization.</span>
<span id="cb16-1427"><a href="#cb16-1427" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb16-1428"><a href="#cb16-1428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1429"><a href="#cb16-1429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1430"><a href="#cb16-1430" aria-hidden="true" tabindex="-1"></a><span class="fu">## Pre-Processing</span></span>
<span id="cb16-1431"><a href="#cb16-1431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1432"><a href="#cb16-1432" aria-hidden="true" tabindex="-1"></a>Typically, images are pre-processed before being processed by the CNN. This is done to make the optimization process converge faster. Common steps include:</span>
<span id="cb16-1433"><a href="#cb16-1433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1434"><a href="#cb16-1434" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Resizing/cropping to a fixed size, e.g., 224x224x3. This is necessary for the images to be processed in a batch (they must have the same dimensions).</span>
<span id="cb16-1435"><a href="#cb16-1435" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Scaling: Images are scaled from the interval <span class="co">[</span><span class="ot">0, 255</span><span class="co">]</span> to the interval <span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>.</span>
<span id="cb16-1436"><a href="#cb16-1436" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Normalization: Normalization is often done along the color channels so that the mean value of the pixel values is 0.</span>
<span id="cb16-1437"><a href="#cb16-1437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1438"><a href="#cb16-1438" aria-hidden="true" tabindex="-1"></a>In <span class="in">`torchvision`</span>, the following pre-processing steps are used: <span class="co">[</span><span class="ot">Link</span><span class="co">](https://github.com/pytorch/vision/blob/main/torchvision/transforms/_presets.py)</span></span>
<span id="cb16-1439"><a href="#cb16-1439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1440"><a href="#cb16-1440" aria-hidden="true" tabindex="-1"></a><span class="fu">## Transfer Learning</span></span>
<span id="cb16-1441"><a href="#cb16-1441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1442"><a href="#cb16-1442" aria-hidden="true" tabindex="-1"></a>Transfer Learning refers to the process of adapting a trained model for Task A to Task B. Adapting pre-trained models often leads to better results and also reduces the number of training iterations. Deep learning frameworks often provide easy access to such models (also known as Model Zoo). For example, from PyTorch: <span class="co">[</span><span class="ot">Link</span><span class="co">](https://pytorch.org/vision/stable/models.html)</span>.</span>
<span id="cb16-1443"><a href="#cb16-1443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1446"><a href="#cb16-1446" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-1447"><a href="#cb16-1447" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb16-1448"><a href="#cb16-1448" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb16-1449"><a href="#cb16-1449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1450"><a href="#cb16-1450" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.models <span class="im">import</span> resnet50, ResNet50_Weights</span>
<span id="cb16-1451"><a href="#cb16-1451" aria-hidden="true" tabindex="-1"></a>resnet50(weights<span class="op">=</span>ResNet50_Weights.IMAGENET1K_V2)</span>
<span id="cb16-1452"><a href="#cb16-1452" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-1453"><a href="#cb16-1453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1454"><a href="#cb16-1454" aria-hidden="true" tabindex="-1"></a><span class="fu">## PyTorch Example</span></span>
<span id="cb16-1455"><a href="#cb16-1455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1456"><a href="#cb16-1456" aria-hidden="true" tabindex="-1"></a><span class="fu">### Architecture</span></span>
<span id="cb16-1457"><a href="#cb16-1457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1458"><a href="#cb16-1458" aria-hidden="true" tabindex="-1"></a>Example of a CNN architecture.</span>
<span id="cb16-1459"><a href="#cb16-1459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1462"><a href="#cb16-1462" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-1463"><a href="#cb16-1463" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb16-1464"><a href="#cb16-1464" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb16-1465"><a href="#cb16-1465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1466"><a href="#cb16-1466" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb16-1467"><a href="#cb16-1467" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb16-1468"><a href="#cb16-1468" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb16-1469"><a href="#cb16-1469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1470"><a href="#cb16-1470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1471"><a href="#cb16-1471" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Net(nn.Module):</span>
<span id="cb16-1472"><a href="#cb16-1472" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb16-1473"><a href="#cb16-1473" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb16-1474"><a href="#cb16-1474" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(<span class="dv">3</span>, <span class="dv">6</span>, <span class="dv">5</span>)</span>
<span id="cb16-1475"><a href="#cb16-1475" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pool <span class="op">=</span> nn.MaxPool2d(<span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb16-1476"><a href="#cb16-1476" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(<span class="dv">6</span>, <span class="dv">16</span>, <span class="dv">5</span>)</span>
<span id="cb16-1477"><a href="#cb16-1477" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">16</span> <span class="op">*</span> <span class="dv">5</span> <span class="op">*</span> <span class="dv">5</span>, <span class="dv">120</span>)</span>
<span id="cb16-1478"><a href="#cb16-1478" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">120</span>, <span class="dv">84</span>)</span>
<span id="cb16-1479"><a href="#cb16-1479" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc3 <span class="op">=</span> nn.Linear(<span class="dv">84</span>, <span class="dv">10</span>)</span>
<span id="cb16-1480"><a href="#cb16-1480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1481"><a href="#cb16-1481" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb16-1482"><a href="#cb16-1482" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(F.relu(<span class="va">self</span>.conv1(x)))</span>
<span id="cb16-1483"><a href="#cb16-1483" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(F.relu(<span class="va">self</span>.conv2(x)))</span>
<span id="cb16-1484"><a href="#cb16-1484" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.flatten(x, <span class="dv">1</span>)</span>
<span id="cb16-1485"><a href="#cb16-1485" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb16-1486"><a href="#cb16-1486" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc2(x))</span>
<span id="cb16-1487"><a href="#cb16-1487" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc3(x)</span>
<span id="cb16-1488"><a href="#cb16-1488" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb16-1489"><a href="#cb16-1489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1490"><a href="#cb16-1490" aria-hidden="true" tabindex="-1"></a>net <span class="op">=</span> Net()</span>
<span id="cb16-1491"><a href="#cb16-1491" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-1492"><a href="#cb16-1492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1493"><a href="#cb16-1493" aria-hidden="true" tabindex="-1"></a><span class="fu">### Loss Function</span></span>
<span id="cb16-1494"><a href="#cb16-1494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1497"><a href="#cb16-1497" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb16-1498"><a href="#cb16-1498" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb16-1499"><a href="#cb16-1499" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb16-1500"><a href="#cb16-1500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1501"><a href="#cb16-1501" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb16-1502"><a href="#cb16-1502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1503"><a href="#cb16-1503" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb16-1504"><a href="#cb16-1504" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(net.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>, momentum<span class="op">=</span><span class="fl">0.9</span>)</span>
<span id="cb16-1505"><a href="#cb16-1505" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb16-1506"><a href="#cb16-1506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1507"><a href="#cb16-1507" aria-hidden="true" tabindex="-1"></a><span class="fu">## References</span></span>
<span id="cb16-1508"><a href="#cb16-1508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-1509"><a href="#cb16-1509" aria-hidden="true" tabindex="-1"></a>::: {#refs}</span>
<span id="cb16-1510"><a href="#cb16-1510" aria-hidden="true" tabindex="-1"></a>:::</span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Â© Copyright 2025, Marco Willi</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2025/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This page is built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>