---
title: "2 - Convolutional Neural Networks"
params:
   images_path: "/assets/images/cnns/"
---

::: {.callout-tip title="Learning Objectives" collapse="true"}

After this lecture you should be able to:

- Explain why CNN inductive biases improve efficiency on images over MLPs.
- Compute output shapes and parameter counts for convolutional layers.
- Distinguish invariance vs equivariance and relate them to layer design.
- Describe receptive field growth through stacking and dilation.
- Compare alternative convolution variants (depthwise separable, 1x1, transposed) and their use cases.
:::

::: {.callout-tip title="TLDR Recap" collapse="true"}

**Why CNNs? The Problem with MLPs on Images**

- Images are high-dimensional (e.g., $224 \times 224 \times 3 = 150{,}528$ pixels)
- MLPs flatten images, losing spatial structure
- Fully connected layers require millions of parameters: $(150{,}528 \times 1{,}000 = 150M$ parameters for just one layer!
- MLPs don't exploit natural image properties (locality, translation invariance)

**The CNN Solution: Three Key Principles**

1. **Local Connectivity**: Each neuron only connects to a small spatial region (receptive field)
2. **Parameter Sharing**: Same filter/detector applied across all spatial locations
3. **Translation Equivariance**: If input shifts, output shifts correspondingly

**Core Operation: Convolution**

$$S(i, j) = (K * I)(i, j) = \sum_m\sum_n I(i + m, j + n)K(m, n)$$

- **Input** $I$: Activation maps $(C_{in} \times H \times W)$
- **Kernel** $K$: Learnable filter $(C_{out} \times C_{in} \times k \times k)$
- **Output** $S$: Feature maps $(C_{out} \times H' \times W')$

**Key Hyperparameters**

- **Kernel Size** $k$: Spatial extent (usually 3√ó3 or 5√ó5)
- **Stride** $s$: Step size (controls downsampling)
- **Padding** $p$: Zero-padding to preserve spatial size
- **Dilation** $d$: Spacing between kernel elements (expands receptive field)

**Output Size Formula**

$$o = \left\lfloor \frac{i + 2p - d\cdot(k-1) - 1}{s} + 1 \right\rfloor$$

**Parameter Count**

$$\text{Total Parameters} = (k \times k \times C_{in} \times C_{out}) + C_{out}$$

**Invariance vs Equivariance**

- **Invariant**: $f(g(x)) = f(x)$ ‚Äî output unchanged by transformation (needed for *classification*)
- **Equivariant**: $f(g(x)) = g(f(x))$ ‚Äî output transforms with input (needed for *detection*, *segmentation*)
- **CNNs are naturally translation equivariant** due to convolution operation
- Add pooling/global average pooling for translation *invariance*

**Receptive Field**

The input region that affects a single output activation:

$$r_0 = \sum_{l=1}^L \left( (k_l - 1) \prod_{i=1}^{l-1} s_i \right) + 1$$

- Grows with depth: stacking small kernels (3√ó3) more efficient than large kernels (5√ó5)
- Example: Two 3√ó3 convs = 18 params/channel vs one 5√ó5 = 25 params/channel

**Common Operations**

- **Pooling**: Spatial downsampling (max-pooling, average-pooling)
- **1√ó1 Convolution**: Change number of channels without spatial effect
- **Global Average Pooling**: $(C \times H \times W) \rightarrow (C \times 1 \times 1)$ for classification
- **Upsampling**: Increase spatial size (interpolation, transposed convolution)
- **Depthwise Separable**: Efficient convolutions with fewer parameters

**Architectural Best Practices**

- ‚úÖ Use multiple small kernels (3√ó3) instead of large ones
- ‚úÖ Add non-linearity (ReLU, etc.) between conv layers
- ‚úÖ Use batch normalization for stable training
- ‚úÖ Stack conv layers to build hierarchical features
- ‚úÖ Use global average pooling instead of large fully-connected layers

**Parameter Efficiency Example**

For CIFAR-10 images (32√ó32√ó3):

- **MLP** (2 hidden layers): ~200K parameters
- **CNN** (2 conv layers): ~5K parameters (40√ó fewer!)

**Key Insight**: CNNs encode our prior knowledge about images as *inductive biases* ‚Äî assumptions baked into the architecture that make learning more efficient and effective.

:::

::: {.content-hidden}
$$
{{< include /assets/_macros.tex >}}
$$
:::

## Introduction & Motivation

Images have unique properties that make them challenging for standard neural networks:

- **High-Dimensional**: An RGB image of size $224 \times 224$ has $224 \times 224 \times 3 = 150{,}528$ values
- **Locality**: Nearby pixels are statistically related (edges, textures, objects)
- **Translation Invariance**: A cat should be recognized regardless of its position in the image
- **Hierarchical Structure**: Images contain features at multiple scales (pixels ‚Üí edges ‚Üí textures ‚Üí objects)

:::{#fig-cnn-example-rgb-image width=600}

![ ]({{< meta params.images_path >}}cows2_small.png){width=600}

High-resolution image. [Image Source](https://unsplash.com/de/fotos/braune-schafe-auf-grunem-grasfeld-tagsuber-olonUwUrmQk)
:::

::: {.callout-note appearance="simple"}

**Question**

Looking at @fig-cnn-example-rgb-image, can you explain how nearby pixels are related?

**Think about it:** What visual patterns or structures do you notice?

<details>
<summary>Click for answer</summary>

**Nearby pixels in images are strongly correlated due to:**

1. **Object Coherence**: Objects occupy contiguous regions‚Äîpixels within an object (like a cow's body) share similar colors and textures
2. **Smooth Transitions**: Natural images have gradual changes in color and intensity, not abrupt random variations
3. **Texture Patterns**: Repeating structures (like grass blades or fur) create local statistical patterns
4. **Edges and Boundaries**: Sharp transitions occur at object boundaries, but even edges span multiple neighboring pixels

</details>

:::

We will see that CNNs are much better suited to model images than MLPs. Unlike MLPs, CNNs explicitly assume that inputs (e.g., pixels) that are close together need to be considered together and that information is locally correlated. This allows certain properties to be embedded in the architecture of CNNs (inductive biases) to define models much more efficiently (with fewer parameters).

### MLPs and Images


The input to an MLP is a vector $\vect{x}^{(i)}$, which is transformed through multiple hidden layers to the output layer. Each hidden layer has a certain number of neurons, each connected to all neurons in the previous layer (fully-connected layers). The fully connected layers can only process 1-D vectors. Therefore, images $\in \mathbb{R}^{H \times W \times C}$ must be flattened into 1-D vectors $\in \mathbb{R}^p$. Here, $p= H \times W \times C$. This causes MLPs to become very large (having many learnable parameters) when applied to high-dimensional inputs such as images. See @fig-cnn-mlp for an illustration.


:::{#fig-cnn-mlp}
![ ]({{< meta params.images_path >}}mlp_images_example.png){width=600}

Illustration of an MLP applied to an image.
:::



 In the CIFAR-10 dataset, which consists of very small images of $\in \mathbb{R}^{32 \times 32 \times 3}$ (height, width, colors), a single neuron in the first hidden layer has 32 * 32 * 3 = 3,072 weights to learn (see @fig-cnn-mlp-images).


:::{#fig-cnn-mlp-images}

![]({{< meta params.images_path >}}mlp_images.jpg){width=600}

Source: @johnson_eecs_2019
:::

<!-- :::{#fig-cnn-spatial-structure-mlp}

![]({{< meta params.images_path >}}mlp-spatial-structure.png){width=600}

Source: @li_cs231n_2023
::: -->

For larger images, which are more common in practice, the number of weights is correspondingly much larger. Furthermore, to identify all significant patterns many neurons are required, further increasing the number of parameters. This may lead to overfitting and difficulty in learning the weights.


The dimensionality of weight matrix $\m{W}$ scales linearly with input size:  $\m{W} \in \mathbb{R}^{d \times k}$ ($d$ is input dimensionality, $k$ the number of neurons). The columns of the weight matrix $\m{W}$ (in the first hidden layer) can be visualized to learn what kind pattern a specific neuron has learned (see @fig-cnn-mlp-example-weights).

:::{#fig-cnn-mlp-example-weights}

![ ]({{< meta params.images_path >}}mlp_images_example_weights.png){width=600}

Illustration of weight matrix $\m{W}$ and how the weights of a single neuron can be reshaped into the input image dimensionality.
:::


Often the patterns we want to learn are not stationary. Ideally we want to recognize them under translations of the inputs, e.g. $g(\vect{x})$ where $g()$ is a spatial translation. @fig-cnns-mlp-pattern-shift illustrates the problem, where the same pattern appears at different spatial positions.


::: {#fig-cnns-mlp-pattern-shift}

![]({{< meta params.images_path >}}mlp_images_example_pattern_shifts.png){width=600}

Illustration of different inputs with the same pattern but shifted spatially.

:::

::: {.callout-note appearance="simple"}

**Question**: Can an MLP learn to detect the same pattern at different spatial locations? If yes, how?

**Think about it:** If an MLP learns to detect a cat in the center of an image, what happens when the same cat appears in the corner?

<details>
<summary>Click for answer</summary>

MLPs must learn separate detectors for each possible position! This is extremely inefficient because:

1. **Parameter explosion**: Need different weights for each spatial location
2. **Data requirements**: Must see target pattern in every possible position during training
3. **Poor generalization**: Fails on positions not seen during training

This is why MLPs with the same number of parameters perform much worse on image tasks compared to CNNs.
</details>

:::

<!-- ::: {.callout-tip title="Interactive MLP Demo"}
Try this visualization to see how an MLP reacts to shifted inputs:

- [MLP vs CNN on 2D patterns](https://adamharley.com/nn_vis/mlp/2d.html)
- [CNN Filter Visualization](https://adamharley.com/nn_vis/cnn/2d.html)

Notice how CNNs reuse the same filter across all positions!
::: -->


::: {.callout-note appearance="simple"}

**Question**

Can an MLP successfully learn patterns in images $\vect{x}$ that are permuted with a permutation matrix $\m{P}$, i.e., $f(P(\vect{x}))$? See @fig-cnns-mlp-pattern-shift.

**Hint:** Think carefully - what if EVERY image is permuted with the SAME permutation matrix?

<details>
<summary>Click for the surprising answer</summary>

**Yes, MLPs CAN learn on consistently permuted images!** This is a crucial insight:

**Why MLPs can handle consistent permutation:**

1. **Consistent mapping**: If every image uses the same permutation $P$, pixel position relationships are preserved
2. **Learning is still possible**: The MLP can learn that "pixel at position 247 (originally position 12) connects to pixel at position 891 (originally position 13)"
3. **No information is lost**: Permutation is just a reordering, not destruction of information

**The real insight:**

- ‚úÖ **Same permutation for all images**: MLP can learn the task
- ‚ùå **Different permutations per image**: MLP cannot learn effectively
- ‚ùå **No permutation but objects move around**: MLP struggles with translation

**Why this matters**: This thought experiment reveals that MLPs' problem with images isn't about permutation per se, but about **spatial relationships and translation invariance**:

1. **Natural images**: Objects appear at different positions ‚Üí MLP must learn separate detectors for each position
2. **Consistently permuted images**: Spatial relationships are preserved (just reordered) ‚Üí MLP can learn
3. **CNNs**: Built-in translation equivariance makes them efficient for natural images regardless of object position

**Key takeaway**: The issue isn't that spatial structure is "destroyed" by permutation, but that CNNs are specifically designed to handle the **translation variability** in natural images efficiently.
</details>

:::


::: {#fig-cnns-mlp-pattern-shift}
![]({{< meta params.images_path >}}mlp_images_examples_permutation.png){width=600}

Inputs transformed with a permuation matrix.
:::



To summarize MLPs on images:

- High-Dimensional: The size of an MLP scales with the input dimensionality which might blow beyond memory and compute budgets.
- Locality: MLPs have no notion of locality and thus can't exploit this inherent bias in natural images.
- Stability under transformations: MLPs need to learn position-dependent patterns which is very inefficient.


::: {.callout-tip title="Try MLPs on images in your browser."}
Try to draw the same digit at different positions and observe what happens!

- [MLP Demo](https://adamharley.com/nn_vis/mlp/2d.html)

:::

### Invariance and Equivariance


For many tasks involving image data, certain variations in the input should either not change the model output (invariance) or should change the output in tandem with the input changes (equivariance).

A function $f(\vect{x})$ is **invariant** to transformation $g(\vect{x})$ if:
$$f(g(\vect{x})) = f(\vect{x})$$

A function $f(\vect{x})$ is **equivariant** to transformation $g(\vect{x})$ if:
$$f(g(\vect{x})) = g(f(\vect{x}))$$

::: {.callout-important title="Intuition"}

- **Invariant** = "I don't care about the transformation"
- **Equivariant** = "I transform my output the same way the input was transformed"
:::



**Example: Classification (Invariance Needed)**

::: {.callout-note appearance="simple"}

**Question**

What type of transformations $g()$ does an image classification model as in @fig-cnns-invariance-example need to be invariant to?

**Think about it:** Consider what changes when the cows move to different positions in the image.

<details>
<summary>Click for answer</summary>

**Translation invariance** is crucial for image classification. The model should output the same class label (e.g., "cow") regardless of where the object appears in the image.

**Key transformations for classification:**

1. **Translation**: Object position in the frame
2. **Scale**: Object size (small vs large)
3. **Rotation**: Object orientation (within reasonable limits)
4. **Lighting**: Brightness, contrast variations

For classification tasks specifically, we need **invariance** because:
- The class label doesn't depend on spatial location
- A cow is a cow whether it's in the center or corner of the image
- The model should "pool" spatial information to make position-independent predictions

This is typically achieved by combining CNNs' translation **equivariance** with operations like:
- **Pooling layers** (max-pooling, average-pooling)
- **Global average pooling** at the end of the network
- These convert equivariant features into invariant predictions

</details>

:::

::: {#fig-cnns-invariance-example}

![ ]({{< meta params.images_path >}}cows2_small_invariance_collage.png)

When objects in the input translate spatially, the output (in this case the classification of the images) does not change. The model  $f(\vect{x})$ is thus invariant to spatial translations.

:::


**Example: Object Detection (Equivariance Needed)**


@fig-cnns-equivariance-example shows an application of object detection. In this case, the model needs to be equivariant with respect to spatial translations (i.e., the bounding boxes that indicate an object‚Äôs location need to translate with the input). The model should remain invariant with respect to small changes in color, such as brightness adjustments.


::: {#fig-cnns-equivariance-example}

![]({{< meta params.images_path >}}cows2_small_detection_collage.png)

When objects in the input (first row) translate spatially, the detections (bounding boxes) change accordingly (bottom row). The model $f(\vect{x})$ that produces the bounding boxes is thus equivariant with respect to spatial translations.

:::

We can summarise the required properties for different tasks:

::: {.callout-note title="When do we want each?"}

| Task Type | Property Needed | Why? |
|-----------|-----------------|------|
| **Classification** | Translation Invariance | Cat is still a cat regardless of position |
| **Object Detection** | Translation Equivariance | If object moves right, bounding box moves right |
| **Semantic Segmentation** | Translation Equivariance | If input shifts, pixel-wise labels shift |

: {tbl-colwidths="[20,20,60]"}

Understanding this distinction is crucial for choosing the right architecture!
:::

<!-- TODO: move or remove this becauwse the convolution operation has not yet been discussed in detail -->
<!-- ::: {.callout-tip title="Key Insight"}
**CNNs naturally provide translation equivariance** due to their convolution operation. However, they need additional components (like pooling or global average pooling) to achieve translation invariance for classification tasks.

This is why CNN architecture design matters so much!
::: -->


## Convolutional Neural Networks

Now that we've seen the limitations of MLPs for images, let's explore how **Convolutional Neural Networks (CNNs)** address these challenges.

::: {.callout-tip title="The Big Idea"}
**Instead of connecting every neuron to every input**, CNNs use three key principles:

1. **Local Connectivity**: Each neuron only looks at a small spatial region
2. **Parameter Sharing**: The same filter/detector is used across all spatial locations
3. **Translation Equivariance**: If the input shifts, the output shifts correspondingly

These aren't just technical tricks‚Äîthey encode our **prior knowledge** about how images work!
:::


### The CNN Advantage

A single neuron in a CNN is only connected to a small portion (local connectivity) of the image (see @fig-cnn-cnn-spatial). As a result:

- ‚úÖ **Fewer Parameters**: Neurons have far fewer connections than in MLPs
- ‚úÖ **Spatial Structure Preserved**: No need to flatten 2D images into 1D vectors
- ‚úÖ **Translation Equivariance**: Same detector works everywhere in the image
- ‚úÖ **Hierarchical Feature Learning**: Stack layers to learn increasingly complex patterns

:::{#fig-cnn-cnn-spatial}

![]({{< meta params.images_path >}}cnn_spatial.jpg){width=200}

Source: @johnson_eecs_2019
:::

<!-- TODO: consider moving this part to a later section -->
<!-- ::: {.callout-note}
CNNs are used not only for image data but also for data with spatial dependencies/local structures. This includes not only images but also time series, videos, audio, and text. The key is that signals that are spatially close together should be interpreted together.
::: -->

### Convolutional Neural Networks

CNNs consist of a sequence of different layers. Each layer transforms activations from the previous layer into new activations through a differentiable operation. Below we look at the main layer types: convolutional layers, pooling layers, activation layers, and fully connected layers. Arranged in a specific sequence, this is referred to as the architecture of the model.

@fig-cnn-convnet shows an example architecture. The activation maps of the various layers are shown, representing the corresponding outputs of the layers.

:::{#fig-cnn-convnet}

![]({{< meta params.images_path >}}convnet.jpeg){width=600}

The activations of a ConvNet architecture are shown. The input image is on the left and the predictions on the right. Source: @li_cs231n_2022.
:::

Sometimes different layers are combined and referred to as a block. For example, the combination of a convolutional layer followed by an activation layer and a pooling layer is often used. This would be a CONV-ACT-POOL block. -->

### Convolution Operation

Convolutional layers are the main layers in CNNs responsible for extracting visual features. The weights of a convolutional layer consist of a set of learnable filters. Each filter is typically small along the spatial dimensions (height, width) relative to the input but extends over the entire input depth. A typical filter in the first layer, for example, has the dimension $7 \times 7 \times 3$ (7 pixels along height/width and 3 along the color channels). During the forward pass, the filters are convolved along height/width over the input. At each position, the dot product (when considering the input and filter as 1-D vectors) between the filter and input is calculated. This produces a 2-D activation map representing the filter's expression at each position in the input. Intuitively, the CNN learns filters corresponding to typical visual patterns, such as edges and colors. A set of $K$ filters produces activation maps with a depth of $K$.

::: {.callout-important title="Filters vs Kernels"}
Filter and kernel are sometimes used synonymously. Here, we differentiate by considering a filter as 3-dimensional (CxHxW) and a kernel as 2-dimensional (HxW). A filter consists of C kernels.
:::


::: {.callout-note title="Mathematical Definition of Convolution" collapse="true"}
Convolution in deep learning is typically implemented as cross-correlation.

\begin{equation}
S(i, j) = (K * I)(i, j) =\sum_m\sum_n I(i + m, j + n)K(m, n)
\end{equation}

$I$ is the input (for example an image), $K$ is the kernel (typically smaller than $I$) and $b$ is a bias term which is being added to the weighted sum.

If $I$ is an RGB image (in the first layer of a CNN for example), the kernel $K$ would have dimensionality $3 \times K \times K$ (assuming a square kernel). More generally we learn kernels of the dimensionality $C_{in} \times K \times K$.

Multiple kernels, let's say $C_o$ kernels, can be grouped together: $C_o \times C_{in} \times K \times K$.

We often refer to such tensors as filters or filter banks.
:::


Convolution on RGB images is nicely illusrated in @fig-cnns-intro-example. We can see that the filter is multiplied element-wise with the corresponding values in the image, the result is summed top produce one single value in the output.

::: {#fig-cnns-intro-example}

![Source:  @prince_understanding_2023]({{< meta params.images_path >}}convolution_prince_rgb.png)

:::

The following figures show further step-by-step illustration of applying convolutions on input activations that produce multiple output activation maps.


@fig-cnn-conv-one-number illustrates how a filter produces one value per position by taking the dot product at a specific location (and adding the bias term).

:::{#fig-cnn-conv-one-number}

![]({{< meta params.images_path >}}cnn_conv_one_number.jpg){width=600}

Source: @johnson_eecs_2019
:::

@fig-cnn-conv-activation-map illustrates how a filter produces one activation map, which is an output with one channel and (almost) the same spatial dimensionality as the input activations.

:::{#fig-cnn-conv-activation-map}

![]({{< meta params.images_path >}}conv_activation_map.jpg){width=600}

Source: @johnson_eecs_2019
:::

@fig-cnn-conv-activation-map2  illustrates how multiple filters produce multiple outputs. After all, we often need to extract / detect different features, each represented by a specific filter.

:::{#fig-cnn-conv-activation-map2}

![]({{< meta params.images_path >}}conv_activation_map2.jpg){width=600}

Source: @johnson_eecs_2019
:::

Finally, @fig-cnn-conv-activation-map3 illustrates how a convolutional layer is defined: it consists of a set of filters and their corresponding biases. Note that the output is essentially a $C_{out}$-dimensional image.

:::{#fig-cnn-conv-activation-map3}

![]({{< meta params.images_path >}}conv_activation_map3.jpg){width=600}

Source: @johnson_eecs_2019
:::

The data is processed in mini-batches, i.e., multiple images at once, as shown in @fig-cnn-conv-activation-map4.

:::{#fig-cnn-conv-activation-map4}

![]({{< meta params.images_path >}}conv_activation_map4.jpg){width=600}

Source: @johnson_eecs_2019
:::

### Padding, Stride, Kernel Size, Dilation

To define a convolution, various hyperparameters need to be set. Some of the most important ones are:

- Padding: How much padding is added to the input?
- Stride: What is the step size (stride) of the kernel?
- Kernel-Size: What is the kernel size?
- Dilation: What is the dilation rate?


**Stride** determines how the filters are convolved over the input activations, essentially the step size. If the stride is 1, the filter moves one pixel at a time to compute the next activation (see @fig-cnn-stride1). If the stride is greater, e.g., 2, it moves two pixels at a time, making the activation maps smaller in width and height (see @fig-cnn-stride2).


:::{#fig-cnn-stride1}
![]({{< meta params.images_path >}}stride1_example.jpg){width=600}

Convolution with stride (1, 1). Left the input activations with (red) the individual positions of the center pixel of the kernel / filter.
:::


:::{#fig-cnn-stride2}
![]({{< meta params.images_path >}}stride2_example.jpg){width=600}

Convolution with stride (2, 2).  Left the input activations with (red) the individual positions of the center pixel of the kernel / filter.
:::

::: {.callout-tip title="Stride > 1 often used!"}
Convolutions with stride provide a way to reduce the spatial dimensionality within a CNN and are commonly used. We will see later that other operations, such as pooling layers, also reduce spatial dimensionality. Using convolutions, however, is sometimes advantageous, since the downsampling is learned.
:::

**Padding** refers to adding (typically) zeros to the border of the input activations before performing the convolution. This can be useful to ensure, for example, that the spatial dimensions of the activation maps are identical to those of the input activations. This is essential for segmentation tasks. @fig-cnn-padding illustrates how padding helps maintain the spatial resolution when padding the input activations.


:::{#fig-cnn-padding}
![]({{< meta params.images_path >}}padding_example_result.jpg){width=600}

Left: Input (Yellow) with Zero-Padding (white border), Middle: Filter, Right: Output.
:::


**Kernel-size** determines the spatial extend from which a value in the output activation map migh aggregate values. It is the receptive field of an output value with respect to the input values. @fig-cnn-kernel-size3x3 and @fig-cnn-kernel-size5x5 show convolutions with different kernel sizes. Note that the amount of padding increases with kernel-size. Kernels usually have an odd number of pixels in height and width to make sure they have a center pixel. Also, the number of parameters increases with kernel-size $(C_{in} \times k \times k) + C_{out}$, which is often an important consideration.

:::{#fig-cnn-kernel-size3x3}

![ ]({{< meta params.images_path >}}kernel_size_example_3x3.jpg){width=600}

Convolution with zero-padding and $3 \times 3$ kernel that produces activations maps with unchanged spatial resolution.
:::

:::{#fig-cnn-kernel-size5x5}
![ ]({{< meta params.images_path >}}kernel_size_example_5x5.jpg){width=600}

Convolution with zero-padding and $5 \times 5$ kernel that produces activations maps with unchanged spatial resolution.
:::



::: {.callout-note appearance="simple"}

**Question**

What is the difference between using 3√ó3 vs 5√ó5 kernels? What is the same?

**Think first**: Consider computational cost, receptive field, and parameter count.

<details>
<summary>Click for detailed comparison</summary>

| Aspect | 3√ó3 Kernel | 5√ó5 Kernel |
|--------|------------|------------|
| **Parameters** | 9 weights (plus bias) per input channel  | 25 weights (plus bias) per input channel |
| **Computation** | Less (fewer multiplications) | More (more multiplications) |
| **Receptive Field** | Smaller (grows slowly) | Larger (grows faster) |
| **Detail Capture** | Fine-grained features | Broader spatial patterns |

**What's the same**:
- Both preserve translation equivariance
- Both use parameter sharing
- Both can learn spatial features

</details>

:::

::: {.callout-tip title="Best Practice"}
**Modern CNN Design**: Use multiple 3√ó3 convolutions instead of larger kernels to get:

- ‚úÖ Fewer parameters
- ‚úÖ More non-linearity (more activation functions)
- ‚úÖ Better gradient flow
- ‚úÖ Similar receptive field coverage
:::

**Dilation** specifies how much the kernel is spread out and thus directly influences the receptive field. @fig-cnn-dilation shows how it works.

:::{#fig-cnn-dilation}
![ ]({{< meta params.images_path >}}dilation_example.jpg){width=600}

Illustration of how a kernel with dilation=2 (middle) is applied to the input activations (left) to produce activations (right). Note the reduced spatial dimensionality of the output activations.
:::

::: {.callout-note appearance="simple"}

**Question**

Why would we use dilation $d > 1$?

**Consider**: What happens to the receptive field? What about computational cost?

<details>
<summary>Click for comprehensive answer</summary>

**Dilation increases receptive field without adding parameters or computation!**

**Benefits of dilation**:

1. **Larger receptive field**: See more context without more layers
2. **Same parameter count**: Filter size stays the same
3. **Multi-scale features**: Different dilation rates capture different scales

**Use cases**:

- **Semantic segmentation**: Need large receptive fields while preserving spatial resolution
- **Dense prediction tasks**: Want to see global context for local predictions
- **Efficient architectures**: Alternative to deeper networks

**Trade-offs**:

- ‚úÖ Larger receptive field with same parameters
- ‚ùå May miss fine-grained spatial relationships
- ‚ùå Can create "grid artifacts" if not used carefully

**Example**: A 3√ó3 kernel with dilation=2 has the receptive field of a 5√ó5 kernel but only 9 parameters instead of 25!

**Atrous/Dilated Convolutions** are especially popular in:

- **DeepLab** (semantic segmentation)
- **WaveNet** (audio generation)
- **Dilated ResNets** (dense prediction)

The key insight: **effectively expand the receptive field**!

</details>
:::

::: {.callout-note appearance="simple"}

@dumoulin_guide_2016 has created some animations for better understanding of convolutions and published them here: [https://github.com/vdumoulin/conv_arithmetic](https://github.com/vdumoulin/conv_arithmetic).


::: {.content-hidden unless-format="html"}

![Convolving a 3x3 kernel over a 5x5 input without padding and with stride 1x1. Source @dumoulin_guide_2016]({{< meta params.images_path >}}no_padding_no_strides.gif){width=200}

:::

:::



### Calculations: Spatial Dimensionality of Activation Maps

You can calculate the dimensionality of the activation maps with the following formulas:

- $i$: Side length of the input activations (assumption: square inputs)
- $k$: Kernel size (assumption: square kernel)
- $o$: Side length of the output activation maps
- $s$: Stride (assumption: same stride along spatial dimensions)
- $p$: Number of paddings on each side (assumption: same number of paddings along spatial dimensions)
- $d$: Dilation rate.


\begin{equation}
o = \left\lfloor \frac{i + 2p - d\cdot(k-1) - 1}{s} + 1 \right\rfloor
\end{equation}

<!-- TODO: hide Answer part of the followig questions  -->
::: {.callout-note title="üìù Quiz: Output Shape Calculation" collapse="true"}

**Problem**: You're designing a CNN layer with the following parameters:

- Input size: $i = 32$ (height and width)
- Kernel size: $k = 3$
- Padding: $p = 1$
- Stride: $s = 2$
- Dilation: $d = 1$

**Question**: What is the output size $o$?

<details>
<summary>Click for step-by-step solution</summary>

**Step-by-step solution**:

1. Apply the formula: $o = \lfloor \frac{i + 2p - d(k-1) - 1}{s} + 1 \rfloor$
2. Substitute: $o = \lfloor \frac{32 + 2(1) - 1(3-1) - 1}{2} + 1 \rfloor$
3. Simplify: $o = \lfloor \frac{32 + 2 - 2 - 1}{2} + 1 \rfloor$
4. Calculate: $o = \lfloor \frac{31}{2} + 1 \rfloor = \lfloor 15.5 + 1 \rfloor = 16$

**Answer**: $o = 16$

**Intuition**: With stride 2, we're roughly halving the spatial dimensions, but padding helps maintain some size.

</details>
:::

Note: If dilation is 1 $d(k-1) - 1$ equals 0.


### Calculations: Number of Weights in a Convolutional Layer

You can calculate the total number of weights in a convolutional layer with the following formula:

- $k$: Kernel size (assumption: square kernel, so kernel has dimensions $k \times k$)
- $C_{in}$: Number of input channels
- $C_{out}$: Number of output channels


\begin{equation}
\text{Total Weights} = (k \times k) \times C_{in} \times C_{out}
\end{equation}

Each filter has a size of $k \times k \times C_{in}$, and there are $C_{out}$ filters in total, resulting in $(k \times k \times C_{in}) \times C_{out}$ weights.


**Biases:**
If each output channel has a bias term, add an additional $C_{out}$ weights for the biases.


\begin{equation}
\text{Total Parameters} = (k \times k \times C_{in} \times C_{out}) + C_{out}
\end{equation}

::: {.callout-note title="üßÆ Practice Quiz: Parameter Counting" collapse="true"}

**Scenario**: Design a convolutional layer with:

- Input: $3 \times 32 \times 32$ (RGB image)
- 64 output channels
- Kernel size: $5 \times 5$
- Include bias terms

**Questions**:

1. How many weights in the filters?
2. How many bias parameters?
3. Total parameters?
4. If we used a fully connected layer instead, how many parameters would that require?

<details>
<summary>Click for solutions</summary>

**Solutions**:

1. **Filter weights**: $(5 \times 5) \times 3 \times 64 = 4{,}800$
2. **Bias parameters**: $64$ (one per output channel)
3. **Total parameters**: $4{,}800 + 64 = 4{,}864$
4. **Fully connected alternative**: $(32 \times 32 \times 3) \times 64 = 196{,}608$

**Insight**: CNNs use **40√ó fewer parameters** while being more effective for images!

</details>
:::

<!-- TODO: verify whether k=7 and justification really makes sense -->
<!-- ::: {.callout-note appearance="simple"}

**Question**

**Real-World Scenario**: You're building a medical imaging system for chest X-ray analysis.

**Scenario:**

- Input X-rays: $1 \times 512 \times 512$ (grayscale)
- First conv layer: 32 filters, $k=7 \times 7$, $s=2$, $p=3$, $d=1$

**Your Tasks:**

1. What is the output activation map size $(C_{out}, H_{out}, W_{out})$?
2. How many parameters does this layer have?
3. Why might we choose $k=7$ for medical images vs $k=3$ for natural images?
4. What's the receptive field of each output pixel?

<details>
<summary>Click for detailed solutions</summary>

**Solutions:**

1. **Output size**:
   - $C_{out} = 32$ (number of filters)
   - $H_{out} = W_{out} = \lfloor \frac{512 + 2(3) - 1(7-1) - 1}{2} \rfloor + 1 = 256$
   - **Answer**: $(32, 256, 256)$

2. **Parameters**:
   - Filter weights: $(7 \times 7) \times 1 \times 32 = 1{,}568$
   - Biases: $32$
   - **Total**: $1{,}600$ parameters

3. **Why $k=7$ for medical images?**
   - Medical features often span larger spatial areas (organ boundaries, lesions)
   - Need to capture broader anatomical context early
   - Higher resolution medical images benefit from larger initial receptive fields
   - Natural images have more fine-grained local features

4. **Receptive field**: $7 \times 7$ (same as kernel size for first layer)

**Key Insight**: Domain knowledge should inform architectural choices!
</details>

::: -->

<!-- ::: {.callout-tip title="Domain-Specific Design"}
Different domains need different CNN architectures:

| Domain | Typical Kernel Sizes | Why? |
|--------|---------------------|------|
| **Natural Images** | 3√ó3, 1√ó1 | Fine details, efficiency |
| **Medical Imaging** | 5√ó5, 7√ó7 | Larger anatomical features |
| **Satellite Imagery** | 7√ó7, 9√ó9 | Geographic features at scale |
| **Text Recognition** | Rectangular (1√ó3, 3√ó1) | Character and word shapes |

: {tbl-colwidths="[30,20,50]"}

Understanding your data is crucial for good architecture design!
::: -->

<!-- TODO: figure out how to handel the Properties section here and some of this information that was already provided previously    -->



## Properties

### Local (Sparse) Connectivity & Parameter Sharing

Fully connected layers are impractical when working with high-dimensional inputs like images. If all neurons in a layer were connected to all previous neurons, the number of parameters to be estimated would increase massively, which is inefficient and leads to overfitting. Each neuron in a CNN is therefore only connected to a local region of the input volume (see @fig-cnn-conv-one-number). The spatial extent of this region is a hyperparameter and is called the receptive field of a neuron (kernel size) on the input volume. The connections along the depth (C) extend over the entire depth of the input volume $C_{in}$. The connections are therefore local along the spatial dimensions (width and height) but complete along the depth.

Parameter sharing in convolutional layers is used to reduce the number of parameters. Since the filters are convolved over the inputs, the individual weights of the filters are identical over the spatial extent of the input volume. One of the main assumptions behind CNNs is the following: If it is useful to learn a specific (visual) feature at a certain position, then it is probably useful at other positions as well. In other words: If I learn filters that detect edges, corners, or cats, then it is a reasonable assumption that I want to do this throughout the image.

::: {.callout-note}
Sometimes parameter sharing does not make sense. This can be the case, for example, if we have centered structures in the images. Then you might want to learn position-dependent features. An example is images of faces that have been centered, where you might want to learn filters that detect the mouth only in the lower middle area (locally connected layers).
:::

@lst-cnns-mlp shows the number of parameters of an MLP, while @lst-cnns-cnn shows the number of parameters of a CNN (each with two hidden layers) on the CIFAR10 dataset.

```{python}
#| eval: true
#| echo: true
#| lst-label: lst-cnns-mlp
#| lst-cap: PyTorch example of an MLP.
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchinfo

class MLP(nn.Module):

    def __init__(self):
        super().__init__()
        self.flatten = nn.Flatten()
        self.hidden_layer1 = nn.Linear(3 * 32 * 32, 64)
        self.hidden_layer2 = nn.Linear(64, 32)
        self.output_layer = nn.Linear(32, 10)

    def forward(self, x):
        x = self.flatten(x)
        x = torch.relu(self.hidden_layer1(x))
        x = torch.relu(self.hidden_layer2(x))
        x = self.output_layer(x)
        return x

net = MLP()
print(torchinfo.summary(net, input_size=(1, 3, 32, 32)))
```

```{python}
#| eval: true
#| echo: true
#| lst-label: lst-cnns-cnn
#| lst-cap: PyTorch example of a CNN.
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchinfo

class CNN(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 16, 7, stride=2, padding=3)
        self.conv2 = nn.Conv2d(16, 16, 3, stride=2, padding=1)
        self.flatten = nn.Flatten()
        self.output_layer = nn.Linear(16 * 8 * 8 , 10)

    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = torch.relu(self.conv2(x))
        x = self.flatten(x)
        x = self.output_layer(x)
        return x

cnn = CNN()
print(torchinfo.summary(cnn, input_size=(1, 3, 32, 32)))
```

We can see that the number of parameters of the CNN is much smaller. Feel free to compare performances.

::: {.callout-note appearance="simple"}

**Question**

Why in @lst-cnns-cnn is `pading=3` in `self.conv1 = nn.Conv2d(3, 16, 7, stride=2, padding=3)`? Would `padding=0`also be possible?

<details>
<summary>Click for answer</summary>

**Answer:**

The `padding=3` is chosen to maintain the spatial dimensions when using `stride=2`.

Using the output size formula:
$$o = \left\lfloor \frac{i + 2p - d\cdot(k-1) - 1}{s} + 1 \right\rfloor$$

With the given parameters:
- Input: $i = 32$
- Kernel: $k = 7$
- Stride: $s = 2$
- Dilation: $d = 1$
- Padding: $p = 3$

$$o = \left\lfloor \frac{32 + 2(3) - 1(7-1) - 1}{2} + 1 \right\rfloor = \left\lfloor \frac{32 + 6 - 6 - 1}{2} + 1 \right\rfloor = \left\lfloor \frac{31}{2} + 1 \right\rfloor = 16$$

**Would `padding=0` work?**

Yes, but it would produce a different output size:
$$o = \left\lfloor \frac{32 + 0 - 6 - 1}{2} + 1 \right\rfloor = \left\lfloor \frac{25}{2} + 1 \right\rfloor = 13$$

**Why `padding=3` is preferred here:**
- Maintains a "nice" output dimension ($16 \times 16$) that's easy to work with in subsequent layers
- Preserves more spatial information from the edges of the input
- Common practice: use padding to keep output size as $\lfloor \frac{i}{s} \rfloor$ when possible

</details>


:::


::: {.callout-note}
A convolution can also be expressed as a single matrix multiplication and is thus a special case of a linear transformation (such as in an MLP). @fig-cnn-conv-as-matrix-illustration shows how a linear layer in an MLP (top left) has different connections between each input $x_1, \cdots, x_6$ and neurons in a hidden layer $h_1, \cdots, h_6$ and the corresponding weight matrix (bottom left). On the right, the same is shown for a 1-D convolution with kernel size 3, however, note that the weight matrix is sparse (white values) and that the weights are shared (repeated patterns across the diagonal).

:::{#fig-cnn-conv-as-matrix-illustration}

![]({{< meta params.images_path >}}1d_example_weight_matrix_mlp_vs_cnn.png){width=600}

Illustration of linear layer in an MLP (left column), including graph (top) and weight matrix (bottom). Convolutional layer (right) represented as a linear transformation. Source @prince_understanding_2023.
:::

:::



::: {.callout-note appearance="simple"}

**Question**

How should the linear transformation be defined to obtain the desired result in @fig-cnn-linear-transf-calc? How many parameters are needed? How could this be done with a convolution?


:::{#fig-cnn-linear-transf-calc}

![]({{< meta params.images_path >}}linear_transf.png){width=800}

Input in 2-D (top left), the flattened version of it (bottom left), expected output (right), and unknown transformation (middle).
:::


:::

### Translation Invariance / Equivariance

Translation invariant is a function that produces the same value under translations $g()$ of the input $x$:

\begin{equation}
f(g(x))=f(x)
\end{equation}

Translation equivariant is a function that produces the same value under translations $g()$ of the input $x$, provided that it is also shifted by $g()$:

\begin{equation}
f(g(x))=g(f(x))
\end{equation}

Convolutions are translation equivariant, as illustrated well in the following example:

{{< video https://www.youtube.com/embed/qoWAFBYOtoU start="50" >}}


<!-- <iframe width="560" height="315" src="https://www.youtube.com/embed/qoWAFBYOtoU?start=50" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
 -->


### Stacking Convolutions

Multiple convolutions can be executed in sequence (stacking). Each convolution is performed on the activation maps of another, preceeding convolution. @fig-cnn-conv-stacking illustrates the process.

:::{#fig-cnn-conv-stacking}

![]({{< meta params.images_path >}}conv_stacking.jpg){width=600}

Source: @johnson_eecs_2019
:::

A convolution is therefore not only performed directly on the input (e.g., images) but is generally defined on inputs of dimensionality $C \times H \times W$.

**The principle of stacking convolutional layers and therefore convolutions are not just applied on raw pixel values from an image is important to understand!** While the filters in the first convolutional layer, which directly operate on the input image, typically learn basic features such as lines in different orientations or colors, filters in subsequent layers learn more complex features by combining the features from preceeding layers. This is similar to our visual cortex. @Zeiler2014 used a special technique to visualize patterns that filters in deeper layers are reacting to (see @fig-cnn-conv-zeiler).


However, non-linear activation functions must be used between the convolutions. Otherwise, the stacked convolution can be expressed with a simple convolution (similar to an MLP, which can be expressed with a linear transformation without activation functions).


:::{#fig-cnn-conv-zeiler}

![]({{< meta params.images_path >}}zeiler_learned_filters.png){width=600}

Shown are patterns the learned flters are reacting to for several layers (in gray) and the corresponding image patches that achieve maximum activation values for these patterns. Source: @Zeiler2014

:::

### Receptive Field  {#sec-cnn-receptive-field}

The receptive field defines which inputs influence the activations of a neuron. The receptive field of a neuron with respect to the input activations (the image pixels) is quite important. If the receptive field is smaller than the objects of interest, the network is unable to "see" the whole object. This is particularly problematic for object detection. See @fig-cnn-receptive-field-1 and @fig-cnn-receptive-field-4 to illustrate the concept.


:::{#fig-cnn-receptive-field-1}
![]({{< meta params.images_path >}}receptive_field1.jpg){width=600}

Receptive field with a 5x5 kernel.
:::


:::{#fig-cnn-receptive-field-2}
![]({{< meta params.images_path >}}receptive_field2.jpg)
Receptive field with a 5x5 kernel and two layers.
:::


:::{#fig-cnn-receptive-field-3}
![ ]({{< meta params.images_path >}}receptive_field3.jpg)

Receptive field with a 5x5 kernel and two layers.
:::


:::{#fig-cnn-receptive-field-4}
![ ]({{< meta params.images_path >}}receptive_field4.jpg)

Receptive field with a 5x5 kernel and two layers. The output activation at the center (right) aggregates values over the union of all blue boxes in the left figure and has thus a receptive field of $9 \times 9$ with respect to the input activations.
:::


The receptive field $r_0$ of a convolutional neural network can be calculated as follows:

1. Define the recurrence relation:

\begin{equation}
r_{l-1} = s_l \cdot r_l + (k_l - s_l)
\end{equation}

- $r_l$: Receptive field at layer $l$.
- $s_l$: Stride of layer $l$.
- $k_l$: Kernel size of layer $l$.

2. Solving this recurrence relation, we get the closed-form solution for the receptive field at the input layer $r_0$:

\begin{equation}
r_0 = \sum_{l=1}^L \left( (k_l - 1) \prod_{i=1}^{l-1} s_i \right) + 1
\end{equation}

- $L$: Total number of layers.
- $k_l$: Kernel size at layer $l$.
- $s_i$: Stride at layer $i$, from layer 1 up to $l-1$.


@araujo_computing_2019 have written a nice article about receptive fields and how to calculate them (also refer to this article for more details on the notation and justification).


::: {.callout-note title="üéØ Challenge Quiz: Receptive Field" collapse="true"}

**Network Architecture**:

- Layer 1: Conv2d(kernel=3, stride=1)
- Layer 2: Conv2d(kernel=3, stride=2)
- Layer 3: Conv2d(kernel=5, stride=1)

**Question**: What is the receptive field of the final layer with respect to the input?

<details>
<summary>Click for solution</summary>

**Solution using the formula**:
$$r_0 = \sum_{l=1}^3 \left( (k_l - 1) \prod_{i=1}^{l-1} s_i \right) + 1$$

- $r_0 = (3-1) \cdot 1 + (3-1) \cdot 1 + (5-1) \cdot (1 \cdot 2) + 1$
- $r_0 = 2 + 2 + 8 + 1 = 13$

**Answer**: $13 \times 13$ receptive field

**Practical insight**: Each output pixel "sees" a $13 \times 13$ region of the input image.

</details>
:::


### Comparing MLPs and CNNs

Now that we understand how CNNs work, let's compare them systematically to MLPs:

:::{.callout-note appearance="simple"}

**Question**

How do MLPs and CNNs differ in their approach to processing images?

**Think first**: Consider parameter count, assumptions about data, and what each architecture "knows" about images.

<details>
<summary>Click for detailed comparison</summary>

| Aspect | MLP | CNN |
|--------|-----|-----|
| **Input** | Flattened vector | Preserve spatial structure (H√óW√óC) |
| **Connections** | Fully connected | Locally connected |
| **Parameter Sharing** | None (each weight unique) | Shared across spatial locations |
| **Translation Equivariance** | ‚ùå No | ‚úÖ Yes (by design) |
| **Assumptions** | None about spatial structure | Locality + stationarity of statistics |
| **Parameter Count** | Very high | Much lower |
| **Learns Position** | Must learn same feature at each position | Learns once, applies everywhere |
| **Best For** | Tabular data, general patterns | Images, spatial data |

: {tbl-colwidths="[20,40,40]"}

**Key Takeaway**: MLPs *can* theoretically learn to process images (universal approximation theorem), but CNNs bake in the right inductive biases, making learning vastly more efficient.

</details>

:::

::: {.callout-tip title="Deep Learning Insight: Inductive Bias"}
**Inductive bias** refers to the assumptions a learning algorithm makes to generalize from training data to unseen data.

**MLPs**: Minimal inductive bias (very general, but requires lots of data)
**CNNs**: Strong inductive bias (assumes locality, translation equivariance ‚Üí more efficient learning)

The right inductive bias for your data is key to success! This is why:

- CNNs excel at images (spatial locality matters)
- Transformers excel at sequences (long-range dependencies matter)
- Graph Neural Networks excel at networks (graph structure matters)

Choosing architecture = choosing which assumptions to bake in!
:::


## More Layers & Operations

CNNs often do not consist solely of convolutional layers but have other layers with specific purposes.


### Downsampling

Spatial downsampling is often a desired property of convolutional layers. We can achieve that by choosing a stride $\gt 1$. However, there are other options, such as pooling layers. @fig-cnn-downsampling-pooling illustrates some pooling variants.

:::{#fig-cnn-downsampling-pooling}
![]({{< meta params.images_path >}}downsampling_with_pooling.jpg)

Shown are a) Sub-Sampling, b) Max Pooling, c) Average Pooling. Source:  @prince_understanding_2023
:::


### Upsampling {#sec-cnn-upsampling}

Sometimes we need to increase the spatial dimensionality of our activation maps.


:::{#fig-cnn-upsampling}
![]({{< meta params.images_path >}}upsampling_options.jpg)

Shown are  a) Duplicate, b) Max-Unpooling, c) Bilinear Interpolation . Source:  @prince_understanding_2023
:::


Upsampling can also be learned with Transposed Convolutions. More on that topic in _Image Segmentation_.

 <!-- [image segmentation](segmentation.qmd#sec-segmentation-upsampling). -->

<!-- :::{#fig-cnn-transposed-convs}
![ ]({{< meta params.images_path >}}transposed_convolution.jpg)
::: -->


### Changing the number of channels

$1 \times 1$ convolutions have a kernel size of 1x1 and thus no spatial extent. These layers are often used in CNNs to change the number ($C$) of activation maps with few parameters. For example, activation maps of dimensionality ($C \times H \times W$) can be changed to a volume of ($C2 \times H \times W$) using only $C2 * (C + 1)$ parameters. This can be useful, for example, to save parameters before more complex layers or at the end of the CNN to adjust the size of the activation maps to the number of classes to be modeled (for classification problems) or to reduce to 3 color channels ($C2=3$) for image generation models. @fig-cnn-1x1-conv shows an example that uses only 6 weights (excluding bias) to reduce the number of channels to one.

:::{#fig-cnn-1x1-conv}

![]({{< meta params.images_path >}}1x1_conv_example.jpg)

$1 \times 1$ convolution. Source: @prince_understanding_2023
:::

### Global Average Pooling

Global Average Pooling is often an important component. It computes the average of the activations along the depth dimension, reducing activation maps from (C x H x W) to (C x 1 x 1). This is useful for directly modeling logits in a classification problem with C classes, enabling architectures that completely eliminate fully-connected layers.

::: {#fig-cnn-global-avg-pooling}

![]({{< meta params.images_path >}}global_average_pooling.jpg)

Global Average pooling, input (left) and output (right).
:::

::: {.callout-note}
This can be achieved using PyTorchs [https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html](https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html)
:::

### Other Layers

There are many other types of layers, also there are more flavours of convolutional layers.


One layer often used to reduce the number of parameters are **depthwise separable convolutions**. Instead of extending filters over the entire depth of the input activations, a separate filter (kernel) is used for each input channel, with the dimensionality ($1 \times k \times k$). @fig-cnn-depthwise shows an example. Subsequently, 1x1 convolutions are used to combine information across the input channels. See @fig-cnn-depthwise-separabel for a comparison of 'normal' convolutions and depthwise separable convolutions. Since 1x1 convolutions require fewer parameters, activation maps can be generated with fewer parameters.

:::{#fig-cnn-depthwise}

![]({{< meta params.images_path >}}depthwise.png){width=600}

Source: [https://paperswithcode.com/method/depthwise-convolution](https://paperswithcode.com/method/depthwise-convolution)
:::

:::{#fig-cnn-depthwise-separabel}

![]({{< meta params.images_path >}}depthwise_separabel.png){width=600}

Source: @yu_multi-scale_2016
:::

Additionally, important layers include the diverse familiy of **normalization layers** which can improve convergence of the optimization procedure dramatically.


## PyTorch Examples

```{python}
#| eval: true
#| echo: true
import numpy as np
import torch
from torch.nn import functional as F
import torchshow as ts
from PIL import Image
from matplotlib import pyplot as plt
```

```{python}
#| eval: true
#| echo: true
#img = Image.open({{< meta params.images_path >}}'cat.jpg')
image_path = "../../assets/images/cnns/cat.jpg"
img = Image.open(image_path)
img
```

```{python}
#| eval: true
#| echo: true
filter_ = torch.tensor(
    [   [[1, 0, -1], [1, 0, -1], [1, 0, -1]], # R
        [[1, 0, -1], [1, 0, -1], [1, 0, -1]], # G
        [[1, 0, -1], [1, 0, -1], [1, 0, -1]], # B
    ]).unsqueeze(0).float()
ts.show(filter_, show_axis=False)
```

```{python}
#| eval: true
#| echo: true
input = torch.tensor(np.array(img)).unsqueeze(0).permute(0, 3, 1, 2).float() # (N, C, H, W)
input /= 255.0
input -= 1.0
result = F.conv2d(input, filter_, stride=1, padding=0, dilation=1, groups=1)
```

```{python}
#| eval: true
#| echo: true
ts.show(result)
```

2D-Convolution:

```{python}
#| eval: true
#| echo: true
result = F.conv2d(input, filter_, stride=6, padding=0, dilation=1, groups=1)
ts.show(result)
```

Transposed convolution:

```{python}
#| eval: true
#| echo: true
result = F.conv2d(input, filter_, stride=6, padding=0, dilation=1, groups=1)
result = F.conv_transpose2d(result, weight=torch.ones_like(filter_))
ts.show(result)
```

Max-Pooling:

```{python}
#| eval: true
#| echo: true
result = F.max_pool2d(input, kernel_size=8, stride=8)
ts.show(result)
```

## References

::: {#refs}
:::
