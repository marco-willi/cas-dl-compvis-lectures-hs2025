<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Marco Willi">

<title>1 - Introduction – CAS Deep Learning - Computer Vision</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../pages/lectures/cnns.html" rel="next">
<link href="../../pages/background/frameworks.html" rel="prev">
<link href="../..//assets/cv_logo_small.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-065a5179aebd64318d7ea99d77b64a9e.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-159896a7fbbc1299bfd73fe3af9cd852.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="../../site_libs/quarto-contrib/videojs/video.min.js"></script>
<link href="../../site_libs/quarto-contrib/videojs/video-js.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-97BR3FTSN1"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-97BR3FTSN1', { 'anonymize_ip': true});
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../pages/lectures/intro.html">Lectures</a></li><li class="breadcrumb-item"><a href="../../pages/lectures/intro.html">1 - Introduction</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-center sidebar-header">
      <a href="../../index.html" class="sidebar-logo-link">
      <img src="../../assets/cv_logo_small.png" alt="" class="sidebar-logo light-content py-0 d-lg-inline d-none">
      <img src="../../assets/cv_logo_small.png" alt="" class="sidebar-logo dark-content py-0 d-lg-inline d-none">
      </a>
      <div class="sidebar-tools-main">
    <a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2025" title="GitHub organization" class="quarto-navigation-tool px-1" aria-label="GitHub organization"><i class="bi bi-github"></i></a>
    <a href="" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-code-square"></i></a>
</div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Background &amp; Preparation</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/background/machine_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1 - Machine Learning Basics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/background/neural_networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2 - Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/background/frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3 - Software &amp; Hardware for Deep Learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Lectures</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/intro.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">1 - Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/cnns.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2 - Convolutional Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3 - Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/representation_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4 - Representation Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/practical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5 - Practical</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/recent_advances.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6 - Recent Advances</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Slides</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/cnns.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Convolutional Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/representation_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Representation Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/practical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Practical Considerations</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false">
 <span class="menu-text">Resources</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/literature.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Books</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/links.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Helpful Links &amp; Resources</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Mathematical Notation</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#motivation" id="toc-motivation" class="nav-link active" data-scroll-target="#motivation"><span class="header-section-number">1</span> Motivation</a>
  <ul class="collapse">
  <li><a href="#applications" id="toc-applications" class="nav-link" data-scroll-target="#applications"><span class="header-section-number">1.1</span> Applications</a></li>
  </ul></li>
  <li><a href="#computer-vision-tasks" id="toc-computer-vision-tasks" class="nav-link" data-scroll-target="#computer-vision-tasks"><span class="header-section-number">2</span> Computer Vision Tasks</a>
  <ul class="collapse">
  <li><a href="#image-classification" id="toc-image-classification" class="nav-link" data-scroll-target="#image-classification"><span class="header-section-number">2.1</span> Image Classification</a></li>
  <li><a href="#object-detection" id="toc-object-detection" class="nav-link" data-scroll-target="#object-detection"><span class="header-section-number">2.2</span> Object Detection</a></li>
  <li><a href="#segmentation" id="toc-segmentation" class="nav-link" data-scroll-target="#segmentation"><span class="header-section-number">2.3</span> Segmentation</a></li>
  <li><a href="#keypoint-detection" id="toc-keypoint-detection" class="nav-link" data-scroll-target="#keypoint-detection"><span class="header-section-number">2.4</span> Keypoint Detection</a></li>
  <li><a href="#image-generation" id="toc-image-generation" class="nav-link" data-scroll-target="#image-generation"><span class="header-section-number">2.5</span> Image Generation</a></li>
  <li><a href="#further-tasks" id="toc-further-tasks" class="nav-link" data-scroll-target="#further-tasks"><span class="header-section-number">2.6</span> Further tasks</a></li>
  <li><a href="#what-about-videos" id="toc-what-about-videos" class="nav-link" data-scroll-target="#what-about-videos"><span class="header-section-number">2.7</span> What about videos?</a></li>
  </ul></li>
  <li><a href="#challenges" id="toc-challenges" class="nav-link" data-scroll-target="#challenges"><span class="header-section-number">3</span> Challenges</a>
  <ul class="collapse">
  <li><a href="#sec-intro-challenges-semantic-gap" id="toc-sec-intro-challenges-semantic-gap" class="nav-link" data-scroll-target="#sec-intro-challenges-semantic-gap"><span class="header-section-number">3.1</span> Semantic Gap</a></li>
  <li><a href="#viewpoint" id="toc-viewpoint" class="nav-link" data-scroll-target="#viewpoint"><span class="header-section-number">3.2</span> Viewpoint</a></li>
  <li><a href="#deformation" id="toc-deformation" class="nav-link" data-scroll-target="#deformation"><span class="header-section-number">3.3</span> Deformation</a></li>
  <li><a href="#illumination" id="toc-illumination" class="nav-link" data-scroll-target="#illumination"><span class="header-section-number">3.4</span> Illumination</a></li>
  <li><a href="#background-clutter" id="toc-background-clutter" class="nav-link" data-scroll-target="#background-clutter"><span class="header-section-number">3.5</span> Background Clutter</a></li>
  <li><a href="#occlusion" id="toc-occlusion" class="nav-link" data-scroll-target="#occlusion"><span class="header-section-number">3.6</span> Occlusion</a></li>
  <li><a href="#intra-class-variation" id="toc-intra-class-variation" class="nav-link" data-scroll-target="#intra-class-variation"><span class="header-section-number">3.7</span> Intra-Class Variation</a></li>
  <li><a href="#context-dependence" id="toc-context-dependence" class="nav-link" data-scroll-target="#context-dependence"><span class="header-section-number">3.8</span> Context Dependence</a></li>
  </ul></li>
  <li><a href="#machine-learning" id="toc-machine-learning" class="nav-link" data-scroll-target="#machine-learning"><span class="header-section-number">4</span> Machine Learning</a>
  <ul class="collapse">
  <li><a href="#machine-learning-basics" id="toc-machine-learning-basics" class="nav-link" data-scroll-target="#machine-learning-basics"><span class="header-section-number">4.1</span> Machine Learning Basics</a></li>
  <li><a href="#ml-with-classical-computer-vision" id="toc-ml-with-classical-computer-vision" class="nav-link" data-scroll-target="#ml-with-classical-computer-vision"><span class="header-section-number">4.2</span> ML with Classical Computer Vision</a></li>
  </ul></li>
  <li><a href="#deep-learning" id="toc-deep-learning" class="nav-link" data-scroll-target="#deep-learning"><span class="header-section-number">5</span> Deep Learning</a>
  <ul class="collapse">
  <li><a href="#deep-learning-history" id="toc-deep-learning-history" class="nav-link" data-scroll-target="#deep-learning-history"><span class="header-section-number">5.1</span> Deep Learning History</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">6</span> References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2025/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../pages/lectures/intro.html">Lectures</a></li><li class="breadcrumb-item"><a href="../../pages/lectures/intro.html">1 - Introduction</a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">1 - Introduction</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Marco Willi </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Institute for Data Science I4DS, FHNW
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-tip callout-titled" title="Learning Objectives">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Learning Objectives
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>After this lecture you should be able to:</p>
<ul>
<li>Distinguish core computer vision tasks.</li>
<li>Describe key real-world application domains and practical challenges.</li>
<li>Explain why deep learning reshaped computer vision.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="TLDR Recap">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>TLDR Recap
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Computer Vision Tasks:</strong></p>
<ul>
<li><strong>Classification</strong>: Assign images to predefined classes</li>
<li><strong>Detection</strong>: Locate and recognize multiple objects with bounding boxes</li>
<li><strong>Segmentation</strong>: Assign each pixel to objects (instance) or classes (semantic)</li>
<li><strong>Generation</strong>: Transform or create images (translation, synthesis, super-resolution, text-to-image)</li>
<li><strong>Keypoint Detection</strong>: Localize specific points (e.g., human joints)</li>
</ul>
<p><strong>Key Challenges:</strong></p>
<ul>
<li>Semantic gap (pixels → meaning)</li>
<li>Viewpoint, illumination, occlusion variations</li>
<li>Intra-class variation and background clutter</li>
<li>Context dependence</li>
</ul>
<p><strong>Classical vs.&nbsp;Modern Approaches:</strong></p>
<ul>
<li><strong>Classical CV</strong>: Hand-crafted features (HOG, SIFT, color histograms) + ML classifiers
<ul>
<li>Limited by manual feature design, poor scalability</li>
</ul></li>
<li><strong>Deep Learning</strong>: End-to-end learned hierarchical representations
<ul>
<li>Automatic feature extraction, better scaling with data</li>
</ul></li>
</ul>
<p><strong>Historical Milestones:</strong></p>
<ul>
<li><strong>1959</strong>: Hubel &amp; Wiesel - receptive fields in visual cortex</li>
<li><strong>1980</strong>: Neocognitron - hierarchical neural network for vision</li>
<li><strong>1986</strong>: Backpropagation - training algorithm for neural networks</li>
<li><strong>1998</strong>: LeNet - first successful CNN for digit recognition</li>
<li><strong>2012</strong>: AlexNet/ImageNet breakthrough - 9.4% error reduction, marked the deep learning revolution</li>
<li><strong>2014-2015</strong>: VGGNet (depth), ResNet (skip connections) - enabled very deep networks</li>
<li><strong>2020s</strong>: Transformers and hybrid architectures competing with CNNs</li>
</ul>
<p><strong>Why Deep Learning Won:</strong></p>
<ul>
<li>Large-scale datasets (ImageNet: 1.4M images, 1K classes)</li>
<li>GPU computing power (parallel processing)</li>
<li>Better architectures (CNNs exploit 2D structure)</li>
<li>End-to-end optimization (learn features + classifier together)</li>
</ul>
<p><strong>Applications:</strong></p>
<ul>
<li>Autonomous driving, medical imaging, facial recognition</li>
<li>Photo editing, image generation, agricultural monitoring</li>
<li>Embedded systems (mobile chips for on-device inference)</li>
</ul>
</div>
</div>
</div>
<section id="motivation" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="motivation"><span class="header-section-number">1</span> Motivation</h2>
<p>Deep Learning models have played a transformative role in <em>Computer Vision</em> over the last decade, significantly enhancing and expanding the capabilities to process visual data.</p>
<p>The integration of deep learning in computer vision has massively improved the accuracy and efficiency of visual recognition, classification, and analysis. This has opened up new possibilities in applications such as automated driving, facial recognition, and medical image analysis. These models are now embedded in a wide variety of products and services.</p>
<p>Let’s take a look at several applications of deep learning assisted computer vision applications.</p>
<section id="applications" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="applications"><span class="header-section-number">1.1</span> Applications</h3>
<p>To monitor wildlife populations biologists might deploy camera traps to capture images from passing animals (see <a href="#fig-intro-kora-lynx" class="quarto-xref">Figure&nbsp;1</a>). These images need to be analysed, for example, biologists need to know which species can be seen in each image. Deep learning can speed up that process significantly (<span class="citation" data-cites="Images2018">Willi et al. (<a href="#ref-Images2018" role="doc-biblioref">2019</a>)</span>).</p>
<div id="fig-intro-kora-lynx" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-kora-lynx-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../assets/images/intro/kora_lynx.png" class="img-fluid figure-img" width="600"></p>
<figcaption>Source: <span class="citation" data-cites="breitenmoser-wursten_projekt_2024">Breitenmoser-Würsten et al. (<a href="#ref-breitenmoser-wursten_projekt_2024" role="doc-biblioref">2024</a>)</span></figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-kora-lynx-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Image from a camera trap.
</figcaption>
</figure>
</div>
<p>Recent advances in generative deep learning enable anyone to create photorealistic synthetic images (see <a href="#fig-intro-synthetic-images" class="quarto-xref">Figure&nbsp;2</a> for examples). While these technologies have many positive applications (art, design, prototyping), they also raise concerns about misinformation and deepfakes, motivating research into synthetic media detection.</p>
<div id="fig-intro-synthetic-images" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-synthetic-images-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../assets/images/intro/sid_trump.jpg" class="img-fluid figure-img"></p>
<figcaption><a href="https://x.com/TheInfiniteDude/status/1637211568692932608">Source</a></figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../assets/images/intro/sid_pope.jpg" class="img-fluid figure-img"></p>
<figcaption><a href="https://x.com/cryptomattk/status/1687908457880367104">Source</a></figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-synthetic-images-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Examples of synthetic images.
</figcaption>
</figure>
</div>
<!-- ![Camera Trap Image Analysis](/assets/images/intro/camtrap.jpg){width=400} -->
<p><a href="#fig-intro-google-lens" class="quarto-xref">Figure&nbsp;3</a> shows an application in which a model identifies objects of interest in photographs, as well as searching additional information about it. Photos can also be used for translation tasks whereby models detect letters (optical character recognition - OCR) and then translate it (using language models in conjunction with visual models).</p>
<div id="fig-intro-google-lens" class="columns quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-intro-google-lens-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="column" style="width:45%;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../assets/images/intro/google_lens_classification.png" class="img-fluid figure-img"></p>
<figcaption>Identification &amp; Search</figcaption>
</figure>
</div>
</div>
<div class="column" style="width:45%;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../assets/images/intro/google_lens_ocr.png" class="img-fluid figure-img"></p>
<figcaption>Translation</figcaption>
</figure>
</div>
</div>
</div><figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-google-lens-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">Figure&nbsp;3: <a href="https://search.google/ways-to-search/lens/">Google Lens</a></figcaption></figure>
</div>
<p>One area with high commercial interest and competition is the development of self-driving capabilities. <a href="#fig-intro-self-driving" class="quarto-xref">Figure&nbsp;4</a> shows a demonstration of the perception system required for autonomous driving, including detecting vehicles, pedestrians, cyclists, and understanding lane markings and traffic signs in real-time.</p>
<div id="fig-intro-self-driving" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-self-driving-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video"><video id="video_shortcode_videojs_video1" width="800" class="video-js vjs-default-skin " controls="" preload="auto" data-setup="{}" title=""><source src="https://storage.googleapis.com/waymo-uploads/files/site-animations/waymo-driver/cameras.webm"></video></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-self-driving-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: <a href="https://waymo.com/waymo-driver/">Example from Waymo</a>. Observe how the system simultaneously tracks multiple objects.
</figcaption>
</figure>
</div>
<p>Face recognition technology, such as Apple’s Face ID (<a href="#fig-intro-face-id" class="quarto-xref">Figure&nbsp;5</a>), has become ubiquitous in consumer devices. The system uses infrared depth sensing to create a 3D map of facial features, making it more secure than 2D image-based recognition and robust to different lighting conditions.</p>
<div id="fig-intro-face-id" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-face-id-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video"><iframe data-external="1" src="https://www.youtube.com/embed/z-t1h0Y8vuM?si=qnEOYDmqyv8zGvMV?start=50" width="600" height="400" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-face-id-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: <a href="https://support.apple.com/en-us/102381">Example from Apple Face ID</a>. Watch how the infrared dot projector creates a depth map of the face.
</figcaption>
</figure>
</div>
<p>Computer vision has also been very useful in agricultur. <a href="#fig-intro-minneapple" class="quarto-xref">Figure&nbsp;6</a> shows an application where apples can be counted from image data. Furthermore, drones might be used to estimate crop yield, type and health.</p>
<div id="fig-intro-minneapple" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-minneapple-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/intro/minneapple.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-minneapple-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Example from <span class="citation" data-cites="hani_minneapple_2020">Häni, Roy, and Isler (<a href="#ref-hani_minneapple_2020" role="doc-biblioref">2020</a>)</span>
</figcaption>
</figure>
</div>
<p>Deep learning has also been applied extensively in the medical area. In particular, many imaging technologies, such as MRI or CT, can be analysed with computer vision techniques. <a href="#fig-intro-sam" class="quarto-xref">Figure&nbsp;7</a> illustrates a generic segmentation model which can be used to identify organs and other anatomical structures on pixel-level.</p>
<div id="fig-intro-sam" class="quarto-float quarto-figure quarto-figure-center anchored" width="400">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-sam-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/intro/medsam.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-sam-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Example from <span class="citation" data-cites="ma_segment_2024">Ma et al. (<a href="#ref-ma_segment_2024" role="doc-biblioref">2024</a>)</span>.
</figcaption>
</figure>
</div>
<p>Photo-editing has been brought to new levels of sophistication with AI-based capabilities. <a href="#fig-intro-magic" class="quarto-xref">Figure&nbsp;8</a> shows an impressive example from the Google Magic Editor. Maybe even a bit too good?</p>
<div id="fig-intro-magic" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-magic-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video"><video id="video_shortcode_videojs_video2" width="800" class="video-js vjs-default-skin " controls="" preload="auto" data-setup="{}" title=""><source src="https://storage.googleapis.com/gweb-mobius-cdn/photos/uploads/6e54ed750f84538fd052b31818127f1e4df5711c.compressed.mp4"></video></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-magic-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: <a href="https://www.google.com/intl/en/photos/editing/">Example from Google Magic Editor</a>
</figcaption>
</figure>
</div>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>🤔 Think About It</strong></p>
<p>What computer vision tasks do you think the model in <a href="#fig-intro-magic" class="quarto-xref">Figure&nbsp;8</a> must perform to enable this editing capability?</p>
<details>
<summary>
Possible tasks involved
</summary>
<ul>
<li>Object detection and segmentation (identifying what to remove/move)</li>
<li>Inpainting (filling in removed areas)</li>
<li>Style transfer (matching lighting/texture)</li>
<li>Scene understanding (maintaining realistic perspective)</li>
</ul>
</details>
</div>
</div>
</div>
<p>The increasingly better models and the ability to run them quickly and resource-efficiently on mobile devices have enabled such applications. <a href="#fig-intro-tensor-phone" class="quarto-xref">Figure&nbsp;9</a> shows that special chips have been developed to process images with Deep Learning models quickly.</p>
<div id="fig-intro-tensor-phone" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-tensor-phone-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/intro/tensor_phone.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-tensor-phone-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: From <a href="https://store.google.com/intl/en/ideas/pixel-camera-features/">Link</a>.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="computer-vision-tasks" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="computer-vision-tasks"><span class="header-section-number">2</span> Computer Vision Tasks</h2>
<section id="image-classification" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="image-classification"><span class="header-section-number">2.1</span> Image Classification</h3>
<p>In image classification, an image is assigned to a predefined set of classes. In multi-class classification, there are <span class="math inline">\(\gt 2\)</span> classes; in binary classification, there are exactly 2 classes; and in multi-label classification, there are multiple sets of classes. <a href="#fig-intro-image_clssifier" class="quarto-xref">Figure&nbsp;10</a> shows an example of multi-class classification.</p>
<div id="fig-intro-image_clssifier" class="quarto-float quarto-figure quarto-figure-center anchored" width="600">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-image_clssifier-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/intro/image_classification_example.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-image_clssifier-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: Multi-Class Image Classification example: a model predicts the most likely class for an image along with confidence scores (<span class="citation" data-cites="krizhevsky_imagenet_2012">Krizhevsky, Sutskever, and Hinton (<a href="#ref-krizhevsky_imagenet_2012" role="doc-biblioref">2012</a>)</span>).
</figcaption>
</figure>
</div>
</section>
<section id="object-detection" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="object-detection"><span class="header-section-number">2.2</span> Object Detection</h3>
<p>Object detection involves locating and recognizing multiple objects in an image. <a href="#fig-intro-yolo-example" class="quarto-xref">Figure&nbsp;11</a> shows an example from the paper by <span class="citation" data-cites="Redmon2016a">Redmon et al. (<a href="#ref-Redmon2016a" role="doc-biblioref">2016</a>)</span>. Each object is localized with a bounding box and assigned an object class with a confidence score.</p>
<div id="fig-intro-yolo-example" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-yolo-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/intro/yolo_object_detection_example.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-yolo-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: Object Detection example (from <span class="citation" data-cites="Redmon2016a">Redmon et al. (<a href="#ref-Redmon2016a" role="doc-biblioref">2016</a>)</span>). Bounding boxes localize the objects, showing the predicted class and confidence for each detection.
</figcaption>
</figure>
</div>
</section>
<section id="segmentation" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="segmentation"><span class="header-section-number">2.3</span> Segmentation</h3>
<p>In segmentation, individual pixels are assigned to specific objects (instance segmentation) or classes (semantic segmentation). <a href="#fig-mask-rcnn-object-segmentation-example" class="quarto-xref">Figure&nbsp;12</a> shows an example of instance segmentation from the paper by <span class="citation" data-cites="he_mask_2018">He et al. (<a href="#ref-he_mask_2018" role="doc-biblioref">2018</a>)</span>, where individual objects are detected and precisely localized at the pixel level.</p>
<div id="fig-mask-rcnn-object-segmentation-example" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mask-rcnn-object-segmentation-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/intro/mask_rcnn_object_segmentation_example.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mask-rcnn-object-segmentation-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: Instance Segmentation example (from <span class="citation" data-cites="he_mask_2018">He et al. (<a href="#ref-he_mask_2018" role="doc-biblioref">2018</a>)</span>). Each person is individually segmented with pixel-level masks.
</figcaption>
</figure>
</div>
<p>The following video shows an example of semantic segmentation:</p>
<div id="fig-intro-semantic-segmentation" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-semantic-segmentation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video"><iframe data-external="1" src="https://www.youtube.com/embed/wfObVKKKJkE?si=GZ6m6vig54Z8XVie" width="600" height="350" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-semantic-segmentation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13: Semantic segmentation applied to aerial imagery of agricultural fields. Each pixel is classified into crop types, field boundaries, and other land categories.
</figcaption>
</figure>
</div>
</section>
<section id="keypoint-detection" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="keypoint-detection"><span class="header-section-number">2.4</span> Keypoint Detection</h3>
<p>In keypoint detection, specific points (keypoints) on objects—typically human body joints—are localized. <a href="#fig-mask-rcnn-keypoint-detection-example" class="quarto-xref">Figure&nbsp;14</a> shows an example of keypoint detection from the paper by <span class="citation" data-cites="he_mask_2018">He et al. (<a href="#ref-he_mask_2018" role="doc-biblioref">2018</a>)</span>, where individual people are detected and their joints localized. This can be used to transfer movements from a person to an avatar or to recognize human activities (action recognition).</p>
<div id="fig-mask-rcnn-keypoint-detection-example" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mask-rcnn-keypoint-detection-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/intro/mask_rcnn_keypoint_detection_example.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mask-rcnn-keypoint-detection-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14: Keypoint Detection example (from <span class="citation" data-cites="he_mask_2018">He et al. (<a href="#ref-he_mask_2018" role="doc-biblioref">2018</a>)</span>).
</figcaption>
</figure>
</div>
</section>
<section id="image-generation" class="level3" data-number="2.5">
<h3 data-number="2.5" class="anchored" data-anchor-id="image-generation"><span class="header-section-number">2.5</span> Image Generation</h3>
<p>There are various applications where models transform input images into specific output images (image-to-image translation) or generate completely new images from scratch (unconditional generation). Below are example applications.</p>
<section id="image-manipulation" class="level4" data-number="2.5.1">
<h4 data-number="2.5.1" class="anchored" data-anchor-id="image-manipulation"><span class="header-section-number">2.5.1</span> Image Manipulation</h4>
<p>Interactive manipulation allows users to modify images by dragging points to reshape objects while maintaining realistic appearance.</p>
<div id="fig-intro-image-manipulation" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-image-manipulation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video"><video id="video_shortcode_videojs_video3" width="800" class="video-js vjs-default-skin " controls="" preload="auto" data-setup="{}" title=""><source src="https://vcai.mpi-inf.mpg.de/projects/DragGAN/data/DragGAN.mp4"></video></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-image-manipulation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15: <a href="https://vcai.mpi-inf.mpg.de/projects/DragGAN/">DragGAN</a> by <span class="citation" data-cites="pan_drag_2023">Pan et al. (<a href="#ref-pan_drag_2023" role="doc-biblioref">2023</a>)</span>
</figcaption>
</figure>
</div>
</section>
<section id="image-translation" class="level4" data-number="2.5.2">
<h4 data-number="2.5.2" class="anchored" data-anchor-id="image-translation"><span class="header-section-number">2.5.2</span> Image Translation</h4>
<p><a href="#fig-intro-pix2pix-example" class="quarto-xref">Figure&nbsp;16</a> shows an example of image generation from the paper by <span class="citation" data-cites="image_to_image_isola2018">Isola et al. (<a href="#ref-image_to_image_isola2018" role="doc-biblioref">2018</a>)</span>, where images are generated conditioned on image inputs (translated). This can be used, for example, to convert a sketch of an object into a photorealistic depiction of it.</p>
<div id="fig-intro-pix2pix-example" class="quarto-float quarto-figure quarto-figure-center anchored" width="600">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-pix2pix-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/intro/pix2pix_examples.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-pix2pix-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16: Image Generation example (from <span class="citation" data-cites="image_to_image_isola2018">Isola et al. (<a href="#ref-image_to_image_isola2018" role="doc-biblioref">2018</a>)</span>).
</figcaption>
</figure>
</div>
</section>
<section id="image-super-resolution" class="level4" data-number="2.5.3">
<h4 data-number="2.5.3" class="anchored" data-anchor-id="image-super-resolution"><span class="header-section-number">2.5.3</span> Image Super Resolution</h4>
<p>In the gaming industry, Deep Learning is used to upscale low-resolution images to high-resolution in real-time (image super resolution), as shown in <a href="#fig-intro-dssl" class="quarto-xref">Figure&nbsp;17</a>. This enables higher frame rates while maintaining visual quality.</p>
<div id="fig-intro-dssl" class="quarto-float quarto-figure quarto-figure-center anchored" width="600">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-dssl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/intro/dssl.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-dssl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17: Nvidia DLSS (Deep Learning Super Sampling) improves gaming performance. <a href="https://images.nvidia.com/aem-dam/Solutions/geforce/news/control-nvidia-dlss-2-0-update/deliver-us-the-moon-nvidia-dlss-2-0-performance-boost.png">Source</a>
</figcaption>
</figure>
</div>
</section>
<section id="image-colorization" class="level4" data-number="2.5.4">
<h4 data-number="2.5.4" class="anchored" data-anchor-id="image-colorization"><span class="header-section-number">2.5.4</span> Image Colorization</h4>
<p>Image colorization can be learned with Deep Learning. This involves transforming a black-and-white image into an RGB image. <a href="#fig-intro-colorization-example" class="quarto-xref">Figure&nbsp;18</a> shows an example.</p>
<div id="fig-intro-colorization-example" class="quarto-float quarto-figure quarto-figure-center anchored" width="600">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-colorization-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/intro/colorization_example.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-colorization-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;18: Norwegian Bride (est late 1890s) from DeOldify: <a href="https://github.com/jantic/DeOldify">Link</a>
</figcaption>
</figure>
</div>
</section>
<section id="view-synthesis" class="level4" data-number="2.5.5">
<h4 data-number="2.5.5" class="anchored" data-anchor-id="view-synthesis"><span class="header-section-number">2.5.5</span> View Synthesis</h4>
<p>In view synthesis, new camera viewpoints of a scene are generated from a model. Neural Radiance Fields (NeRFs) are models that can generate novel views from a set of input images taken from known viewpoints. <a href="#fig-intro-nerf" class="quarto-xref">Figure&nbsp;19</a> shows training data (input images from different angles) and the synthesized novel views the model can generate.</p>
<div id="fig-intro-nerf" class="quarto-float quarto-figure quarto-figure-center anchored" width="600">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-nerf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/intro/nerf.png" class="img-fluid figure-img" width="800">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-nerf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;19: Neural Radiance Fields - example (from <span class="citation" data-cites="mildenhall_nerf_2020">Mildenhall et al. (<a href="#ref-mildenhall_nerf_2020" role="doc-biblioref">2020</a>)</span>).
</figcaption>
</figure>
</div>
<p>Modern variants can produce impressive quality, as shown in <a href="#fig-intro-gaussian-splatting" class="quarto-xref">Figure&nbsp;20</a>.</p>
<div id="fig-intro-gaussian-splatting" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-gaussian-splatting-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video"><video id="video_shortcode_videojs_video4" width="800" class="video-js vjs-default-skin " controls="" preload="auto" data-setup="{}" title=""><source src="https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/content/videos/bicycle.mp4"></video></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-gaussian-splatting-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;20: 3D Gaussian Splatting for real-time novel view synthesis <span class="citation" data-cites="kerbl_3d_2023">Kerbl et al. (<a href="#ref-kerbl_3d_2023" role="doc-biblioref">2023</a>)</span>.
</figcaption>
</figure>
</div>
</section>
<section id="unconditional-generation" class="level4" data-number="2.5.6">
<h4 data-number="2.5.6" class="anchored" data-anchor-id="unconditional-generation"><span class="header-section-number">2.5.6</span> Unconditional Generation</h4>
<p>In unconditional image generation, images are generated that resemble the training data distribution without specific input guidance. While you have no direct control over what specific image is generated, you can often manipulate latent variables to modify generated images or interpolate between samples. <a href="#fig-intro-stylegan3" class="quarto-xref">Figure&nbsp;21</a> shows photorealistic portraits generated by a model trained on human faces.</p>
<div id="fig-intro-stylegan3" class="quarto-float quarto-figure quarto-figure-center anchored" width="400">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-stylegan3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/intro/stylegan3_examples.png" class="img-fluid figure-img" width="400">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-stylegan3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;21: StyleGan3 (from <span class="citation" data-cites="karras_alias-free_2021">Karras et al. (<a href="#ref-karras_alias-free_2021" role="doc-biblioref">2021</a>)</span>).
</figcaption>
</figure>
</div>
</section>
<section id="text-to-image" class="level4" data-number="2.5.7">
<h4 data-number="2.5.7" class="anchored" data-anchor-id="text-to-image"><span class="header-section-number">2.5.7</span> Text to Image</h4>
<p><a href="#fig-stable-diffusion-example" class="quarto-xref">Figure&nbsp;22</a> shows an example of image generation from the paper by <span class="citation" data-cites="rombach_high-resolution_2022">Rombach et al. (<a href="#ref-rombach_high-resolution_2022" role="doc-biblioref">2022</a>)</span>, where images are generated conditioned on text inputs.</p>
<div id="fig-stable-diffusion-example" class="quarto-float quarto-figure quarto-figure-center anchored" width="600">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-stable-diffusion-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/intro/stable_diffusion_example.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-stable-diffusion-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;22: Image Generation example (from <span class="citation" data-cites="rombach_high-resolution_2022">Rombach et al. (<a href="#ref-rombach_high-resolution_2022" role="doc-biblioref">2022</a>)</span>.)
</figcaption>
</figure>
</div>
<p>On <a href="https://civitai.com/">civitai</a>, there are numerous examples and models to admire or download.</p>
</section>
</section>
<section id="further-tasks" class="level3" data-number="2.6">
<h3 data-number="2.6" class="anchored" data-anchor-id="further-tasks"><span class="header-section-number">2.6</span> Further tasks</h3>
<p>There are many more computer vision tasks. The following list is not exhaustive:</p>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>Image Classification</li>
<li>Object Detection (and Tracking)</li>
<li>Image Segmentation
<ul>
<li>Semantic Segmentation</li>
<li>Instance Segmentation</li>
</ul></li>
<li>Optical Character Recognition (OCR)</li>
<li>Pose Estimation</li>
<li>Facial Recognition</li>
<li>Action Recognition</li>
</ul>
</div><div class="column" style="width:50%;">
<ul>
<li>Image Generation
<ul>
<li>Style Transfer</li>
<li>Image Inpainting</li>
<li>Super-Resolution</li>
<li>Text-to-Image (and more)</li>
</ul></li>
<li>Image Captioning</li>
<li>3D Reconstruction</li>
<li>Image Retrieval</li>
</ul>
</div>
</div>
</section>
<section id="what-about-videos" class="level3" data-number="2.7">
<h3 data-number="2.7" class="anchored" data-anchor-id="what-about-videos"><span class="header-section-number">2.7</span> What about videos?</h3>
<p>Many of the image-level tasks can be extended to videos. After all, videos are nothing but a sequence of images.</p>
<p>The most recent impressive examples are from video generative models:</p>
<div id="fig-intro-sora2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-sora2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video"><video id="video_shortcode_videojs_video5" width="800" class="video-js vjs-default-skin " controls="" preload="auto" data-setup="{}" title=""><source src="https://openaiassets.blob.core.windows.net/$web/nf2/blog-final2/20250929_0337_New%20Video_simple_compose_01k6agr8ctey29mze208t29w38%20(1).mp4"></video></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-sora2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;23: <a href="https://openai.com/index/sora-2/">OpenAI Sora 2</a> - Prompt: a man rides a horse which is on another horse
</figcaption>
</figure>
</div>
<p>But also object / keypoint tracking:</p>
<div id="fig-intro-cotracker" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-cotracker-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-video"><video id="video_shortcode_videojs_video6" width="800" class="video-js vjs-default-skin " controls="" preload="auto" data-setup="{}" title=""><source src="https://co-tracker.github.io/videos/teaser/rollerblade.mp4"></video></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-cotracker-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;24: <a href="https://co-tracker.github.io/">CoTracker</a> - Tracking points across video frames for motion analysis (<span class="citation" data-cites="karaev_cotracker_2024">Karaev et al. (<a href="#ref-karaev_cotracker_2024" role="doc-biblioref">2024</a>)</span>)
</figcaption>
</figure>
</div>
</section>
</section>
<section id="challenges" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="challenges"><span class="header-section-number">3</span> Challenges</h2>
<p>We will now explore some challenges that must be overcome when analyzing images with machine learning models.</p>
<section id="sec-intro-challenges-semantic-gap" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="sec-intro-challenges-semantic-gap"><span class="header-section-number">3.1</span> Semantic Gap</h3>
<p>The semantic gap refers to the discrepancy between the low-level pixel information that can be directly extracted from an image and the high-level semantic interpretation understood by humans. In simpler terms: an image consists of millions of pixel values, but deriving semantically meaningful concepts like “cat” or “sitting” from these raw numbers is extremely challenging.</p>
<div id="fig-cat-number-grid" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cat-number-grid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/intro/semantic_gap_comparison.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cat-number-grid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;25: Illustration of the semantic gap: computers see numbers, humans see meaning.
</figcaption>
</figure>
</div>
</section>
<section id="viewpoint" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="viewpoint"><span class="header-section-number">3.2</span> Viewpoint</h3>
<p>The semantic meaning of an image remains constant regardless of camera viewpoint, but the pixel values change dramatically. A model must learn to recognize the same object from different angles.</p>
<div id="fig-intro-challenge-viewpoint" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-challenge-viewpoint-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/intro/viewpoint_challenge.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-challenge-viewpoint-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;26: The same cat from different viewpoints looks completely different at the pixel level.
</figcaption>
</figure>
</div>
</section>
<section id="deformation" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="deformation"><span class="header-section-number">3.3</span> Deformation</h3>
<p>Objects, especially living beings, are often flexible and can appear in vastly different shapes and poses. Models must recognize a cat whether it’s curled up sleeping or stretched out running.</p>
<div id="fig-challenge-deformation" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-challenge-deformation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/intro/challenge_deformation.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-challenge-deformation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;27: Cats exhibit significant shape variation due to pose changes. <a href="http://cs231n.stanford.edu/">Source</a>
</figcaption>
</figure>
</div>
</section>
<section id="illumination" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="illumination"><span class="header-section-number">3.4</span> Illumination</h3>
<p>Changes in lighting conditions dramatically affect pixel values and can obscure object features. The same object under different lighting can produce vastly different images.</p>
<div id="fig-challenge-illumination" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-challenge-illumination-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/intro/challenge_illumination.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-challenge-illumination-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;28: The same scene under different lighting conditions. <a href="http://cs231n.stanford.edu/">Source</a>
</figcaption>
</figure>
</div>
</section>
<section id="background-clutter" class="level3" data-number="3.5">
<h3 data-number="3.5" class="anchored" data-anchor-id="background-clutter"><span class="header-section-number">3.5</span> Background Clutter</h3>
<p>Background pixels can visually resemble the object of interest, making precise object localization and segmentation difficult. The model must learn to distinguish foreground from background.</p>
<div id="fig-challenge-background" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-challenge-background-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/intro/challenge_background.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-challenge-background-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;29: Cat camouflaged against a similarly-textured background. <a href="http://cs231n.stanford.edu/">Source</a>
</figcaption>
</figure>
</div>
</section>
<section id="occlusion" class="level3" data-number="3.6">
<h3 data-number="3.6" class="anchored" data-anchor-id="occlusion"><span class="header-section-number">3.6</span> Occlusion</h3>
<p>Objects are often partially hidden by other objects in the scene. Models must learn to recognize objects even when only part of them is visible.</p>
<div id="fig-challenge-occlusion" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-challenge-occlusion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/intro/challenge_occlusion.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-challenge-occlusion-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;30: Partially occluded cat - only part of the object is visible. <a href="http://cs231n.stanford.edu/">Source</a>
</figcaption>
</figure>
</div>
</section>
<section id="intra-class-variation" class="level3" data-number="3.7">
<h3 data-number="3.7" class="anchored" data-anchor-id="intra-class-variation"><span class="header-section-number">3.7</span> Intra-Class Variation</h3>
<p>Objects belonging to the same class can exhibit enormous variability in appearance, color, size, and texture. All cats are cats, but they can look very different from each other.</p>
<div id="fig-challenge-intra-class" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-challenge-intra-class-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/intro/challenge_intra_class_variation.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-challenge-intra-class-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;31: High intra-class variation: all are cats but with vastly different appearances. <a href="https://www.maxpixel.net/Cat-Kittens-Free-Float-Kitten-Rush-Cat-Puppy-555822">Source</a>
</figcaption>
</figure>
</div>
</section>
<section id="context-dependence" class="level3" data-number="3.8">
<h3 data-number="3.8" class="anchored" data-anchor-id="context-dependence"><span class="header-section-number">3.8</span> Context Dependence</h3>
<p><a href="#fig-tiger-context" class="quarto-xref">Figure&nbsp;32</a> shows that contextual information can be crucial for correct object classification. The surrounding scene provides important cues about what objects are likely to be present.</p>
<div id="fig-tiger-context" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tiger-context-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/intro/tiger_context.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tiger-context-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;32: Is this a tiger? Context clues help determine the answer. <a href="https://www.linkedin.com/posts/ralph-aboujaoude-diaz-40838313_technology-artificialintelligence-computervision-activity-6912446088364875776-h-Iq?utm_source=linkedin_share&amp;utm_medium=member_desktop_web">Source</a>
</figcaption>
</figure>
</div>
<p><a href="#fig-blop-context" class="quarto-xref">Figure&nbsp;33</a> represents an interesting example where context is important: Shown are four images with severe quality degradation (blur) and oval objects indicated with dashed black lines. Notably, each marked object is exactly identical on pixel level (apart from a rotation) - yet we can form clear hypotheses what each of the object represents.</p>
<div id="fig-blop-context" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-blop-context-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/intro/challenges_context_blop.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-blop-context-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;33: In the presence of image degradation a visual system (e.g.&nbsp;humans) heavily use context information. Even though the circled blops are identical, we interpret them as either: plate, bottle, cell phone, car, pedestrian or shoe. Source <span class="citation" data-cites="torralba_using_2010">Torralba, Murphy, and Freeman (<a href="#ref-torralba_using_2010" role="doc-biblioref">2010</a>)</span>
</figcaption>
</figure>
</div>
</section>
</section>
<section id="machine-learning" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="machine-learning"><span class="header-section-number">4</span> Machine Learning</h2>
<section id="machine-learning-basics" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="machine-learning-basics"><span class="header-section-number">4.1</span> Machine Learning Basics</h3>
<div class="callout callout-style-default callout-note callout-titled" title="📚 Quick ML Refresher">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>📚 Quick ML Refresher
</div>
</div>
<div class="callout-body-container callout-body">
<p>This course uses machine learning concepts throughout. If you need a detailed refresher on ML fundamentals, see our <a href="../../pages/background/machine_learning.html">Machine Learning Basics</a> background page.</p>
<p><strong>Quick summary</strong>: We follow a data-driven approach:</p>
<ol type="1">
<li><strong>Collect</strong> a dataset of images and labels</li>
<li><strong>Train</strong> a model that learns to map images to labels</li>
<li><strong>Evaluate</strong> the model on new data</li>
</ol>
<div id="c218f038" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(images, labels):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a> <span class="co">""" Train a Model """</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a> <span class="co"># Fit Model here</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a> <span class="cf">return</span> model</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict(test_images, model):</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a> <span class="co">""" Predict """</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a> predictions <span class="op">=</span> model(test_images)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a> <span class="cf">return</span> predictions</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p><strong>Key concepts:</strong></p>
<ul>
<li><strong>Model</strong>: Function <span class="math inline">\(f_{\theta}(\mathbf{x}) = \hat{y}\)</span> with learnable parameters <span class="math inline">\(\theta\)</span></li>
<li><strong>Optimization</strong>: Find <span class="math inline">\(\theta\)</span> that minimizes loss <span class="math inline">\(J(f_{\theta}(\mathbf{X}), \mathbf{y})\)</span></li>
<li><strong>Model Selection</strong>: Use train/validation/test splits to avoid overfitting</li>
</ul>
</div>
</div>
</section>
<section id="ml-with-classical-computer-vision" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="ml-with-classical-computer-vision"><span class="header-section-number">4.2</span> ML with Classical Computer Vision</h3>
<p>Images typically have very high dimensionality. For example, an RGB image with a resolution of <span class="math inline">\(800 \times 600\)</span> has a dimensionality of <span class="math inline">\(800 \times 600 \times 3 = 1,440,000\)</span>. Classical machine learning algorithms often struggle with such high dimensionalities:</p>
<ul>
<li>They are very slow or require a lot of memory.</li>
<li>They cannot exploit the 2-D structure of images.</li>
<li>They are very sensitive to slight changes in images (e.g., rotations).</li>
<li>They can easily overfit, as the number of features is close to the number of observations (training set).</li>
</ul>
<p>When modeling images with (classical) machine learning algorithms, methods from (classical) computer vision are often used. With the help of such methods, features can be extracted from images, and the algorithms can learn on these features. This avoids modeling high-dimensional raw data. <a href="#fig-intro-color-histograms" class="quarto-xref">Figure&nbsp;34</a>, <a href="#fig-intro-hog-features" class="quarto-xref">Figure&nbsp;35</a>, and <a href="#fig-intro-bow-features" class="quarto-xref">Figure&nbsp;36</a> show various feature extraction methods.</p>
<p><a href="#fig-intro-color-histograms" class="quarto-xref">Figure&nbsp;34</a> shows that, for example, the distribution over the color spectrum can be extracted from an image. This could be an important feature.</p>
<div id="fig-intro-color-histograms" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-color-histograms-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/intro/color_histogram.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-color-histograms-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;34: Color Histograms as Features (Source: <span class="citation" data-cites="johnson_eecs_2022">Johnson (<a href="#ref-johnson_eecs_2022" role="doc-biblioref">2022</a>)</span>)
</figcaption>
</figure>
</div>
<p><a href="#fig-intro-hog-features" class="quarto-xref">Figure&nbsp;35</a> shows that techniques like Histogram of Oriented Gradients (HOG) <span class="citation" data-cites="dalal_histograms_2005">Dalal and Triggs (<a href="#ref-dalal_histograms_2005" role="doc-biblioref">2005</a>)</span> can be used to extract structures from images. Such features were successfully used for pedestrian detection <span class="citation" data-cites="dalal_histograms_2005">Dalal and Triggs (<a href="#ref-dalal_histograms_2005" role="doc-biblioref">2005</a>)</span>.</p>
<div id="fig-intro-hog-features" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-hog-features-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/intro/hog_features.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-hog-features-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;35: HOG as Features (Source: <span class="citation" data-cites="johnson_eecs_2022">Johnson (<a href="#ref-johnson_eecs_2022" role="doc-biblioref">2022</a>)</span>)
</figcaption>
</figure>
</div>
<p><a href="#fig-intro-bow-features" class="quarto-xref">Figure&nbsp;36</a> shows another feature variant. Visual patches can be extracted from a dataset, clustered, and then used as descriptors.</p>
<div id="fig-intro-bow-features" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-bow-features-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/intro/bag_of_words_features.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-bow-features-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;36: Bag of (visual) words Features (Source: <span class="citation" data-cites="johnson_eecs_2022">Johnson (<a href="#ref-johnson_eecs_2022" role="doc-biblioref">2022</a>)</span>)
</figcaption>
</figure>
</div>
<p>Finally, all features can be combined, often more is better, as shown in <a href="#fig-intro-concat-features" class="quarto-xref">Figure&nbsp;37</a>.</p>
<div id="fig-intro-concat-features" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-concat-features-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/intro/features_concat.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-concat-features-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;37: Image Features (Source: <span class="citation" data-cites="johnson_eecs_2022">Johnson (<a href="#ref-johnson_eecs_2022" role="doc-biblioref">2022</a>)</span>)
</figcaption>
</figure>
</div>
<p>Depending on the parameterization, HOG descriptors, for example, can be very large for images. The resulting feature vector from <a href="#fig-intro-concat-features" class="quarto-xref">Figure&nbsp;37</a> can still be reduced in dimensionality, e.g., with Principal Component Analysis.</p>
<p><a href="#fig-intro-cifar10" class="quarto-xref">Figure&nbsp;38</a> shows the CIFAR10 dataset. A well-known dataset for testing models, consisting of 10 classes with 6,000 images each.</p>
<div id="fig-intro-cifar10" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-cifar10-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/intro/cifar10.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-cifar10-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;38: CIFAR10 Dataset <a href="https://www.cs.toronto.edu/~kriz/cifar.html">Source</a>
</figcaption>
</figure>
</div>
</section>
</section>
<section id="deep-learning" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="deep-learning"><span class="header-section-number">5</span> Deep Learning</h2>
<p>To accelerate research in computer vision and make progress more measurable, the PASCAL VOC Challenges were introduced <span class="citation" data-cites="everingham_pascal_2007">Everingham et al. (<a href="#ref-everingham_pascal_2007" role="doc-biblioref">2007</a>)</span>. These involved various tasks, such as detecting objects in photographs (<a href="#fig-intro-pascal" class="quarto-xref">Figure&nbsp;39</a>).</p>
<div id="fig-intro-pascal" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-pascal-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/intro/pascal_voc_2007.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-pascal-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;39: Images/illustrations from <a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2007/">Link</a> and <span class="citation" data-cites="johnson_eecs_2022">Johnson (<a href="#ref-johnson_eecs_2022" role="doc-biblioref">2022</a>)</span>. On the left, you see object annotations in images, and on the right, you see the development of Mean Average Precision over the years.
</figcaption>
</figure>
</div>
<p>The easy availability of images on the internet has made it possible to collect increasingly larger datasets. <a href="https://www.image-net.org/">ImageNet</a> is such a very large, hierarchically annotated image dataset <span class="citation" data-cites="deng_imagenet_2009">Deng et al. (<a href="#ref-deng_imagenet_2009" role="doc-biblioref">2009</a>)</span> with over 1.4 million images, categorized into 1,000 object classes. <a href="#fig-intro-imagenet" class="quarto-xref">Figure&nbsp;40</a> illustrates the dataset.</p>
<div id="fig-intro-imagenet" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-imagenet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/intro/imagenet.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-imagenet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;40: <a href="https://www.image-net.org/">ImageNet</a>, <a href="https://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_full_1k.jpg">Image Source</a>, details in <span class="citation" data-cites="deng_imagenet_2009">Deng et al. (<a href="#ref-deng_imagenet_2009" role="doc-biblioref">2009</a>)</span>
</figcaption>
</figure>
</div>
<p>Since 2010, challenges have been regularly conducted on the ImageNet dataset <span class="citation" data-cites="russakovsky_imagenet_2015">Russakovsky et al. (<a href="#ref-russakovsky_imagenet_2015" role="doc-biblioref">2015</a>)</span>, such as image classification and object detection. <a href="#fig-intro-imagenet-perf" class="quarto-xref">Figure&nbsp;41</a> shows the development of the error rate over time.</p>
<div id="fig-intro-imagenet-perf" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-imagenet-perf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/intro/image_net_perf.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-imagenet-perf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;41: Source: <span class="citation" data-cites="johnson_eecs_2022">Johnson (<a href="#ref-johnson_eecs_2022" role="doc-biblioref">2022</a>)</span>
</figcaption>
</figure>
</div>
<section id="the-imagenet-breakthrough-2012" class="level4" data-number="5.0.1">
<h4 data-number="5.0.1" class="anchored" data-anchor-id="the-imagenet-breakthrough-2012"><span class="header-section-number">5.0.1</span> The ImageNet Breakthrough (2012)</h4>
<p>The year 2012 marked a watershed moment in computer vision. Until then, the state-of-the-art in the ImageNet competition relied on classical approaches. In 2011, the winning team <span class="citation" data-cites="perronnin_large-scale_2010">Perronnin et al. (<a href="#ref-perronnin_large-scale_2010" role="doc-biblioref">2010</a>)</span> combined various hand-crafted feature extraction methods (SIFT features) with Support Vector Machines (SVMs), achieving a top-5 error rate of 25.8%.</p>
<p>Then came the breakthrough: Krizhevsky et al. <span class="citation" data-cites="krizhevsky_imagenet_2012">Krizhevsky, Sutskever, and Hinton (<a href="#ref-krizhevsky_imagenet_2012" role="doc-biblioref">2012</a>)</span> implemented <strong>AlexNet</strong>, a convolutional neural network (CNN) with multiple layers, and achieved a top-5 error rate of just 16.4%—a dramatic 9.4 percentage point improvement. This wasn’t just an incremental gain; it fundamentally demonstrated that deep learning could outperform decades of hand-crafted computer vision methods.</p>
<div id="fig-intro-alexnet" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-alexnet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/intro/alexnet.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-alexnet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;42: AlexNet architecture <span class="citation" data-cites="krizhevsky_imagenet_2012">Krizhevsky, Sutskever, and Hinton (<a href="#ref-krizhevsky_imagenet_2012" role="doc-biblioref">2012</a>)</span>: 5 convolutional layers followed by 3 fully connected layers, trained on dual GPUs.
</figcaption>
</figure>
</div>
<p><strong>What made AlexNet work?</strong></p>
<ul>
<li><strong>Deep architecture</strong>: 8 layers (5 convolutional + 3 fully connected)</li>
<li><strong>GPU training</strong>: Used dual GPUs to handle 60 million parameters</li>
<li><strong>ReLU activations</strong>: Faster training than traditional sigmoid/tanh</li>
<li><strong>Dropout regularization</strong>: Reduced overfitting</li>
</ul>
<p>This success marked the end of classical computer vision methods in many areas and triggered an explosion of deep learning research.</p>
<p>While classical computer vision trains a machine learning model on features extracted with hand-crafted algorithms <a href="#fig-intro-classical-ml" class="quarto-xref">Figure&nbsp;43</a>, the development is increasingly moving towards end-to-end learning. In this approach, one avoids as much as possible static/hand-designed components and learns everything, including feature extraction, with machine learning <a href="#fig-intro-end-to-end" class="quarto-xref">Figure&nbsp;44</a>.</p>
<div id="fig-intro-classical-ml" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-classical-ml-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/intro/classical_ml.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-classical-ml-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;43: Illustration from <span class="citation" data-cites="johnson_eecs_2022">Johnson (<a href="#ref-johnson_eecs_2022" role="doc-biblioref">2022</a>)</span>
</figcaption>
</figure>
</div>
<div id="fig-intro-end-to-end" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro-end-to-end-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/intro/end-to-end-ml.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro-end-to-end-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;44: Illustration from <span class="citation" data-cites="johnson_eecs_2022">Johnson (<a href="#ref-johnson_eecs_2022" role="doc-biblioref">2022</a>)</span>
</figcaption>
</figure>
</div>
<p>Deep learning-based approaches have several key advantages over classical machine learning methods:</p>
<ul>
<li><strong>Automatic feature extraction</strong>: No need for hand-crafted features like SIFT or HOG.</li>
<li><strong>Hierarchical representations</strong>: Learn features at multiple levels of abstraction (edges → textures → parts → objects).</li>
<li><strong>Better scaling</strong>: Performance improves with more training data and compute.</li>
<li><strong>End-to-end learning</strong>: Unified optimization from raw inputs to task outputs.</li>
<li><strong>Built-in invariances</strong>: Architectures like CNNs are naturally invariant to certain transformations (e.g., translation).</li>
<li><strong>Transfer learning</strong>: Pre-trained models can be adapted to new tasks with limited data.</li>
</ul>
</section>
<section id="deep-learning-history" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="deep-learning-history"><span class="header-section-number">5.1</span> Deep Learning History</h3>
<p>We will now explore the most important milestones in deep learning for image analysis.</p>
<section id="hubel-and-wiesel" class="level4" data-number="5.1.1">
<h4 data-number="5.1.1" class="anchored" data-anchor-id="hubel-and-wiesel"><span class="header-section-number">5.1.1</span> Hubel and Wiesel</h4>
<p><span class="citation" data-cites="hubel_receptive_1959">Hubel and Wiesel (<a href="#ref-hubel_receptive_1959" role="doc-biblioref">1959</a>)</span> showed in experiments on cats that there are complex cells/neurons in the visual cortex that only respond to certain patterns. In addition to these complex cells, there are also simple cells that only respond to lines at a certain angle.</p>
<div id="fig-huber-wiesel" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-huber-wiesel-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/intro/huber_wiesel_cat_experiment.jpg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-huber-wiesel-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;45: Illustration <a href="https://link.springer.com/chapter/10.1007/978-3-030-28954-6_4/figures/1">Source</a>
</figcaption>
</figure>
</div>
</section>
<section id="neocognitron" class="level4" data-number="5.1.2">
<h4 data-number="5.1.2" class="anchored" data-anchor-id="neocognitron"><span class="header-section-number">5.1.2</span> Neocognitron</h4>
<p><span class="citation" data-cites="fukushima_neocognitron_1980">Fukushima (<a href="#ref-fukushima_neocognitron_1980" role="doc-biblioref">1980</a>)</span> defined a model of a neural network that can recognize visual patterns. It already has the hierarchical structure of a modern convolutional neural network and is inspired by biological neural networks, particularly from the insights of <span class="citation" data-cites="hubel_receptive_1959">Hubel and Wiesel (<a href="#ref-hubel_receptive_1959" role="doc-biblioref">1959</a>)</span>.</p>
<div id="fig-neocognitron" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-neocognitron-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/intro/neocogitron.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-neocognitron-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;46: The Neocognitron <span class="citation" data-cites="fukushima_neocognitron_1980">Fukushima (<a href="#ref-fukushima_neocognitron_1980" role="doc-biblioref">1980</a>)</span>.
</figcaption>
</figure>
</div>
</section>
<section id="backpropagation" class="level4" data-number="5.1.3">
<h4 data-number="5.1.3" class="anchored" data-anchor-id="backpropagation"><span class="header-section-number">5.1.3</span> Backpropagation</h4>
<p><span class="citation" data-cites="rumelhart_learning_1986">Rumelhart, Hinton, and Williams (<a href="#ref-rumelhart_learning_1986" role="doc-biblioref">1986</a>)</span> introduced the backpropagation algorithm in the context of neural networks. This algorithm is used to train modern neural networks: it finds the parameters of an artificial neural network (ANN) to solve a specific task. Backpropagation is based on the chain rule from calculus and is also important for other machine learning models.</p>
<div id="fig-rumalhart-backprop" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rumalhart-backprop-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/intro/rumelhart_backprop.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rumalhart-backprop-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;47: Backpropagation in neural networks <span class="citation" data-cites="rumelhart_learning_1986">Rumelhart, Hinton, and Williams (<a href="#ref-rumelhart_learning_1986" role="doc-biblioref">1986</a>)</span>.
</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>A good video on backpropagation: <a href="https://www.youtube.com/watch?v=tIeHLnjs5U8">3Blue1Brown Backpropagation Calculus</a></p>
</div>
</div>
</section>
<section id="convolutional-neural-networks-cnns" class="level4" data-number="5.1.4">
<h4 data-number="5.1.4" class="anchored" data-anchor-id="convolutional-neural-networks-cnns"><span class="header-section-number">5.1.4</span> Convolutional Neural Networks (CNNs)</h4>
<p><span class="citation" data-cites="lecun_gradient-based_1998">Lecun et al. (<a href="#ref-lecun_gradient-based_1998" role="doc-biblioref">1998</a>)</span> implemented convolutional neural networks (CNNs) to recognize handwritten digits. The architecture is specialized for the 2-D structure of image data and trained a model very similar to modern CNNs, as shown in <a href="#fig-lecun-cnn" class="quarto-xref">Figure&nbsp;48</a>.</p>
<div id="fig-lecun-cnn" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lecun-cnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/intro/lecun_cnn.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lecun-cnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;48: LeNet-5 architecture <span class="citation" data-cites="lecun_gradient-based_1998">Lecun et al. (<a href="#ref-lecun_gradient-based_1998" role="doc-biblioref">1998</a>)</span>: the first successful CNN for digit recognition.
</figcaption>
</figure>
</div>
<p>After AlexNet’s breakthrough in 2012 (see previous section), CNNs rapidly evolved and were applied to increasingly complex tasks. Notable developments include:</p>
<ul>
<li><strong>VGGNet (2014)</strong>: Showed that depth matters—16-19 layers with small 3×3 filters</li>
<li><strong>ResNet (2015)</strong>: Introduced skip connections, enabling networks with 50-152 layers</li>
<li><strong>COCO Challenges</strong>: Expanded beyond ImageNet to multi-task benchmarks (detection, segmentation, keypoints)</li>
<li><strong>Mobile architectures</strong>: Efficient CNNs for deployment on phones and embedded devices</li>
</ul>
</section>
<section id="other-architectures-in-computer-vision" class="level4" data-number="5.1.5">
<h4 data-number="5.1.5" class="anchored" data-anchor-id="other-architectures-in-computer-vision"><span class="header-section-number">5.1.5</span> Other Architectures in Computer Vision</h4>
<p>CNNs are still in use today (2025). Meanwhile, there are alternative architectures, such as transformer-based models (<span class="citation" data-cites="dosovitskiy_image_2020">Dosovitskiy et al. (<a href="#ref-dosovitskiy_image_2020" role="doc-biblioref">2020</a>)</span>), which are extremely successful in language modeling, or multilayer perceptron-based architectures (<span class="citation" data-cites="liu_pay_2021">Liu et al. (<a href="#ref-liu_pay_2021" role="doc-biblioref">2021</a>)</span>).</p>
<p>The question of which architecture type will prevail is open. Currently, it seems that both CNNs and transformer-based models have their place, and often perform similarly (<span class="citation" data-cites="smith_convnets_2023">Smith et al. (<a href="#ref-smith_convnets_2023" role="doc-biblioref">2023</a>)</span>). Furthermore, hybrid-architectures, combining the strengths of CNNs and transformers have been proposed <span class="citation" data-cites="vasu_fastvit_2023">Vasu et al. (<a href="#ref-vasu_fastvit_2023" role="doc-biblioref">2023</a>)</span>. Some works use both architecture types, depending on whether parameter efficiency (CNNs) or raw performance (transformers) are prioritized <span class="citation" data-cites="simeoni_dinov3_2025">Siméoni et al. (<a href="#ref-simeoni_dinov3_2025" role="doc-biblioref">2025</a>)</span>.</p>
</section>
</section>
</section>
<section id="references" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="references"><span class="header-section-number">6</span> References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-breitenmoser-wursten_projekt_2024" class="csl-entry" role="listitem">
Breitenmoser-Würsten, Christine, Fridolin Zimmermann, Kristina Vogt, Manuela von Arx, Sven Signe, and Christian Stauffer. 2024. <span>“Das <span>Projekt</span> <span>LUNO</span> - <span>Abschlussbericht</span>.”</span> 121. Stiftung KORA. <a href="https://portal.issn.org/#">https://portal.issn.org/#</a>.
</div>
<div id="ref-dalal_histograms_2005" class="csl-entry" role="listitem">
Dalal, N., and B. Triggs. 2005. <span>“Histograms of <span>Oriented</span> <span>Gradients</span> for <span>Human</span> <span>Detection</span>.”</span> In <em>2005 <span>IEEE</span> <span>Computer</span> <span>Society</span> <span>Conference</span> on <span>Computer</span> <span>Vision</span> and <span>Pattern</span> <span>Recognition</span> (<span>CVPR</span>’05)</em>, 1:886–93. San Diego, CA, USA: IEEE. <a href="https://doi.org/10.1109/CVPR.2005.177">https://doi.org/10.1109/CVPR.2005.177</a>.
</div>
<div id="ref-deng_imagenet_2009" class="csl-entry" role="listitem">
Deng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. <span>“<span>ImageNet</span>: <span>A</span> Large-Scale Hierarchical Image Database.”</span> In <em>2009 <span>IEEE</span> <span>Conference</span> on <span>Computer</span> <span>Vision</span> and <span>Pattern</span> <span>Recognition</span></em>, 248–55. Miami, FL: IEEE. <a href="https://doi.org/10.1109/CVPR.2009.5206848">https://doi.org/10.1109/CVPR.2009.5206848</a>.
</div>
<div id="ref-dosovitskiy_image_2020" class="csl-entry" role="listitem">
Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2020. <span>“An <span>Image</span> Is <span>Worth</span> 16x16 <span>Words</span>: <span>Transformers</span> for <span>Image</span> <span>Recognition</span> at <span>Scale</span>.”</span> <em>arXiv:2010.11929 [Cs]</em>, October. <a href="http://arxiv.org/abs/2010.11929">http://arxiv.org/abs/2010.11929</a>.
</div>
<div id="ref-everingham_pascal_2007" class="csl-entry" role="listitem">
Everingham, M., L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. 2007. <span>“The <span>PASCAL</span> <span>Visual</span> <span>Object</span> <span>Classes</span> <span>Challenge</span> 2007 (<span>VOC2007</span>) <span>Results</span>.”</span> <a href="http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html">http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html</a>.
</div>
<div id="ref-fukushima_neocognitron_1980" class="csl-entry" role="listitem">
Fukushima, Kunihiko. 1980. <span>“Neocognitron: <span>A</span> Self-Organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position.”</span> <em>Biological Cybernetics</em> 36 (4): 193–202. <a href="https://doi.org/10.1007/BF00344251">https://doi.org/10.1007/BF00344251</a>.
</div>
<div id="ref-hani_minneapple_2020" class="csl-entry" role="listitem">
Häni, Nicolai, Pravakar Roy, and Volkan Isler. 2020. <span>“<span>MinneApple</span>: <span>A</span> <span>Benchmark</span> <span>Dataset</span> for <span>Apple</span> <span>Detection</span> and <span>Segmentation</span>.”</span> <em>IEEE Robotics and Automation Letters</em> 5 (2): 852–58. <a href="https://doi.org/10.1109/LRA.2020.2965061">https://doi.org/10.1109/LRA.2020.2965061</a>.
</div>
<div id="ref-he_mask_2018" class="csl-entry" role="listitem">
He, Kaiming, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. 2018. <span>“Mask <span>R</span>-<span>CNN</span>.”</span> arXiv. <a href="http://arxiv.org/abs/1703.06870">http://arxiv.org/abs/1703.06870</a>.
</div>
<div id="ref-hubel_receptive_1959" class="csl-entry" role="listitem">
Hubel, D. H., and T. N. Wiesel. 1959. <span>“Receptive Fields of Single Neurones in the Cat’s Striate Cortex.”</span> <em>The Journal of Physiology</em> 148 (3): 574–91. <a href="https://doi.org/10.1113/jphysiol.1959.sp006308">https://doi.org/10.1113/jphysiol.1959.sp006308</a>.
</div>
<div id="ref-image_to_image_isola2018" class="csl-entry" role="listitem">
Isola, Phillip, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. 2018. <span>“Image-to-<span>Image</span> <span>Translation</span> with <span>Conditional</span> <span>Adversarial</span> <span>Networks</span>.”</span> arXiv. <a href="http://arxiv.org/abs/1611.07004">http://arxiv.org/abs/1611.07004</a>.
</div>
<div id="ref-johnson_eecs_2022" class="csl-entry" role="listitem">
Johnson, Justin. 2022. <span>“<span>EECS</span> 498.008 / 598.008 <span>Deep</span> <span>Learning</span> for <span>Computer</span> <span>Vision</span>.”</span> Lecture {Notes} / {Slides}. <a href="https://web.eecs.umich.edu/~justincj/teaching/eecs498/WI2022/">https://web.eecs.umich.edu/~justincj/teaching/eecs498/WI2022/</a>.
</div>
<div id="ref-karaev_cotracker_2024" class="csl-entry" role="listitem">
Karaev, Nikita, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. 2024. <span>“<span>CoTracker</span>: <span>It</span> Is <span>Better</span> to <span>Track</span> <span>Together</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2307.07635">https://doi.org/10.48550/arXiv.2307.07635</a>.
</div>
<div id="ref-karras_alias-free_2021" class="csl-entry" role="listitem">
Karras, Tero, Miika Aittala, Samuli Laine, Erik Härkönen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. 2021. <span>“Alias-<span>Free</span> <span>Generative</span> <span>Adversarial</span> <span>Networks</span>.”</span> arXiv. <a href="http://arxiv.org/abs/2106.12423">http://arxiv.org/abs/2106.12423</a>.
</div>
<div id="ref-kerbl_3d_2023" class="csl-entry" role="listitem">
Kerbl, Bernhard, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 2023. <span>“<span>3D</span> <span>Gaussian</span> <span>Splatting</span> for <span>Real</span>-<span>Time</span> <span>Radiance</span> <span>Field</span> <span>Rendering</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2308.04079">https://doi.org/10.48550/arXiv.2308.04079</a>.
</div>
<div id="ref-krizhevsky_imagenet_2012" class="csl-entry" role="listitem">
Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. <span>“<span>ImageNet</span> <span>Classification</span> with <span>Deep</span> <span>Convolutional</span> <span>Neural</span> <span>Networks</span>.”</span> In <em>Advances in <span>Neural</span> <span>Information</span> <span>Processing</span> <span>Systems</span></em>, edited by F. Pereira, C. J. Burges, L. Bottou, and K. Q. Weinberger. Vol. 25. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf</a>.
</div>
<div id="ref-lecun_gradient-based_1998" class="csl-entry" role="listitem">
Lecun, Y., L. Bottou, Y. Bengio, and P. Haffner. 1998. <span>“Gradient-Based Learning Applied to Document Recognition.”</span> <em>Proceedings of the IEEE</em> 86 (11): 2278–2324. <a href="https://doi.org/10.1109/5.726791">https://doi.org/10.1109/5.726791</a>.
</div>
<div id="ref-liu_pay_2021" class="csl-entry" role="listitem">
Liu, Hanxiao, Zihang Dai, David R. So, and Quoc V. Le. 2021. <span>“Pay <span>Attention</span> to <span>MLPs</span>.”</span> <em>arXiv:2105.08050 [Cs]</em>, June. <a href="http://arxiv.org/abs/2105.08050">http://arxiv.org/abs/2105.08050</a>.
</div>
<div id="ref-ma_segment_2024" class="csl-entry" role="listitem">
Ma, Jun, Yuting He, Feifei Li, Lin Han, Chenyu You, and Bo Wang. 2024. <span>“Segment Anything in Medical Images.”</span> <em>Nature Communications</em> 15 (1): 654. <a href="https://doi.org/10.1038/s41467-024-44824-z">https://doi.org/10.1038/s41467-024-44824-z</a>.
</div>
<div id="ref-mildenhall_nerf_2020" class="csl-entry" role="listitem">
Mildenhall, Ben, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. 2020. <span>“<span>NeRF</span>: <span>Representing</span> <span>Scenes</span> as <span>Neural</span> <span>Radiance</span> <span>Fields</span> for <span>View</span> <span>Synthesis</span>.”</span> arXiv. <a href="http://arxiv.org/abs/2003.08934">http://arxiv.org/abs/2003.08934</a>.
</div>
<div id="ref-pan_drag_2023" class="csl-entry" role="listitem">
Pan, Xingang, Ayush Tewari, Thomas Leimkühler, Lingjie Liu, Abhimitra Meka, and Christian Theobalt. 2023. <span>“Drag <span>Your</span> <span>GAN</span>: <span>Interactive</span> <span>Point</span>-Based <span>Manipulation</span> on the <span>Generative</span> <span>Image</span> <span>Manifold</span>.”</span> arXiv. <a href="http://arxiv.org/abs/2305.10973">http://arxiv.org/abs/2305.10973</a>.
</div>
<div id="ref-perronnin_large-scale_2010" class="csl-entry" role="listitem">
Perronnin, Florent, Yan Liu, Jorge Sanchez, and Herve Poirier. 2010. <span>“Large-Scale Image Retrieval with Compressed <span>Fisher</span> Vectors.”</span> In <em>2010 <span>IEEE</span> <span>Computer</span> <span>Society</span> <span>Conference</span> on <span>Computer</span> <span>Vision</span> and <span>Pattern</span> <span>Recognition</span></em>, 3384–91. San Francisco, CA, USA: IEEE. <a href="https://doi.org/10.1109/CVPR.2010.5540009">https://doi.org/10.1109/CVPR.2010.5540009</a>.
</div>
<div id="ref-Redmon2016a" class="csl-entry" role="listitem">
Redmon, Joseph, Santosh Divvala, Ross Girshick, and Ali Farhadi. 2016. <span>“You Only Look Once: <span>Unified</span>, Real-Time Object Detection.”</span> <em>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</em> 2016-Decem: 779–88. <a href="https://doi.org/10.1109/CVPR.2016.91">https://doi.org/10.1109/CVPR.2016.91</a>.
</div>
<div id="ref-rombach_high-resolution_2022" class="csl-entry" role="listitem">
Rombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. <span>“High-<span>Resolution</span> <span>Image</span> <span>Synthesis</span> with <span>Latent</span> <span>Diffusion</span> <span>Models</span>.”</span> arXiv. <a href="http://arxiv.org/abs/2112.10752">http://arxiv.org/abs/2112.10752</a>.
</div>
<div id="ref-rumelhart_learning_1986" class="csl-entry" role="listitem">
Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. 1986. <span>“Learning Representations by Back-Propagating Errors.”</span> <em>Nature</em> 323 (6088): 533–36. <a href="https://doi.org/10.1038/323533a0">https://doi.org/10.1038/323533a0</a>.
</div>
<div id="ref-russakovsky_imagenet_2015" class="csl-entry" role="listitem">
Russakovsky, Olga, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, et al. 2015. <span>“<span>ImageNet</span> <span>Large</span> <span>Scale</span> <span>Visual</span> <span>Recognition</span> <span>Challenge</span>.”</span> arXiv. <a href="http://arxiv.org/abs/1409.0575">http://arxiv.org/abs/1409.0575</a>.
</div>
<div id="ref-simeoni_dinov3_2025" class="csl-entry" role="listitem">
Siméoni, Oriane, Huy V. Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, et al. 2025. <span>“<span>DINOv3</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2508.10104">https://doi.org/10.48550/arXiv.2508.10104</a>.
</div>
<div id="ref-smith_convnets_2023" class="csl-entry" role="listitem">
Smith, Samuel L., Andrew Brock, Leonard Berrada, and Soham De. 2023. <span>“<span>ConvNets</span> <span>Match</span> <span>Vision</span> <span>Transformers</span> at <span>Scale</span>.”</span> arXiv. <a href="http://arxiv.org/abs/2310.16764">http://arxiv.org/abs/2310.16764</a>.
</div>
<div id="ref-torralba_using_2010" class="csl-entry" role="listitem">
Torralba, Antonio, Kevin P Murphy, and William T Freeman. 2010. <span>“Using the Forest to See the Trees: Exploiting Context for Visual Object Detection and Localization.”</span> <em>Communications of the ACM</em> 53 (3): 107–14.
</div>
<div id="ref-vasu_fastvit_2023" class="csl-entry" role="listitem">
Vasu, Pavan Kumar Anasosalu, James Gabriel, Jeff Zhu, Oncel Tuzel, and Anurag Ranjan. 2023. <span>“<span>FastViT</span>: <span>A</span> <span>Fast</span> <span>Hybrid</span> <span>Vision</span> <span>Transformer</span> Using <span>Structural</span> <span>Reparameterization</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2303.14189">https://doi.org/10.48550/arXiv.2303.14189</a>.
</div>
<div id="ref-Images2018" class="csl-entry" role="listitem">
Willi, Marco, Ross T. Pitman, Anabelle W. Cardoso, Christina Locke, Alexandra Swanson, Amy Boyer, Marten Veldthuis, and Lucy Fortson. 2019. <span>“Identifying Animal Species in Camera Trap Images Using Deep Learning and Citizen Science.”</span> Edited by Oscar Gaggiotti. <em>Methods in Ecology and Evolution</em> 10 (1): 80–91. <a href="https://doi.org/10.1111/2041-210X.13099">https://doi.org/10.1111/2041-210X.13099</a>.
</div>
</div>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/marco-willi\.github\.io\/cas-dl-compvis-lectures-hs2025\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../pages/background/frameworks.html" class="pagination-link" aria-label="3 - Software &amp; Hardware for Deep Learning">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">3 - Software &amp; Hardware for Deep Learning</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../pages/lectures/cnns.html" class="pagination-link" aria-label="2 - Convolutional Neural Networks">
        <span class="nav-page-text">2 - Convolutional Neural Networks</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "1 - Introduction"</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="an">params:</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co">   images_path: "/assets/images/intro/"</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title="Learning Objectives"  collapse="true"}</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>After this lecture you should be able to:</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Distinguish core computer vision tasks.</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Describe key real-world application domains and practical challenges.</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Explain why deep learning reshaped computer vision.</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title="TLDR Recap" collapse="true"}</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>**Computer Vision Tasks:**</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Classification**: Assign images to predefined classes</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Detection**: Locate and recognize multiple objects with bounding boxes</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Segmentation**: Assign each pixel to objects (instance) or classes (semantic)</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Generation**: Transform or create images (translation, synthesis, super-resolution, text-to-image)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Keypoint Detection**: Localize specific points (e.g., human joints)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>**Key Challenges:**</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Semantic gap (pixels → meaning)</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Viewpoint, illumination, occlusion variations</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Intra-class variation and background clutter</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Context dependence</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>**Classical vs. Modern Approaches:**</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Classical CV**: Hand-crafted features (HOG, SIFT, color histograms) + ML classifiers</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Limited by manual feature design, poor scalability</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Deep Learning**: End-to-end learned hierarchical representations</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Automatic feature extraction, better scaling with data</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>**Historical Milestones:**</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**1959**: Hubel &amp; Wiesel - receptive fields in visual cortex</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**1980**: Neocognitron - hierarchical neural network for vision</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**1986**: Backpropagation - training algorithm for neural networks</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**1998**: LeNet - first successful CNN for digit recognition</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**2012**: AlexNet/ImageNet breakthrough - 9.4% error reduction, marked the deep learning revolution</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**2014-2015**: VGGNet (depth), ResNet (skip connections) - enabled very deep networks</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**2020s**: Transformers and hybrid architectures competing with CNNs</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>**Why Deep Learning Won:**</span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Large-scale datasets (ImageNet: 1.4M images, 1K classes)</span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>GPU computing power (parallel processing)</span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Better architectures (CNNs exploit 2D structure)</span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>End-to-end optimization (learn features + classifier together)</span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a>**Applications:**</span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Autonomous driving, medical imaging, facial recognition</span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Photo editing, image generation, agricultural monitoring</span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Embedded systems (mobile chips for on-device inference)</span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a><span class="fu">## Motivation</span></span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a>Deep Learning models have played a transformative role in _Computer Vision_ over the last decade, significantly enhancing and expanding the capabilities to process visual data.</span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a>The integration of deep learning in computer vision has massively improved the accuracy and efficiency of visual recognition, classification, and analysis. This has opened up new possibilities in applications such as automated driving, facial recognition, and medical image analysis. These models are now embedded in a wide variety of products and services.</span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a>Let’s take a look at several applications of deep learning assisted computer vision applications.</span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a><span class="fu">### Applications</span></span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a>To monitor wildlife populations biologists might deploy camera traps to capture images from passing animals (see @fig-intro-kora-lynx). These images need to be analysed, for example, biologists need to know which species can be seen in each image. Deep learning can speed up that process significantly (@Images2018).</span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-kora-lynx}</span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a><span class="al">![Source: @breitenmoser-wursten_projekt_2024]({{&lt; meta params.images_path &gt;}}kora_lynx.png)</span>{width=600}</span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-83"><a href="#cb2-83" aria-hidden="true" tabindex="-1"></a>Image from a camera trap.</span>
<span id="cb2-84"><a href="#cb2-84" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-85"><a href="#cb2-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-86"><a href="#cb2-86" aria-hidden="true" tabindex="-1"></a>Recent advances in generative deep learning enable anyone to create photorealistic synthetic images (see @fig-intro-synthetic-images for examples). While these technologies have many positive applications (art, design, prototyping), they also raise concerns about misinformation and deepfakes, motivating research into synthetic media detection.</span>
<span id="cb2-87"><a href="#cb2-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-88"><a href="#cb2-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-89"><a href="#cb2-89" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-synthetic-images layout-ncol=2}</span>
<span id="cb2-90"><a href="#cb2-90" aria-hidden="true" tabindex="-1"></a>![<span class="co">[</span><span class="ot">Source</span><span class="co">](https://x.com/TheInfiniteDude/status/1637211568692932608)</span>]({{&lt; meta params.images_path &gt;}}sid_trump.jpg)</span>
<span id="cb2-91"><a href="#cb2-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-92"><a href="#cb2-92" aria-hidden="true" tabindex="-1"></a>![<span class="co">[</span><span class="ot">Source</span><span class="co">](https://x.com/cryptomattk/status/1687908457880367104)</span>]({{&lt; meta params.images_path &gt;}}sid_pope.jpg)</span>
<span id="cb2-93"><a href="#cb2-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-94"><a href="#cb2-94" aria-hidden="true" tabindex="-1"></a>Examples of synthetic images.</span>
<span id="cb2-95"><a href="#cb2-95" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-96"><a href="#cb2-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-97"><a href="#cb2-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-98"><a href="#cb2-98" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ![Camera Trap Image Analysis]({{&lt; meta params.images_path &gt;}}camtrap.jpg){width=400} --&gt;</span></span>
<span id="cb2-99"><a href="#cb2-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-100"><a href="#cb2-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-101"><a href="#cb2-101" aria-hidden="true" tabindex="-1"></a>@fig-intro-google-lens shows an application in which a model identifies objects of interest in photographs, as well as searching additional information about it. Photos can also be used for translation tasks whereby models detect letters (optical character recognition - OCR) and then translate it (using language models in conjunction with visual models).</span>
<span id="cb2-102"><a href="#cb2-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-103"><a href="#cb2-103" aria-hidden="true" tabindex="-1"></a>:::: {.columns #fig-intro-google-lens}</span>
<span id="cb2-104"><a href="#cb2-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-105"><a href="#cb2-105" aria-hidden="true" tabindex="-1"></a>::: {.column width="45%"}</span>
<span id="cb2-106"><a href="#cb2-106" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Identification &amp; Search</span><span class="co">]</span>({{&lt; meta params.images_path &gt;}}google_lens_classification.png</span>
<span id="cb2-107"><a href="#cb2-107" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-108"><a href="#cb2-108" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-109"><a href="#cb2-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-110"><a href="#cb2-110" aria-hidden="true" tabindex="-1"></a>::: {.column width="45%"}</span>
<span id="cb2-111"><a href="#cb2-111" aria-hidden="true" tabindex="-1"></a><span class="al">![Translation]({{&lt; meta params.images_path &gt;}}google_lens_ocr.png)</span></span>
<span id="cb2-112"><a href="#cb2-112" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-113"><a href="#cb2-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-114"><a href="#cb2-114" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Google Lens</span><span class="co">](https://search.google/ways-to-search/lens/)</span></span>
<span id="cb2-115"><a href="#cb2-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-116"><a href="#cb2-116" aria-hidden="true" tabindex="-1"></a>::::</span>
<span id="cb2-117"><a href="#cb2-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-118"><a href="#cb2-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-119"><a href="#cb2-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-120"><a href="#cb2-120" aria-hidden="true" tabindex="-1"></a>One area with high commercial interest and competition is the development of self-driving capabilities. @fig-intro-self-driving shows a demonstration of the perception system required for autonomous driving, including detecting vehicles, pedestrians, cyclists, and understanding lane markings and traffic signs in real-time.</span>
<span id="cb2-121"><a href="#cb2-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-122"><a href="#cb2-122" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-self-driving}</span>
<span id="cb2-123"><a href="#cb2-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-124"><a href="#cb2-124" aria-hidden="true" tabindex="-1"></a>{{&lt; video https://storage.googleapis.com/waymo-uploads/files/site-animations/waymo-driver/cameras.webm width=800 &gt;}}</span>
<span id="cb2-125"><a href="#cb2-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-126"><a href="#cb2-126" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Example from Waymo</span><span class="co">](https://waymo.com/waymo-driver/)</span>. Observe how the system simultaneously tracks multiple objects.</span>
<span id="cb2-127"><a href="#cb2-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-128"><a href="#cb2-128" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-129"><a href="#cb2-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-130"><a href="#cb2-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-131"><a href="#cb2-131" aria-hidden="true" tabindex="-1"></a>Face recognition technology, such as Apple's Face ID (@fig-intro-face-id), has become ubiquitous in consumer devices. The system uses infrared depth sensing to create a 3D map of facial features, making it more secure than 2D image-based recognition and robust to different lighting conditions.</span>
<span id="cb2-132"><a href="#cb2-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-133"><a href="#cb2-133" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-face-id}</span>
<span id="cb2-134"><a href="#cb2-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-135"><a href="#cb2-135" aria-hidden="true" tabindex="-1"></a>{{&lt; video https://www.youtube.com/embed/z-t1h0Y8vuM?si=qnEOYDmqyv8zGvMV start="50" width=600 height=400 &gt;}}</span>
<span id="cb2-136"><a href="#cb2-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-137"><a href="#cb2-137" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Example from Apple Face ID</span><span class="co">](https://support.apple.com/en-us/102381)</span>. Watch how the infrared dot projector creates a depth map of the face.</span>
<span id="cb2-138"><a href="#cb2-138" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-139"><a href="#cb2-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-140"><a href="#cb2-140" aria-hidden="true" tabindex="-1"></a>Computer vision has also been very useful in agricultur. @fig-intro-minneapple shows an application where apples can be counted from image data. Furthermore, drones might be used to estimate crop yield, type and health.</span>
<span id="cb2-141"><a href="#cb2-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-142"><a href="#cb2-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-143"><a href="#cb2-143" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-minneapple}</span>
<span id="cb2-144"><a href="#cb2-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-145"><a href="#cb2-145" aria-hidden="true" tabindex="-1"></a><span class="al">![ ]({{&lt; meta params.images_path &gt;}}minneapple.png)</span>{width=600}</span>
<span id="cb2-146"><a href="#cb2-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-147"><a href="#cb2-147" aria-hidden="true" tabindex="-1"></a>Example from @hani_minneapple_2020</span>
<span id="cb2-148"><a href="#cb2-148" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-149"><a href="#cb2-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-150"><a href="#cb2-150" aria-hidden="true" tabindex="-1"></a>Deep learning has also been applied extensively in the medical area. In particular, many imaging technologies, such as MRI or CT, can be analysed with computer vision techniques.  @fig-intro-sam illustrates a generic segmentation model which can be used to identify organs and other anatomical structures on pixel-level.</span>
<span id="cb2-151"><a href="#cb2-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-152"><a href="#cb2-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-153"><a href="#cb2-153" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-sam width=400}</span>
<span id="cb2-154"><a href="#cb2-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-155"><a href="#cb2-155" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}medsam.png)</span>{width=600}</span>
<span id="cb2-156"><a href="#cb2-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-157"><a href="#cb2-157" aria-hidden="true" tabindex="-1"></a>Example from @ma_segment_2024.</span>
<span id="cb2-158"><a href="#cb2-158" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-159"><a href="#cb2-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-160"><a href="#cb2-160" aria-hidden="true" tabindex="-1"></a>Photo-editing has been brought to new levels of sophistication with AI-based capabilities.  @fig-intro-magic shows an impressive example from the Google Magic Editor. Maybe even a bit too good?</span>
<span id="cb2-161"><a href="#cb2-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-162"><a href="#cb2-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-163"><a href="#cb2-163" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-magic}</span>
<span id="cb2-164"><a href="#cb2-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-165"><a href="#cb2-165" aria-hidden="true" tabindex="-1"></a>{{&lt; video https://storage.googleapis.com/gweb-mobius-cdn/photos/uploads/6e54ed750f84538fd052b31818127f1e4df5711c.compressed.mp4 width=800 &gt;}}</span>
<span id="cb2-166"><a href="#cb2-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-167"><a href="#cb2-167" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Example from Google Magic Editor</span><span class="co">](https://www.google.com/intl/en/photos/editing/)</span></span>
<span id="cb2-168"><a href="#cb2-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-169"><a href="#cb2-169" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-170"><a href="#cb2-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-171"><a href="#cb2-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-172"><a href="#cb2-172" aria-hidden="true" tabindex="-1"></a>::: {.callout-note appearance="simple"}</span>
<span id="cb2-173"><a href="#cb2-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-174"><a href="#cb2-174" aria-hidden="true" tabindex="-1"></a>**🤔 Think About It**</span>
<span id="cb2-175"><a href="#cb2-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-176"><a href="#cb2-176" aria-hidden="true" tabindex="-1"></a>What computer vision tasks do you think the model in @fig-intro-magic must perform to enable this editing capability?</span>
<span id="cb2-177"><a href="#cb2-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-178"><a href="#cb2-178" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb2-179"><a href="#cb2-179" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Possible tasks involved<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb2-180"><a href="#cb2-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-181"><a href="#cb2-181" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Object detection and segmentation (identifying what to remove/move)</span>
<span id="cb2-182"><a href="#cb2-182" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Inpainting (filling in removed areas)</span>
<span id="cb2-183"><a href="#cb2-183" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Style transfer (matching lighting/texture)</span>
<span id="cb2-184"><a href="#cb2-184" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Scene understanding (maintaining realistic perspective)</span>
<span id="cb2-185"><a href="#cb2-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-186"><a href="#cb2-186" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb2-187"><a href="#cb2-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-188"><a href="#cb2-188" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-189"><a href="#cb2-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-190"><a href="#cb2-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-191"><a href="#cb2-191" aria-hidden="true" tabindex="-1"></a>The increasingly better models and the ability to run them quickly and resource-efficiently on mobile devices have enabled such applications. @fig-intro-tensor-phone shows that special chips have been developed to process images with Deep Learning models quickly.</span>
<span id="cb2-192"><a href="#cb2-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-193"><a href="#cb2-193" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-tensor-phone}</span>
<span id="cb2-194"><a href="#cb2-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-195"><a href="#cb2-195" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}tensor_phone.png)</span>{width=600}</span>
<span id="cb2-196"><a href="#cb2-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-197"><a href="#cb2-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-198"><a href="#cb2-198" aria-hidden="true" tabindex="-1"></a>From <span class="co">[</span><span class="ot">Link</span><span class="co">](https://store.google.com/intl/en/ideas/pixel-camera-features/)</span>.</span>
<span id="cb2-199"><a href="#cb2-199" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-200"><a href="#cb2-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-201"><a href="#cb2-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-202"><a href="#cb2-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-203"><a href="#cb2-203" aria-hidden="true" tabindex="-1"></a><span class="fu">## Computer Vision Tasks</span></span>
<span id="cb2-204"><a href="#cb2-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-205"><a href="#cb2-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-206"><a href="#cb2-206" aria-hidden="true" tabindex="-1"></a><span class="fu">### Image Classification</span></span>
<span id="cb2-207"><a href="#cb2-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-208"><a href="#cb2-208" aria-hidden="true" tabindex="-1"></a>In image classification, an image is assigned to a predefined set of classes. In multi-class classification, there are $\gt 2$ classes; in binary classification, there are exactly 2 classes; and in multi-label classification, there are multiple sets of classes. @fig-intro-image_clssifier shows an example of multi-class classification.</span>
<span id="cb2-209"><a href="#cb2-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-210"><a href="#cb2-210" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-image_clssifier width=600}</span>
<span id="cb2-211"><a href="#cb2-211" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}image_classification_example.png)</span>{width=600}</span>
<span id="cb2-212"><a href="#cb2-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-213"><a href="#cb2-213" aria-hidden="true" tabindex="-1"></a>Multi-Class Image Classification example: a model predicts the most likely class for an image along with confidence scores (@krizhevsky_imagenet_2012).</span>
<span id="cb2-214"><a href="#cb2-214" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-215"><a href="#cb2-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-216"><a href="#cb2-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-217"><a href="#cb2-217" aria-hidden="true" tabindex="-1"></a><span class="fu">### Object Detection</span></span>
<span id="cb2-218"><a href="#cb2-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-219"><a href="#cb2-219" aria-hidden="true" tabindex="-1"></a>Object detection involves locating and recognizing multiple objects in an image. @fig-intro-yolo-example shows an example from the paper by @Redmon2016a. Each object is localized with a bounding box and assigned an object class with a confidence score.</span>
<span id="cb2-220"><a href="#cb2-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-221"><a href="#cb2-221" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-yolo-example}</span>
<span id="cb2-222"><a href="#cb2-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-223"><a href="#cb2-223" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}yolo_object_detection_example.png)</span>{width=600}</span>
<span id="cb2-224"><a href="#cb2-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-225"><a href="#cb2-225" aria-hidden="true" tabindex="-1"></a>Object Detection example (from @Redmon2016a). Bounding boxes localize the objects, showing the predicted class and confidence for each detection.</span>
<span id="cb2-226"><a href="#cb2-226" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-227"><a href="#cb2-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-228"><a href="#cb2-228" aria-hidden="true" tabindex="-1"></a><span class="fu">### Segmentation</span></span>
<span id="cb2-229"><a href="#cb2-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-230"><a href="#cb2-230" aria-hidden="true" tabindex="-1"></a>In segmentation, individual pixels are assigned to specific objects (instance segmentation) or classes (semantic segmentation). @fig-mask-rcnn-object-segmentation-example shows an example of instance segmentation from the paper by @he_mask_2018, where individual objects are detected and precisely localized at the pixel level.</span>
<span id="cb2-231"><a href="#cb2-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-232"><a href="#cb2-232" aria-hidden="true" tabindex="-1"></a>::: {#fig-mask-rcnn-object-segmentation-example}</span>
<span id="cb2-233"><a href="#cb2-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-234"><a href="#cb2-234" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}mask_rcnn_object_segmentation_example.png)</span>{width=600}</span>
<span id="cb2-235"><a href="#cb2-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-236"><a href="#cb2-236" aria-hidden="true" tabindex="-1"></a>Instance Segmentation example (from @he_mask_2018). Each person is individually segmented with pixel-level masks.</span>
<span id="cb2-237"><a href="#cb2-237" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-238"><a href="#cb2-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-239"><a href="#cb2-239" aria-hidden="true" tabindex="-1"></a>The following video shows an example of semantic segmentation:</span>
<span id="cb2-240"><a href="#cb2-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-241"><a href="#cb2-241" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-semantic-segmentation}</span>
<span id="cb2-242"><a href="#cb2-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-243"><a href="#cb2-243" aria-hidden="true" tabindex="-1"></a>{{&lt; video https://www.youtube.com/embed/wfObVKKKJkE?si=GZ6m6vig54Z8XVie width=600 height=350 &gt;}}</span>
<span id="cb2-244"><a href="#cb2-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-245"><a href="#cb2-245" aria-hidden="true" tabindex="-1"></a>Semantic segmentation applied to aerial imagery of agricultural fields. Each pixel is classified into crop types, field boundaries, and other land categories.</span>
<span id="cb2-246"><a href="#cb2-246" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-247"><a href="#cb2-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-248"><a href="#cb2-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-249"><a href="#cb2-249" aria-hidden="true" tabindex="-1"></a><span class="fu">### Keypoint Detection</span></span>
<span id="cb2-250"><a href="#cb2-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-251"><a href="#cb2-251" aria-hidden="true" tabindex="-1"></a>In keypoint detection, specific points (keypoints) on objects—typically human body joints—are localized. @fig-mask-rcnn-keypoint-detection-example shows an example of keypoint detection from the paper by @he_mask_2018, where individual people are detected and their joints localized. This can be used to transfer movements from a person to an avatar or to recognize human activities (action recognition).</span>
<span id="cb2-252"><a href="#cb2-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-253"><a href="#cb2-253" aria-hidden="true" tabindex="-1"></a>::: {#fig-mask-rcnn-keypoint-detection-example}</span>
<span id="cb2-254"><a href="#cb2-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-255"><a href="#cb2-255" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}mask_rcnn_keypoint_detection_example.png)</span>{width=600}</span>
<span id="cb2-256"><a href="#cb2-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-257"><a href="#cb2-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-258"><a href="#cb2-258" aria-hidden="true" tabindex="-1"></a>Keypoint Detection example (from @he_mask_2018).</span>
<span id="cb2-259"><a href="#cb2-259" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-260"><a href="#cb2-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-261"><a href="#cb2-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-262"><a href="#cb2-262" aria-hidden="true" tabindex="-1"></a><span class="fu">### Image Generation</span></span>
<span id="cb2-263"><a href="#cb2-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-264"><a href="#cb2-264" aria-hidden="true" tabindex="-1"></a>There are various applications where models transform input images into specific output images (image-to-image translation) or generate completely new images from scratch (unconditional generation). Below are example applications.</span>
<span id="cb2-265"><a href="#cb2-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-266"><a href="#cb2-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-267"><a href="#cb2-267" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Image Manipulation</span></span>
<span id="cb2-268"><a href="#cb2-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-269"><a href="#cb2-269" aria-hidden="true" tabindex="-1"></a>Interactive manipulation allows users to modify images by dragging points to reshape objects while maintaining realistic appearance.</span>
<span id="cb2-270"><a href="#cb2-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-271"><a href="#cb2-271" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-image-manipulation}</span>
<span id="cb2-272"><a href="#cb2-272" aria-hidden="true" tabindex="-1"></a>{{&lt; video https://vcai.mpi-inf.mpg.de/projects/DragGAN/data/DragGAN.mp4 width=800 &gt;}}</span>
<span id="cb2-273"><a href="#cb2-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-274"><a href="#cb2-274" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">DragGAN</span><span class="co">](https://vcai.mpi-inf.mpg.de/projects/DragGAN/)</span> by @pan_drag_2023</span>
<span id="cb2-275"><a href="#cb2-275" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-276"><a href="#cb2-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-277"><a href="#cb2-277" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Image Translation</span></span>
<span id="cb2-278"><a href="#cb2-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-279"><a href="#cb2-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-280"><a href="#cb2-280" aria-hidden="true" tabindex="-1"></a>@fig-intro-pix2pix-example shows an example of image generation from the paper by @image_to_image_isola2018, where images are generated conditioned on image inputs (translated). This can be used, for example, to convert a sketch of an object into a photorealistic depiction of it.</span>
<span id="cb2-281"><a href="#cb2-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-282"><a href="#cb2-282" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-pix2pix-example width=600}</span>
<span id="cb2-283"><a href="#cb2-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-284"><a href="#cb2-284" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}pix2pix_examples.png)</span></span>
<span id="cb2-285"><a href="#cb2-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-286"><a href="#cb2-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-287"><a href="#cb2-287" aria-hidden="true" tabindex="-1"></a>Image Generation example (from @image_to_image_isola2018).</span>
<span id="cb2-288"><a href="#cb2-288" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-289"><a href="#cb2-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-290"><a href="#cb2-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-291"><a href="#cb2-291" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Image Super Resolution</span></span>
<span id="cb2-292"><a href="#cb2-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-293"><a href="#cb2-293" aria-hidden="true" tabindex="-1"></a>In the gaming industry, Deep Learning is used to upscale low-resolution images to high-resolution in real-time (image super resolution), as shown in @fig-intro-dssl. This enables higher frame rates while maintaining visual quality.</span>
<span id="cb2-294"><a href="#cb2-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-295"><a href="#cb2-295" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-dssl  width=600}</span>
<span id="cb2-296"><a href="#cb2-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-297"><a href="#cb2-297" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}dssl.png)</span></span>
<span id="cb2-298"><a href="#cb2-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-299"><a href="#cb2-299" aria-hidden="true" tabindex="-1"></a>Nvidia DLSS (Deep Learning Super Sampling) improves gaming performance. <span class="co">[</span><span class="ot">Source</span><span class="co">](https://images.nvidia.com/aem-dam/Solutions/geforce/news/control-nvidia-dlss-2-0-update/deliver-us-the-moon-nvidia-dlss-2-0-performance-boost.png)</span></span>
<span id="cb2-300"><a href="#cb2-300" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-301"><a href="#cb2-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-302"><a href="#cb2-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-303"><a href="#cb2-303" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Image Colorization</span></span>
<span id="cb2-304"><a href="#cb2-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-305"><a href="#cb2-305" aria-hidden="true" tabindex="-1"></a>Image colorization can be learned with Deep Learning. This involves transforming a black-and-white image into an RGB image. @fig-intro-colorization-example shows an example.</span>
<span id="cb2-306"><a href="#cb2-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-307"><a href="#cb2-307" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-colorization-example width=600}</span>
<span id="cb2-308"><a href="#cb2-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-309"><a href="#cb2-309" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}colorization_example.png)</span>{width=600}</span>
<span id="cb2-310"><a href="#cb2-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-311"><a href="#cb2-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-312"><a href="#cb2-312" aria-hidden="true" tabindex="-1"></a>Norwegian Bride (est late 1890s) from DeOldify: <span class="co">[</span><span class="ot">Link</span><span class="co">](https://github.com/jantic/DeOldify)</span></span>
<span id="cb2-313"><a href="#cb2-313" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-314"><a href="#cb2-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-315"><a href="#cb2-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-316"><a href="#cb2-316" aria-hidden="true" tabindex="-1"></a><span class="fu">#### View Synthesis</span></span>
<span id="cb2-317"><a href="#cb2-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-318"><a href="#cb2-318" aria-hidden="true" tabindex="-1"></a>In view synthesis, new camera viewpoints of a scene are generated from a model. Neural Radiance Fields (NeRFs) are models that can generate novel views from a set of input images taken from known viewpoints. @fig-intro-nerf shows training data (input images from different angles) and the synthesized novel views the model can generate.</span>
<span id="cb2-319"><a href="#cb2-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-320"><a href="#cb2-320" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-nerf width=600}</span>
<span id="cb2-321"><a href="#cb2-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-322"><a href="#cb2-322" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}nerf.png)</span>{width=800}</span>
<span id="cb2-323"><a href="#cb2-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-324"><a href="#cb2-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-325"><a href="#cb2-325" aria-hidden="true" tabindex="-1"></a>Neural Radiance Fields - example (from @mildenhall_nerf_2020).</span>
<span id="cb2-326"><a href="#cb2-326" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-327"><a href="#cb2-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-328"><a href="#cb2-328" aria-hidden="true" tabindex="-1"></a>Modern variants can produce impressive quality, as shown in @fig-intro-gaussian-splatting.</span>
<span id="cb2-329"><a href="#cb2-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-330"><a href="#cb2-330" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-gaussian-splatting}</span>
<span id="cb2-331"><a href="#cb2-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-332"><a href="#cb2-332" aria-hidden="true" tabindex="-1"></a>{{&lt; video https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/content/videos/bicycle.mp4 width=800 &gt;}}</span>
<span id="cb2-333"><a href="#cb2-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-334"><a href="#cb2-334" aria-hidden="true" tabindex="-1"></a>3D Gaussian Splatting for real-time novel view synthesis @kerbl_3d_2023.</span>
<span id="cb2-335"><a href="#cb2-335" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-336"><a href="#cb2-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-337"><a href="#cb2-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-338"><a href="#cb2-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-339"><a href="#cb2-339" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Unconditional Generation</span></span>
<span id="cb2-340"><a href="#cb2-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-341"><a href="#cb2-341" aria-hidden="true" tabindex="-1"></a>In unconditional image generation, images are generated that resemble the training data distribution without specific input guidance. While you have no direct control over what specific image is generated, you can often manipulate latent variables to modify generated images or interpolate between samples. @fig-intro-stylegan3 shows photorealistic portraits generated by a model trained on human faces.</span>
<span id="cb2-342"><a href="#cb2-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-343"><a href="#cb2-343" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-stylegan3 width=400}</span>
<span id="cb2-344"><a href="#cb2-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-345"><a href="#cb2-345" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}stylegan3_examples.png)</span>{width=400}</span>
<span id="cb2-346"><a href="#cb2-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-347"><a href="#cb2-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-348"><a href="#cb2-348" aria-hidden="true" tabindex="-1"></a>StyleGan3 (from @karras_alias-free_2021).</span>
<span id="cb2-349"><a href="#cb2-349" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-350"><a href="#cb2-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-351"><a href="#cb2-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-352"><a href="#cb2-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-353"><a href="#cb2-353" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Text to Image</span></span>
<span id="cb2-354"><a href="#cb2-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-355"><a href="#cb2-355" aria-hidden="true" tabindex="-1"></a>@fig-stable-diffusion-example shows an example of image generation from the paper by @rombach_high-resolution_2022, where images are generated conditioned on text inputs.</span>
<span id="cb2-356"><a href="#cb2-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-357"><a href="#cb2-357" aria-hidden="true" tabindex="-1"></a>::: {#fig-stable-diffusion-example width=600}</span>
<span id="cb2-358"><a href="#cb2-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-359"><a href="#cb2-359" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}stable_diffusion_example.png)</span>{width=600}</span>
<span id="cb2-360"><a href="#cb2-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-361"><a href="#cb2-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-362"><a href="#cb2-362" aria-hidden="true" tabindex="-1"></a>Image Generation example (from @rombach_high-resolution_2022.)</span>
<span id="cb2-363"><a href="#cb2-363" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-364"><a href="#cb2-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-365"><a href="#cb2-365" aria-hidden="true" tabindex="-1"></a>On <span class="co">[</span><span class="ot">civitai</span><span class="co">](https://civitai.com/)</span>, there are numerous examples and models to admire or download.</span>
<span id="cb2-366"><a href="#cb2-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-367"><a href="#cb2-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-368"><a href="#cb2-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-369"><a href="#cb2-369" aria-hidden="true" tabindex="-1"></a><span class="fu">### Further tasks</span></span>
<span id="cb2-370"><a href="#cb2-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-371"><a href="#cb2-371" aria-hidden="true" tabindex="-1"></a>There are many more computer vision tasks. The following list is not exhaustive:</span>
<span id="cb2-372"><a href="#cb2-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-373"><a href="#cb2-373" aria-hidden="true" tabindex="-1"></a>:::: {.columns}</span>
<span id="cb2-374"><a href="#cb2-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-375"><a href="#cb2-375" aria-hidden="true" tabindex="-1"></a>::: {.column width="50%"}</span>
<span id="cb2-376"><a href="#cb2-376" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Image Classification</span>
<span id="cb2-377"><a href="#cb2-377" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Object Detection (and Tracking)</span>
<span id="cb2-378"><a href="#cb2-378" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Image Segmentation</span>
<span id="cb2-379"><a href="#cb2-379" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Semantic Segmentation</span>
<span id="cb2-380"><a href="#cb2-380" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Instance Segmentation</span>
<span id="cb2-381"><a href="#cb2-381" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Optical Character Recognition (OCR)</span>
<span id="cb2-382"><a href="#cb2-382" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Pose Estimation</span>
<span id="cb2-383"><a href="#cb2-383" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Facial Recognition</span>
<span id="cb2-384"><a href="#cb2-384" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Action Recognition</span>
<span id="cb2-385"><a href="#cb2-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-386"><a href="#cb2-386" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-387"><a href="#cb2-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-388"><a href="#cb2-388" aria-hidden="true" tabindex="-1"></a>::: {.column width="50%"}</span>
<span id="cb2-389"><a href="#cb2-389" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Image Generation</span>
<span id="cb2-390"><a href="#cb2-390" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Style Transfer</span>
<span id="cb2-391"><a href="#cb2-391" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Image Inpainting</span>
<span id="cb2-392"><a href="#cb2-392" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Super-Resolution</span>
<span id="cb2-393"><a href="#cb2-393" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Text-to-Image (and more)</span>
<span id="cb2-394"><a href="#cb2-394" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Image Captioning</span>
<span id="cb2-395"><a href="#cb2-395" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>3D Reconstruction</span>
<span id="cb2-396"><a href="#cb2-396" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Image Retrieval</span>
<span id="cb2-397"><a href="#cb2-397" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-398"><a href="#cb2-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-399"><a href="#cb2-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-400"><a href="#cb2-400" aria-hidden="true" tabindex="-1"></a>::::</span>
<span id="cb2-401"><a href="#cb2-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-402"><a href="#cb2-402" aria-hidden="true" tabindex="-1"></a><span class="fu">### What about videos?</span></span>
<span id="cb2-403"><a href="#cb2-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-404"><a href="#cb2-404" aria-hidden="true" tabindex="-1"></a>Many of the image-level tasks can be extended to videos. After all, videos are nothing but a sequence of images.</span>
<span id="cb2-405"><a href="#cb2-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-406"><a href="#cb2-406" aria-hidden="true" tabindex="-1"></a>The most recent impressive examples are from video generative models:</span>
<span id="cb2-407"><a href="#cb2-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-408"><a href="#cb2-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-409"><a href="#cb2-409" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-sora2}</span>
<span id="cb2-410"><a href="#cb2-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-411"><a href="#cb2-411" aria-hidden="true" tabindex="-1"></a>{{&lt; video https://openaiassets.blob.core.windows.net/$web/nf2/blog-final2/20250929_0337_New%20Video_simple_compose_01k6agr8ctey29mze208t29w38%20(1).mp4 width=800 &gt;}}</span>
<span id="cb2-412"><a href="#cb2-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-413"><a href="#cb2-413" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">OpenAI Sora 2</span><span class="co">](https://openai.com/index/sora-2/)</span> - Prompt: a man rides a horse which is on another horse</span>
<span id="cb2-414"><a href="#cb2-414" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-415"><a href="#cb2-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-416"><a href="#cb2-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-417"><a href="#cb2-417" aria-hidden="true" tabindex="-1"></a>But also object / keypoint tracking:</span>
<span id="cb2-418"><a href="#cb2-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-419"><a href="#cb2-419" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-cotracker}</span>
<span id="cb2-420"><a href="#cb2-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-421"><a href="#cb2-421" aria-hidden="true" tabindex="-1"></a>{{&lt; video https://co-tracker.github.io/videos/teaser/rollerblade.mp4 width=800 &gt;}}</span>
<span id="cb2-422"><a href="#cb2-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-423"><a href="#cb2-423" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">CoTracker</span><span class="co">](https://co-tracker.github.io/)</span> - Tracking points across video frames for motion analysis (@karaev_cotracker_2024)</span>
<span id="cb2-424"><a href="#cb2-424" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-425"><a href="#cb2-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-426"><a href="#cb2-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-427"><a href="#cb2-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-428"><a href="#cb2-428" aria-hidden="true" tabindex="-1"></a><span class="fu">## Challenges</span></span>
<span id="cb2-429"><a href="#cb2-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-430"><a href="#cb2-430" aria-hidden="true" tabindex="-1"></a>We will now explore some challenges that must be overcome when analyzing images with machine learning models.</span>
<span id="cb2-431"><a href="#cb2-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-432"><a href="#cb2-432" aria-hidden="true" tabindex="-1"></a><span class="fu">### Semantic Gap {#sec-intro-challenges-semantic-gap}</span></span>
<span id="cb2-433"><a href="#cb2-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-434"><a href="#cb2-434" aria-hidden="true" tabindex="-1"></a>The semantic gap refers to the discrepancy between the low-level pixel information that can be directly extracted from an image and the high-level semantic interpretation understood by humans. In simpler terms: an image consists of millions of pixel values, but deriving semantically meaningful concepts like "cat" or "sitting" from these raw numbers is extremely challenging.</span>
<span id="cb2-435"><a href="#cb2-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-436"><a href="#cb2-436" aria-hidden="true" tabindex="-1"></a>::: {#fig-cat-number-grid}</span>
<span id="cb2-437"><a href="#cb2-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-438"><a href="#cb2-438" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}semantic_gap_comparison.png)</span>{width=600}</span>
<span id="cb2-439"><a href="#cb2-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-440"><a href="#cb2-440" aria-hidden="true" tabindex="-1"></a>Illustration of the semantic gap: computers see numbers, humans see meaning.</span>
<span id="cb2-441"><a href="#cb2-441" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-442"><a href="#cb2-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-443"><a href="#cb2-443" aria-hidden="true" tabindex="-1"></a><span class="fu">### Viewpoint</span></span>
<span id="cb2-444"><a href="#cb2-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-445"><a href="#cb2-445" aria-hidden="true" tabindex="-1"></a>The semantic meaning of an image remains constant regardless of camera viewpoint, but the pixel values change dramatically. A model must learn to recognize the same object from different angles.</span>
<span id="cb2-446"><a href="#cb2-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-447"><a href="#cb2-447" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-challenge-viewpoint}</span>
<span id="cb2-448"><a href="#cb2-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-449"><a href="#cb2-449" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}viewpoint_challenge.png)</span>{width=600}</span>
<span id="cb2-450"><a href="#cb2-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-451"><a href="#cb2-451" aria-hidden="true" tabindex="-1"></a>The same cat from different viewpoints looks completely different at the pixel level.</span>
<span id="cb2-452"><a href="#cb2-452" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-453"><a href="#cb2-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-454"><a href="#cb2-454" aria-hidden="true" tabindex="-1"></a><span class="fu">### Deformation</span></span>
<span id="cb2-455"><a href="#cb2-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-456"><a href="#cb2-456" aria-hidden="true" tabindex="-1"></a>Objects, especially living beings, are often flexible and can appear in vastly different shapes and poses. Models must recognize a cat whether it's curled up sleeping or stretched out running.</span>
<span id="cb2-457"><a href="#cb2-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-458"><a href="#cb2-458" aria-hidden="true" tabindex="-1"></a>::: {#fig-challenge-deformation}</span>
<span id="cb2-459"><a href="#cb2-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-460"><a href="#cb2-460" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}challenge_deformation.png)</span>{width=600}</span>
<span id="cb2-461"><a href="#cb2-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-462"><a href="#cb2-462" aria-hidden="true" tabindex="-1"></a>Cats exhibit significant shape variation due to pose changes. <span class="co">[</span><span class="ot">Source</span><span class="co">](http://cs231n.stanford.edu/)</span></span>
<span id="cb2-463"><a href="#cb2-463" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-464"><a href="#cb2-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-465"><a href="#cb2-465" aria-hidden="true" tabindex="-1"></a><span class="fu">### Illumination</span></span>
<span id="cb2-466"><a href="#cb2-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-467"><a href="#cb2-467" aria-hidden="true" tabindex="-1"></a>Changes in lighting conditions dramatically affect pixel values and can obscure object features. The same object under different lighting can produce vastly different images.</span>
<span id="cb2-468"><a href="#cb2-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-469"><a href="#cb2-469" aria-hidden="true" tabindex="-1"></a>::: {#fig-challenge-illumination}</span>
<span id="cb2-470"><a href="#cb2-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-471"><a href="#cb2-471" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}challenge_illumination.png)</span>{width=600}</span>
<span id="cb2-472"><a href="#cb2-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-473"><a href="#cb2-473" aria-hidden="true" tabindex="-1"></a>The same scene under different lighting conditions. <span class="co">[</span><span class="ot">Source</span><span class="co">](http://cs231n.stanford.edu/)</span></span>
<span id="cb2-474"><a href="#cb2-474" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-475"><a href="#cb2-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-476"><a href="#cb2-476" aria-hidden="true" tabindex="-1"></a><span class="fu">### Background Clutter</span></span>
<span id="cb2-477"><a href="#cb2-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-478"><a href="#cb2-478" aria-hidden="true" tabindex="-1"></a>Background pixels can visually resemble the object of interest, making precise object localization and segmentation difficult. The model must learn to distinguish foreground from background.</span>
<span id="cb2-479"><a href="#cb2-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-480"><a href="#cb2-480" aria-hidden="true" tabindex="-1"></a>::: {#fig-challenge-background}</span>
<span id="cb2-481"><a href="#cb2-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-482"><a href="#cb2-482" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}challenge_background.png)</span>{width=600}</span>
<span id="cb2-483"><a href="#cb2-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-484"><a href="#cb2-484" aria-hidden="true" tabindex="-1"></a>Cat camouflaged against a similarly-textured background. <span class="co">[</span><span class="ot">Source</span><span class="co">](http://cs231n.stanford.edu/)</span></span>
<span id="cb2-485"><a href="#cb2-485" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-486"><a href="#cb2-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-487"><a href="#cb2-487" aria-hidden="true" tabindex="-1"></a><span class="fu">### Occlusion</span></span>
<span id="cb2-488"><a href="#cb2-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-489"><a href="#cb2-489" aria-hidden="true" tabindex="-1"></a>Objects are often partially hidden by other objects in the scene. Models must learn to recognize objects even when only part of them is visible.</span>
<span id="cb2-490"><a href="#cb2-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-491"><a href="#cb2-491" aria-hidden="true" tabindex="-1"></a>::: {#fig-challenge-occlusion}</span>
<span id="cb2-492"><a href="#cb2-492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-493"><a href="#cb2-493" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}challenge_occlusion.png)</span>{width=600}</span>
<span id="cb2-494"><a href="#cb2-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-495"><a href="#cb2-495" aria-hidden="true" tabindex="-1"></a>Partially occluded cat - only part of the object is visible. <span class="co">[</span><span class="ot">Source</span><span class="co">](http://cs231n.stanford.edu/)</span></span>
<span id="cb2-496"><a href="#cb2-496" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-497"><a href="#cb2-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-498"><a href="#cb2-498" aria-hidden="true" tabindex="-1"></a><span class="fu">### Intra-Class Variation</span></span>
<span id="cb2-499"><a href="#cb2-499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-500"><a href="#cb2-500" aria-hidden="true" tabindex="-1"></a>Objects belonging to the same class can exhibit enormous variability in appearance, color, size, and texture. All cats are cats, but they can look very different from each other.</span>
<span id="cb2-501"><a href="#cb2-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-502"><a href="#cb2-502" aria-hidden="true" tabindex="-1"></a>::: {#fig-challenge-intra-class}</span>
<span id="cb2-503"><a href="#cb2-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-504"><a href="#cb2-504" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}challenge_intra_class_variation.jpg)</span>{width=600}</span>
<span id="cb2-505"><a href="#cb2-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-506"><a href="#cb2-506" aria-hidden="true" tabindex="-1"></a>High intra-class variation: all are cats but with vastly different appearances. <span class="co">[</span><span class="ot">Source</span><span class="co">](https://www.maxpixel.net/Cat-Kittens-Free-Float-Kitten-Rush-Cat-Puppy-555822)</span></span>
<span id="cb2-507"><a href="#cb2-507" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-508"><a href="#cb2-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-509"><a href="#cb2-509" aria-hidden="true" tabindex="-1"></a><span class="fu">### Context Dependence</span></span>
<span id="cb2-510"><a href="#cb2-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-511"><a href="#cb2-511" aria-hidden="true" tabindex="-1"></a>@fig-tiger-context shows that contextual information can be crucial for correct object classification. The surrounding scene provides important cues about what objects are likely to be present.</span>
<span id="cb2-512"><a href="#cb2-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-513"><a href="#cb2-513" aria-hidden="true" tabindex="-1"></a>::: {#fig-tiger-context}</span>
<span id="cb2-514"><a href="#cb2-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-515"><a href="#cb2-515" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}tiger_context.jpg)</span>{width=600}</span>
<span id="cb2-516"><a href="#cb2-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-517"><a href="#cb2-517" aria-hidden="true" tabindex="-1"></a>Is this a tiger? Context clues help determine the answer. <span class="co">[</span><span class="ot">Source</span><span class="co">](https://www.linkedin.com/posts/ralph-aboujaoude-diaz-40838313_technology-artificialintelligence-computervision-activity-6912446088364875776-h-Iq?utm_source=linkedin_share&amp;utm_medium=member_desktop_web)</span></span>
<span id="cb2-518"><a href="#cb2-518" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-519"><a href="#cb2-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-520"><a href="#cb2-520" aria-hidden="true" tabindex="-1"></a>@fig-blop-context represents an interesting example where context is important: Shown are four images with severe quality degradation (blur) and oval objects indicated with dashed black lines. Notably, each marked object is exactly identical on pixel level (apart from a rotation) - yet we can form clear hypotheses what each of the object represents.</span>
<span id="cb2-521"><a href="#cb2-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-522"><a href="#cb2-522" aria-hidden="true" tabindex="-1"></a>::: {#fig-blop-context}</span>
<span id="cb2-523"><a href="#cb2-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-524"><a href="#cb2-524" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}challenges_context_blop.png)</span>{width=600}</span>
<span id="cb2-525"><a href="#cb2-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-526"><a href="#cb2-526" aria-hidden="true" tabindex="-1"></a>In the presence of image degradation a visual system (e.g. humans) heavily use context information. Even though the circled blops are identical, we interpret them as either: plate, bottle, cell phone, car, pedestrian or shoe. Source @torralba_using_2010</span>
<span id="cb2-527"><a href="#cb2-527" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-528"><a href="#cb2-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-529"><a href="#cb2-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-530"><a href="#cb2-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-531"><a href="#cb2-531" aria-hidden="true" tabindex="-1"></a><span class="fu">## Machine Learning</span></span>
<span id="cb2-532"><a href="#cb2-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-533"><a href="#cb2-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-534"><a href="#cb2-534" aria-hidden="true" tabindex="-1"></a><span class="fu">### Machine Learning Basics</span></span>
<span id="cb2-535"><a href="#cb2-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-536"><a href="#cb2-536" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="📚 Quick ML Refresher"}</span>
<span id="cb2-537"><a href="#cb2-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-538"><a href="#cb2-538" aria-hidden="true" tabindex="-1"></a>This course uses machine learning concepts throughout. If you need a detailed refresher on ML fundamentals, see our <span class="co">[</span><span class="ot">Machine Learning Basics</span><span class="co">](../background/machine_learning.qmd)</span> background page.</span>
<span id="cb2-539"><a href="#cb2-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-540"><a href="#cb2-540" aria-hidden="true" tabindex="-1"></a>**Quick summary**: We follow a data-driven approach:</span>
<span id="cb2-541"><a href="#cb2-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-542"><a href="#cb2-542" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Collect** a dataset of images and labels</span>
<span id="cb2-543"><a href="#cb2-543" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Train** a model that learns to map images to labels</span>
<span id="cb2-544"><a href="#cb2-544" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Evaluate** the model on new data</span>
<span id="cb2-545"><a href="#cb2-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-548"><a href="#cb2-548" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb2-549"><a href="#cb2-549" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb2-550"><a href="#cb2-550" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb2-551"><a href="#cb2-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-552"><a href="#cb2-552" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(images, labels):</span>
<span id="cb2-553"><a href="#cb2-553" aria-hidden="true" tabindex="-1"></a> <span class="co">""" Train a Model """</span></span>
<span id="cb2-554"><a href="#cb2-554" aria-hidden="true" tabindex="-1"></a> <span class="co"># Fit Model here</span></span>
<span id="cb2-555"><a href="#cb2-555" aria-hidden="true" tabindex="-1"></a> <span class="cf">return</span> model</span>
<span id="cb2-556"><a href="#cb2-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-557"><a href="#cb2-557" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict(test_images, model):</span>
<span id="cb2-558"><a href="#cb2-558" aria-hidden="true" tabindex="-1"></a> <span class="co">""" Predict """</span></span>
<span id="cb2-559"><a href="#cb2-559" aria-hidden="true" tabindex="-1"></a> predictions <span class="op">=</span> model(test_images)</span>
<span id="cb2-560"><a href="#cb2-560" aria-hidden="true" tabindex="-1"></a> <span class="cf">return</span> predictions</span>
<span id="cb2-561"><a href="#cb2-561" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb2-562"><a href="#cb2-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-563"><a href="#cb2-563" aria-hidden="true" tabindex="-1"></a>**Key concepts:**</span>
<span id="cb2-564"><a href="#cb2-564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-565"><a href="#cb2-565" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Model**: Function $f_{\theta}(\mathbf{x}) = \hat{y}$ with learnable parameters $\theta$</span>
<span id="cb2-566"><a href="#cb2-566" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Optimization**: Find $\theta$ that minimizes loss $J(f_{\theta}(\mathbf{X}), \mathbf{y})$</span>
<span id="cb2-567"><a href="#cb2-567" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Model Selection**: Use train/validation/test splits to avoid overfitting</span>
<span id="cb2-568"><a href="#cb2-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-569"><a href="#cb2-569" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-570"><a href="#cb2-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-571"><a href="#cb2-571" aria-hidden="true" tabindex="-1"></a><span class="fu">### ML with Classical Computer Vision</span></span>
<span id="cb2-572"><a href="#cb2-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-573"><a href="#cb2-573" aria-hidden="true" tabindex="-1"></a>Images typically have very high dimensionality. For example, an RGB image with a resolution of $800 \times 600$ has a dimensionality of $800 \times 600 \times 3 = 1,440,000$. Classical machine learning algorithms often struggle with such high dimensionalities:</span>
<span id="cb2-574"><a href="#cb2-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-575"><a href="#cb2-575" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>They are very slow or require a lot of memory.</span>
<span id="cb2-576"><a href="#cb2-576" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>They cannot exploit the 2-D structure of images.</span>
<span id="cb2-577"><a href="#cb2-577" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>They are very sensitive to slight changes in images (e.g., rotations).</span>
<span id="cb2-578"><a href="#cb2-578" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>They can easily overfit, as the number of features is close to the number of observations (training set).</span>
<span id="cb2-579"><a href="#cb2-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-580"><a href="#cb2-580" aria-hidden="true" tabindex="-1"></a>When modeling images with (classical) machine learning algorithms, methods from (classical) computer vision are often used. With the help of such methods, features can be extracted from images, and the algorithms can learn on these features. This avoids modeling high-dimensional raw data. @fig-intro-color-histograms, @fig-intro-hog-features, and @fig-intro-bow-features show various feature extraction methods.</span>
<span id="cb2-581"><a href="#cb2-581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-582"><a href="#cb2-582" aria-hidden="true" tabindex="-1"></a>@fig-intro-color-histograms shows that, for example, the distribution over the color spectrum can be extracted from an image. This could be an important feature.</span>
<span id="cb2-583"><a href="#cb2-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-584"><a href="#cb2-584" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-color-histograms}</span>
<span id="cb2-585"><a href="#cb2-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-586"><a href="#cb2-586" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}color_histogram.png)</span>{width=600}</span>
<span id="cb2-587"><a href="#cb2-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-588"><a href="#cb2-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-589"><a href="#cb2-589" aria-hidden="true" tabindex="-1"></a>Color Histograms as Features (Source: @johnson_eecs_2022)</span>
<span id="cb2-590"><a href="#cb2-590" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-591"><a href="#cb2-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-592"><a href="#cb2-592" aria-hidden="true" tabindex="-1"></a>@fig-intro-hog-features shows that techniques like Histogram of Oriented Gradients (HOG) @dalal_histograms_2005 can be used to extract structures from images. Such features were successfully used for pedestrian detection @dalal_histograms_2005.</span>
<span id="cb2-593"><a href="#cb2-593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-594"><a href="#cb2-594" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-hog-features}</span>
<span id="cb2-595"><a href="#cb2-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-596"><a href="#cb2-596" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}hog_features.png)</span>{width=600}</span>
<span id="cb2-597"><a href="#cb2-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-598"><a href="#cb2-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-599"><a href="#cb2-599" aria-hidden="true" tabindex="-1"></a>HOG as Features (Source: @johnson_eecs_2022)</span>
<span id="cb2-600"><a href="#cb2-600" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-601"><a href="#cb2-601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-602"><a href="#cb2-602" aria-hidden="true" tabindex="-1"></a>@fig-intro-bow-features shows another feature variant. Visual patches can be extracted from a dataset, clustered, and then used as descriptors.</span>
<span id="cb2-603"><a href="#cb2-603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-604"><a href="#cb2-604" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-bow-features}</span>
<span id="cb2-605"><a href="#cb2-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-606"><a href="#cb2-606" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}bag_of_words_features.png)</span>{width=600}</span>
<span id="cb2-607"><a href="#cb2-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-608"><a href="#cb2-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-609"><a href="#cb2-609" aria-hidden="true" tabindex="-1"></a>Bag of (visual) words Features (Source: @johnson_eecs_2022)</span>
<span id="cb2-610"><a href="#cb2-610" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-611"><a href="#cb2-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-612"><a href="#cb2-612" aria-hidden="true" tabindex="-1"></a>Finally, all features can be combined, often more is better, as shown in @fig-intro-concat-features.</span>
<span id="cb2-613"><a href="#cb2-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-614"><a href="#cb2-614" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-concat-features}</span>
<span id="cb2-615"><a href="#cb2-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-616"><a href="#cb2-616" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}features_concat.png)</span>{width=600}</span>
<span id="cb2-617"><a href="#cb2-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-618"><a href="#cb2-618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-619"><a href="#cb2-619" aria-hidden="true" tabindex="-1"></a>Image Features (Source: @johnson_eecs_2022)</span>
<span id="cb2-620"><a href="#cb2-620" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-621"><a href="#cb2-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-622"><a href="#cb2-622" aria-hidden="true" tabindex="-1"></a>Depending on the parameterization, HOG descriptors, for example, can be very large for images. The resulting feature vector from @fig-intro-concat-features can still be reduced in dimensionality, e.g., with Principal Component Analysis.</span>
<span id="cb2-623"><a href="#cb2-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-624"><a href="#cb2-624" aria-hidden="true" tabindex="-1"></a>@fig-intro-cifar10 shows the CIFAR10 dataset. A well-known dataset for testing models, consisting of 10 classes with 6,000 images each.</span>
<span id="cb2-625"><a href="#cb2-625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-626"><a href="#cb2-626" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-cifar10}</span>
<span id="cb2-627"><a href="#cb2-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-628"><a href="#cb2-628" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}cifar10.jpg)</span>{width=600}</span>
<span id="cb2-629"><a href="#cb2-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-630"><a href="#cb2-630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-631"><a href="#cb2-631" aria-hidden="true" tabindex="-1"></a>CIFAR10 Dataset <span class="co">[</span><span class="ot">Source</span><span class="co">](https://www.cs.toronto.edu/~kriz/cifar.html)</span></span>
<span id="cb2-632"><a href="#cb2-632" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-633"><a href="#cb2-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-634"><a href="#cb2-634" aria-hidden="true" tabindex="-1"></a><span class="fu">## Deep Learning</span></span>
<span id="cb2-635"><a href="#cb2-635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-636"><a href="#cb2-636" aria-hidden="true" tabindex="-1"></a>To accelerate research in computer vision and make progress more measurable, the PASCAL VOC Challenges were introduced @everingham_pascal_2007. These involved various tasks, such as detecting objects in photographs (@fig-intro-pascal).</span>
<span id="cb2-637"><a href="#cb2-637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-638"><a href="#cb2-638" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-pascal}</span>
<span id="cb2-639"><a href="#cb2-639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-640"><a href="#cb2-640" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}pascal_voc_2007.png)</span>{width=600}</span>
<span id="cb2-641"><a href="#cb2-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-642"><a href="#cb2-642" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-643"><a href="#cb2-643" aria-hidden="true" tabindex="-1"></a>Images/illustrations from <span class="co">[</span><span class="ot">Link</span><span class="co">](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/)</span> and @johnson_eecs_2022. On the left, you see object annotations in images, and on the right, you see the development of Mean Average Precision over the years.</span>
<span id="cb2-644"><a href="#cb2-644" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-645"><a href="#cb2-645" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-646"><a href="#cb2-646" aria-hidden="true" tabindex="-1"></a>The easy availability of images on the internet has made it possible to collect increasingly larger datasets. <span class="co">[</span><span class="ot">ImageNet</span><span class="co">](https://www.image-net.org/)</span> is such a very large, hierarchically annotated image dataset @deng_imagenet_2009 with over 1.4 million images, categorized into 1,000 object classes. @fig-intro-imagenet illustrates the dataset.</span>
<span id="cb2-647"><a href="#cb2-647" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-648"><a href="#cb2-648" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-imagenet}</span>
<span id="cb2-649"><a href="#cb2-649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-650"><a href="#cb2-650" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}imagenet.jpg)</span>{width=600}</span>
<span id="cb2-651"><a href="#cb2-651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-652"><a href="#cb2-652" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-653"><a href="#cb2-653" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">ImageNet</span><span class="co">](https://www.image-net.org/)</span>, <span class="co">[</span><span class="ot">Image Source</span><span class="co">](https://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_full_1k.jpg)</span>, details in @deng_imagenet_2009</span>
<span id="cb2-654"><a href="#cb2-654" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-655"><a href="#cb2-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-656"><a href="#cb2-656" aria-hidden="true" tabindex="-1"></a>Since 2010, challenges have been regularly conducted on the ImageNet dataset @russakovsky_imagenet_2015, such as image classification and object detection. @fig-intro-imagenet-perf shows the development of the error rate over time.</span>
<span id="cb2-657"><a href="#cb2-657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-658"><a href="#cb2-658" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-imagenet-perf}</span>
<span id="cb2-659"><a href="#cb2-659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-660"><a href="#cb2-660" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}image_net_perf.png)</span>{width=600}</span>
<span id="cb2-661"><a href="#cb2-661" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-662"><a href="#cb2-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-663"><a href="#cb2-663" aria-hidden="true" tabindex="-1"></a>Source: @johnson_eecs_2022</span>
<span id="cb2-664"><a href="#cb2-664" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-665"><a href="#cb2-665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-666"><a href="#cb2-666" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The ImageNet Breakthrough (2012)</span></span>
<span id="cb2-667"><a href="#cb2-667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-668"><a href="#cb2-668" aria-hidden="true" tabindex="-1"></a>The year 2012 marked a watershed moment in computer vision. Until then, the state-of-the-art in the ImageNet competition relied on classical approaches. In 2011, the winning team @perronnin_large-scale_2010 combined various hand-crafted feature extraction methods (SIFT features) with Support Vector Machines (SVMs), achieving a top-5 error rate of 25.8%.</span>
<span id="cb2-669"><a href="#cb2-669" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-670"><a href="#cb2-670" aria-hidden="true" tabindex="-1"></a>Then came the breakthrough: Krizhevsky et al. @krizhevsky_imagenet_2012 implemented **AlexNet**, a convolutional neural network (CNN) with multiple layers, and achieved a top-5 error rate of just 16.4%—a dramatic 9.4 percentage point improvement. This wasn't just an incremental gain; it fundamentally demonstrated that deep learning could outperform decades of hand-crafted computer vision methods.</span>
<span id="cb2-671"><a href="#cb2-671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-672"><a href="#cb2-672" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-alexnet}</span>
<span id="cb2-673"><a href="#cb2-673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-674"><a href="#cb2-674" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}alexnet.png)</span>{width=600}</span>
<span id="cb2-675"><a href="#cb2-675" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-676"><a href="#cb2-676" aria-hidden="true" tabindex="-1"></a>AlexNet architecture @krizhevsky_imagenet_2012: 5 convolutional layers followed by 3 fully connected layers, trained on dual GPUs.</span>
<span id="cb2-677"><a href="#cb2-677" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-678"><a href="#cb2-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-679"><a href="#cb2-679" aria-hidden="true" tabindex="-1"></a>**What made AlexNet work?**</span>
<span id="cb2-680"><a href="#cb2-680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-681"><a href="#cb2-681" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Deep architecture**: 8 layers (5 convolutional + 3 fully connected)</span>
<span id="cb2-682"><a href="#cb2-682" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**GPU training**: Used dual GPUs to handle 60 million parameters</span>
<span id="cb2-683"><a href="#cb2-683" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**ReLU activations**: Faster training than traditional sigmoid/tanh</span>
<span id="cb2-684"><a href="#cb2-684" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Dropout regularization**: Reduced overfitting</span>
<span id="cb2-685"><a href="#cb2-685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-686"><a href="#cb2-686" aria-hidden="true" tabindex="-1"></a>This success marked the end of classical computer vision methods in many areas and triggered an explosion of deep learning research.</span>
<span id="cb2-687"><a href="#cb2-687" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-688"><a href="#cb2-688" aria-hidden="true" tabindex="-1"></a>While classical computer vision trains a machine learning model on features extracted with hand-crafted algorithms @fig-intro-classical-ml, the development is increasingly moving towards end-to-end learning. In this approach, one avoids as much as possible static/hand-designed components and learns everything, including feature extraction, with machine learning @fig-intro-end-to-end.</span>
<span id="cb2-689"><a href="#cb2-689" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-690"><a href="#cb2-690" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-classical-ml}</span>
<span id="cb2-691"><a href="#cb2-691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-692"><a href="#cb2-692" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}classical_ml.png)</span>{width=600}</span>
<span id="cb2-693"><a href="#cb2-693" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-694"><a href="#cb2-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-695"><a href="#cb2-695" aria-hidden="true" tabindex="-1"></a>Illustration from @johnson_eecs_2022</span>
<span id="cb2-696"><a href="#cb2-696" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-697"><a href="#cb2-697" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-698"><a href="#cb2-698" aria-hidden="true" tabindex="-1"></a>::: {#fig-intro-end-to-end}</span>
<span id="cb2-699"><a href="#cb2-699" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-700"><a href="#cb2-700" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}end-to-end-ml.png)</span>{width=600}</span>
<span id="cb2-701"><a href="#cb2-701" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-702"><a href="#cb2-702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-703"><a href="#cb2-703" aria-hidden="true" tabindex="-1"></a>Illustration from @johnson_eecs_2022</span>
<span id="cb2-704"><a href="#cb2-704" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-705"><a href="#cb2-705" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-706"><a href="#cb2-706" aria-hidden="true" tabindex="-1"></a>Deep learning-based approaches have several key advantages over classical machine learning methods:</span>
<span id="cb2-707"><a href="#cb2-707" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-708"><a href="#cb2-708" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Automatic feature extraction**: No need for hand-crafted features like SIFT or HOG.</span>
<span id="cb2-709"><a href="#cb2-709" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Hierarchical representations**: Learn features at multiple levels of abstraction (edges → textures → parts → objects).</span>
<span id="cb2-710"><a href="#cb2-710" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Better scaling**: Performance improves with more training data and compute.</span>
<span id="cb2-711"><a href="#cb2-711" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**End-to-end learning**: Unified optimization from raw inputs to task outputs.</span>
<span id="cb2-712"><a href="#cb2-712" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Built-in invariances**: Architectures like CNNs are naturally invariant to certain transformations (e.g., translation).</span>
<span id="cb2-713"><a href="#cb2-713" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Transfer learning**: Pre-trained models can be adapted to new tasks with limited data.</span>
<span id="cb2-714"><a href="#cb2-714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-715"><a href="#cb2-715" aria-hidden="true" tabindex="-1"></a><span class="fu">### Deep Learning History</span></span>
<span id="cb2-716"><a href="#cb2-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-717"><a href="#cb2-717" aria-hidden="true" tabindex="-1"></a>We will now explore the most important milestones in deep learning for image analysis.</span>
<span id="cb2-718"><a href="#cb2-718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-719"><a href="#cb2-719" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Hubel and Wiesel</span></span>
<span id="cb2-720"><a href="#cb2-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-721"><a href="#cb2-721" aria-hidden="true" tabindex="-1"></a>@hubel_receptive_1959 showed in experiments on cats that there are complex cells/neurons in the visual cortex that only respond to certain patterns. In addition to these complex cells, there are also simple cells that only respond to lines at a certain angle.</span>
<span id="cb2-722"><a href="#cb2-722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-723"><a href="#cb2-723" aria-hidden="true" tabindex="-1"></a>::: {#fig-huber-wiesel}</span>
<span id="cb2-724"><a href="#cb2-724" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-725"><a href="#cb2-725" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}huber_wiesel_cat_experiment.jpg)</span>{width=600}</span>
<span id="cb2-726"><a href="#cb2-726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-727"><a href="#cb2-727" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-728"><a href="#cb2-728" aria-hidden="true" tabindex="-1"></a>Illustration <span class="co">[</span><span class="ot">Source</span><span class="co">](https://link.springer.com/chapter/10.1007/978-3-030-28954-6_4/figures/1)</span></span>
<span id="cb2-729"><a href="#cb2-729" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-730"><a href="#cb2-730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-731"><a href="#cb2-731" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Neocognitron</span></span>
<span id="cb2-732"><a href="#cb2-732" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-733"><a href="#cb2-733" aria-hidden="true" tabindex="-1"></a>@fukushima_neocognitron_1980 defined a model of a neural network that can recognize visual patterns. It already has the hierarchical structure of a modern convolutional neural network and is inspired by biological neural networks, particularly from the insights of @hubel_receptive_1959.</span>
<span id="cb2-734"><a href="#cb2-734" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-735"><a href="#cb2-735" aria-hidden="true" tabindex="-1"></a>::: {#fig-neocognitron}</span>
<span id="cb2-736"><a href="#cb2-736" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-737"><a href="#cb2-737" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}neocogitron.png)</span>{width=600}</span>
<span id="cb2-738"><a href="#cb2-738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-739"><a href="#cb2-739" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-740"><a href="#cb2-740" aria-hidden="true" tabindex="-1"></a>The Neocognitron @fukushima_neocognitron_1980.</span>
<span id="cb2-741"><a href="#cb2-741" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-742"><a href="#cb2-742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-743"><a href="#cb2-743" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Backpropagation</span></span>
<span id="cb2-744"><a href="#cb2-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-745"><a href="#cb2-745" aria-hidden="true" tabindex="-1"></a>@rumelhart_learning_1986 introduced the backpropagation algorithm in the context of neural networks. This algorithm is used to train modern neural networks: it finds the parameters of an artificial neural network (ANN) to solve a specific task. Backpropagation is based on the chain rule from calculus and is also important for other machine learning models.</span>
<span id="cb2-746"><a href="#cb2-746" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-747"><a href="#cb2-747" aria-hidden="true" tabindex="-1"></a>::: {#fig-rumalhart-backprop}</span>
<span id="cb2-748"><a href="#cb2-748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-749"><a href="#cb2-749" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}rumelhart_backprop.png)</span>{width=600}</span>
<span id="cb2-750"><a href="#cb2-750" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-751"><a href="#cb2-751" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-752"><a href="#cb2-752" aria-hidden="true" tabindex="-1"></a>Backpropagation in neural networks @rumelhart_learning_1986.</span>
<span id="cb2-753"><a href="#cb2-753" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-754"><a href="#cb2-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-755"><a href="#cb2-755" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-756"><a href="#cb2-756" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb2-757"><a href="#cb2-757" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-758"><a href="#cb2-758" aria-hidden="true" tabindex="-1"></a>A good video on backpropagation: <span class="co">[</span><span class="ot">3Blue1Brown Backpropagation Calculus</span><span class="co">](https://www.youtube.com/watch?v=tIeHLnjs5U8)</span></span>
<span id="cb2-759"><a href="#cb2-759" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-760"><a href="#cb2-760" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-761"><a href="#cb2-761" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-762"><a href="#cb2-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-763"><a href="#cb2-763" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-764"><a href="#cb2-764" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Convolutional Neural Networks (CNNs)</span></span>
<span id="cb2-765"><a href="#cb2-765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-766"><a href="#cb2-766" aria-hidden="true" tabindex="-1"></a>@lecun_gradient-based_1998 implemented convolutional neural networks (CNNs) to recognize handwritten digits. The architecture is specialized for the 2-D structure of image data and trained a model very similar to modern CNNs, as shown in @fig-lecun-cnn.</span>
<span id="cb2-767"><a href="#cb2-767" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-768"><a href="#cb2-768" aria-hidden="true" tabindex="-1"></a>::: {#fig-lecun-cnn}</span>
<span id="cb2-769"><a href="#cb2-769" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-770"><a href="#cb2-770" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}lecun_cnn.png)</span>{width=600}</span>
<span id="cb2-771"><a href="#cb2-771" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-772"><a href="#cb2-772" aria-hidden="true" tabindex="-1"></a>LeNet-5 architecture @lecun_gradient-based_1998: the first successful CNN for digit recognition.</span>
<span id="cb2-773"><a href="#cb2-773" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb2-774"><a href="#cb2-774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-775"><a href="#cb2-775" aria-hidden="true" tabindex="-1"></a>After AlexNet's breakthrough in 2012 (see previous section), CNNs rapidly evolved and were applied to increasingly complex tasks. Notable developments include:</span>
<span id="cb2-776"><a href="#cb2-776" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-777"><a href="#cb2-777" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**VGGNet (2014)**: Showed that depth matters—16-19 layers with small 3×3 filters</span>
<span id="cb2-778"><a href="#cb2-778" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**ResNet (2015)**: Introduced skip connections, enabling networks with 50-152 layers</span>
<span id="cb2-779"><a href="#cb2-779" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**COCO Challenges**: Expanded beyond ImageNet to multi-task benchmarks (detection, segmentation, keypoints)</span>
<span id="cb2-780"><a href="#cb2-780" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Mobile architectures**: Efficient CNNs for deployment on phones and embedded devices</span>
<span id="cb2-781"><a href="#cb2-781" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-782"><a href="#cb2-782" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Other Architectures in Computer Vision</span></span>
<span id="cb2-783"><a href="#cb2-783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-784"><a href="#cb2-784" aria-hidden="true" tabindex="-1"></a>CNNs are still in use today (2025). Meanwhile, there are alternative architectures, such as transformer-based models (@dosovitskiy_image_2020), which are extremely successful in language modeling, or multilayer perceptron-based architectures (@liu_pay_2021).</span>
<span id="cb2-785"><a href="#cb2-785" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-786"><a href="#cb2-786" aria-hidden="true" tabindex="-1"></a>The question of which architecture type will prevail is open. Currently, it seems that both CNNs and transformer-based models have their place, and often perform similarly (@smith_convnets_2023). Furthermore, hybrid-architectures, combining the strengths of CNNs and transformers have been proposed @vasu_fastvit_2023. Some works use both architecture types, depending on whether parameter efficiency (CNNs) or raw performance (transformers) are prioritized @simeoni_dinov3_2025.</span>
<span id="cb2-787"><a href="#cb2-787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-788"><a href="#cb2-788" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-789"><a href="#cb2-789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-790"><a href="#cb2-790" aria-hidden="true" tabindex="-1"></a><span class="fu">## References</span></span>
<span id="cb2-791"><a href="#cb2-791" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-792"><a href="#cb2-792" aria-hidden="true" tabindex="-1"></a>::: {#refs}</span>
<span id="cb2-793"><a href="#cb2-793" aria-hidden="true" tabindex="-1"></a>:::</span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© Copyright 2025, Marco Willi</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2025/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This page is built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>
<script>videojs(video_shortcode_videojs_video1);</script>
<script>videojs(video_shortcode_videojs_video2);</script>
<script>videojs(video_shortcode_videojs_video3);</script>
<script>videojs(video_shortcode_videojs_video4);</script>
<script>videojs(video_shortcode_videojs_video5);</script>
<script>videojs(video_shortcode_videojs_video6);</script>




</body></html>