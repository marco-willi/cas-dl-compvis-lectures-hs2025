---
title: "3 - Generative Models "
params:
   images_path: "/assets/images/genai/"
---

::: {.callout-tip title="Learning Objectives"  collapse="true"}

After this lecture you should be able to:

- Understand the core challenge of generative modeling: mapping from latent variables to diverse, realistic outputs
- Explain the key differences between GANs, VAEs, and Diffusion models in terms of training objectives and trade-offs
- Describe how conditioning enables controllable generation for tasks like image-to-image translation
- Understand practical considerations for using pre-trained models including hardware requirements and fine-tuning approaches

:::


::: {.callout-tip title="TLDR Recap" collapse="true"}

**Three families of generative models** dominate modern image generation:

- **GANs**: Adversarial training (generator vs. discriminator) produces sharp outputs but can suffer from mode collapse and training instability
- **VAEs**: Probabilistic encoder-decoder with ELBO objective provides structured latent spaces but historically blurrier outputs
- **Diffusion Models**: Learn to reverse gradual noising process, achieving state-of-the-art quality with stable training but slow inference

**No model dominates all metrics**—choose based on your constraints (speed, quality, interpretability). Modern systems often combine approaches (e.g., Latent Diffusion).

**Pre-trained models** (especially Stable Diffusion) democratize access. **LoRA fine-tuning** enables customization with minimal compute. Open-source (HuggingFace, CivitAI) and commercial (OpenAI, Midjourney) platforms offer diverse trade-offs.

**Key considerations**: computational cost, sample quality, training stability, latent space structure, and inference speed.

:::

## Introduction: The Generative Modeling Landscape

Generative modeling tackles a fundamental challenge in machine learning: learning to produce realistic, diverse outputs from simple inputs. Unlike discriminative models (e.g., classifiers) that perform many-to-one mappings (images → labels), generative models perform **one-to-many mappings** (latent variables → diverse images). This inherent stochasticity—generating different plausible outputs from the same input—is what makes generative modeling both powerful and challenging.

The key insight across modern generative models is the use of **latent variables** (typically sampled from $\mathcal{N}(0, I)$) that control attributes of the generated output. By manipulating these latent codes, we can generate diverse samples, interpolate between examples, and control specific properties of the output.

### Three Main Approaches

Over the past decade, three families of generative models have emerged as dominant approaches, each with distinct philosophies and trade-offs:

| Aspect | GANs | VAEs | Diffusion Models |
|--------|------|------|------------------|
| **Training** | Adversarial game | Variational inference | Denoising objective |
| **Output Quality** | Sharp, high-quality | Can be blurry | High-quality, diverse |
| **Training Stability** | Unstable, mode collapse | Stable | Stable |
| **Inference Speed** | Fast | Fast | Slow (iterative) |
| **Latent Structure** | Implicit | Explicit, interpretable | Implicit |
| **Likelihood** | No tractable likelihood | Tractable lower bound | Tractable (slow) |

: {tbl-colwidths="[20,25,25,30]"}

**Generative Adversarial Networks (GANs)** pit two neural networks against each other: a generator creates samples, while a discriminator tries to distinguish real from fake. This adversarial game leads to sharp, realistic outputs but can suffer from training instability.

**Variational Autoencoders (VAEs)** learn an explicit encoder-decoder pair with a principled probabilistic foundation. They optimize a variational lower bound (ELBO) that balances reconstruction quality with latent space structure. VAEs provide interpretable latent spaces but historically produced blurrier outputs.

**Diffusion Models** reverse a gradual noising process, learning to denoise images step-by-step. They achieve state-of-the-art sample quality and training stability but require many inference steps, making them slower than GANs or VAEs.

Let's explore each approach in detail.

## Generative Adversarial Networks

GANs [@goodfellow_generative_2014] frame generative modeling as an **adversarial game** between two neural networks: a **generator** $G$ that creates fake samples, and a **discriminator** $D$ that tries to distinguish real data from generated fakes. This adversarial setup drives both networks to improve: the generator learns to create increasingly realistic samples, while the discriminator becomes better at spotting fakes.

::: {#fig-genai-gan-idea}

![]({{< meta params.images_path >}}gan-architecture.jpg)

GAN idea: The generator creates fake samples from random noise $z \sim \mathcal{N}(0, I)$, while the discriminator judges whether samples are real or fake. From [Link](https://microsoft.github.io/GenStudio/)
:::

### Training Objective

The GAN training objective can be expressed as a minimax game:

$$
\min_{G} \max_{D} \mathbb{E}_{x \sim p_{data}}[\log D(x)] + \mathbb{E}_{z \sim p_z}[\log(1 - D(G(z)))]
$$

The discriminator $D$ maximizes its ability to correctly classify real ($D(x) \approx 1$) vs. fake ($D(G(z)) \approx 0$) samples. The generator $G$ minimizes the discriminator's success, trying to produce samples where $D(G(z)) \approx 1$ (fooling the discriminator).

In practice, training alternates between:
1. **Discriminator step**: Update $D$ to better distinguish real from fake
2. **Generator step**: Update $G$ to better fool the current discriminator


::: {#fig-genai-gan-learning}

![]({{< meta params.images_path >}}gan_learning.png)

GAN learning process: As training progresses, the generator's distribution (green) moves closer to the real data distribution (black dots). The discriminator's decision boundary (blue) adapts accordingly. From @foster_generative_2023
:::

The figure above illustrates how the generator progressively learns to match the data distribution. Initially, the generator produces poor samples and the discriminator easily separates real from fake. As training continues, the generator improves and the discriminator's task becomes harder, until ideally the generator perfectly matches the real data distribution.

### Conditional Generation

While basic GANs generate random samples from $z \sim \mathcal{N}(0, I)$, many applications require **controlled generation**—producing outputs that satisfy specific constraints. **Conditional GANs (cGANs)** [@mirza_conditional_2014] extend the framework by conditioning both networks on additional input $x$:

$$
\min_{G} \max_{D} \mathbb{E}_{x,y \sim p_{data}}[\log D(x,y)] + \mathbb{E}_{x,z \sim p_z}[\log(1 - D(x, G(x,z)))]
$$

Now the discriminator evaluates input-output **pairs** $(x, y)$, learning whether $y$ is a plausible output for input $x$. The generator must produce outputs $G(x, z)$ that match the conditional distribution $p(y|x)$.

::: {#fig-genai-gan-conditional}

![]({{< meta params.images_path >}}gan_conditional_generation.png)

Conditional GAN architecture: Both generator and discriminator receive the conditioning input $x$, enabling controlled generation.
:::


::: {#fig-genai-gan-translation layout-ncol=2}

![Model architecture]({{< meta params.images_path >}}gan_image_translation_model.png)

![Translation examples]({{< meta params.images_path >}}gan_image_translation_examples.png)

Image-to-image translation with Pix2Pix [@isola_image--image_2018]: The generator learns to map from input domain (sketches, edges) to output domain (photos). The PatchGAN discriminator evaluates local patches rather than whole images, improving statistical efficiency.
:::

A particularly successful application of cGANs is **image-to-image translation** (e.g., Pix2Pix, CycleGAN), where the model learns mappings like edges→photos, sketches→images, or day→night. The discriminator learns a task-specific loss function adapted to the data, often outperforming hand-designed losses.


::: {.callout-note title="Pros and Cons of GANs"}

**Pros**

- High-Quality Output, Sharp Details
- Fast Generation

**Cons**

- Training instability → mode collapse and vanishing gradients
- Difficult to evaluate, no likelihood

:::

::: {.callout-tip title="Key Takeaway: GANs" collapse="true"}

GANs frame generation as an adversarial game: $\min_{G} \max_{D} \mathbb{E}_{x \sim p_{data}}[\log D(x)] + \mathbb{E}_{z}[\log(1 - D(G(z)))]$

**Intuition**: Generator creates fakes, discriminator spots fakes, both improve through competition. Results in sharp, realistic samples but training can be unstable (mode collapse, vanishing gradients).

**Conditional GANs** enable controllable generation by conditioning both networks on input $x$, enabling applications like image-to-image translation (Pix2Pix, CycleGAN).

:::

## Variational Autoencoders

VAEs [@kingma_auto-encoding_2022] take a different approach to generative modeling, building on the autoencoder architecture but with a crucial probabilistic twist. A VAE learns two networks that perform (approximate) inverse operations:

- **Encoder** $q_{\psi}(z|x)$: Maps data $x$ to a distribution over latent codes $z$
- **Decoder** $p_{\theta}(x|z)$: Maps latent codes $z$ back to data $x$

Unlike standard autoencoders, VAEs enforce structure on the latent space through probabilistic modeling, ensuring that randomly sampled latent codes produce meaningful outputs.

::: {#fig-genai-vae-illustration}

![]({{< meta params.images_path >}}vae_illustration.png)

VAE architecture: The encoder maps inputs to latent distributions $q(z|x)$, while the decoder reconstructs from latent samples. From [Source](https://lilianweng.github.io/posts/2018-08-12-vae/).
:::

::: {#fig-genai-vae-illustration2}

![]({{< meta params.images_path >}}vae_illustration2.png)

VAE computational graph: The encoder outputs mean $\mu$ and variance $\sigma^2$ for each latent dimension. The **reparameterization trick** ($z = \mu + \epsilon \cdot \sigma$, where $\epsilon \sim \mathcal{N}(0, 1)$) enables gradient-based optimization. From [Source](https://lilianweng.github.io/posts/2018-08-12-vae/).
:::

### Training Objective: The ELBO

VAEs optimize the **Evidence Lower Bound (ELBO)**, which consists of two terms:

$$
\text{ELBO} = \underbrace{\mathbb{E}_{z \sim q_{\psi}(z|x)}[\log p_{\theta}(x|z)]}_{\text{Reconstruction}} - \underbrace{\text{KL}(q_{\psi}(z|x) \| p(z))}_{\text{Regularization}}
$$

The **reconstruction term** ensures the decoder can recover the input from the latent code. The **KL regularization term** (typically with prior $p(z) = \mathcal{N}(0, I)$) prevents the latent space from developing "gaps and tendrils"—ensuring that random samples from $\mathcal{N}(0, I)$ decode to meaningful outputs.

This balance is key to VAE behavior: the reconstruction loss wants to use the latent space flexibly, while KL regularization pushes latent codes toward a standard normal distribution. The result is a **structured, continuous latent space** where interpolation and attribute manipulation work smoothly.

::: {#fig-genai-vae-latent-space}

![]({{< meta params.images_path >}}vae_latent_space_face.png)

VAE latent space: Traversing the latent space produces smooth interpolations between faces, demonstrating the structured representation learned by the model. From @prince_understanding_2023.
:::

::: {.callout-note title="Pros and Cons of VAEs"}

**Pros**

- Principled probabilistic foundation
- Latent space structure: can interpolate in latent space and interpret factors
- Stable training
- Can be used for representation learning

**Cons**

- Blurry outputs (more complex variants can reduce this)

:::

::: {.callout-tip title="Key Takeaway: VAEs" collapse="true"}

VAEs learn encoder $q_{\psi}(z|x)$ and decoder $p_{\theta}(x|z)$ by optimizing the ELBO:
$$\text{ELBO} = \underbrace{\mathbb{E}_{z}[\log p_{\theta}(x|z)]}_{\text{Reconstruction}} - \underbrace{\text{KL}(q_{\psi}(z|x) \| p(z))}_{\text{Regularization}}$$

**Intuition**: Reconstruction loss wants accurate decoding; KL regularization prevents "gaps" in latent space, ensuring random $z \sim \mathcal{N}(0, I)$ produces meaningful outputs.

**Result**: Structured, continuous latent spaces enabling smooth interpolation and disentangled representations. Trade-off: blurrier outputs than GANs (though modern variants improve this).

:::

## Diffusion Models

Diffusion models [@ho_denoising_2020; @sohl-dickstein_deep_2015] have emerged as the state-of-the-art approach for high-quality image generation, powering systems like DALL-E 2, Stable Diffusion, and Imagen. The core insight is remarkably simple: learn to reverse a gradual noising process.

::: {#fig-genai-diffusion-overview}

![]({{< meta params.images_path >}}diffusion_overview.png)

Diffusion model overview: The forward process (top) gradually corrupts data with noise over $T$ steps. The reverse process (bottom) learns to denoise, generating samples from noise. From @prince_understanding_2023.
:::

Diffusion models consist of two processes:

- **Forward Process**: Gradually adds Gaussian noise to data over $T$ steps (typically 1000), following a pre-specified noise schedule. This is deterministic—no neural network needed.
- **Reverse Process**: Learns to gradually remove noise, step by step, using a neural network. Starting from pure noise $z_T \sim \mathcal{N}(0, I)$, the model denoises iteratively to produce realistic samples.

The key advantage: by breaking generation into many small denoising steps, each step becomes easier to learn than generating an image in one shot (as in GANs or VAEs).


::: {#fig-genai-diffusion-forward}

![]({{< meta params.images_path >}}diffusion_forward.png)

Forward process: At each step $t$, noise is added according to $z_t = \sqrt{1-\beta_t} z_{t-1} + \sqrt{\beta_t} \epsilon_t$, where $\epsilon_t \sim \mathcal{N}(0, I)$. From @prince_understanding_2023.
:::

### The Noise Schedule

The parameter $\beta_t$ controls how much noise is added at step $t$—the **noise schedule**. Common schedules (linear, cosine) start with small $\beta_t$ values and gradually increase, ensuring smooth degradation from data to noise.

A crucial mathematical property: we can **directly jump to any timestep** $z_t$ without iterating through all intermediate steps. This enables efficient training, where we randomly sample timesteps and train the denoiser on all noise levels simultaneously.



::: {#fig-genai-diffusion-denoising}

![]({{< meta params.images_path >}}diffusion_denoising.png)

Reverse (denoising) process: A neural network $f_\theta(z_t, t)$ predicts the noise $\epsilon_t$ added at step $t$, enabling reconstruction of $z_{t-1}$. From @prince_understanding_2023.
:::

### Learning to Denoise

The neural network learns to predict the noise $\epsilon_t$ that was added at each timestep. Crucially, the network is **conditioned on the timestep** $t$, since the noise level varies dramatically from early (almost clean) to late (almost pure noise) steps. The training objective is simple:

$$
\mathcal{L} = \mathbb{E}_{t, x, \epsilon} \| \epsilon - f_\theta(z_t, t) \|^2
$$

Train the network to predict the noise $\epsilon$ that was added to create $z_t$ from $x$.

::: {#fig-genai-diffusion-algorithm}

![]({{< meta params.images_path >}}diffusion_algorithm.png)

Training and sampling algorithms: Training samples random timesteps and noise; sampling iteratively denoises from $T$ to 0. Guidance techniques (not shown) can further improve sample quality.
:::

### Architecture: The U-Net

The denoising network typically uses a **U-Net architecture**—a convolutional encoder-decoder with skip connections. The U-Net processes noisy images at multiple resolutions, with timestep $t$ embedded and injected throughout the network.

::: {#fig-genai-diffusion-unet}

![]({{< meta params.images_path >}}diffusion_unet.png)

U-Net architecture for diffusion models: Downsampling path (left) captures context, upsampling path (right) enables precise localization, and skip connections (horizontal) preserve spatial details. Timestep embeddings condition the network on noise level. From @prince_understanding_2023.
:::

::: {.callout-note title="Pros and Cons of Diffusion Models"}

**Pros**

- High-quality, diverse outputs
- Stable and reliable training

**Cons**

- Slow inference time
- High compute (training + inference) cost

:::

::: {.callout-tip title="Key Takeaway: Diffusion Models" collapse="true"}

Diffusion models learn to reverse a gradual noising process:

1. **Forward process** (fixed): Add Gaussian noise over $T$ steps: $z_t = \sqrt{1-\beta_t} z_{t-1} + \sqrt{\beta_t} \epsilon_t$
2. **Reverse process** (learned): Neural network predicts noise $\epsilon$ to denoise: $\mathcal{L} = \mathbb{E} \| \epsilon - f_\theta(z_t, t) \|^2$

**Intuition**: Breaking generation into many small denoising steps makes each step easier to learn. U-Net architecture processes noisy images at multiple scales, conditioned on timestep.

**Result**: State-of-the-art sample quality and training stability. Trade-off: requires $T$ sequential steps for inference (slow).

:::

## Comparing GANs, VAEs, and Diffusion Models

Having explored each model family, how do they actually behave differently? Let's examine their behavior on simple 1D and 2D distributions to build intuition about their strengths and failure modes.

### 1D Distribution Comparison

::: {#fig-genai-1d-true}

![]({{< meta params.images_path >}}1d_comparison/true_distribution.png)

True 1D data distribution: A bimodal distribution that the generative models must learn to reproduce.
:::

::: {#fig-genai-1d-comparison layout-ncol=3}

![GAN training]({{< meta params.images_path >}}1d_comparison/gan_training_progress.png)

![VAE training]({{< meta params.images_path >}}1d_comparison/vae_training_progress.png)

![Diffusion training]({{< meta params.images_path >}}1d_comparison/diffusion_training_progress.png)

Training progression on 1D bimodal distribution. Each panel shows how the learned distribution (orange) approaches the true distribution (blue) over training iterations. GANs converge quickly but risk mode collapse; VAEs converge smoothly but may oversmooth between modes; Diffusion models reliably capture both modes with stable convergence.
:::

::: {#fig-genai-1d-final}

![]({{< meta params.images_path >}}1d_comparison/final_comparison.png)

Final comparison: All three models after convergence, showing their different biases in capturing the bimodal distribution.
:::

**Key observations**:

- **GANs**: Sharp samples matching the data manifold, but prone to **mode collapse**—missing entire modes of the distribution
- **VAEs**: Smooth distributions that can **fill in gaps** between modes, producing samples in low-density regions
- **Diffusion**: Reliably captures all modes with high fidelity, though at computational cost

### 2D Distribution Comparison

::: {#fig-genai-2d-true}

![]({{< meta params.images_path >}}2d_comparison/true_distribution.png)

True 2D data distribution: A complex spiral pattern in 2D space.
:::

::: {#fig-genai-2d-comparison layout-ncol=3}

![GAN training]({{< meta params.images_path >}}2d_comparison/gan_training_progress.png)

![VAE training]({{< meta params.images_path >}}2d_comparison/vae_training_progress.png)

![Diffusion training]({{< meta params.images_path >}}2d_comparison/diffusion_training_progress.png)

Training progression on 2D spiral distribution. The models must learn a complex curved manifold. GANs produce sharp samples along the spiral but may miss regions; VAEs spread probability mass more smoothly but may blur the structure; Diffusion models capture fine geometric details reliably.
:::

::: {#fig-genai-2d-final}

![]({{< meta params.images_path >}}2d_comparison/final_comparison.png)

Final 2D comparison: Converged distributions overlaid on the true spiral. Notice how each model type handles the complex geometry differently.
:::

**Practical implications**:

- **Choose GANs** when you need fast inference and sharp outputs, and can afford to monitor training carefully
- **Choose VAEs** when you need interpretable latent spaces, stable training, or downstream representation learning
- **Choose Diffusion** when sample quality is paramount and you have sufficient compute budget

The "no free lunch" theorem applies: no model dominates across all metrics. Modern systems often combine approaches (e.g., Latent Diffusion runs diffusion in a VAE latent space).

## Pre-Trained Models

Training generative models from scratch requires massive datasets (millions of images), significant compute resources (weeks on multiple GPUs), and careful hyperparameter tuning. For most practical applications, **pre-trained models** provide a much more accessible starting point.

The landscape has been transformed by models like **Stable Diffusion**, which bring state-of-the-art generation capabilities to consumer hardware. These models are trained on billions of image-text pairs and can generate high-quality, diverse images from text prompts—with minimal customization required for many use cases.

::: {.callout-note title="Pros and Cons of Pre-Trained Models"}

**Pros**

- High-quality image generation out-of-the-box
- Generate images via natural language prompts
- Easy to customize or fine-tune with minimal investment (LoRA, DreamBooth)
- Active community sharing models, techniques, and improvements

**Cons**

- Not suitable if your data is from a highly specific domain (e.g., medical imaging, satellite imagery)
- May not work well for very specific output constraints (e.g., exact branding requirements)
- Biases from training data may propagate to outputs
- Licensing considerations for commercial use

:::

### Stable Diffusion: Practical State-of-the-Art

**Stable Diffusion** [@rombach_high-resolution_2022] is an open-source text-to-image model that has become the de facto standard for accessible, high-quality image generation. Unlike pure pixel-space diffusion (which is computationally expensive), Stable Diffusion uses **latent diffusion**—running the diffusion process in a compressed latent space.

::: {#fig-genai-stable-diffusion-xl-examples}

![]({{< meta params.images_path >}}pract_stable_diffusion_xl_examples.png)

Stable Diffusion XL: Example generations showing diverse styles and subjects from text prompts. [Source](https://huggingface.co/stabilityai/stable-diffusion-3.5-large).
:::


::: {#fig-genai-diffusion-sd-architecture}

![]({{< meta params.images_path >}}diffusion_sd_architecture_genbook.png)

Stable Diffusion pipeline: Text is encoded via CLIP, diffusion runs in VAE latent space (much smaller than pixel space), and the VAE decoder produces the final image. Adapted from GenBook.
:::

### Why Latent Diffusion?

Running diffusion directly on high-resolution images (e.g., 512×512×3) is computationally expensive—the U-Net must process millions of values at each denoising step. **Latent Diffusion** solves this by:

1. **Pre-training a VAE** to compress images into a lower-dimensional latent space (e.g., 64×64×4)
2. **Running diffusion in the latent space** (much faster!)
3. **Decoding** the final latent representation back to pixel space

This provides an **8× reduction** in memory and computation, making high-resolution generation practical on consumer GPUs.

### Conditioning on Text (and More)

Stable Diffusion supports **text-conditional generation** via cross-attention to CLIP text embeddings. The U-Net attends to text features at multiple scales, enabling precise control over generated content. Beyond text, the framework supports conditioning on:

- **Images**: For inpainting, outpainting, and image-to-image translation
- **Depth maps**: For 3D-aware generation
- **Segmentation masks**: For spatial control
- **Style embeddings**: For artistic control

::: {#fig-genai-diffusion-latent-diff-architecture}

![]({{< meta params.images_path >}}diffusion_latent_diff_architecture.png)

Latent Diffusion architecture: The denoising U-Net operates in VAE latent space and uses cross-attention to condition on text (or other modalities). From @rombach_high-resolution_2022.
:::

::: {#fig-genai-lora-architecture}

![]({{< meta params.images_path >}}lora.png)

LoRA (Low-Rank Adaptation): Instead of fine-tuning all weights $W$, LoRA trains low-rank matrices $A$ and $B$ where $\Delta W = BA$. This drastically reduces trainable parameters. From @hu_lora_2021.
:::

### Fine-Tuning with LoRA

While pre-trained models are powerful, you often want to adapt them to specific styles, subjects, or domains. Traditional fine-tuning updates all model parameters (billions of weights!), requiring massive memory and compute. **LoRA (Low-Rank Adaptation)** [@hu_lora_2021] offers a far more efficient alternative.

**Key idea**: Instead of updating the full weight matrix $W$, LoRA learns a low-rank update $\Delta W = BA$ where:

- $B \in \mathbb{R}^{d \times r}$ and $A \in \mathbb{R}^{r \times k}$
- $r \ll \min(d, k)$ (typically $r = 4$ to $16$)
- Only $A$ and $B$ are trained; original weights $W$ remain frozen

This reduces trainable parameters by **1000×** or more, enabling fine-tuning on a single consumer GPU in hours rather than days. LoRA adapters can be easily shared, combined, and switched, creating a thriving ecosystem of specialized models.

**Popular use cases**:

- **Style transfer**: Train on artworks to mimic specific artistic styles
- **Subject learning**: DreamBooth + LoRA to generate images of specific people/objects
- **Domain adaptation**: Adapt to specialized domains (anime, architecture, product photography)

### Hardware Requirements

The computational demands vary significantly between inference and training:

#### Inference (Image Generation)

- **Minimum**: 4-6 GB VRAM (GPU)
  - Stable Diffusion 1.5 at 512×512 resolution
  - Longer generation times, limited batch sizes
- **Recommended**: 8-12 GB VRAM
  - Stable Diffusion XL at 1024×1024
  - Comfortable batch sizes, faster generation
- **High-end**: 16+ GB VRAM
  - Multiple models loaded simultaneously
  - Video generation, 3D-aware models

**CPU-only** inference is possible but **extremely slow** (minutes per image vs. seconds). Many optimization techniques exist: quantization, attention slicing, model offloading.

#### Fine-Tuning (LoRA)

- **Minimum**: 12-16 GB VRAM
  - LoRA fine-tuning with small batch sizes
  - Gradient checkpointing, mixed precision required
- **Recommended**: 24 GB VRAM
  - Comfortable batch sizes for faster convergence
  - Multiple LoRA ranks simultaneously

#### Full Training (From Scratch)

- **Minimum**: Multiple A100 GPUs (40-80 GB each)
- **Typical**: Clusters with 100+ GPUs
- **Duration**: Days to weeks
- **Cost**: $10,000s to $100,000s

For most researchers and practitioners, pre-trained models + LoRA fine-tuning offer the best trade-off.

### Open-Source Ecosystem

The open-source community has created a rich ecosystem of tools, models, and resources:

#### HuggingFace

[HuggingFace](https://huggingface.co/) is the central hub for open-source generative models:

- **Diffusers library**: Unified Python API for diffusion models with extensive documentation
  ```python
  from diffusers import StableDiffusionPipeline
  pipe = StableDiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-2-1")
  image = pipe("A photo of a cat wearing sunglasses").images[0]
  ```
- **Model Hub**: 100,000+ models including base models, fine-tunes, and LoRA adapters
- **Spaces**: Interactive demos for trying models before downloading
- **Documentation**: Comprehensive guides for inference, fine-tuning, and training

**Best for**: Programmatic access, research, production deployments, custom workflows

#### CivitAI

[CivitAI](https://civitai.com/) is a community-driven platform specializing in Stable Diffusion models:

- **Extensive model library**: Thousands of community-created models, LoRAs, and embeddings
- **Style-focused**: Strong emphasis on artistic styles, anime, photorealism
- **Preview images**: See example outputs before downloading
- **Version control**: Track model updates and improvements

**Best for**: Exploring diverse styles, finding specialized models, community engagement

#### Automatic1111 WebUI

[Automatic1111](https://github.com/AUTOMATIC1111/stable-diffusion-webui) provides a feature-rich web interface for local generation:

- **Local execution**: Run models on your own hardware with full control
- **Advanced features**: Inpainting, img2img, ControlNet, depth guidance
- **Extensions ecosystem**: Hundreds of community plugins
- **Batch processing**: Generate and iterate efficiently

**Best for**: Interactive experimentation, full control over generation, no API costs

### Closed-Source Platforms

Commercial platforms offer convenient APIs and frontier models:

#### OpenAI (DALL-E)

- **Latest model**: DALL-E 3 with enhanced prompt following and coherence
- **API access**: Programmatic generation, inpainting, variations
- **Pricing**: Pay per image generated
- **Strengths**: Strong text understanding, safe outputs, reliable API

#### Google Gemini (Imagen)

- **High photorealism**: Particularly strong at realistic images
- **Multimodal**: Integration with Gemini's broader capabilities
- **Controlled access**: Available through Google Cloud
- **Strengths**: Photo-quality outputs, factual accuracy

#### Replicate

- **Hybrid platform**: Both open- and closed-source models via unified API
- **Pay-per-use**: No infrastructure management, pay only for inference
- **Host custom models**: Deploy your own fine-tuned models
- **Wide selection**: SDXL, Midjourney alternatives, domain-specific models

#### Midjourney

- **Artistic focus**: Highly curated, aesthetic, and stylized outputs
- **Discord interface**: Community-driven prompt engineering
- **No API**: Interface only, no programmatic access
- **Strengths**: Artistic coherence, aesthetic quality, strong defaults

**Choosing a platform**:

- **For prototyping**: HuggingFace Spaces or Replicate (fast, no setup)
- **For production**: HuggingFace Diffusers or OpenAI API (reliable, scalable)
- **For experimentation**: Automatic1111 (full control, no API costs)
- **For art/design**: Midjourney or CivitAI models (aesthetic quality)

## Conclusions & Discussions

::: {#fig-genai-deterioration-on-pollution}

![]({{< meta params.images_path >}}deterioration_on_pollution.png)

Example: Deterioration of generated image quality under pollution or domain shift. From @alemohammad_self-consuming_2023.
:::


Zunächst hatte die Polizei … die Aktion habe nicht stattgefunden, das Bild davon sei ein »Fake«.

... Auf Anfrage des SPIEGEL sagte ein Sprecher der Brandenburger Polizei: »Ob die Projektion tatsächlich so ablief, ist Teil dieser Ermittlungen. Derzeit können wir es jedenfalls nicht ausschließen.«


::: {#fig-genai-real-or-not-musk}

![]({{< meta params.images_path >}}real_or_not_musk.png)

"Real or Not?" [Link](https://www.spiegel.de/panorama/elon-musk-staatsschutz-ermittelt-nach-projektion-tesla-werk-in-gruenheide-a-8268048f-050b-4ae7-adf8-bb9cf572cf25?sara_ref=re-so-app-sh)
:::


::: {#fig-genai-ai-bias-wp-front}

![]({{< meta params.images_path >}}ai_bias_wp_front.png)

AI bias: Example from a whitepaper front page. [Link](https://www.washingtonpost.com/technology/interactive/2024/ai-bias-beautiful-women-ugly-images/)
:::



::: {#fig-genai-ai-bias-wp-examples}

![]({{< meta params.images_path >}}ai_bias_wp_examples.png)

AI bias: Additional examples. [Link](https://www.washingtonpost.com/technology/interactive/2024/ai-bias-beautiful-women-ugly-images/)
:::

### Project Ideas

Fine-tune Stable-Diffusion with Dreambooth approach to generate images of yourself.
Inspect degradation of generative models when trained on synthetic data.
Implement Diffusion Model from scratch.
Chain different models into a creative pipeline.

## References

Understanding Deep Learning, Price, https://udlbook.github.io/udlbook/
Variational Autoencoders: https://lilianweng.github.io/posts/2018-08-12-vae/
Generative Deep Learning, 2nd Edition, David Foster



## References

::: {#refs}
:::
