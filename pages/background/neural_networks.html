<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Marco Willi">

<title>2 - Neural Networks – CAS Deep Learning - Computer Vision</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../pages/background/frameworks.html" rel="next">
<link href="../../pages/background/machine_learning.html" rel="prev">
<link href="../..//assets/cv_logo_small.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-065a5179aebd64318d7ea99d77b64a9e.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-159896a7fbbc1299bfd73fe3af9cd852.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-97BR3FTSN1"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-97BR3FTSN1', { 'anonymize_ip': true});
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../pages/background/machine_learning.html">Background &amp; Preparation</a></li><li class="breadcrumb-item"><a href="../../pages/background/neural_networks.html">2 - Neural Networks</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-center sidebar-header">
      <a href="../../index.html" class="sidebar-logo-link">
      <img src="../../assets/cv_logo_small.png" alt="" class="sidebar-logo light-content py-0 d-lg-inline d-none">
      <img src="../../assets/cv_logo_small.png" alt="" class="sidebar-logo dark-content py-0 d-lg-inline d-none">
      </a>
      <div class="sidebar-tools-main">
    <a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2025" title="GitHub organization" class="quarto-navigation-tool px-1" aria-label="GitHub organization"><i class="bi bi-github"></i></a>
    <a href="" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-code-square"></i></a>
</div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Background &amp; Preparation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/background/machine_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1 - Machine Learning Basics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/background/neural_networks.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">2 - Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/background/frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3 - Software &amp; Hardware for Deep Learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Lectures</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1 - Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/cnns.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2 - Convolutional Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3 - Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/representation_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4 - Representation Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/practical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5 - Practical</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/recent_advances.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6 - Recent Advances</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Slides</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/cnns.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Convolutional Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/representation_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Representation Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/practical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Practical Considerations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/recent_advances.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Recent Advances</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false">
 <span class="menu-text">Resources</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/literature.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Books</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/links.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Helpful Links &amp; Resources</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Mathematical Notation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/student_projects.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Project Ideas</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#biological-neural-networks" id="toc-biological-neural-networks" class="nav-link active" data-scroll-target="#biological-neural-networks"><span class="header-section-number">1</span> Biological Neural Networks</a></li>
  <li><a href="#artificial-neural-networks" id="toc-artificial-neural-networks" class="nav-link" data-scroll-target="#artificial-neural-networks"><span class="header-section-number">2</span> Artificial Neural Networks</a></li>
  <li><a href="#sec-nn-mlps" id="toc-sec-nn-mlps" class="nav-link" data-scroll-target="#sec-nn-mlps"><span class="header-section-number">3</span> From Linear Models to Neural Networks</a>
  <ul class="collapse">
  <li><a href="#activation-functions" id="toc-activation-functions" class="nav-link" data-scroll-target="#activation-functions"><span class="header-section-number">3.1</span> Activation Functions</a></li>
  <li><a href="#universal-approximation-theorem" id="toc-universal-approximation-theorem" class="nav-link" data-scroll-target="#universal-approximation-theorem"><span class="header-section-number">3.2</span> Universal Approximation Theorem</a></li>
  </ul></li>
  <li><a href="#deep-learning" id="toc-deep-learning" class="nav-link" data-scroll-target="#deep-learning"><span class="header-section-number">4</span> Deep Learning</a></li>
  <li><a href="#implementation-in-pytorch" id="toc-implementation-in-pytorch" class="nav-link" data-scroll-target="#implementation-in-pytorch"><span class="header-section-number">5</span> Implementation in PyTorch</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">6</span> References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2025/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../pages/background/machine_learning.html">Background &amp; Preparation</a></li><li class="breadcrumb-item"><a href="../../pages/background/neural_networks.html">2 - Neural Networks</a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">2 - Neural Networks</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Marco Willi </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Institute for Data Science I4DS, FHNW
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-tip callout-titled" title="Learning Objectives">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Learning Objectives
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>After this lecture you should be able to:</p>
<ul>
<li>Explain the relationship between linear models and multilayer perceptrons.</li>
<li>Describe why activation functions enable non-linear function approximation.</li>
<li>Interpret the role of depth vs width (efficiency of representation, hierarchical features).</li>
<li>Implement and configure simple MLP architectures in PyTorch.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="TLDR Recap">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>TLDR Recap
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>From Linear to Non-Linear:</strong></p>
<ul>
<li>Linear models: <span class="math inline">\(f(\mathbf{x}) = \mathbf{W}\mathbf{x} + \mathbf{b}\)</span> (limited to linear decision boundaries)</li>
<li>Neural networks add <strong>activation functions</strong> (ReLU, sigmoid, tanh) enabling non-linear mappings</li>
<li>Without activation functions, multiple layers collapse to a single linear transformation</li>
</ul>
<p><strong>Key Concepts:</strong></p>
<ul>
<li><strong>Layers</strong>: Input → Hidden (intermediate computations) → Output</li>
<li><strong>Activation Functions</strong>: ReLU most common (<span class="math inline">\(\max(0,x)\)</span>), enables modeling complex patterns</li>
<li><strong>Universal Approximation</strong>: Shallow networks can approximate any continuous function (theoretically)</li>
<li><strong>Depth vs Width</strong>: Deep networks more efficient than wide shallow ones for complex functions</li>
</ul>
<p><strong>Why Deep Learning Works:</strong></p>
<ul>
<li><strong>Hierarchical Features</strong>: Early layers learn simple patterns (edges, colors) → deeper layers learn complex concepts (objects, faces)</li>
<li><strong>Efficiency</strong>: Adding depth requires fewer neurons than increasing width</li>
<li><strong>Biological Inspiration</strong>: Mimics hierarchical processing in visual cortex</li>
</ul>
<p><strong>Architecture Components:</strong></p>
<ul>
<li><strong>Multilayer Perceptron (MLP)</strong>: Fully connected layers with activation functions</li>
<li><strong>Parameters</strong>: Weights <span class="math inline">\(\mathbf{W}\)</span> and biases <span class="math inline">\(\mathbf{b}\)</span> learned via optimization (backpropagation)</li>
<li><strong>Example</strong>: 784 → 128 → 10 network has ~102K parameters (784×128 + 128×10 + biases)</li>
</ul>
<p><strong>Practical Implementation (PyTorch):</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    nn.Flatten(),</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">784</span>, <span class="dv">128</span>),</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">128</span>, <span class="dv">10</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Key Takeaway:</strong> Neural networks transform inputs through learned hierarchical representations, with activation functions enabling the non-linearity needed to solve complex real-world problems.</p>
</div>
</div>
</div>
<section id="biological-neural-networks" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="biological-neural-networks"><span class="header-section-number">1</span> Biological Neural Networks</h2>
<p>A biological neural network is a part of the nervous system and consists of interconnected neurons. A neuron is connected to other neurons via dendrites (these are “weighted” input signals) and via the axon (output signal) (see <a href="#fig-nn-neurons" class="quarto-xref">Figure&nbsp;1</a>). If the input signals exceed a certain threshold, the neuron “fires” and sends a signal through the axon, which then serves as an input signal for other neurons. Humans have about 86 billion neurons, each connected to about 1000 others on average (<a href="https://en.wikipedia.org/wiki/Brain">See Wikipedia</a>).</p>
<div id="fig-nn-neurons" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nn-neurons-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/neural_networks/connected_neurons.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nn-neurons-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Schematic representation of connected neurons. <span class="citation" data-cites="phillips_speed_2015">Phillips (<a href="#ref-phillips_speed_2015" role="doc-biblioref">2015</a>)</span>
</figcaption>
</figure>
</div>
<p>Optical signals are processed, among other things, in the visual cortex (<a href="#fig-nn-visual-cortex" class="quarto-xref">Figure&nbsp;2</a>). Signals are processed hierarchically, with the first layers recognizing simple patterns and later layers recognizing shapes and objects. See also the work of Hubel and Wiesel <span class="citation" data-cites="hubel_receptive_1959">Hubel and Wiesel (<a href="#ref-hubel_receptive_1959" role="doc-biblioref">1959</a>)</span>.</p>
<div id="fig-nn-visual-cortex" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nn-visual-cortex-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/neural_networks/ventralvisualstream_v2.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nn-visual-cortex-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Representation of transformations in the visual cortex. <span class="citation" data-cites="kubilius_ventral_2017">Kubilius (<a href="#ref-kubilius_ventral_2017" role="doc-biblioref">2017</a>)</span>
</figcaption>
</figure>
</div>
<p>The question that arises is:</p>
<p><strong>Can we create artificial neural networks and reproduce the performance of biological neural networks?</strong></p>
</section>
<section id="artificial-neural-networks" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="artificial-neural-networks"><span class="header-section-number">2</span> Artificial Neural Networks</h2>
<p>Artificial neural networks are models of biological networks. Such models were created and implemented technically as early as the 1940s.</p>
<p>Neural networks are often represented with a graph. The <em>nodes</em> are individual neurons in the network, and the <em>edges</em> are connections between the neurons (see <a href="#fig-mlp-structure" class="quarto-xref">Figure&nbsp;3</a>). The neurons are arranged in <em>layers</em>, with each neuron in a layer connected to every neuron in adjacent layers. The <em>input layer</em> represents the data, the <em>output layer</em> the (observable) outputs, and the <em>hidden layers</em> are within the network. The connections between the neurons are weighted.</p>
<div id="fig-mlp-structure" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mlp-structure-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/neural_networks/mlp.jpeg" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mlp-structure-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: A neural network with two <em>hidden layers</em>. The lines show connections between neurons. Source: <span class="citation" data-cites="li_cs231n_2022">Li (<a href="#ref-li_cs231n_2022" role="doc-biblioref">2022</a>)</span>.
</figcaption>
</figure>
</div>
</section>
<section id="sec-nn-mlps" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="sec-nn-mlps"><span class="header-section-number">3</span> From Linear Models to Neural Networks</h2>
<p>In the following, we will describe neural networks mathematically. We start with a linear model. A linear model has the following form (see <a href="#eq-nn-linear-model" class="quarto-xref">Equation&nbsp;1</a>):</p>
<p><span id="eq-nn-linear-model"><span class="math display">\[
   f(\mathbf{x}^{(i)}) = \mathbf{W} \mathbf{x}^{(i)}  +  \mathbf{b}
\tag{1}\]</span></span></p>
<p>With the following elements:</p>
<ul>
<li>The <span class="math inline">\(i\)</span>-th data point <span class="math inline">\(\mathbf{x}^{(i)}\)</span> is a <span class="math inline">\(p\)</span>-dimensional vector: <span class="math inline">\(\in \mathbb{R}^{p \times 1}\)</span></li>
<li>Model weights <span class="math inline">\(\mathbf{W} \in \mathbb{R}^{k \times p}\)</span> linearily project the data point to <span class="math inline">\(k\)</span> dimensions.</li>
<li>A bias term <span class="math inline">\(\mathbf{b} \in \mathbb{R}^{k \times 1}\)</span> is added to the result.</li>
</ul>
<p><span class="math inline">\(f(\mathbf{x}^{(i)})\)</span> is thus <span class="math inline">\(\in  \mathbb{R}^{k \times 1}\)</span>.</p>
<p>The following parameters must be learned from training data using an optimization method: <span class="math inline">\(\mathbf{W}\)</span> and <span class="math inline">\(\mathbf{b}\)</span>.</p>
<p>In neural networks, the linear model is extended with additional <em>layers</em>. <a href="#eq-nn-deep-linear-model" class="quarto-xref">Equation&nbsp;2</a> defines a neural network with two layers (one hidden layer <span class="math inline">\(\mathbf{W}^{(1)}, \mathbf{b}^{(1)}\)</span> and one output layer <span class="math inline">\(\mathbf{W}^{(2)}, \mathbf{b}^{(2)}\)</span>).</p>
<p><span id="eq-nn-deep-linear-model"><span class="math display">\[
   f(\mathbf{x}^{(i)}) = \mathbf{W}^{(2)} g\big(\mathbf{W}^{(1)} \mathbf{x}^{(i)}  +  \mathbf{b}^{(1)} \big)  +  \mathbf{b}^{(2)}
\tag{2}\]</span></span></p>
<p>Where <span class="math inline">\(g()\)</span> is an <em>activation function</em>, such as the <em>ReLU</em> function shown in <a href="#eq-nn-relu" class="quarto-xref">Equation&nbsp;3</a>. Only due to <em>activation functions</em> non-linear relationships can be modelled. Without activation functions, the model collapses to a simple linear model with one layer.</p>
<p><span id="eq-nn-relu"><span class="math display">\[
\text{ReLU}(x) = \begin{cases}
x, &amp; \text{if } x \geq 0 \\
0, &amp; \text{if } x &lt; 0
\end{cases}
\tag{3}\]</span></span></p>
<p>Every layer between the input and output layer is called a <em>hidden layer</em>.</p>
<p>Typically, not just one data point is processed but a <em>mini-batch</em> of data, or even the entire dataset. The data points are arranged in a data matrix <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{n \times p}\)</span>. The linear model from <a href="#eq-nn-linear-model" class="quarto-xref">Equation&nbsp;1</a> can be rewritten in matrix notation as shown in <a href="#eq-nn-linear-model-matrix" class="quarto-xref">Equation&nbsp;4</a>.</p>
<p><span id="eq-nn-linear-model-matrix"><span class="math display">\[
   f( \mathbf{X}) = \mathbf{X} \mathbf{W}^T  +  \mathbf{b}^T
\tag{4}\]</span></span></p>
<p>The bias term <span class="math inline">\(\mathbf{b}\)</span> is <em>broadcast</em> (<a href="https://pytorch.org/docs/stable/notes/broadcasting.html">details here</a>), so it can be added to each projected input.</p>
<section id="activation-functions" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="activation-functions"><span class="header-section-number">3.1</span> Activation Functions</h3>
<p>The following code shows how <em>activation</em> functions are important for modeling non-linear relationships. The model has a hidden layer with several neurons but (left) no activation function and (right) with <em>ReLU</em> activation.</p>
<div id="cell-fig-nn-linear-regions" class="cell" width="600" data-execution_count="1">
<div class="cell-output cell-output-display">
<div id="fig-nn-linear-regions" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nn-linear-regions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="neural_networks_files/figure-html/fig-nn-linear-regions-output-1.png" width="941" height="506" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nn-linear-regions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Linear (left) vs non-linear (right) activation function.
</figcaption>
</figure>
</div>
</div>
</div>
<p><a href="#fig-nn-mlp-shallow" class="quarto-xref">Figure&nbsp;5</a> now shows a neural network including <em>activation functions</em> (ReLU). Sometimes the biases are also shown as nodes that feed into the next layer.</p>
<div id="fig-nn-mlp-shallow" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nn-mlp-shallow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/neural_networks/mlp_shallow.jpg" class="img-fluid figure-img" width="800">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nn-mlp-shallow-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: A neural network with a <em>hidden layer</em>. The lines show connections between neurons and their weights <span class="math inline">\(w_{i,j}\)</span>.
</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Activation Function Cheat Sheet">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Activation Function Cheat Sheet
</div>
</div>
<div class="callout-body-container callout-body">
<p>Common choices:</p>
<ul>
<li>ReLU: <span class="math inline">\(\max(0, x)\)</span> (sparse activations, mitigates vanishing gradients).</li>
<li>LeakyReLU / GELU: Smoother or with negative slope; GELU often used in transformers.</li>
<li>Sigmoid: Saturates; mainly for probabilities/logits output.</li>
<li>Tanh: Zero-centered but still saturates; rarely preferred over ReLU-family now.</li>
<li>Softplus / Mish / Swish: Smooth alternatives; sometimes modest gains, higher compute.</li>
</ul>
<p>Guideline: Start with ReLU (or GELU for transformer-like blocks); change only if you have empirical evidence.</p>
</div>
</div>
</section>
<section id="universal-approximation-theorem" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="universal-approximation-theorem"><span class="header-section-number">3.2</span> Universal Approximation Theorem</h3>
<p>With a shallow neural network, any continuous function can be modeled with arbitrary accuracy (<em>Universal Approximation Theorem</em>). The following graphic illustrates that as the number of linear functions (and thus piecewise linear regions) increases, the approximation of the underlying function becomes more accurate.</p>
<div id="fig-nn-universal-approx" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nn-universal-approx-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/neural_networks/linear_regions.jpg" class="img-fluid figure-img" width="800">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nn-universal-approx-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Approximation of a 1-D function with piecewise linear regions. The more regions, the more accurate the approximation. Source: <span class="citation" data-cites="prince_understanding_2023">Prince (<a href="#ref-prince_understanding_2023" role="doc-biblioref">2023</a>)</span>
</figcaption>
</figure>
</div>
<p><strong>Neural networks are therefore a particularly powerful class of models!</strong></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Refresh your neural network know-how!
</div>
</div>
<div class="callout-body-container callout-body">
<div class="quarto-video"><iframe data-external="1" src="https://www.youtube.com/embed/aircAruvnKk?feature=shared" width="800" height="450" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
</div>
</section>
</section>
<section id="deep-learning" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="deep-learning"><span class="header-section-number">4</span> Deep Learning</h2>
<p>Using neural networks with multiple <em>hidden layers</em> is refered to as deep learning. <a href="#fig-nn-deep-learning" class="quarto-xref">Figure&nbsp;7</a> illustrates such a model with 5 hidden layers and ReLU activation functions. Such a neural network is also called a <em>multilayer perceptron</em> (MLP) or a <em>fully connected neural network</em> or a network with <em>dense layers</em>. It consists of linear layers and activation functions.</p>
<div id="fig-nn-deep-learning" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nn-deep-learning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/neural_networks/deep_neural_net.png" class="img-fluid figure-img" width="800">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nn-deep-learning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Illustration of a <em>deep learning</em> model with 5 <em>hidden layers</em>, from <span class="citation" data-cites="johnson_eecs_2022">Johnson (<a href="#ref-johnson_eecs_2022" role="doc-biblioref">2022</a>)</span>
</figcaption>
</figure>
</div>
<p>Although <em>shallow neural networks</em> can theoretically model arbitrarily complex functions, the number of neurons required is often impractical. It can be shown that adding <em>hidden layers</em> (increasing the depth of a network) is much more efficient (requires fewer neurons) for modeling complex functions.</p>
<p>In practice, it has been shown that the first layers in a network learn simple features. These are combined in deeper layers to learn and detect increasingly abstract concepts. This is well illustrated with images. When a neural network is applied to images, the first layers learn to detect simple features, such as edges and colors. Further layers then detect shapes and objects, leading to specific items or people (see <a href="#fig-nn-efficience-deep-learning" class="quarto-xref">Figure&nbsp;8</a>).</p>
<div id="fig-nn-efficience-deep-learning" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nn-efficience-deep-learning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/neural_networks/hierarchical_features.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nn-efficience-deep-learning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Hierarchical features, from <span class="citation" data-cites="lee_unsupervised_2011">Lee et al. (<a href="#ref-lee_unsupervised_2011" role="doc-biblioref">2011</a>)</span>
</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Train Neural Networks in your Browser!</strong></p>
<p>To directly observe the functioning of a neural network and try out the influence of various configurations, you can train networks directly in the browser using the following link: <a href="https://playground.tensorflow.org/">TensorFlow Playground</a>.</p>
<p><img src="../../assets/images/neural_networks/tf_playground.png" class="img-fluid" width="600"></p>
</div>
</div>
</section>
<section id="implementation-in-pytorch" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="implementation-in-pytorch"><span class="header-section-number">5</span> Implementation in PyTorch</h2>
<p>The following code shows how we can implement a configurable neural network.</p>
<div id="3d96d089" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MLP(nn.Module):</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size, hidden_layer_sizes, num_outputs):</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize submodules of your module - typically layers</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># that your module needs - these can then be used in the</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># forward pass</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.flatten <span class="op">=</span> nn.Flatten()</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># a ModuleList allows you to flexibly chain submodules</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># in a list - depending e.g. on configuration parameters</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layers <span class="op">=</span> nn.ModuleList()</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>        last_size <span class="op">=</span> input_size</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> size <span class="kw">in</span> hidden_layer_sizes:</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.hidden_layers.append(nn.Linear(last_size, size))</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>            last_size <span class="op">=</span> size</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_layer <span class="op">=</span> nn.Linear(last_size, num_outputs)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""The forward pass of your module."""</span></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.flatten(x)</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.hidden_layers:</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> F.relu(layer(x))</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.output_layer(x)</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage:</span></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the model with input size 784, one hidden layer of size 128, and 10 output units.</span></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> MLP(<span class="dv">784</span>, [<span class="dv">128</span>], <span class="dv">10</span>)</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Example input vector (batch size of 32, input size of 784)</span></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">32</span>, <span class="dv">784</span>)</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Forward pass</span></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> model(x)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>We can easily display the architecture details with <code>torchinfo</code></p>
<div id="7f15951c" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the model architecture</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchinfo <span class="im">import</span> summary</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>summary(model, input_size<span class="op">=</span>(<span class="dv">32</span>, <span class="dv">784</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
MLP                                      [32, 10]                  --
├─Flatten: 1-1                           [32, 784]                 --
├─ModuleList: 1-2                        --                        --
│    └─Linear: 2-1                       [32, 128]                 100,480
├─Linear: 1-3                            [32, 10]                  1,290
==========================================================================================
Total params: 101,770
Trainable params: 101,770
Non-trainable params: 0
Total mult-adds (Units.MEGABYTES): 3.26
==========================================================================================
Input size (MB): 0.10
Forward/backward pass size (MB): 0.04
Params size (MB): 0.41
Estimated Total Size (MB): 0.54
==========================================================================================</code></pre>
</div>
</div>
<div class="callout callout-style-simple callout-note">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Question</strong></p>
<p>How many parameters does this neural network have, and why?</p>
<details>
<summary>
Answer
</summary>
<p>Based on the <code>torchinfo</code> summary output above, the network has:</p>
<p><strong>Layer 1</strong>: 784 inputs × 128 hidden units + 128 biases = <strong>100,480 parameters</strong></p>
<p><strong>Layer 2</strong>: 128 inputs × 10 outputs + 10 biases = <strong>1,290 parameters</strong></p>
<p><strong>Total: 101,770 parameters</strong></p>
<p>Each fully connected layer has: - Weight matrix of size (input_dim × output_dim) - Bias vector of size (output_dim)</p>
<p>This is why the number of parameters grows quickly with network size!</p>
</details>
</div>
</div>
</div>
</section>
<section id="references" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="references"><span class="header-section-number">6</span> References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-hubel_receptive_1959" class="csl-entry" role="listitem">
Hubel, D. H., and T. N. Wiesel. 1959. <span>“Receptive Fields of Single Neurones in the Cat’s Striate Cortex.”</span> <em>The Journal of Physiology</em> 148 (3): 574–91. <a href="https://doi.org/10.1113/jphysiol.1959.sp006308">https://doi.org/10.1113/jphysiol.1959.sp006308</a>.
</div>
<div id="ref-johnson_eecs_2022" class="csl-entry" role="listitem">
Johnson, Justin. 2022. <span>“<span>EECS</span> 498.008 / 598.008 <span>Deep</span> <span>Learning</span> for <span>Computer</span> <span>Vision</span>.”</span> Lecture {Notes} / {Slides}. <a href="https://web.eecs.umich.edu/~justincj/teaching/eecs498/WI2022/">https://web.eecs.umich.edu/~justincj/teaching/eecs498/WI2022/</a>.
</div>
<div id="ref-kubilius_ventral_2017" class="csl-entry" role="listitem">
Kubilius, Jonas. 2017. <span>“Ventral Visual Stream.”</span> <a href="https://figshare.com/articles/figure/Ventral_visual_stream/106794">https://figshare.com/articles/figure/Ventral_visual_stream/106794</a>.
</div>
<div id="ref-lee_unsupervised_2011" class="csl-entry" role="listitem">
Lee, Honglak, Roger Grosse, Rajesh Ranganath, and Andrew Y. Ng. 2011. <span>“Unsupervised Learning of Hierarchical Representations with Convolutional Deep Belief Networks.”</span> <em>Communications of the ACM</em> 54 (10): 95–103. <a href="https://doi.org/10.1145/2001269.2001295">https://doi.org/10.1145/2001269.2001295</a>.
</div>
<div id="ref-li_cs231n_2022" class="csl-entry" role="listitem">
Li, Fei-Fei. 2022. <span>“<span>CS231n</span> <span>Convolutional</span> <span>Neural</span> <span>Networks</span> for <span>Visual</span> <span>Recognition</span>.”</span> Lecture {Notes}. <a href="https://cs231n.github.io">https://cs231n.github.io</a>.
</div>
<div id="ref-phillips_speed_2015" class="csl-entry" role="listitem">
Phillips, Devin K. 2015. <span>“Speed of the <span>Human</span> <span>Brain</span>.”</span> <em>Ask A Biologist</em>, May. <a href="https://askabiologist.asu.edu/plosable/speed-human-brain">https://askabiologist.asu.edu/plosable/speed-human-brain</a>.
</div>
<div id="ref-prince_understanding_2023" class="csl-entry" role="listitem">
Prince, Simon J. D. 2023. <em>Understanding <span>Deep</span> <span>Learning</span></em>. MIT Press. <a href="https://udlbook.github.io/udlbook/">https://udlbook.github.io/udlbook/</a>.
</div>
</div>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/marco-willi\.github\.io\/cas-dl-compvis-lectures-hs2025\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../pages/background/machine_learning.html" class="pagination-link" aria-label="1 - Machine Learning Basics">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">1 - Machine Learning Basics</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../pages/background/frameworks.html" class="pagination-link" aria-label="3 - Software &amp; Hardware for Deep Learning">
        <span class="nav-page-text">3 - Software &amp; Hardware for Deep Learning</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "2 - Neural Networks"</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="an">params:</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co">   images_path: "/assets/images/neural_networks/"</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title="Learning Objectives" collapse="true"}</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>After this lecture you should be able to:</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Explain the relationship between linear models and multilayer perceptrons.</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Describe why activation functions enable non-linear function approximation.</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Interpret the role of depth vs width (efficiency of representation, hierarchical features).</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Implement and configure simple MLP architectures in PyTorch.</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip title="TLDR Recap" collapse="true"}</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>**From Linear to Non-Linear:**</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Linear models: $f(\mathbf{x}) = \mathbf{W}\mathbf{x} + \mathbf{b}$ (limited to linear decision boundaries)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Neural networks add **activation functions** (ReLU, sigmoid, tanh) enabling non-linear mappings</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Without activation functions, multiple layers collapse to a single linear transformation</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>**Key Concepts:**</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Layers**: Input → Hidden (intermediate computations) → Output</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Activation Functions**: ReLU most common ($\max(0,x)$), enables modeling complex patterns</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Universal Approximation**: Shallow networks can approximate any continuous function (theoretically)</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Depth vs Width**: Deep networks more efficient than wide shallow ones for complex functions</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>**Why Deep Learning Works:**</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Hierarchical Features**: Early layers learn simple patterns (edges, colors) → deeper layers learn complex concepts (objects, faces)</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Efficiency**: Adding depth requires fewer neurons than increasing width</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Biological Inspiration**: Mimics hierarchical processing in visual cortex</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>**Architecture Components:**</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Multilayer Perceptron (MLP)**: Fully connected layers with activation functions</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Parameters**: Weights $\mathbf{W}$ and biases $\mathbf{b}$ learned via optimization (backpropagation)</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Example**: 784 → 128 → 10 network has ~102K parameters (784×128 + 128×10 + biases)</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>**Practical Implementation (PyTorch):**</span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(</span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>    nn.Flatten(),</span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">784</span>, <span class="dv">128</span>),</span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">128</span>, <span class="dv">10</span>)</span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a>**Key Takeaway:** Neural networks transform inputs through learned hierarchical representations, with activation functions enabling the non-linearity needed to solve complex real-world problems.</span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a>::: {.content-hidden}</span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a>{{&lt; include /assets/_macros.tex &gt;}}</span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true" tabindex="-1"></a><span class="fu">## Biological Neural Networks</span></span>
<span id="cb5-65"><a href="#cb5-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-66"><a href="#cb5-66" aria-hidden="true" tabindex="-1"></a>A biological neural network is a part of the nervous system and consists of interconnected neurons. A neuron is connected to other neurons via dendrites (these are "weighted" input signals) and via the axon (output signal) (see @fig-nn-neurons). If the input signals exceed a certain threshold, the neuron "fires" and sends a signal through the axon, which then serves as an input signal for other neurons. Humans have about 86 billion neurons, each connected to about 1000 others on average (<span class="co">[</span><span class="ot">See Wikipedia</span><span class="co">](https://en.wikipedia.org/wiki/Brain)</span>).</span>
<span id="cb5-67"><a href="#cb5-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-68"><a href="#cb5-68" aria-hidden="true" tabindex="-1"></a>::: {#fig-nn-neurons}</span>
<span id="cb5-69"><a href="#cb5-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-70"><a href="#cb5-70" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}connected_neurons.png)</span>{width=600}</span>
<span id="cb5-71"><a href="#cb5-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-72"><a href="#cb5-72" aria-hidden="true" tabindex="-1"></a>Schematic representation of connected neurons. @phillips_speed_2015</span>
<span id="cb5-73"><a href="#cb5-73" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-74"><a href="#cb5-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-75"><a href="#cb5-75" aria-hidden="true" tabindex="-1"></a>Optical signals are processed, among other things, in the visual cortex (@fig-nn-visual-cortex). Signals are processed hierarchically, with the first layers recognizing simple patterns and later layers recognizing shapes and objects. See also the work of Hubel and Wiesel @hubel_receptive_1959.</span>
<span id="cb5-76"><a href="#cb5-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-77"><a href="#cb5-77" aria-hidden="true" tabindex="-1"></a>::: {#fig-nn-visual-cortex}</span>
<span id="cb5-78"><a href="#cb5-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-79"><a href="#cb5-79" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}ventralvisualstream_v2.png)</span>{width=600}</span>
<span id="cb5-80"><a href="#cb5-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-81"><a href="#cb5-81" aria-hidden="true" tabindex="-1"></a>Representation of transformations in the visual cortex. @kubilius_ventral_2017</span>
<span id="cb5-82"><a href="#cb5-82" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-83"><a href="#cb5-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-84"><a href="#cb5-84" aria-hidden="true" tabindex="-1"></a>The question that arises is:</span>
<span id="cb5-85"><a href="#cb5-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-86"><a href="#cb5-86" aria-hidden="true" tabindex="-1"></a>**Can we create artificial neural networks and reproduce the performance of biological neural networks?**</span>
<span id="cb5-87"><a href="#cb5-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-88"><a href="#cb5-88" aria-hidden="true" tabindex="-1"></a><span class="fu">## Artificial Neural Networks</span></span>
<span id="cb5-89"><a href="#cb5-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-90"><a href="#cb5-90" aria-hidden="true" tabindex="-1"></a>Artificial neural networks are models of biological networks. Such models were created and implemented technically as early as the 1940s.</span>
<span id="cb5-91"><a href="#cb5-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-92"><a href="#cb5-92" aria-hidden="true" tabindex="-1"></a>Neural networks are often represented with a graph. The _nodes_ are individual neurons in the network, and the _edges_ are connections between the neurons (see @fig-mlp-structure). The neurons are arranged in _layers_, with each neuron in a layer connected to every neuron in adjacent layers. The _input layer_ represents the data, the _output layer_ the (observable) outputs, and the _hidden layers_ are within the network. The connections between the neurons are weighted.</span>
<span id="cb5-93"><a href="#cb5-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-94"><a href="#cb5-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-95"><a href="#cb5-95" aria-hidden="true" tabindex="-1"></a>::: {#fig-mlp-structure}</span>
<span id="cb5-96"><a href="#cb5-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-97"><a href="#cb5-97" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}mlp.jpeg)</span>{width=600}</span>
<span id="cb5-98"><a href="#cb5-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-99"><a href="#cb5-99" aria-hidden="true" tabindex="-1"></a>A neural network with two _hidden layers_. The lines show connections between neurons. Source: @li_cs231n_2022.</span>
<span id="cb5-100"><a href="#cb5-100" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-101"><a href="#cb5-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-102"><a href="#cb5-102" aria-hidden="true" tabindex="-1"></a><span class="fu">## From Linear Models to Neural Networks {#sec-nn-mlps}</span></span>
<span id="cb5-103"><a href="#cb5-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-104"><a href="#cb5-104" aria-hidden="true" tabindex="-1"></a>In the following, we will describe neural networks mathematically. We start with a linear model. A linear model has the following form (see @eq-nn-linear-model):</span>
<span id="cb5-105"><a href="#cb5-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-106"><a href="#cb5-106" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-107"><a href="#cb5-107" aria-hidden="true" tabindex="-1"></a>   f(\mathbf{x}^{(i)}) = \mathbf{W} \mathbf{x}^{(i)}  +  \mathbf{b}</span>
<span id="cb5-108"><a href="#cb5-108" aria-hidden="true" tabindex="-1"></a>$$ {#eq-nn-linear-model}</span>
<span id="cb5-109"><a href="#cb5-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-110"><a href="#cb5-110" aria-hidden="true" tabindex="-1"></a>With the following elements:</span>
<span id="cb5-111"><a href="#cb5-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-112"><a href="#cb5-112" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The $i$-th data point $\vect{x}^{(i)}$ is a $p$-dimensional vector: $\in \mathbb{R}^{p \times 1}$</span>
<span id="cb5-113"><a href="#cb5-113" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Model weights $\m{W} \in \mathbb{R}^{k \times p}$ linearily project the data point to $k$ dimensions.</span>
<span id="cb5-114"><a href="#cb5-114" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A bias term $\vect{b} \in \mathbb{R}^{k \times 1}$ is added to the result.</span>
<span id="cb5-115"><a href="#cb5-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-116"><a href="#cb5-116" aria-hidden="true" tabindex="-1"></a>$f(\mathbf{x}^{(i)})$ is thus $\in  \mathbb{R}^{k \times 1}$.</span>
<span id="cb5-117"><a href="#cb5-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-118"><a href="#cb5-118" aria-hidden="true" tabindex="-1"></a>The following parameters must be learned from training data using an optimization method: $\m{W}$ and $\vect{b}$.</span>
<span id="cb5-119"><a href="#cb5-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-120"><a href="#cb5-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-121"><a href="#cb5-121" aria-hidden="true" tabindex="-1"></a>In neural networks, the linear model is extended with additional _layers_. @eq-nn-deep-linear-model defines a neural network with two layers (one hidden layer $\m{W}^{(1)}, \vect{b}^{(1)}$ and one output layer $\m{W}^{(2)}, \vect{b}^{(2)}$).</span>
<span id="cb5-122"><a href="#cb5-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-123"><a href="#cb5-123" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-124"><a href="#cb5-124" aria-hidden="true" tabindex="-1"></a>   f(\vect{x}^{(i)}) = \m{W}^{(2)} g\big(\m{W}^{(1)} \vect{x}^{(i)}  +  \vect{b}^{(1)} \big)  +  \vect{b}^{(2)}</span>
<span id="cb5-125"><a href="#cb5-125" aria-hidden="true" tabindex="-1"></a>$$ {#eq-nn-deep-linear-model}</span>
<span id="cb5-126"><a href="#cb5-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-127"><a href="#cb5-127" aria-hidden="true" tabindex="-1"></a>Where $g()$ is an _activation function_, such as the _ReLU_ function shown in @eq-nn-relu. Only due to _activation functions_ non-linear relationships can be modelled. Without activation functions, the model collapses to a simple linear model with one layer.</span>
<span id="cb5-128"><a href="#cb5-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-129"><a href="#cb5-129" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-130"><a href="#cb5-130" aria-hidden="true" tabindex="-1"></a>\text{ReLU}(x) = \begin{cases}</span>
<span id="cb5-131"><a href="#cb5-131" aria-hidden="true" tabindex="-1"></a>x, &amp; \text{if } x \geq 0 <span class="sc">\\</span></span>
<span id="cb5-132"><a href="#cb5-132" aria-hidden="true" tabindex="-1"></a>0, &amp; \text{if } x &lt; 0</span>
<span id="cb5-133"><a href="#cb5-133" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb5-134"><a href="#cb5-134" aria-hidden="true" tabindex="-1"></a>$$ {#eq-nn-relu}</span>
<span id="cb5-135"><a href="#cb5-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-136"><a href="#cb5-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-137"><a href="#cb5-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-138"><a href="#cb5-138" aria-hidden="true" tabindex="-1"></a>Every layer between the input and output layer is called a _hidden layer_.</span>
<span id="cb5-139"><a href="#cb5-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-140"><a href="#cb5-140" aria-hidden="true" tabindex="-1"></a>Typically, not just one data point is processed but a _mini-batch_ of data, or even the entire dataset. The data points are arranged in a data matrix $\m{X} \in \mathbb{R}^{n \times p}$. The linear model from @eq-nn-linear-model can be rewritten in matrix notation as shown in @eq-nn-linear-model-matrix.</span>
<span id="cb5-141"><a href="#cb5-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-142"><a href="#cb5-142" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb5-143"><a href="#cb5-143" aria-hidden="true" tabindex="-1"></a>   f( \m{X}) = \m{X} \m{W}^T  +  \vect{b}^T</span>
<span id="cb5-144"><a href="#cb5-144" aria-hidden="true" tabindex="-1"></a>$$ {#eq-nn-linear-model-matrix}</span>
<span id="cb5-145"><a href="#cb5-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-146"><a href="#cb5-146" aria-hidden="true" tabindex="-1"></a>The bias term $\vect{b}$ is _broadcast_ (<span class="co">[</span><span class="ot">details here</span><span class="co">](https://pytorch.org/docs/stable/notes/broadcasting.html)</span>), so it can be added to each projected input.</span>
<span id="cb5-147"><a href="#cb5-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-148"><a href="#cb5-148" aria-hidden="true" tabindex="-1"></a><span class="fu">### Activation Functions</span></span>
<span id="cb5-149"><a href="#cb5-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-150"><a href="#cb5-150" aria-hidden="true" tabindex="-1"></a>The following code shows how _activation_ functions are important for modeling non-linear relationships. The model has a hidden layer with several neurons but (left) no activation function and (right) with _ReLU_ activation.</span>
<span id="cb5-151"><a href="#cb5-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-154"><a href="#cb5-154" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb5-155"><a href="#cb5-155" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb5-156"><a href="#cb5-156" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb5-157"><a href="#cb5-157" aria-hidden="true" tabindex="-1"></a><span class="co">#| width: 600</span></span>
<span id="cb5-158"><a href="#cb5-158" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-nn-linear-regions</span></span>
<span id="cb5-159"><a href="#cb5-159" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap:</span></span>
<span id="cb5-160"><a href="#cb5-160" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - "Linear (left) vs non-linear (right) activation function."</span></span>
<span id="cb5-161"><a href="#cb5-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-162"><a href="#cb5-162" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb5-163"><a href="#cb5-163" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-164"><a href="#cb5-164" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_blobs</span>
<span id="cb5-165"><a href="#cb5-165" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neural_network <span class="im">import</span> MLPClassifier</span>
<span id="cb5-166"><a href="#cb5-166" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mlxtend.plotting <span class="im">import</span> plot_decision_regions</span>
<span id="cb5-167"><a href="#cb5-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-168"><a href="#cb5-168" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_non_linear_vs_linear():</span>
<span id="cb5-169"><a href="#cb5-169" aria-hidden="true" tabindex="-1"></a>    fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>), ncols<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb5-170"><a href="#cb5-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-171"><a href="#cb5-171" aria-hidden="true" tabindex="-1"></a>    X, y <span class="op">=</span> make_blobs(</span>
<span id="cb5-172"><a href="#cb5-172" aria-hidden="true" tabindex="-1"></a>        n_samples<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb5-173"><a href="#cb5-173" aria-hidden="true" tabindex="-1"></a>        n_features<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb5-174"><a href="#cb5-174" aria-hidden="true" tabindex="-1"></a>        random_state<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb5-175"><a href="#cb5-175" aria-hidden="true" tabindex="-1"></a>        cluster_std<span class="op">=</span><span class="fl">0.5</span>,</span>
<span id="cb5-176"><a href="#cb5-176" aria-hidden="true" tabindex="-1"></a>        centers<span class="op">=</span>[(<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>), (<span class="dv">1</span>, <span class="dv">1</span>), (<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>), (<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)],</span>
<span id="cb5-177"><a href="#cb5-177" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb5-178"><a href="#cb5-178" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> np.where(y <span class="op">&lt;</span> <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb5-179"><a href="#cb5-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-180"><a href="#cb5-180" aria-hidden="true" tabindex="-1"></a>    clf <span class="op">=</span> MLPClassifier(</span>
<span id="cb5-181"><a href="#cb5-181" aria-hidden="true" tabindex="-1"></a>        hidden_layer_sizes<span class="op">=</span>[<span class="dv">10</span>],</span>
<span id="cb5-182"><a href="#cb5-182" aria-hidden="true" tabindex="-1"></a>        activation<span class="op">=</span><span class="st">"identity"</span>,</span>
<span id="cb5-183"><a href="#cb5-183" aria-hidden="true" tabindex="-1"></a>        max_iter<span class="op">=</span><span class="dv">200</span>,</span>
<span id="cb5-184"><a href="#cb5-184" aria-hidden="true" tabindex="-1"></a>        random_state<span class="op">=</span><span class="dv">123</span>,</span>
<span id="cb5-185"><a href="#cb5-185" aria-hidden="true" tabindex="-1"></a>        learning_rate_init<span class="op">=</span><span class="fl">1.0</span>,</span>
<span id="cb5-186"><a href="#cb5-186" aria-hidden="true" tabindex="-1"></a>    ).fit(X, y)</span>
<span id="cb5-187"><a href="#cb5-187" aria-hidden="true" tabindex="-1"></a>    _ <span class="op">=</span> plot_decision_regions(X, y, clf, ax<span class="op">=</span>ax[<span class="dv">0</span>])</span>
<span id="cb5-188"><a href="#cb5-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-189"><a href="#cb5-189" aria-hidden="true" tabindex="-1"></a>    clf <span class="op">=</span> MLPClassifier(</span>
<span id="cb5-190"><a href="#cb5-190" aria-hidden="true" tabindex="-1"></a>        hidden_layer_sizes<span class="op">=</span>[<span class="dv">10</span>],</span>
<span id="cb5-191"><a href="#cb5-191" aria-hidden="true" tabindex="-1"></a>        activation<span class="op">=</span><span class="st">"relu"</span>,</span>
<span id="cb5-192"><a href="#cb5-192" aria-hidden="true" tabindex="-1"></a>        max_iter<span class="op">=</span><span class="dv">400</span>,</span>
<span id="cb5-193"><a href="#cb5-193" aria-hidden="true" tabindex="-1"></a>        random_state<span class="op">=</span><span class="dv">123</span>,</span>
<span id="cb5-194"><a href="#cb5-194" aria-hidden="true" tabindex="-1"></a>        learning_rate_init<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb5-195"><a href="#cb5-195" aria-hidden="true" tabindex="-1"></a>    ).fit(X, y)</span>
<span id="cb5-196"><a href="#cb5-196" aria-hidden="true" tabindex="-1"></a>    _ <span class="op">=</span> plot_decision_regions(X, y, clf, ax<span class="op">=</span>ax[<span class="dv">1</span>])</span>
<span id="cb5-197"><a href="#cb5-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-198"><a href="#cb5-198" aria-hidden="true" tabindex="-1"></a>    _ <span class="op">=</span> ax[<span class="dv">0</span>].set_title(<span class="st">"Linear: $g(x) = x$"</span>)</span>
<span id="cb5-199"><a href="#cb5-199" aria-hidden="true" tabindex="-1"></a>    _ <span class="op">=</span> ax[<span class="dv">1</span>].set_title(<span class="st">"Non-Linear: $g(x) = ReLU(x)$"</span>)</span>
<span id="cb5-200"><a href="#cb5-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-201"><a href="#cb5-201" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> fig</span>
<span id="cb5-202"><a href="#cb5-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-203"><a href="#cb5-203" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plot_non_linear_vs_linear()</span>
<span id="cb5-204"><a href="#cb5-204" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb5-205"><a href="#cb5-205" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb5-206"><a href="#cb5-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-207"><a href="#cb5-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-208"><a href="#cb5-208" aria-hidden="true" tabindex="-1"></a>@fig-nn-mlp-shallow now shows a neural network including _activation functions_ (ReLU). Sometimes the biases are also shown as nodes that feed into the next layer.</span>
<span id="cb5-209"><a href="#cb5-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-210"><a href="#cb5-210" aria-hidden="true" tabindex="-1"></a>::: {#fig-nn-mlp-shallow}</span>
<span id="cb5-211"><a href="#cb5-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-212"><a href="#cb5-212" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}mlp_shallow.jpg)</span>{width=800}</span>
<span id="cb5-213"><a href="#cb5-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-214"><a href="#cb5-214" aria-hidden="true" tabindex="-1"></a>A neural network with a _hidden layer_. The lines show connections between neurons and their weights $w_{i,j}$.</span>
<span id="cb5-215"><a href="#cb5-215" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-216"><a href="#cb5-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-217"><a href="#cb5-217" aria-hidden="true" tabindex="-1"></a>::: {.callout-note title="Activation Function Cheat Sheet"}</span>
<span id="cb5-218"><a href="#cb5-218" aria-hidden="true" tabindex="-1"></a>Common choices:</span>
<span id="cb5-219"><a href="#cb5-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-220"><a href="#cb5-220" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>ReLU: $\max(0, x)$ (sparse activations, mitigates vanishing gradients).</span>
<span id="cb5-221"><a href="#cb5-221" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>LeakyReLU / GELU: Smoother or with negative slope; GELU often used in transformers.</span>
<span id="cb5-222"><a href="#cb5-222" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Sigmoid: Saturates; mainly for probabilities/logits output.</span>
<span id="cb5-223"><a href="#cb5-223" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Tanh: Zero-centered but still saturates; rarely preferred over ReLU-family now.</span>
<span id="cb5-224"><a href="#cb5-224" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Softplus / Mish / Swish: Smooth alternatives; sometimes modest gains, higher compute.</span>
<span id="cb5-225"><a href="#cb5-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-226"><a href="#cb5-226" aria-hidden="true" tabindex="-1"></a>Guideline: Start with ReLU (or GELU for transformer-like blocks); change only if you have empirical evidence.</span>
<span id="cb5-227"><a href="#cb5-227" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-228"><a href="#cb5-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-229"><a href="#cb5-229" aria-hidden="true" tabindex="-1"></a><span class="fu">### Universal Approximation Theorem</span></span>
<span id="cb5-230"><a href="#cb5-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-231"><a href="#cb5-231" aria-hidden="true" tabindex="-1"></a>With a shallow neural network, any continuous function can be modeled with arbitrary accuracy (_Universal Approximation Theorem_). The following graphic illustrates that as the number of linear functions (and thus piecewise linear regions) increases, the approximation of the underlying function becomes more accurate.</span>
<span id="cb5-232"><a href="#cb5-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-233"><a href="#cb5-233" aria-hidden="true" tabindex="-1"></a>::: {#fig-nn-universal-approx}</span>
<span id="cb5-234"><a href="#cb5-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-235"><a href="#cb5-235" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}linear_regions.jpg)</span>{width=800}</span>
<span id="cb5-236"><a href="#cb5-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-237"><a href="#cb5-237" aria-hidden="true" tabindex="-1"></a>Approximation of a 1-D function with piecewise linear regions. The more regions, the more accurate the approximation. Source: @prince_understanding_2023</span>
<span id="cb5-238"><a href="#cb5-238" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-239"><a href="#cb5-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-240"><a href="#cb5-240" aria-hidden="true" tabindex="-1"></a>**Neural networks are therefore a particularly powerful class of models!**</span>
<span id="cb5-241"><a href="#cb5-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-242"><a href="#cb5-242" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb5-243"><a href="#cb5-243" aria-hidden="true" tabindex="-1"></a><span class="fu">## Refresh your neural network know-how!</span></span>
<span id="cb5-244"><a href="#cb5-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-245"><a href="#cb5-245" aria-hidden="true" tabindex="-1"></a>{{&lt; video https://youtu.be/aircAruvnKk?feature=shared width=800 height=450 &gt;}}</span>
<span id="cb5-246"><a href="#cb5-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-247"><a href="#cb5-247" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-248"><a href="#cb5-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-249"><a href="#cb5-249" aria-hidden="true" tabindex="-1"></a><span class="fu">## Deep Learning</span></span>
<span id="cb5-250"><a href="#cb5-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-251"><a href="#cb5-251" aria-hidden="true" tabindex="-1"></a>Using neural networks with multiple _hidden layers_ is refered to as deep learning. @fig-nn-deep-learning illustrates such a model with 5 hidden layers and ReLU activation functions. Such a neural network is also called a _multilayer perceptron_ (MLP) or a _fully connected neural network_ or a network with _dense layers_. It consists of linear layers and activation functions.</span>
<span id="cb5-252"><a href="#cb5-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-253"><a href="#cb5-253" aria-hidden="true" tabindex="-1"></a>::: {#fig-nn-deep-learning}</span>
<span id="cb5-254"><a href="#cb5-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-255"><a href="#cb5-255" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}deep_neural_net.png)</span>{width=800}</span>
<span id="cb5-256"><a href="#cb5-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-257"><a href="#cb5-257" aria-hidden="true" tabindex="-1"></a>Illustration of a _deep learning_ model with 5 _hidden layers_, from @johnson_eecs_2022</span>
<span id="cb5-258"><a href="#cb5-258" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-259"><a href="#cb5-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-260"><a href="#cb5-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-261"><a href="#cb5-261" aria-hidden="true" tabindex="-1"></a>Although _shallow neural networks_ can theoretically model arbitrarily complex functions, the number of neurons required is often impractical. It can be shown that adding _hidden layers_ (increasing the depth of a network) is much more efficient (requires fewer neurons) for modeling complex functions.</span>
<span id="cb5-262"><a href="#cb5-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-263"><a href="#cb5-263" aria-hidden="true" tabindex="-1"></a>In practice, it has been shown that the first layers in a network learn simple features. These are combined in deeper layers to learn and detect increasingly abstract concepts. This is well illustrated with images. When a neural network is applied to images, the first layers learn to detect simple features, such as edges and colors. Further layers then detect shapes and objects, leading to specific items or people (see @fig-nn-efficience-deep-learning).</span>
<span id="cb5-264"><a href="#cb5-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-265"><a href="#cb5-265" aria-hidden="true" tabindex="-1"></a>::: {#fig-nn-efficience-deep-learning}</span>
<span id="cb5-266"><a href="#cb5-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-267"><a href="#cb5-267" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}hierarchical_features.jpg)</span></span>
<span id="cb5-268"><a href="#cb5-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-269"><a href="#cb5-269" aria-hidden="true" tabindex="-1"></a>Hierarchical features, from @lee_unsupervised_2011</span>
<span id="cb5-270"><a href="#cb5-270" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-271"><a href="#cb5-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-272"><a href="#cb5-272" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb5-273"><a href="#cb5-273" aria-hidden="true" tabindex="-1"></a>**Train Neural Networks in your Browser!**</span>
<span id="cb5-274"><a href="#cb5-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-275"><a href="#cb5-275" aria-hidden="true" tabindex="-1"></a>To directly observe the functioning of a neural network and try out the influence of various configurations, you can train networks directly in the browser using the following link: <span class="co">[</span><span class="ot">TensorFlow Playground</span><span class="co">](https://playground.tensorflow.org/)</span>.</span>
<span id="cb5-276"><a href="#cb5-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-277"><a href="#cb5-277" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}tf_playground.png)</span>{width=600}</span>
<span id="cb5-278"><a href="#cb5-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-279"><a href="#cb5-279" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-280"><a href="#cb5-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-281"><a href="#cb5-281" aria-hidden="true" tabindex="-1"></a><span class="fu">## Implementation in PyTorch</span></span>
<span id="cb5-282"><a href="#cb5-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-283"><a href="#cb5-283" aria-hidden="true" tabindex="-1"></a>The following code shows how we can implement a configurable neural network.</span>
<span id="cb5-284"><a href="#cb5-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-287"><a href="#cb5-287" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb5-288"><a href="#cb5-288" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb5-289"><a href="#cb5-289" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb5-290"><a href="#cb5-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-291"><a href="#cb5-291" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb5-292"><a href="#cb5-292" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb5-293"><a href="#cb5-293" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb5-294"><a href="#cb5-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-295"><a href="#cb5-295" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MLP(nn.Module):</span>
<span id="cb5-296"><a href="#cb5-296" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size, hidden_layer_sizes, num_outputs):</span>
<span id="cb5-297"><a href="#cb5-297" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb5-298"><a href="#cb5-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-299"><a href="#cb5-299" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize submodules of your module - typically layers</span></span>
<span id="cb5-300"><a href="#cb5-300" aria-hidden="true" tabindex="-1"></a>        <span class="co"># that your module needs - these can then be used in the</span></span>
<span id="cb5-301"><a href="#cb5-301" aria-hidden="true" tabindex="-1"></a>        <span class="co"># forward pass</span></span>
<span id="cb5-302"><a href="#cb5-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-303"><a href="#cb5-303" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.flatten <span class="op">=</span> nn.Flatten()</span>
<span id="cb5-304"><a href="#cb5-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-305"><a href="#cb5-305" aria-hidden="true" tabindex="-1"></a>        <span class="co"># a ModuleList allows you to flexibly chain submodules</span></span>
<span id="cb5-306"><a href="#cb5-306" aria-hidden="true" tabindex="-1"></a>        <span class="co"># in a list - depending e.g. on configuration parameters</span></span>
<span id="cb5-307"><a href="#cb5-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-308"><a href="#cb5-308" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_layers <span class="op">=</span> nn.ModuleList()</span>
<span id="cb5-309"><a href="#cb5-309" aria-hidden="true" tabindex="-1"></a>        last_size <span class="op">=</span> input_size</span>
<span id="cb5-310"><a href="#cb5-310" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> size <span class="kw">in</span> hidden_layer_sizes:</span>
<span id="cb5-311"><a href="#cb5-311" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.hidden_layers.append(nn.Linear(last_size, size))</span>
<span id="cb5-312"><a href="#cb5-312" aria-hidden="true" tabindex="-1"></a>            last_size <span class="op">=</span> size</span>
<span id="cb5-313"><a href="#cb5-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-314"><a href="#cb5-314" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_layer <span class="op">=</span> nn.Linear(last_size, num_outputs)</span>
<span id="cb5-315"><a href="#cb5-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-316"><a href="#cb5-316" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb5-317"><a href="#cb5-317" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""The forward pass of your module."""</span></span>
<span id="cb5-318"><a href="#cb5-318" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.flatten(x)</span>
<span id="cb5-319"><a href="#cb5-319" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.hidden_layers:</span>
<span id="cb5-320"><a href="#cb5-320" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> F.relu(layer(x))</span>
<span id="cb5-321"><a href="#cb5-321" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.output_layer(x)</span>
<span id="cb5-322"><a href="#cb5-322" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb5-323"><a href="#cb5-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-324"><a href="#cb5-324" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage:</span></span>
<span id="cb5-325"><a href="#cb5-325" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the model with input size 784, one hidden layer of size 128, and 10 output units.</span></span>
<span id="cb5-326"><a href="#cb5-326" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> MLP(<span class="dv">784</span>, [<span class="dv">128</span>], <span class="dv">10</span>)</span>
<span id="cb5-327"><a href="#cb5-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-328"><a href="#cb5-328" aria-hidden="true" tabindex="-1"></a><span class="co"># Example input vector (batch size of 32, input size of 784)</span></span>
<span id="cb5-329"><a href="#cb5-329" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">32</span>, <span class="dv">784</span>)</span>
<span id="cb5-330"><a href="#cb5-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-331"><a href="#cb5-331" aria-hidden="true" tabindex="-1"></a><span class="co"># Forward pass</span></span>
<span id="cb5-332"><a href="#cb5-332" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> model(x)</span>
<span id="cb5-333"><a href="#cb5-333" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb5-334"><a href="#cb5-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-335"><a href="#cb5-335" aria-hidden="true" tabindex="-1"></a>We can easily display the architecture details with <span class="in">`torchinfo`</span></span>
<span id="cb5-336"><a href="#cb5-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-339"><a href="#cb5-339" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb5-340"><a href="#cb5-340" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb5-341"><a href="#cb5-341" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb5-342"><a href="#cb5-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-343"><a href="#cb5-343" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the model architecture</span></span>
<span id="cb5-344"><a href="#cb5-344" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchinfo <span class="im">import</span> summary</span>
<span id="cb5-345"><a href="#cb5-345" aria-hidden="true" tabindex="-1"></a>summary(model, input_size<span class="op">=</span>(<span class="dv">32</span>, <span class="dv">784</span>))</span>
<span id="cb5-346"><a href="#cb5-346" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb5-347"><a href="#cb5-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-348"><a href="#cb5-348" aria-hidden="true" tabindex="-1"></a>::: {.callout-note appearance="simple"}</span>
<span id="cb5-349"><a href="#cb5-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-350"><a href="#cb5-350" aria-hidden="true" tabindex="-1"></a>**Question**</span>
<span id="cb5-351"><a href="#cb5-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-352"><a href="#cb5-352" aria-hidden="true" tabindex="-1"></a>How many parameters does this neural network have, and why?</span>
<span id="cb5-353"><a href="#cb5-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-354"><a href="#cb5-354" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb5-355"><a href="#cb5-355" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Answer<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb5-356"><a href="#cb5-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-357"><a href="#cb5-357" aria-hidden="true" tabindex="-1"></a>Based on the <span class="in">`torchinfo`</span> summary output above, the network has:</span>
<span id="cb5-358"><a href="#cb5-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-359"><a href="#cb5-359" aria-hidden="true" tabindex="-1"></a>**Layer 1**: 784 inputs × 128 hidden units + 128 biases = **100,480 parameters**</span>
<span id="cb5-360"><a href="#cb5-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-361"><a href="#cb5-361" aria-hidden="true" tabindex="-1"></a>**Layer 2**: 128 inputs × 10 outputs + 10 biases = **1,290 parameters**</span>
<span id="cb5-362"><a href="#cb5-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-363"><a href="#cb5-363" aria-hidden="true" tabindex="-1"></a>**Total: 101,770 parameters**</span>
<span id="cb5-364"><a href="#cb5-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-365"><a href="#cb5-365" aria-hidden="true" tabindex="-1"></a>Each fully connected layer has:</span>
<span id="cb5-366"><a href="#cb5-366" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Weight matrix of size (input_dim × output_dim)</span>
<span id="cb5-367"><a href="#cb5-367" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Bias vector of size (output_dim)</span>
<span id="cb5-368"><a href="#cb5-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-369"><a href="#cb5-369" aria-hidden="true" tabindex="-1"></a>This is why the number of parameters grows quickly with network size!</span>
<span id="cb5-370"><a href="#cb5-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-371"><a href="#cb5-371" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb5-372"><a href="#cb5-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-373"><a href="#cb5-373" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb5-374"><a href="#cb5-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-375"><a href="#cb5-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-376"><a href="#cb5-376" aria-hidden="true" tabindex="-1"></a><span class="fu">## References</span></span>
<span id="cb5-377"><a href="#cb5-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-378"><a href="#cb5-378" aria-hidden="true" tabindex="-1"></a>::: {#refs}</span>
<span id="cb5-379"><a href="#cb5-379" aria-hidden="true" tabindex="-1"></a>:::</span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© Copyright 2025, Marco Willi</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2025/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This page is built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>