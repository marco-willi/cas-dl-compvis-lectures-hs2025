---
title: "Project Ideas"
params:
   images_path: "/assets/images/student_projects/"
---

::: {.content-hidden}
$$
{{< include /assets/_macros.tex >}}
$$
:::


## Topic List

You can choose your own topic!

Alternatively, feel free to pick one of the following topics.


### Image Classification: EuroSAT Land Use and Land Cover Classification

**Goal**: Develop a model to classify satellite images. You should consider RGB images and images with 13 spectral bands (see [wiki](https://en.wikipedia.org/wiki/Sentinel-2)) in modeling. There are 10 classes and 27,000 images.

**Approach**: Investigate various model architectures and decide what works best. Compare pre-trained models with those you train from scratch. Use appropriate data augmentation techniques. Since the dataset is relatively small, you should be careful of overfitting and robustly compare different models. Use RGB-only models for simplicity.

**Focus**: Compare different pre-trained models (foundation models) and a model trained from scratch.

**Dataset**: The two datasets can be found here: [https://github.com/phelber/eurosat](https://github.com/phelber/eurosat). There is a dataset in RGB format and a dataset with 13 spectral bands.

:::{#fig-mp-eurosat}
![]({{< meta params.images_path >}}eurosat.png){width=600}

Source: [Link](https://github.com/phelber/eurosat)
:::

**Difficulty/Effort**: Small - Medium



### Image Classification: Food Images

**Goal**: Develop a model to classify images of food. Pick 10 food classes (≈10k imgs)

**Approach**: Use pre-trained (foundation) models.

**Focus**: Compare different models, including CLIP zero-shot learning, and a DINO variant. Investigate how DINO improves with more labels.

**Dataset**: The dataset can be found here: [https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/), see @bossard_food-101_2014. It is 5 GB.

:::{#fig-mp-eurosat}
![]({{< meta params.images_path >}}food101.png){width=600}

Source: [Link](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/)
:::

**Difficulty/Effort**: Small - Medium



### Image Classification: Synthetic Image Detection with Foundation Models

**Goal**: Develop a model to classify images into either synthetic or not (binary classification). You have to use pre-trained CLIP (@radford_learning_2021) models and fine-tune them for this task.

**Approach**: Choose a CLIP model (for example from here:  [HuggingFace](https://huggingface.co/models)). Understand how it works, and how the outputs of this model are organized. Use the pre-trained image encoder as a feature extractor and learn a small model on top of these features. Carefully split the dataset to train and evaluate it (image id, datasets, generative models).

**Focus**: Focus on making the model work (establish a working baseline). Train your model on a train-split of the Synthbuster/RAISE1-K dataset and evaluate on a validation-split. Test your model on SynthwildX. Report overall and model-specific performances.

**Dataset**: There are three ready-made datasets (Synthbuster, SynthWildX and [RAISE-1K](http://loki.disi.unitn.it/RAISE/index.php)) in the [dataset](https://huggingface.co/docs/datasets/en/index) format. It is important to read the documentation to efficiently use this format. The datasets consist of real images (RAISE-1K) and synthetic images (SynthwildX and Synthbuster) from different generative models. The Synthbuster dataset was created to match the RAISE-1K dataset: each real image was used to create a corresponding synthetic image (see @fig-mp-synthbuster). The SynthwildX dataset was scraped from social media and is thus a good independent test set.

- [SynthwildX - 450 MB](https://drive.google.com/file/d/1v3224djuHqMX2zbxoqPd6CDxA4Ali9qc/view?usp=drive_link)
- [Synthbuster and RAISE-1K - 2.3 GB](https://drive.google.com/file/d/1deG5gRNsHvnAA4nJzzIjL_iGpjHQqBY7/view?usp=drive_link)
- [(original link) Synthbuster](https://zenodo.org/records/10066460)
- [(original link) RAISE-1k](http://loki.disi.unitn.it/RAISE/index.php)


:::{#fig-mp-synthbuster width=800}
![]({{< meta params.images_path >}}synth_detection.png)

Images from @bammey_synthbuster_2023 and @dang-nguyen_raise_2015. The first row are real photographs that represent templates to generate synthetic images from (see second row onward).
:::

**Difficulty/Effort**: HIGH!


### Counting Sea Turtles from Drone Images

**Goal**: Develop a model to process high-resolution images taken from drones to count sea turtles. The  results can be used to survey marine turtles. The dataset includes approximately 1'000 images of high resolution (4,048 × 3,048 pixels).

**Approach**: Read the paper @gray_convolutional_2019 as inspiration. It contains an easy way to break the problem down to an image classification task (see @fig-mp-turtle). Implement your own CNN architecture and train a model. Use the train/validation splits as described in the paper. Compare different architectures and compare to the results reported in the paper. You can test different variations and techniques: CNN architectures, window-size, data augmentation and regularization.


**Dataset**: Available here: [https://zenodo.org/records/5004596#.ZChnr3ZBxD8](https://zenodo.org/records/5004596#.ZChnr3ZBxD8).


:::{#fig-mp-turtle}
![]({{< meta params.images_path >}}sea_turtles.png){width=600}

Source @gray_convolutional_2019
:::


**Difficulty/Effort**: Medium - High


### Semantic Segmentation: Underwater Imagery

**Goal**: Develop a segmentation model to segment underwater camera images. You should classify pixels into 8 classes on 1,500 images.

**Approach**: Develop your architecture based on your intuition and knowledge from the course. Read the paper @islam_semantic_2020 and implement one of the architectures presented (SUIM-Net RSB or SUIM-Net VGG) and compare with your architecture. Since the dataset is relatively small, you should be careful of overfitting and robustly compare different models.

**Dataset**: Available here: [https://irvlab.cs.umn.edu/resources/suim-dataset](https://irvlab.cs.umn.edu/resources/suim-dataset).

:::{#fig-mp-suim}
![]({{< meta params.images_path >}}semantic_seg_underwater.png){width=600}

Source: [Link](https://github.com/phelber/eurosat)
:::

**Difficulty/Effort**: Medium


### Semantic Segmentation: Pothole and Crack Detection

**Goal**: Segment images from street surfaces to detect regions with potholes and cracks. Classify each pixel into either normal, pothole or crack. The dataset consists of 4'340 images and masks.

**Approach**. This dataset was used in a competition. Make sure to consult a description of the best methods as described in @thompson_shrec_2022. Start with developing your architecture based on your intuition and knowledge from the course. One problem is the class imbalance, with many pixels being normal. Consider this when designing your loss function.

**Dataset**: Available here: [https://data.mendeley.com/datasets/kfth5g2xk3/2](https://data.mendeley.com/datasets/kfth5g2xk3/2). See also @andrea_ranieri_pothole_2022. This datset is actually a collection of different datasets. See @fig-mp-potholes for examples.

**Difficulty/Effort**: Medium

:::{#fig-mp-potholes}
![]({{< meta params.images_path >}}potholes_n_cracks.png){width=600}

Examples from different datasets. Red are cracks, blue potholes. From @thompson_shrec_2022.
:::



### Object Detection: Polyp Detection


**Goal**: Develop an object detection model to detect polyps in images from endoscopic procedures. The goal is to detect each polyp with a bounding box. There are 1'000 images for training. See left and mid image of @fig-mp-polyp.

**Approach**: Develop **your own architecture** based on the lecture notes. Follow the best practices and implement your ML pipeline carefully from the ground up by gradually adding complexity. Compare a few architectures and hyper-parameters.

**Dataset**: Available here: [https://datasets.simula.no/kvasir-seg/](https://datasets.simula.no/kvasir-seg/).


:::{#fig-mp-polyp}
![]({{< meta params.images_path >}}polyp_segmentation.png){width=600}

Input image (left), semantic segmentation (middle) and object detection (right). From @ro_kvasir-seg_2020.
:::

**Difficulty/Effort**: High

<!--


## Pet Breed Transfer Learning (small fine-tune)

**Task.** Fine-tune MobileNet/ResNet to classify 6–10 chosen breeds; visualize Grad-CAM on a few images.
**Dataset.** Oxford-IIIT Pet — <https://www.robots.ox.ac.uk/~vgg/data/pets/>
**Visual (pipeline).**
224×224 resize → pretrained backbone → small classification head → Grad-CAM on sample images



## Food-101 (Few-Class Mini-Challenge)

**Task.** Pick 10 food classes (≈10k imgs). Fine-tune a small net; discuss class imbalance & noisy labels.
**Dataset.** Food-101 — <https://www.vision.ee.ethz.ch/datasets_extra/food-101/>
**Visual (pipeline).**
food images → stratified split → transfer learning → metrics (Top-1, F1) → error gallery



## EuroSAT: Land-Cover from Space

**Task.** Classify land-use classes using EuroSAT RGB; compare training from scratch vs. ImageNet-pretrained; map most confused classes.
**Dataset.** EuroSAT — <https://github.com/phelber/EuroSAT>
**Visual (pipeline).**
Sentinel-2 patch (RGB) → preprocess → {ResNet18 scratch | ResNet18 pretrained} → compare metrics


## Industrial Anomaly Detection (one category)

**Task.** Choose one MVTec AD class (e.g., *bottle*). Train a simple autoencoder or kNN-embedding baseline on **only normal** images; detect defects on the test set.
**Dataset.** MVTec AD — <https://www.mvtec.com/company/research/datasets/mvtec-ad>
**Visual (pipeline).**
(normal train set) → {autoencoder | CNN embeddings + kNN} → fit “normality”
(test image) → reconstruct/embed → anomaly score → threshold → pass/fail


## Tiny Image Retrieval (CBIR) with Embeddings

**Task.** Compute image embeddings (pretrained CNN). Given a query image, return top-k similar images (cosine). Evaluate with precision@k on CIFAR-10 or a 10-class Food-101 subset.
**Dataset.** CIFAR-10 — <https://www.cs.toronto.edu/~kriz/cifar.html> or Food-101 subset
**Visual (pipeline).**
index images → embed with CNN → build vector index
query image → embed → nearest neighbors → precision@k


## Natural Scenes (Intel) — Quick Transfer Learn

**Task.** Classify 6 scene types (buildings/forest/glacier/mountain/sea/street). Great for a clean, single-evening fine-tune with tidy splits.
**Dataset.** Intel Image Classification (Kaggle) — <https://www.kaggle.com/datasets/puneet6060/intel-image-classification>
**Visual (pipeline).**
scene image → resize (~160) → MobileNetV3-Small → accuracy + ROC


## Suggested Time Split (per project)

- **Setup & data** (env, download, sanity checks): 2–3 h
- **Baseline model** (train once, save metrics): 4–5 h
- **One improvement** (augmentation / lr-schedule / transfer): 3–4 h
- **Evaluation & visuals** (confusion matrix, sample errors): 2–3 h
- **Short report** (1–2 pages with plots): 2 h


### Notes & Tips

- Favor **small backbones** and **few classes** to stay under the time budget.
- When datasets are large, **sample 2–10 classes** or **subset N images per class**.
- Track basic metrics (accuracy, F1), save a confusion matrix, and include 8–12 **failure cases** (thumbnails) in your report.
 -->
