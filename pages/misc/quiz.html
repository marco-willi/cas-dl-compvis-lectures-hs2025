<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Marco Willi">

<title>Quiz &amp; Exam Prep – CAS Deep Learning - Computer Vision</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../pages/misc/literature.html" rel="next">
<link href="../../pages/misc/exercises.html" rel="prev">
<link href="../..//assets/cv_logo_small.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-065a5179aebd64318d7ea99d77b64a9e.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-159896a7fbbc1299bfd73fe3af9cd852.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-97BR3FTSN1"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-97BR3FTSN1', { 'anonymize_ip': true});
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../pages/misc/quiz.html">Quiz &amp; Exam Prep</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-center sidebar-header">
      <a href="../../index.html" class="sidebar-logo-link">
      <img src="../../assets/cv_logo_small.png" alt="" class="sidebar-logo light-content py-0 d-lg-inline d-none">
      <img src="../../assets/cv_logo_small.png" alt="" class="sidebar-logo dark-content py-0 d-lg-inline d-none">
      </a>
      <div class="sidebar-tools-main">
    <a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2025" title="GitHub organization" class="quarto-navigation-tool px-1" aria-label="GitHub organization"><i class="bi bi-github"></i></a>
    <a href="" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-code-square"></i></a>
</div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Background &amp; Preparation</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/background/machine_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1 - Machine Learning Basics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/background/neural_networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2 - Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/background/frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3 - Software &amp; Hardware for Deep Learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Lectures</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1 - Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/cnns.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2 - Convolutional Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3 - Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/representation_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4 - Representation Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/practical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5 - Practical</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/recent_advances.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6 - Recent Advances</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Slides</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/cnns.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Convolutional Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/representation_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Representation Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/practical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Practical Considerations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/recent_advances.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Recent Advances</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/quiz.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Quiz &amp; Exam Prep</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false">
 <span class="menu-text">Resources</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/literature.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Books</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/links.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Helpful Links &amp; Resources</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Mathematical Notation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/student_projects.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Project Ideas</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#intro" id="toc-intro" class="nav-link active" data-scroll-target="#intro"><span class="header-section-number">1</span> Intro</a></li>
  <li><a href="#modellierung-von-bildern-mit-neuronalen-netzwerken" id="toc-modellierung-von-bildern-mit-neuronalen-netzwerken" class="nav-link" data-scroll-target="#modellierung-von-bildern-mit-neuronalen-netzwerken"><span class="header-section-number">2</span> Modellierung von Bildern mit Neuronalen Netzwerken</a></li>
  <li><a href="#convolutional-neural-networks-cnns" id="toc-convolutional-neural-networks-cnns" class="nav-link" data-scroll-target="#convolutional-neural-networks-cnns"><span class="header-section-number">3</span> Convolutional Neural Networks (CNNs)</a></li>
  <li><a href="#image-classification" id="toc-image-classification" class="nav-link" data-scroll-target="#image-classification"><span class="header-section-number">4</span> Image Classification</a></li>
  <li><a href="#representation-learning" id="toc-representation-learning" class="nav-link" data-scroll-target="#representation-learning"><span class="header-section-number">5</span> Representation Learning</a></li>
  <li><a href="#praktische-aspekte" id="toc-praktische-aspekte" class="nav-link" data-scroll-target="#praktische-aspekte"><span class="header-section-number">6</span> Praktische Aspekte</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2025/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Quiz &amp; Exam Prep</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Marco Willi </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Institute for Data Science I4DS, FHNW
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  


</header>


<p>Die folgenden Fragen helfen das gelernte nochmal zu überprüfen und könnten Prüfungsfragen sein.</p>
<p>Beachten Sie auch die Quiz-Fragen in den Unerrichtsunterlagen.</p>
<section id="intro" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="intro"><span class="header-section-number">1</span> Intro</h2>
<ol type="1">
<li>Inwiefern definiert das <em>Semantic Gap</em> eine grundsätzliche Herausforderung in Computer Vision?</li>
</ol>
<details>
<summary>
Antwort anzeigen
</summary>
<p>Das Semantic Gap beschreibt die Diskrepanz oder enorme kognitive Leistung um die Pixelrepräsentation von Bildern (2-D Gitter von Pixel-Werte) semantisch zu inerpretieren. Also von Pixeln zu Katzen. Diese fundamentale Herausforderung macht es schwierig, direkt von Pixelwerten auf semantische Bedeutung zu schliessen. Deep Learning hilft, diese Lücke zu überbrücken, indem es hierarchische Repräsentationen lernt, die schrittweise von lokalen Features (Kanten, Farben) zu hochdimensionalen semantischen Konzepten (Objekte, Szenen) führen.</p>
</details>
<ol start="2" type="1">
<li>Sie möchten ein Objekt in einem Bild klassifizieren: Ist es besser das ganze Bild zur Verfügung zu haben oder nur einen Ausschnitt vom Objekt? Begründen Sie.</li>
</ol>
<details>
<summary>
Antwort anzeigen
</summary>
<p>Das hängt vom Anwendungsfall ab</p>
<p><strong>Vorteile ganzes Bild:</strong></p>
<ul>
<li>Kontextinformationen sind verfügbar (z.B. Umgebung hilft bei Klassifikation)</li>
<li>Räumliche Beziehungen zu anderen Objekten können genutzt werden</li>
</ul>
<p><strong>Vorteile eines Ausschnitts:</strong></p>
<ul>
<li>Fokus auf das relevante Objekt, weniger Ablenkung durch Hintergrund</li>
<li>Objekt nimmt mehr Pixel ein → mehr Details sichtbar (je nach Architektur)</li>
</ul>
<p><strong>Empfehlung:</strong> In der Regel ist das ganze Bild vorzuziehen, da Kontext wichtig ist.</p>
</details>
</section>
<section id="modellierung-von-bildern-mit-neuronalen-netzwerken" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="modellierung-von-bildern-mit-neuronalen-netzwerken"><span class="header-section-number">2</span> Modellierung von Bildern mit Neuronalen Netzwerken</h2>
<ol type="1">
<li>Sie möchten ein MLP auf Bilddaten trainieren. Die Bilder haben jedoch unterschiedliche räumliche Auflösungen, die von <span class="math inline">\(64 \times 64\)</span> bis zu <span class="math inline">\(512 \times 512\)</span> reichen. Ist das ein Problem? Wenn ja, was tun Sie?</li>
</ol>
<details>
<summary>
Antwort anzeigen
</summary>
<p><strong>Ja, das ist ein Problem!</strong> MLPs erwarten eine fixe Input-Dimensionalität. Ein Bild mit <span class="math inline">\(64 \times 64 \times 3 = 12{,}288\)</span> Pixeln ergibt einen anderen Input-Vektor als ein Bild mit <span class="math inline">\(512 \times 512 \times 3 = 786{,}432\)</span> Pixeln.</p>
<p><strong>Lösungen:</strong></p>
<ol type="1">
<li><strong>Resizing:</strong> Alle Bilder auf eine einheitliche Auflösung skalieren (z.B. <span class="math inline">\(224 \times 224\)</span>)</li>
<li><strong>Cropping:</strong> Zentrale Crops oder Random Crops auf einheitliche Grösse</li>
</ol>
</details>
<ol start="2" type="1">
<li>Sie möchten ein MLP auf Bildern mit einer Input Dimensionalität von <span class="math inline">\(3 \times 16 \times 16\)</span> (CHW) trainieren. Der erste Hidden Layer hat 4 Neuronen, der Output Layer 2. Wie viele Gewichte hat Ihr Modell? Wie viele, wenn sich die räumliche Auflösung auf <span class="math inline">\(32 \times 32\)</span> verdoppelt?</li>
</ol>
<details>
<summary>
Antwort anzeigen
</summary>
<p><strong>Berechnung für <span class="math inline">\(16 \times 16\)</span>:</strong></p>
<ul>
<li>Input: <span class="math inline">\(3 \times 16 \times 16 = 768\)</span> Neuronen</li>
<li>Hidden Layer: 4 Neuronen</li>
<li>Output Layer: 2 Neuronen</li>
</ul>
<p>Gewichte:</p>
<ul>
<li>Input → Hidden: <span class="math inline">\(768 \times 4 = 3{,}072\)</span> Gewichte (+ 4 Bias)</li>
<li>Hidden → Output: <span class="math inline">\(4 \times 2 = 8\)</span> Gewichte (+ 2 Bias)</li>
<li><strong>Total: 3,086 Parameter</strong></li>
</ul>
<p><strong>Berechnung für <span class="math inline">\(32 \times 32\)</span>:</strong></p>
<ul>
<li>Input: <span class="math inline">\(3 \times 32 \times 32 = 3{,}072\)</span> Neuronen</li>
<li>Input → Hidden: <span class="math inline">\(3{,}072 \times 4 = 12{,}288\)</span> Gewichte (+ 4 Bias)</li>
<li>Hidden → Output: <span class="math inline">\(4 \times 2 = 8\)</span> Gewichte (+ 2 Bias)</li>
<li><strong>Total: 12,302 Parameter</strong></li>
</ul>
<p>Die Anzahl der Parameter vervierfacht sich! Dies zeigt, warum MLPs nicht gut für Bilder skalieren.</p>
</details>
<ol start="3" type="1">
<li>Sie möchten ein neuronales Netzwerk lernen, das alle Bilder mit einem vertikalen Balken mit vier Pixeln (wobei jeder dieser vier Pixel einen hohe, identischen Wert hat), sieh erste zwei Bilder in <a href="#fig-quiz-nn" class="quarto-xref">Figure&nbsp;1</a>. Es muss in der Lage sein, zwischen Bildern zu unterscheiden, die ein anderes Muster haben (z.B. drittes und viertes Bild in <a href="#fig-quiz-nn" class="quarto-xref">Figure&nbsp;1</a>). Wie viele Neuronen benötigen Sie mindestens in einem Netzwerk mit einem Hidden Layer?</li>
</ol>
<details>
<summary>
Antwort anzeigen
</summary>
<p><strong>Mindestens 3 Neuronen pro Zeile im Hidden Layer. Also 3x6=18</strong></p>
<p><strong>Begründung:</strong> - Mit einem Neuron kann ein Muster erkannt werden, z.B. hohe Gewichte auf Muster-Positionen und negatie auf restlichen. Falls Muster -&gt; Aktivierung positiv, sonst negativ oder 0. - Jede Gruppe von 3 Neuronen kann lernen, eine bestimmte vertikale Position (pro Zeile) zu detektieren (es gibt davon jeweils 3) - Dies muss dann noch pro Zeile gemacht werden (6 Zeilen).</p>
<p><strong>Alternativ:</strong> Mit geschickter Gewichtswahl könnten theoretisch auch weniger Neuronen ausreichen, aber 36 ist eine sichere praktische Antwort.</p>
</details>
<div id="fig-quiz-nn" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-quiz-nn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/quiz/neural_network_patterns.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-quiz-nn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1
</figcaption>
</figure>
</div>
<ol start="4" type="1">
<li>Ihr Kollege ist am verzweifeln. Sein MLP funktioniert nicht sehr gut. Das kann natürlich viele Gründe haben. Nachdem Sie jedoch seinen Code angeschaut haben, haben Sie einen Verdacht. Welcher?</li>
</ol>
<div id="3f7fa921" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MLP(nn.Module):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size, hidden_size1, hidden_size2, hidden_size3, output_size):</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(MLP, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(input_size, hidden_size1)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(hidden_size1, hidden_size2)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc3 <span class="op">=</span> nn.Linear(hidden_size2, hidden_size3)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc4 <span class="op">=</span> nn.Linear(hidden_size3, output_size)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc1(x)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc2(x)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc3(x)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc4(x)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<details>
<summary>
Antwort anzeigen
</summary>
<p><strong>Das Problem: Fehlende Aktivierungsfunktionen!</strong></p>
<p>Das Netzwerk besteht nur aus linearen Transformationen. Ohne nichtlineare Aktivierungsfunktionen (wie ReLU, Sigmoid, Tanh) kann das Netzwerk nur lineare Funktionen lernen - egal wie viele Layer es hat.</p>
<p><strong>Mathematisch:</strong> Die Komposition mehrerer linearer Funktionen ist wieder eine lineare Funktion: <span class="math display">\[f_4(f_3(f_2(f_1(\vec{x})))) = W_4 W_3 W_2 W_1 x = W_{\text{combined}} \vec{x}\]</span></p>
<p>Das Netzwerk ist also äquivalent zu einem einzigen linearen Layer!</p>
<p><strong>Lösung:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> F.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> F.relu(<span class="va">self</span>.fc2(x))</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> F.relu(<span class="va">self</span>.fc3(x))</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="va">self</span>.fc4(x)  <span class="co"># Kein ReLU vor dem Output</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
</section>
<section id="convolutional-neural-networks-cnns" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="convolutional-neural-networks-cnns"><span class="header-section-number">3</span> Convolutional Neural Networks (CNNs)</h2>
<ol type="1">
<li>Ein Convolutional Layer hat 64 Input Activations (<span class="math inline">\(C_{in} = 64\)</span> und <span class="math inline">\(H=16\)</span>, <span class="math inline">\(W=16\)</span>). Sie möchten die räumliche Dimensionierung um die Hälfte reduzieren, während Sie die Anzahl der Channels verdoppeln. Wie parametrisieren Sie Ihren Convolutional Layer? Geben Sie ein Beispiel an. Wieviele Gewichte müssen gelernt werden?</li>
</ol>
<details>
<summary>
Antwort anzeigen
</summary>
<p><strong>Ziel:</strong> <span class="math inline">\(64 \times 16 \times 16 \rightarrow 128 \times 8 \times 8\)</span></p>
<p><strong>Parametrisierung:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>nn.Conv2d(in_channels<span class="op">=</span><span class="dv">64</span>, out_channels<span class="op">=</span><span class="dv">128</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Begründung:</strong></p>
<ul>
<li><code>out_channels=128</code>: Verdoppelt die Channels von 64 auf 128</li>
<li><code>stride=2</code>: Halbiert räumliche Auflösung (16 → 8)</li>
<li><code>kernel_size=3, padding=1</code>: Erhält kompatible Dimensionen</li>
</ul>
<p><strong>Gewichte:</strong></p>
<ul>
<li>Pro Filter: <span class="math inline">\(64 \times 3 \times 3 = 576\)</span> Gewichte</li>
<li>Anzahl Filter: 128</li>
<li><strong>Total: <span class="math inline">\(128 \times 576 = 73{,}728\)</span> Gewichte</strong></li>
<li>Plus 128 Bias-Parameter: <strong>73,856 Parameter</strong></li>
</ul>
</details>
<ol start="2" type="1">
<li>Sie möchten ein CNN auf Bildern mit einer Input Dimensionalität von <span class="math inline">\(3 \times 16 \times 16\)</span> (CHW) trainieren. Sie verwenden 3 Convolutional Layer mit je 8 Filtern und Kernel-Size 3. Wie viele Gewichte haben ihre Convolutional Layers ingesamt? Wie viele, wenn sich die räumliche Auflösung der Bilder auf <span class="math inline">\(32 \times 32\)</span> verdoppelt?</li>
</ol>
<details>
<summary>
Antwort anzeigen
</summary>
<p><strong>Wichtig:</strong> Die Anzahl der Gewichte in Convolutional Layers hängt NICHT von der räumlichen Auflösung ab!</p>
<p><strong>Berechnung:</strong></p>
<ul>
<li><strong>Layer 1:</strong> <span class="math inline">\(3 \times 3 \times 3 \times 8 = 216\)</span> Gewichte (+ 8 Bias)</li>
<li><strong>Layer 2:</strong> <span class="math inline">\(8 \times 3 \times 3 \times 8 = 576\)</span> Gewichte (+ 8 Bias)</li>
<li><strong>Layer 3:</strong> <span class="math inline">\(8 \times 3 \times 3 \times 8 = 576\)</span> Gewichte (+ 8 Bias)</li>
<li><strong>Total: 1,392 Parameter</strong></li>
</ul>
<p><strong>Bei <span class="math inline">\(32 \times 32\)</span>:</strong></p>
<ul>
<li><strong>Immer noch 1,392 Parameter!</strong></li>
</ul>
<p>Dies ist ein Hauptvorteil von CNNs: Parameter Sharing macht sie unabhängig von der Bildgrösse (im Gegensatz zu MLPs).</p>
</details>
<ol start="3" type="1">
<li>Sie haben sehr grosse Bilder (<span class="math inline">\(8000 \times 8000\)</span> Pixel). Ihr Modell stürzt ständig mit Out-Of-Memory Fehlern ab. Warum könnte das so sein? Welche Optionen haben Sie bei der Parametrisierung Ihrer Convolutions um die Out-Of-Memory Fehler zu umgehen?</li>
</ol>
<details>
<summary>
Antwort anzeigen
</summary>
<p><strong>Ursache:</strong> Grosse Bilder erzeugen riesige Activation Maps, die viel GPU-Memory benötigen.</p>
<p><strong>Lösungen:</strong></p>
<ol type="1">
<li><strong>Grösserer Stride in frühen Layern:</strong>
<ul>
<li><code>stride=2</code> oder <code>stride=4</code> reduziert Auflösung schnell</li>
<li>Beispiel: <span class="math inline">\(8000 \times 8000 \xrightarrow{\text{stride=4}} 2000 \times 2000\)</span></li>
</ul></li>
<li><strong>Aggressive Pooling:</strong>
<ul>
<li><code>nn.MaxPool2d(kernel_size=4, stride=4)</code> früh einsetzen</li>
</ul></li>
<li><strong>Kleinere Batch Size:</strong>
<ul>
<li>Batch size = 1 statt 32 (weniger parallel)</li>
</ul></li>
<li><strong>Weniger Filter in frühen Layern:</strong>
<ul>
<li>Statt 64 nur 32 oder 16 Filter verwenden</li>
</ul></li>
<li><strong>Image Resizing vor dem Modell:</strong>
<ul>
<li>Bilder auf z.B. <span class="math inline">\(2000 \times 2000\)</span> downsampen</li>
</ul></li>
<li><strong>Dilated Convolutions:</strong>
<ul>
<li>Grösseres receptive field ohne mehr Parameter</li>
</ul></li>
</ol>
<p><strong>Beste Kombination:</strong> Stride=2-4 in ersten Layern + weniger Filter + kleinere Batch Size</p>
</details>
<ol start="4" type="1">
<li>Können CNNs verwendet werden, um Inputs wie z.B. Satellitendaten zu modellieren, die keine RGB-Bilder sind, d.h. mehr als 3 Channels haben? Begründen Sie Ihre Antwort.</li>
</ol>
<details>
<summary>
Antwort anzeigen
</summary>
<p><strong>Ja, absolut!</strong> CNNs können beliebig viele Input-Channels verarbeiten.</p>
<p><strong>Begründung:</strong></p>
<ul>
<li>Der erste Conv-Layer spezifiziert <code>in_channels</code> - dies kann jede Zahl sein</li>
<li>Satellitendaten haben oft 10+ Bänder (multispektral, hyperspektral)</li>
<li>Medizinische Bilder können mehrere Modalitäten kombinieren (CT + MRI)</li>
</ul>
<p><strong>Beispiel:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Satellitendaten mit 12 Bändern</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>nn.Conv2d(in_channels<span class="op">=</span><span class="dv">12</span>, out_channels<span class="op">=</span><span class="dv">64</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Vorteile:</strong></p>
<ul>
<li>CNNs lernen automatisch relevante Kombinationen der Channels</li>
<li>Räumliche Struktur bleibt erhalten (2D Convolutions)</li>
<li>Gleiche Architektur-Prinzipien wie bei RGB</li>
</ul>
<p><strong>Hinweis:</strong> Bei Transfer Learning von ImageNet (3 Channels) muss der erste Layer angepasst werden!</p>
</details>
<ol start="5" type="1">
<li>Sie möchten Bilder modellieren, die nicht quadratisch sind. Sie haben eine räumliche Auflösung von 800x400. Was ist bei einem CNN anders wenn es mit nicht quadratischen Bildern trainiert wird (im Vergleich zu quadratischen Bildern)?</li>
</ol>
<details>
<summary>
Antwort anzeigen
</summary>
<p><strong>Hauptsächlich nichts Fundamentales!</strong> CNNs funktionieren auch mit nicht-quadratischen Bildern.</p>
<p><strong>Unterschiede:</strong></p>
<ol type="1">
<li><strong>Activation Maps bleiben nicht-quadratisch:</strong>
<ul>
<li>Input <span class="math inline">\(800 \times 400\)</span> → nach Conv mit stride=1, padding=1 immer noch <span class="math inline">\(800 \times 400\)</span></li>
<li>Nach Pooling (2x2): <span class="math inline">\(400 \times 200\)</span></li>
</ul></li>
<li><strong>Fully Connected Layer:</strong>
<ul>
<li>FC Layer müssen die genaue Auflösung kennen</li>
<li>Bei <span class="math inline">\(32 \times 128 \times 8 \times 4\)</span>: <code>nn.Linear(128*8*4, num_classes)</code></li>
</ul></li>
<li><strong>Global Pooling vorteilhaft:</strong>
<ul>
<li><code>nn.AdaptiveAvgPool2d((1,1))</code> funktioniert unabhängig von Auflösung</li>
<li>Macht Modell flexibler für verschiedene Grössen</li>
</ul></li>
</ol>
<p><strong>Kein Unterschied:</strong></p>
<ul>
<li>Convolutions funktionieren identisch</li>
<li>Receptive fields sind gleich</li>
<li>Training-Algorithmus ist gleich</li>
</ul>
<p><strong>Best Practice:</strong> Global Average Pooling verwenden statt FC-Layers mit fixer Grösse.</p>
</details>
<ol start="6" type="1">
<li>Im obigen Beispiel mit Bildern mit einer räumlichen Auflösung von 800x400. Wenn Sie wirklich quadratische Activation Maps in Ihrem Modell haben wollten, wie könnten Sie dies mit dem ersten Convolutional Layer erreichen? Wie müssten Sie diesen parametrisieren? Konsultieren Sie bei Bedarf die PyTorch Dokumentation <a href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html">torch.nn.Conv2d</a>.</li>
</ol>
<details>
<summary>
Antwort anzeigen
</summary>
<p><strong>Lösung: Asymmetrische Stride-Parameter verwenden</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>nn.Conv2d(</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    in_channels<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    out_channels<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    kernel_size<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    stride<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">1</span>),  <span class="co"># Höhe um Faktor 2 reduzieren, Breite beibehalten</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    padding<span class="op">=</span><span class="dv">1</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Berechnung:</strong></p>
<ul>
<li>Input: <span class="math inline">\(800 \times 400\)</span></li>
<li>Nach Conv mit <code>stride=(2, 1)</code>: <span class="math inline">\(400 \times 400\)</span></li>
</ul>
<p><strong>Alternative mit Pooling:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Convolution mit stride=1</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>conv <span class="op">=</span> nn.Conv2d(<span class="dv">3</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Asymmetrisches Pooling</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>pool <span class="op">=</span> nn.MaxPool2d(kernel_size<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">1</span>), stride<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">1</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<ol start="7" type="1">
<li>Wenn die Pixel Ihrer Input Bilder mit einer fixen Permutationsmatrix zufällig neu sortiert werden (jedes Bild wird identisch neu sortiert), bevorzugen Sie ein CNN oder ein MLP? Begründen Sie Ihre Antwort.</li>
</ol>
<details>
<summary>
Antwort anzeigen
</summary>
<p><strong>Antwort: MLP ist vorzuziehen!</strong></p>
<p><strong>Begründung:</strong></p>
<p><strong>CNNs basieren auf räumlicher Lokalität:</strong></p>
<ul>
<li>Convolutions nutzen aus, dass benachbarte Pixel zusammenhängen</li>
<li>Nach zufälliger Permutation ist diese Struktur zerstört</li>
<li>Ein 3×3 Filter würde auf räumlich nicht-zusammenhängende Pixel angewendet</li>
<li>Der Hauptvorteil von CNNs (lokale Feature-Extraktion) geht verloren</li>
</ul>
<p><strong>MLPs behandeln alle Positionen gleich:</strong></p>
<ul>
<li>MLPs haben keine Annahmen über räumliche Struktur</li>
<li>Jedes Pixel wird unabhängig behandelt (volles weight sharing)</li>
<li>Permutation ändert nur die Position im Eingangsvektor, nicht die Funktionsweise</li>
</ul>
</details>
<ol start="8" type="1">
<li>Schauen Sie sich das folgende CNN an. Können Sie die räumliche Auflösung der Bilder ableiten? Wenn ja, berechnen Sie diese. Wenn nein, begründen Sie.</li>
</ol>
<details>
<summary>
Antwort anzeigen
</summary>
<p><strong>Ja, die räumliche Auflösung kann berechnet werden!</strong></p>
<p><strong>Rückwärts-Rechnung vom FC-Layer:</strong></p>
<p>Der FC-Layer erwartet: <code>32 * 8 * 8 = 2048</code> Eingänge</p>
<p>Dies bedeutet nach 2× Pooling: <span class="math inline">\(32 \times 8 \times 8\)</span></p>
<p><strong>Berechnung:</strong></p>
<ul>
<li>Nach pool2: <span class="math inline">\(32 \times 8 \times 8\)</span></li>
<li>Vor pool2 (nach conv2): <span class="math inline">\(32 \times 16 \times 16\)</span> (Pooling halbiert)</li>
<li>Nach pool1: <span class="math inline">\(16 \times 16 \times 16\)</span></li>
<li>Vor pool1 (nach conv1): <span class="math inline">\(16 \times 32 \times 32\)</span> (Pooling halbiert)</li>
<li><strong>Input:</strong> <span class="math inline">\(3 \times 32 \times 32\)</span></li>
</ul>
<p><strong>Antwort: Die Input-Bilder haben eine räumliche Auflösung von <span class="math inline">\(32 \times 32\)</span></strong></p>
</details>
<div id="36961f12" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CNN1(nn.Module):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(CNN1, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(<span class="dv">3</span>, <span class="dv">16</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(<span class="dv">16</span>, <span class="dv">32</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu <span class="op">=</span> nn.ReLU()</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pool <span class="op">=</span> nn.MaxPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc <span class="op">=</span> nn.Linear(<span class="dv">32</span> <span class="op">*</span> <span class="dv">8</span> <span class="op">*</span> <span class="dv">8</span>, <span class="dv">10</span>)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.conv1(x))</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(x)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.conv2(x))</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(x)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(x.size(<span class="dv">0</span>), <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc(x)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<ol start="9" type="1">
<li>Schauen Sie sich die folgenden beiden CNNs an. Was sind die wichtigsten praktischen Unterschiede?</li>
</ol>
<div id="f170cc14" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CNN1(nn.Module):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(CNN1, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(<span class="dv">3</span>, <span class="dv">16</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(<span class="dv">16</span>, <span class="dv">32</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu <span class="op">=</span> nn.ReLU()</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pool <span class="op">=</span> nn.MaxPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc <span class="op">=</span> nn.Linear(<span class="dv">32</span> <span class="op">*</span> <span class="dv">8</span> <span class="op">*</span> <span class="dv">8</span>, <span class="dv">10</span>)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.conv1(x))</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(x)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.conv2(x))</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(x)</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(x.size(<span class="dv">0</span>), <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc(x)</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<div id="c34affed" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CNN2(nn.Module):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(CNN2, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(<span class="dv">3</span>, <span class="dv">16</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(<span class="dv">16</span>, <span class="dv">32</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu <span class="op">=</span> nn.ReLU()</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pool <span class="op">=</span> nn.MaxPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.global_avg_pool <span class="op">=</span> nn.AdaptiveAvgPool2d((<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc <span class="op">=</span> nn.Linear(<span class="dv">32</span>, <span class="dv">10</span>)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.conv1(x))</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(x)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.conv2(x))</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(x)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.global_avg_pool(x)</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(x.size(<span class="dv">0</span>), <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc(x)</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<details>
<summary>
Antwort anzeigen
</summary>
<p><strong>Hauptunterschied: Global Average Pooling vs Fixed-size FC Layer</strong></p>
<p><strong>CNN1:</strong></p>
<ul>
<li>Verwendet <code>nn.Linear(32 * 8 * 8, 10)</code> - fest an Input-Grösse <span class="math inline">\(32 \times 32\)</span> gebunden</li>
<li>Kann nur mit Bildern dieser Grösse umgehen</li>
<li>Mehr Parameter im FC-Layer: <span class="math inline">\(32 \times 8 \times 8 \times 10 = 20{,}480\)</span> Parameter</li>
</ul>
<p><strong>CNN2:</strong></p>
<ul>
<li>Verwendet <code>nn.AdaptiveAvgPool2d((1, 1))</code> - reduziert jede feature map auf 1×1</li>
<li>Funktioniert mit <strong>beliebigen</strong> Input-Grössen!</li>
<li>Weniger Parameter: <span class="math inline">\(32 \times 10 = 320\)</span> Parameter</li>
</ul>
<p><strong>Praktische Implikationen:</strong></p>
<ol type="1">
<li><strong>Flexibilität:</strong> CNN2 kann <span class="math inline">\(16\times16\)</span>, <span class="math inline">\(64\times64\)</span>, <span class="math inline">\(128\times256\)</span> verarbeiten</li>
<li><strong>Effizienz:</strong> CNN2 hat drastisch weniger Parameter</li>
<li><strong>Overfitting:</strong> CNN2 ist robuster (weniger Parameter)</li>
<li><strong>Modern Best Practice:</strong> Global Average Pooling wird in modernen Architekturen bevorzugt</li>
</ol>
<p><strong>Empfehlung: CNN2 (Global Average Pooling) ist deutlich besser!</strong></p>
</details>
<ol start="10" type="1">
<li>Zeichnen Sie das Receptive Field vom mittleren Wert der Activation map ganz rechts (grau) in <a href="#fig-quiz-cnn-receptive-field" class="quarto-xref">Figure&nbsp;2</a> bezüglich der Input Activation ganz links ein. Die Convs sind folgendermasen parametrisiert:</li>
</ol>
<ul>
<li>CONV1: kernel_size = 5, stride = 1, dilation = 1, padding=2</li>
<li>CONV2: kernel_size = 3, stride = 1, dilation = 1, padding=1</li>
</ul>
<div id="fig-quiz-cnn-receptive-field" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-quiz-cnn-receptive-field-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/quiz/cnn_receptive_field.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-quiz-cnn-receptive-field-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2
</figcaption>
</figure>
</div>
<details>
<summary>
Antwort anzeigen
</summary>
<p><strong>Berechnung des Receptive Fields:</strong></p>
<p><strong>Rückwärts von Layer 2 zu Layer 1:</strong></p>
<ul>
<li>Mittlerer Pixel in Layer 2 Activation</li>
<li>CONV2 hat kernel_size=3, stride=1</li>
<li>Dieser Pixel “sieht” 3×3 Bereich in Layer 1 Activation</li>
</ul>
<p><strong>Rückwärts von Layer 1 zu Input:</strong></p>
<ul>
<li>Jeder dieser 3×3 Pixel in Layer 1 Activation</li>
<li>CONV1 hat kernel_size=5, stride=1</li>
<li>Jeder “sieht” 5×5 Bereich im Input</li>
</ul>
<p><strong>Receptive Field Berechnung:</strong></p>
<ul>
<li>Nach CONV1: 5×5</li>
<li>Nach CONV2: <span class="math inline">\(5 + 2 \times (3-1) = 5 + 4 = 9 \times 9\)</span></li>
</ul>
</details>
<ol start="11" type="1">
<li>Berechnen Sie den fehlenden Wert in der Activation Map von <a href="#fig-quiz-cnn-calc-act" class="quarto-xref">Figure&nbsp;3</a>.</li>
</ol>
<p>Die Convolution ist folgendermassen parametrisiert:</p>
<pre><code>stride=1
padding=1</code></pre>
<div id="fig-quiz-cnn-calc-act" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-quiz-cnn-calc-act-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/quiz/conv_calc.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-quiz-cnn-calc-act-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3
</figcaption>
</figure>
</div>
<details>
<summary>
Antwort anzeigen
</summary>
todo
</details>
<ol start="12" type="1">
<li>Können CNNs verwendet werden um Objekte zu zählen? Schauen Sie sich <a href="#fig-quiz-cnn-counting" class="quarto-xref">Figure&nbsp;4</a> an. Sie möchten zählen in wie vielen Quadranten ein Objekt vorkommt. Begründen Sie Ihre Antwort.</li>
</ol>
<div id="fig-quiz-cnn-counting" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-quiz-cnn-counting-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/quiz/counting.jpg" class="img-fluid figure-img" width="300">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-quiz-cnn-counting-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4
</figcaption>
</figure>
</div>
<details>
<summary>
Antwort anzeigen
</summary>
<p>Ja, CNNs können zum Zählen verwendet werden. Die Aktivation-Maps werden je nach Anzahl Objekten an verschiedenen Orten aktiv. Diese Aktivierung kann mit z.B. einer Regression (Output ist Anzahl Objekte) modelliert werden.</p>
</details>
</section>
<section id="image-classification" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="image-classification"><span class="header-section-number">4</span> Image Classification</h2>
<ol type="1">
<li>Sie haben folgende logits <code>[3.0, 1.0, 1.0]</code>.</li>
</ol>
<ul>
<li>Wieviele Klassen hat ihr Klassifikationsproblem?</li>
<li>Die Summe über die Logits ist 5.0. Was ist die Summe nach einer Transformation mit der Softmax Funktion? Warum ist das wichtig?</li>
</ul>
<details>
<summary>
Antwort anzeigen
</summary>
<p><strong>Teil 1: Anzahl Klassen</strong> - <strong>3 Klassen</strong> (ein Logit pro Klasse)</p>
<p><strong>Teil 2: Softmax Summe</strong> - <strong>Die Summe ist immer 1.0</strong> nach Softmax!</p>
</details>
<ol start="2" type="1">
<li>Beschreiben Sie wie Image pre-processing das Training eines CNNs beeinflusst. Auf was muss man bezüglich Pre-Processing achten, wenn man ein vortrainiertes Modell verwenden möchte (Transfer-Learning)?</li>
</ol>
<details>
<summary>
Antwort anzeigen
</summary>
<p><strong>Einfluss von Pre-Processing auf Training:</strong></p>
<ol type="1">
<li><strong>Normalisierung:</strong> Beschleunigt Konvergenz, stabilisiert Training</li>
<li><strong>Data Augmentation:</strong> Erhöht Generalisierung, reduziert Overfitting</li>
<li><strong>Resizing:</strong> Beeinflusst sichtbare Details, Rechenaufwand</li>
</ol>
<p><strong>Bei Transfer Learning - KRITISCH:</strong></p>
<p><strong>Must Do:</strong></p>
<ul>
<li><p><strong>Gleiche Normalisierung wie Vortraining verwenden!</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ImageNet Preprocessing</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>transforms.Normalize(mean<span class="op">=</span>[<span class="fl">0.485</span>, <span class="fl">0.456</span>, <span class="fl">0.406</span>],</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>                     std<span class="op">=</span>[<span class="fl">0.229</span>, <span class="fl">0.224</span>, <span class="fl">0.225</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div></li>
<li><p><strong>Gleiche Input-Grösse:</strong> Meist 224×224 für ImageNet-Modelle</p></li>
</ul>
<p><strong>Warum wichtig?</strong></p>
<ul>
<li>Vortrainiertes Modell erwartet spezifische Input-Distribution</li>
<li>Falsche Normalisierung → Features “out of distribution” → schlechte Performance</li>
<li>Dokumentation des Modells konsultieren!</li>
</ul>
<p><strong>Tipp:</strong> Verwenden Sie die offiziellen Preprocessing-Pipelines des Modells (z.B. von timm oder Hugging Face).</p>
</details>
<ol start="3" type="1">
<li>Sie haben ein Modell trainiert um synthetische (fake) Bilder zu identifizieren. Das Modell ist ziemlich gut aber nicht perfekt. Wenn Sie das Modell deployen haben Sie die Möglichkeit einige der Bilder von einem Experten überprüfen zu lassen. Welche Bilder wählen Sie aus? Begründen Sie die Antwort.</li>
</ol>
<details>
<summary>
Antwort anzeigen
</summary>
<p><strong>Strategie</strong>: Bilder mit <strong>unsicheren Vorhersagen</strong> auswählen</p>
<p><strong>Konkret:</strong></p>
<ul>
<li>Wählen Sie Bilder, bei denen die Confidence nahe bei 0.5 liegt</li>
<li>Beispiel: P(fake) = 0.45 bis 0.55</li>
</ul>
<p><strong>Begründung:</strong></p>
<ol type="1">
<li><strong>Maximaler Informationsgewinn:</strong>
<ul>
<li>Unsichere Predictions zeigen schwierige Fälle</li>
<li>Experten-Labels für diese sind am wertvollsten</li>
</ul></li>
<li><strong>Effiziente Ressourcennutzung:</strong>
<ul>
<li>Sehr sichere Predictions (<span class="math inline">\(P &gt; 0.95\)</span>) sind wahrscheinlich korrekt</li>
<li>Experte bestätigt nur Bekanntes → wenig Mehrwert</li>
</ul></li>
<li><strong>Active Learning Prinzip:</strong>
<ul>
<li>Modell lernt am meisten an der Entscheidungsgrenze</li>
<li>Diese Bilder können fürs Re-Training genutzt werden</li>
</ul></li>
</ol>
<p>Auch extreme Misclassifications (sehr sichere aber falsche) anschauen, um systematische Fehler zu finden.</p>
</details>
<ol start="4" type="1">
<li>Sie möchten ein Modell trainieren um Bilder von Kamerafallen zu klassifizieren. Die Biologen möchten ein Modell, welches für ein Bild aussagt welche Spezies zu sehen ist (immer nur jeweils eine) und was die Tiere machen (jweils eine von 5 Aktivitäten, wie z.B. Essen, Ruhen, etc.) ist möglich. Was ist das für eine Problemstellung? Wie würden Sie diese modellieren?</li>
</ol>
<details>
<summary>
Antwort anzeigen
</summary>
<p><strong>Problemstellung: Multi-Task Learning mit zwei Klassifikations-Aufgaben</strong></p>
<ul>
<li><strong>Task 1:</strong> Spezies-Klassifikation (single-label, N Klassen)</li>
<li><strong>Task 2:</strong> Aktivitäts-Klassifikation (single-label, 5 Klassen)</li>
</ul>
<p><strong>Modellierung:</strong></p>
<p>Shared Backbone + Two Heads (empfohlen)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiTaskModel(nn.Module):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_species, num_activities<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Shared feature extractor</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.backbone <span class="op">=</span> resnet50(pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.backbone.fc <span class="op">=</span> nn.Identity()  <span class="co"># Remove final layer</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Two separate classification heads</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.species_head <span class="op">=</span> nn.Linear(<span class="dv">2048</span>, num_species)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activity_head <span class="op">=</span> nn.Linear(<span class="dv">2048</span>, num_activities)</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>        features <span class="op">=</span> <span class="va">self</span>.backbone(x)</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>        species_logits <span class="op">=</span> <span class="va">self</span>.species_head(features)</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>        activity_logits <span class="op">=</span> <span class="va">self</span>.activity_head(features)</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> species_logits, activity_logits</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Loss Function:</strong></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> lambda_species <span class="op">*</span> CE_loss(species_logits, species_labels) <span class="op">+</span> <span class="op">\</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>       lambda_activity <span class="op">*</span> CE_loss(activity_logits, activity_labels)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</details>
<ol start="5" type="1">
<li>Sie möchten Bilder von Kamerafallen klassifizieren. Insbesondere wollen Sie ein Modell, welches die Spezies bestimmt, welche in einem Bild sichtbar ist. Sie haben Bilder, die am Tag aufgenommen worden sind und solche die bei Nacht aufgenommen worden sind. Die Tagbilder sind RGB, die Nachtbilder sind graustufen Bilder. Erwähnen Sie 2 Optionen um Tag- und Nachtbilder zu modellieren. Wählen Sie eine aus und begründen Sie.</li>
</ol>
<details>
<summary>
Antwort anzeigen
</summary>
<p>Graustufen zu RGB konvertieren. Umgekehrt gibt es einen Informationsverlust.</p>
<p>Vortrainierte Modelle erwarten RGB!</p>
</details>
<ol start="6" type="1">
<li>Sind tiefere Netzwerke in der Regel besser als weniger tiefe? Auf was muss man achten? Begründe die Antwort.</li>
</ol>
<details>
<summary>
Antwort anzeigen
</summary>
<p>Ja. Siehe Arbeit von <span class="citation" data-cites="He2016">He et al. (<a href="#ref-He2016" role="doc-biblioref">2016</a>)</span> an ResNet. Overfitting und schlechte Konvergenz könnten Probleme bereiten.</p>
</details>
<ol start="7" type="1">
<li>Sie werden beauftragt ein Modell zu entwickeln mit dem man in Echtzeit Bilder von einem Kamerasystem klassifizieren kann. Was für Eigenschaften muss die Architektur haben, welche Sie auswählen?</li>
</ol>
<details>
<summary>
Antwort anzeigen
</summary>
<p>Geschwindigkeit ist zentral. Die Architektur muss also wohl eher klein sein um die Anforderung “Echtzeit” zu erfüllen.</p>
</details>
</section>
<section id="representation-learning" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="representation-learning"><span class="header-section-number">5</span> Representation Learning</h2>
<ol type="1">
<li>Warum ist die Darstellung von Bildern als Pixel-Werte ineffizient? Wie hilft Representation Learning?</li>
</ol>
<details>
<summary>
Antwort anzeigen
</summary>
<p><strong>Problem der Pixel-Repräsentation:</strong></p>
<ul>
<li>Der Raum aller möglichen Bilder ist astronomisch gross</li>
<li>Für RGB-Bilder mit <span class="math inline">\(224 \times 224\)</span> Pixeln und 255 Intensitätswerten: <span class="math inline">\(255^{224 \times 224 \times 3} \approx 255^{150{,}528}\)</span> mögliche Bilder</li>
<li>Natürliche (reale) Bilder machen nur einen winzigen Bruchteil dieses Raums aus</li>
<li>Pixel-Repräsentation ist daher extrem ineffizient</li>
</ul>
<p><strong>Lösung durch Representation Learning:</strong></p>
<ul>
<li>Findet <strong>kompakte Repräsentationen</strong>, die viel kleiner sind als Raw-Pixel</li>
<li>Behält dabei <strong>relevante Information</strong> für praktische Aufgaben</li>
</ul>
<p><strong>Vorteile:</strong></p>
<ul>
<li>Effizienter für Downstream-Tasks (z.B. Klassifikation)</li>
<li>Reduziert Dimensionalität drastisch</li>
<li>Konzentriert sich auf semantisch bedeutsame Merkmale</li>
<li>Ermöglicht bessere Generalisierung</li>
</ul>
<p><strong>Intuition:</strong> Natürliche Bilder haben Struktur und Regularität - Representation Learning nutzt dies aus, um einen besser geeigneten “Koordinatenraum” zu finden.</p>
</details>
<ol start="2" type="1">
<li>Sie möchten eine App entwickeln, welche Bilder in “Screenshots” oder “Fotos” klassifiziert. Allerdings sollte es die Option geben, dass der Anwender zusätzliche Klassen hinzufügen kann. Welches Modell wählen Sie und warum?</li>
</ol>
<details>
<summary>
Antwort anzeigen
</summary>
<p><strong>Empfehlung: CLIP oder DINOvx (Foundation Models mit Feature Extraction)</strong></p>
<p><strong>Begründung:</strong></p>
<p><strong>CLIP (beste Wahl):</strong></p>
<ul>
<li>Zero-shot fähig: Kann neue Klassen ohne Training hinzufügen</li>
<li>Nutzer kann einfach Text-Labels definieren (z.B. “diagram”, “meme”)</li>
<li>Bereits ausgezeichnet für “Screenshot” vs “Photo” Unterscheidung</li>
</ul>
<p><strong>DINOv2 (Alternative):</strong></p>
<ul>
<li>Exzellente visuelle Features</li>
<li>k-NN classifier kann schnell erweitert werden</li>
<li>Wenige Beispiele pro neuer Klasse nötig</li>
</ul>
<p><strong>Warum nicht supervised (ResNet)?</strong></p>
<ul>
<li>Müsste für jede neue Klasse neu trainiert werden</li>
<li>Benötigt viele Labels pro Klasse</li>
<li>Nicht flexibel für user-defined Klassen</li>
</ul>
</details>
</section>
<section id="praktische-aspekte" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="praktische-aspekte"><span class="header-section-number">6</span> Praktische Aspekte</h2>
<ol type="1">
<li>Sie untersuchen die Rohdaten und stellen fest, dass die Labels in 5% aller Fälle inkorrekt sind. Welche Performance dürfen Sie maximal erwarten, wenn Sie ihr bestes Modell auf einem Testsplit auf diesen Rohdaten evaluieren?</li>
</ol>
<details>
<summary>
Antwort anzeigen
</summary>
<p><strong>Maximal 95% Accuracy!</strong></p>
<p><strong>Begründung:</strong></p>
<ul>
<li>5% der Labels sind falsch (Label Noise)</li>
<li>Selbst ein perfektes Modell, das die wahre Funktion lernt, wird diese 5% “falsch” klassifizieren</li>
<li>Das Modell wird für korrekte Predictions bestraft, weil die Ground Truth falsch ist</li>
</ul>
<p><strong>Wichtige Implikationen:</strong></p>
<ul>
<li><strong>Human Baseline</strong> ist maximal 95%</li>
<li>Models können nicht besser sein als die Label-Qualität</li>
<li>Bei Accuracy &gt; 95% liegt vermutlich Overfitting vor (Modell lernt Noise)</li>
</ul>
</details>
<ol start="2" type="1">
<li>Sie möchten ein Bildklassifikationsproblem modellieren. Welche der folgenden Modelle probieren Sie zuerst aus nachdem Sie ihre Pipeline validiert und eine Baseline etabliert haben? Begründen Sie ihre Antwort.</li>
</ol>
<ul>
<li>A: ResNet50 vortrainiert auf ImageNet bei dem Sie einen linearen Layer hinzufügen</li>
<li>B: ResNet50 vortrainiert auf ImageNet, welches Sie mit einem Squeeze-and-Excite Block und einem linearen Layer erweitern</li>
<li>C: ResNet50 ohne Vortraining</li>
</ul>
<details>
<summary>
Antwort anzeigen
</summary>
<p><strong>Antwort: A (ResNet50 pretrained + Linear Layer)</strong></p>
<p><strong>Begründung:</strong></p>
<p><strong>Prinzip: “Don’t be a hero”</strong> (Karpathy)</p>
<ul>
<li>Beginnen Sie mit einfachsten, etablierten Lösungen</li>
<li>Fügen Sie Komplexität nur hinzu, wenn nötig</li>
</ul>
<p><strong>Warum A?</strong></p>
<ol type="1">
<li><strong>Transfer Learning funktioniert:</strong> ImageNet Features sind sehr allgemein</li>
<li><strong>Schnelles Training:</strong> Nur Linear Layer trainieren (Feature Extraction)</li>
<li><strong>Gute Baseline:</strong> Meist ~90% der finalen Performance</li>
<li><strong>Wenige Hyperparameter:</strong> Reduziert Fehlerquellen</li>
</ol>
<p><strong>Warum nicht B?</strong> - Squeeze-and-Excite Blocks sind fancy, aber erhöhen Komplexität - Erst hinzufügen, wenn A nicht gut genug ist - Mehr Hyperparameter zum Tunen</p>
<p><strong>Warum nicht C?</strong> - Training from scratch benötigt viel mehr Daten - Länger Trainingszeit - Meist schlechter als pretrained (ausser sehr domain-spezifisch)</p>
<p><strong>Workflow:</strong> 1. A: Pretrained + Linear → Baseline 2. A mit Fine-tuning → Wenn mehr Performance nötig 3. B: Custom Blocks → Nur bei speziellen Requirements</p>
</details>
<ol start="3" type="1">
<li>Sie möchten Bilder von Kamerafallen im Wald klassifizieren. Diese Kamerafallen werden durch Bewegung ausgelöst und enthalten viele False Positives (kein Tier im Bild). In einem ersten Schritt möchten Sie leere Bilder (kein Tier) von nicht-leeren Bildern (hat Tier) unterscheiden. Sie stellen fest, dass einige Spezies (Eichhörnchen!) sehr klein sind. Was müssen Sie beachten?</li>
</ol>
<details>
<summary>
Antwort anzeigen
</summary>
<p><strong>Hauptherausforderung: Kleine Objekte im grossen Bild</strong></p>
<p><strong>Kritische Aspekte:</strong></p>
<ol type="1">
<li><strong>Räumliche Auflösung beibehalten:</strong></li>
</ol>
<ul>
<li>Nicht zu aggressiv downsampling in frühen Layern</li>
<li>Stride=1 oder kleinere Pooling-Operationen verwenden</li>
<li>Eichhörnchen könnten nur 20×20 Pixel einnehmen</li>
</ul>
<ol start="2" type="1">
<li><strong>Input-Auflösung:</strong></li>
</ol>
<ul>
<li>Höhere Input-Auflösung verwenden (z.B. 512×512 statt 224×224)</li>
<li>Trade-off: Mehr Memory und Rechenzeit</li>
</ul>
<ol start="3" type="1">
<li><strong>Evaluation:</strong></li>
</ol>
<ul>
<li>Recall für “animal”-Klasse ist wichtig (keine Tiere verpassen)</li>
<li>Precision für “empty” weniger kritisch (False Positives OK)</li>
</ul>
</details>


<!-- -->


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-He2016" class="csl-entry" role="listitem">
He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. <span>“Deep <span>Residual</span> <span>Learning</span> for <span>Image</span> <span>Recognition</span>.”</span> In <em>2016 <span>IEEE</span> <span>Conference</span> on <span>Computer</span> <span>Vision</span> and <span>Pattern</span> <span>Recognition</span> (<span>CVPR</span>)</em>, 770–78. IEEE. <a href="https://doi.org/10.1109/CVPR.2016.90">https://doi.org/10.1109/CVPR.2016.90</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/marco-willi\.github\.io\/cas-dl-compvis-lectures-hs2025\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../pages/misc/exercises.html" class="pagination-link" aria-label="Exercises">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Exercises</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../pages/misc/literature.html" class="pagination-link" aria-label="Books">
        <span class="nav-page-text">Books</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb14" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Quiz &amp; Exam Prep"</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="an">params:</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co">  images_path: /assets/images/quiz/</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>::: {.content-hidden}</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>{{&lt; include /assets/_macros.tex &gt;}}</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>Die folgenden Fragen helfen das gelernte nochmal zu überprüfen und könnten Prüfungsfragen sein.</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>Beachten Sie auch die Quiz-Fragen in den Unerrichtsunterlagen.</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a><span class="fu">## Intro</span></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Inwiefern definiert das _Semantic Gap_ eine grundsätzliche Herausforderung in Computer Vision?</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Antwort anzeigen<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>Das Semantic Gap beschreibt die Diskrepanz oder enorme kognitive Leistung um die Pixelrepräsentation von Bildern (2-D Gitter von Pixel-Werte) semantisch zu inerpretieren. Also von Pixeln zu Katzen. Diese fundamentale Herausforderung macht es schwierig, direkt von Pixelwerten auf semantische Bedeutung zu schliessen. Deep Learning hilft, diese Lücke zu überbrücken, indem es hierarchische Repräsentationen lernt, die schrittweise von lokalen Features (Kanten, Farben) zu hochdimensionalen semantischen Konzepten (Objekte, Szenen) führen.</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Sie möchten ein Objekt in einem Bild klassifizieren: Ist es besser das ganze Bild zur Verfügung zu haben oder nur einen Ausschnitt vom Objekt? Begründen Sie.</span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Antwort anzeigen<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>Das hängt vom Anwendungsfall ab</span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>**Vorteile ganzes Bild:**</span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Kontextinformationen sind verfügbar (z.B. Umgebung hilft bei Klassifikation)</span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Räumliche Beziehungen zu anderen Objekten können genutzt werden</span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a>**Vorteile eines Ausschnitts:**</span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Fokus auf das relevante Objekt, weniger Ablenkung durch Hintergrund</span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Objekt nimmt mehr Pixel ein → mehr Details sichtbar (je nach Architektur)</span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a>**Empfehlung:** In der Regel ist das ganze Bild vorzuziehen, da Kontext wichtig ist.</span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a><span class="fu">## Modellierung von Bildern mit Neuronalen Netzwerken</span></span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Sie möchten ein MLP auf Bilddaten trainieren. Die Bilder haben jedoch unterschiedliche räumliche Auflösungen, die von $64 \times 64$ bis zu $512 \times 512$ reichen. Ist das ein Problem? Wenn ja, was tun Sie?</span>
<span id="cb14-54"><a href="#cb14-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-55"><a href="#cb14-55" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-56"><a href="#cb14-56" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Antwort anzeigen<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb14-57"><a href="#cb14-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-58"><a href="#cb14-58" aria-hidden="true" tabindex="-1"></a>**Ja, das ist ein Problem!** MLPs erwarten eine fixe Input-Dimensionalität. Ein Bild mit $64 \times 64 \times 3 = 12{,}288$ Pixeln ergibt einen anderen Input-Vektor als ein Bild mit $512 \times 512 \times 3 = 786{,}432$ Pixeln.</span>
<span id="cb14-59"><a href="#cb14-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-60"><a href="#cb14-60" aria-hidden="true" tabindex="-1"></a>**Lösungen:**</span>
<span id="cb14-61"><a href="#cb14-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-62"><a href="#cb14-62" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Resizing:** Alle Bilder auf eine einheitliche Auflösung skalieren (z.B. $224 \times 224$)</span>
<span id="cb14-63"><a href="#cb14-63" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Cropping:** Zentrale Crops oder Random Crops auf einheitliche Grösse</span>
<span id="cb14-64"><a href="#cb14-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-65"><a href="#cb14-65" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-66"><a href="#cb14-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-67"><a href="#cb14-67" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Sie möchten ein MLP auf Bildern mit einer Input Dimensionalität von $3 \times 16 \times 16$ (CHW) trainieren. Der erste Hidden Layer hat 4 Neuronen, der Output Layer 2. Wie viele Gewichte hat Ihr Modell? Wie viele, wenn sich die räumliche Auflösung auf $32 \times 32$ verdoppelt?</span>
<span id="cb14-68"><a href="#cb14-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-69"><a href="#cb14-69" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-70"><a href="#cb14-70" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Antwort anzeigen<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb14-71"><a href="#cb14-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-72"><a href="#cb14-72" aria-hidden="true" tabindex="-1"></a>**Berechnung für $16 \times 16$:**</span>
<span id="cb14-73"><a href="#cb14-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-74"><a href="#cb14-74" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Input: $3 \times 16 \times 16 = 768$ Neuronen</span>
<span id="cb14-75"><a href="#cb14-75" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Hidden Layer: 4 Neuronen</span>
<span id="cb14-76"><a href="#cb14-76" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Output Layer: 2 Neuronen</span>
<span id="cb14-77"><a href="#cb14-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-78"><a href="#cb14-78" aria-hidden="true" tabindex="-1"></a>Gewichte:</span>
<span id="cb14-79"><a href="#cb14-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-80"><a href="#cb14-80" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Input → Hidden: $768 \times 4 = 3{,}072$ Gewichte (+ 4 Bias)</span>
<span id="cb14-81"><a href="#cb14-81" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Hidden → Output: $4 \times 2 = 8$ Gewichte (+ 2 Bias)</span>
<span id="cb14-82"><a href="#cb14-82" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Total: 3,086 Parameter**</span>
<span id="cb14-83"><a href="#cb14-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-84"><a href="#cb14-84" aria-hidden="true" tabindex="-1"></a>**Berechnung für $32 \times 32$:**</span>
<span id="cb14-85"><a href="#cb14-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-86"><a href="#cb14-86" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Input: $3 \times 32 \times 32 = 3{,}072$ Neuronen</span>
<span id="cb14-87"><a href="#cb14-87" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Input → Hidden: $3{,}072 \times 4 = 12{,}288$ Gewichte (+ 4 Bias)</span>
<span id="cb14-88"><a href="#cb14-88" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Hidden → Output: $4 \times 2 = 8$ Gewichte (+ 2 Bias)</span>
<span id="cb14-89"><a href="#cb14-89" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Total: 12,302 Parameter**</span>
<span id="cb14-90"><a href="#cb14-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-91"><a href="#cb14-91" aria-hidden="true" tabindex="-1"></a>Die Anzahl der Parameter vervierfacht sich! Dies zeigt, warum MLPs nicht gut für Bilder skalieren.</span>
<span id="cb14-92"><a href="#cb14-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-93"><a href="#cb14-93" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-94"><a href="#cb14-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-95"><a href="#cb14-95" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Sie möchten ein neuronales Netzwerk lernen, das alle Bilder mit einem vertikalen Balken mit vier Pixeln (wobei jeder dieser vier Pixel einen hohe, identischen Wert hat), sieh erste zwei Bilder in @fig-quiz-nn. Es muss in der Lage sein, zwischen Bildern zu unterscheiden, die ein anderes Muster haben (z.B. drittes und viertes Bild in @fig-quiz-nn). Wie viele Neuronen benötigen Sie mindestens in einem Netzwerk mit einem Hidden Layer?</span>
<span id="cb14-96"><a href="#cb14-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-97"><a href="#cb14-97" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-98"><a href="#cb14-98" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Antwort anzeigen<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb14-99"><a href="#cb14-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-100"><a href="#cb14-100" aria-hidden="true" tabindex="-1"></a>**Mindestens 3 Neuronen pro Zeile im Hidden Layer. Also 3x6=18**</span>
<span id="cb14-101"><a href="#cb14-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-102"><a href="#cb14-102" aria-hidden="true" tabindex="-1"></a>**Begründung:**</span>
<span id="cb14-103"><a href="#cb14-103" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Mit einem Neuron kann ein Muster erkannt werden, z.B. hohe Gewichte auf Muster-Positionen und negatie auf restlichen. Falls Muster -&gt; Aktivierung positiv, sonst negativ oder 0.</span>
<span id="cb14-104"><a href="#cb14-104" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Jede Gruppe von 3 Neuronen kann lernen, eine bestimmte vertikale Position (pro Zeile) zu detektieren (es gibt davon jeweils 3)</span>
<span id="cb14-105"><a href="#cb14-105" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Dies muss dann noch pro Zeile gemacht werden (6 Zeilen).</span>
<span id="cb14-106"><a href="#cb14-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-107"><a href="#cb14-107" aria-hidden="true" tabindex="-1"></a>**Alternativ:** Mit geschickter Gewichtswahl könnten theoretisch auch weniger Neuronen ausreichen, aber 36 ist eine sichere praktische Antwort.</span>
<span id="cb14-108"><a href="#cb14-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-109"><a href="#cb14-109" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-110"><a href="#cb14-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-111"><a href="#cb14-111" aria-hidden="true" tabindex="-1"></a>::: {#fig-quiz-nn}</span>
<span id="cb14-112"><a href="#cb14-112" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}neural_network_patterns.png)</span></span>
<span id="cb14-113"><a href="#cb14-113" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-114"><a href="#cb14-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-115"><a href="#cb14-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-116"><a href="#cb14-116" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Ihr Kollege ist am verzweifeln. Sein MLP funktioniert nicht sehr gut. Das kann natürlich viele Gründe haben. Nachdem Sie jedoch seinen Code angeschaut haben, haben Sie einen Verdacht. Welcher?</span>
<span id="cb14-117"><a href="#cb14-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-120"><a href="#cb14-120" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-121"><a href="#cb14-121" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb14-122"><a href="#cb14-122" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-123"><a href="#cb14-123" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MLP(nn.Module):</span>
<span id="cb14-124"><a href="#cb14-124" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size, hidden_size1, hidden_size2, hidden_size3, output_size):</span>
<span id="cb14-125"><a href="#cb14-125" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(MLP, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb14-126"><a href="#cb14-126" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(input_size, hidden_size1)</span>
<span id="cb14-127"><a href="#cb14-127" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(hidden_size1, hidden_size2)</span>
<span id="cb14-128"><a href="#cb14-128" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc3 <span class="op">=</span> nn.Linear(hidden_size2, hidden_size3)</span>
<span id="cb14-129"><a href="#cb14-129" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc4 <span class="op">=</span> nn.Linear(hidden_size3, output_size)</span>
<span id="cb14-130"><a href="#cb14-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-131"><a href="#cb14-131" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb14-132"><a href="#cb14-132" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc1(x)</span>
<span id="cb14-133"><a href="#cb14-133" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc2(x)</span>
<span id="cb14-134"><a href="#cb14-134" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc3(x)</span>
<span id="cb14-135"><a href="#cb14-135" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc4(x)</span>
<span id="cb14-136"><a href="#cb14-136" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb14-137"><a href="#cb14-137" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-138"><a href="#cb14-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-139"><a href="#cb14-139" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-140"><a href="#cb14-140" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Antwort anzeigen<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb14-141"><a href="#cb14-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-142"><a href="#cb14-142" aria-hidden="true" tabindex="-1"></a>**Das Problem: Fehlende Aktivierungsfunktionen!**</span>
<span id="cb14-143"><a href="#cb14-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-144"><a href="#cb14-144" aria-hidden="true" tabindex="-1"></a>Das Netzwerk besteht nur aus linearen Transformationen. Ohne nichtlineare Aktivierungsfunktionen (wie ReLU, Sigmoid, Tanh) kann das Netzwerk nur lineare Funktionen lernen - egal wie viele Layer es hat.</span>
<span id="cb14-145"><a href="#cb14-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-146"><a href="#cb14-146" aria-hidden="true" tabindex="-1"></a>**Mathematisch:** Die Komposition mehrerer linearer Funktionen ist wieder eine lineare Funktion:</span>
<span id="cb14-147"><a href="#cb14-147" aria-hidden="true" tabindex="-1"></a>$$f_4(f_3(f_2(f_1(\vec{x})))) = W_4 W_3 W_2 W_1 x = W_{\text{combined}} \vec{x}$$</span>
<span id="cb14-148"><a href="#cb14-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-149"><a href="#cb14-149" aria-hidden="true" tabindex="-1"></a>Das Netzwerk ist also äquivalent zu einem einzigen linearen Layer!</span>
<span id="cb14-150"><a href="#cb14-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-151"><a href="#cb14-151" aria-hidden="true" tabindex="-1"></a>**Lösung:**</span>
<span id="cb14-152"><a href="#cb14-152" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb14-153"><a href="#cb14-153" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb14-154"><a href="#cb14-154" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> F.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb14-155"><a href="#cb14-155" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> F.relu(<span class="va">self</span>.fc2(x))</span>
<span id="cb14-156"><a href="#cb14-156" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> F.relu(<span class="va">self</span>.fc3(x))</span>
<span id="cb14-157"><a href="#cb14-157" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> <span class="va">self</span>.fc4(x)  <span class="co"># Kein ReLU vor dem Output</span></span>
<span id="cb14-158"><a href="#cb14-158" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x</span>
<span id="cb14-159"><a href="#cb14-159" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-160"><a href="#cb14-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-161"><a href="#cb14-161" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-162"><a href="#cb14-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-163"><a href="#cb14-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-164"><a href="#cb14-164" aria-hidden="true" tabindex="-1"></a><span class="fu">## Convolutional Neural Networks (CNNs)</span></span>
<span id="cb14-165"><a href="#cb14-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-166"><a href="#cb14-166" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Ein Convolutional Layer hat 64 Input Activations ($C_{in} = 64$ und $H=16$, $W=16$). Sie möchten die räumliche Dimensionierung um die Hälfte reduzieren, während Sie die Anzahl der Channels verdoppeln. Wie parametrisieren Sie Ihren Convolutional Layer? Geben Sie ein Beispiel an. Wieviele Gewichte müssen gelernt werden?</span>
<span id="cb14-167"><a href="#cb14-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-168"><a href="#cb14-168" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-169"><a href="#cb14-169" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Antwort anzeigen<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb14-170"><a href="#cb14-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-171"><a href="#cb14-171" aria-hidden="true" tabindex="-1"></a>**Ziel:** $64 \times 16 \times 16 \rightarrow 128 \times 8 \times 8$</span>
<span id="cb14-172"><a href="#cb14-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-173"><a href="#cb14-173" aria-hidden="true" tabindex="-1"></a>**Parametrisierung:**</span>
<span id="cb14-174"><a href="#cb14-174" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb14-175"><a href="#cb14-175" aria-hidden="true" tabindex="-1"></a>nn.Conv2d(in_channels<span class="op">=</span><span class="dv">64</span>, out_channels<span class="op">=</span><span class="dv">128</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-176"><a href="#cb14-176" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-177"><a href="#cb14-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-178"><a href="#cb14-178" aria-hidden="true" tabindex="-1"></a>**Begründung:**</span>
<span id="cb14-179"><a href="#cb14-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-180"><a href="#cb14-180" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`out_channels=128`</span>: Verdoppelt die Channels von 64 auf 128</span>
<span id="cb14-181"><a href="#cb14-181" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`stride=2`</span>: Halbiert räumliche Auflösung (16 → 8)</span>
<span id="cb14-182"><a href="#cb14-182" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`kernel_size=3, padding=1`</span>: Erhält kompatible Dimensionen</span>
<span id="cb14-183"><a href="#cb14-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-184"><a href="#cb14-184" aria-hidden="true" tabindex="-1"></a>**Gewichte:**</span>
<span id="cb14-185"><a href="#cb14-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-186"><a href="#cb14-186" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Pro Filter: $64 \times 3 \times 3 = 576$ Gewichte</span>
<span id="cb14-187"><a href="#cb14-187" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Anzahl Filter: 128</span>
<span id="cb14-188"><a href="#cb14-188" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Total: $128 \times 576 = 73{,}728$ Gewichte**</span>
<span id="cb14-189"><a href="#cb14-189" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Plus 128 Bias-Parameter: **73,856 Parameter**</span>
<span id="cb14-190"><a href="#cb14-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-191"><a href="#cb14-191" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-192"><a href="#cb14-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-193"><a href="#cb14-193" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Sie möchten ein CNN auf Bildern mit einer Input Dimensionalität von $3 \times 16 \times 16$ (CHW) trainieren. Sie verwenden 3 Convolutional Layer mit je 8 Filtern und Kernel-Size 3. Wie viele Gewichte haben ihre Convolutional Layers ingesamt? Wie viele, wenn sich die räumliche Auflösung der Bilder auf $32 \times 32$ verdoppelt?</span>
<span id="cb14-194"><a href="#cb14-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-195"><a href="#cb14-195" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-196"><a href="#cb14-196" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Antwort anzeigen<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb14-197"><a href="#cb14-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-198"><a href="#cb14-198" aria-hidden="true" tabindex="-1"></a>**Wichtig:** Die Anzahl der Gewichte in Convolutional Layers hängt NICHT von der räumlichen Auflösung ab!</span>
<span id="cb14-199"><a href="#cb14-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-200"><a href="#cb14-200" aria-hidden="true" tabindex="-1"></a>**Berechnung:**</span>
<span id="cb14-201"><a href="#cb14-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-202"><a href="#cb14-202" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Layer 1:** $3 \times 3 \times 3 \times 8 = 216$ Gewichte (+ 8 Bias)</span>
<span id="cb14-203"><a href="#cb14-203" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Layer 2:** $8 \times 3 \times 3 \times 8 = 576$ Gewichte (+ 8 Bias)</span>
<span id="cb14-204"><a href="#cb14-204" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Layer 3:** $8 \times 3 \times 3 \times 8 = 576$ Gewichte (+ 8 Bias)</span>
<span id="cb14-205"><a href="#cb14-205" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Total: 1,392 Parameter**</span>
<span id="cb14-206"><a href="#cb14-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-207"><a href="#cb14-207" aria-hidden="true" tabindex="-1"></a>**Bei $32 \times 32$:**</span>
<span id="cb14-208"><a href="#cb14-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-209"><a href="#cb14-209" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Immer noch 1,392 Parameter!**</span>
<span id="cb14-210"><a href="#cb14-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-211"><a href="#cb14-211" aria-hidden="true" tabindex="-1"></a>Dies ist ein Hauptvorteil von CNNs: Parameter Sharing macht sie unabhängig von der Bildgrösse (im Gegensatz zu MLPs).</span>
<span id="cb14-212"><a href="#cb14-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-213"><a href="#cb14-213" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-214"><a href="#cb14-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-215"><a href="#cb14-215" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Sie haben sehr grosse Bilder ($8000 \times 8000$ Pixel). Ihr Modell stürzt ständig mit Out-Of-Memory Fehlern ab. Warum könnte das so sein? Welche Optionen haben Sie bei der Parametrisierung Ihrer Convolutions um die Out-Of-Memory Fehler zu umgehen?</span>
<span id="cb14-216"><a href="#cb14-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-217"><a href="#cb14-217" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-218"><a href="#cb14-218" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Antwort anzeigen<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb14-219"><a href="#cb14-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-220"><a href="#cb14-220" aria-hidden="true" tabindex="-1"></a>**Ursache:** Grosse Bilder erzeugen riesige Activation Maps, die viel GPU-Memory benötigen.</span>
<span id="cb14-221"><a href="#cb14-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-222"><a href="#cb14-222" aria-hidden="true" tabindex="-1"></a>**Lösungen:**</span>
<span id="cb14-223"><a href="#cb14-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-224"><a href="#cb14-224" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Grösserer Stride in frühen Layern:**</span>
<span id="cb14-225"><a href="#cb14-225" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span><span class="in">`stride=2`</span> oder <span class="in">`stride=4`</span> reduziert Auflösung schnell</span>
<span id="cb14-226"><a href="#cb14-226" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Beispiel: $8000 \times 8000 \xrightarrow{\text{stride=4}} 2000 \times 2000$</span>
<span id="cb14-227"><a href="#cb14-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-228"><a href="#cb14-228" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Aggressive Pooling:**</span>
<span id="cb14-229"><a href="#cb14-229" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span><span class="in">`nn.MaxPool2d(kernel_size=4, stride=4)`</span> früh einsetzen</span>
<span id="cb14-230"><a href="#cb14-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-231"><a href="#cb14-231" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Kleinere Batch Size:**</span>
<span id="cb14-232"><a href="#cb14-232" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Batch size = 1 statt 32 (weniger parallel)</span>
<span id="cb14-233"><a href="#cb14-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-234"><a href="#cb14-234" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Weniger Filter in frühen Layern:**</span>
<span id="cb14-235"><a href="#cb14-235" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Statt 64 nur 32 oder 16 Filter verwenden</span>
<span id="cb14-236"><a href="#cb14-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-237"><a href="#cb14-237" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Image Resizing vor dem Modell:**</span>
<span id="cb14-238"><a href="#cb14-238" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Bilder auf z.B. $2000 \times 2000$ downsampen</span>
<span id="cb14-239"><a href="#cb14-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-240"><a href="#cb14-240" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>**Dilated Convolutions:**</span>
<span id="cb14-241"><a href="#cb14-241" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Grösseres receptive field ohne mehr Parameter</span>
<span id="cb14-242"><a href="#cb14-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-243"><a href="#cb14-243" aria-hidden="true" tabindex="-1"></a>**Beste Kombination:** Stride=2-4 in ersten Layern + weniger Filter + kleinere Batch Size</span>
<span id="cb14-244"><a href="#cb14-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-245"><a href="#cb14-245" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-246"><a href="#cb14-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-247"><a href="#cb14-247" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Können CNNs verwendet werden, um Inputs wie z.B. Satellitendaten zu modellieren, die keine RGB-Bilder sind, d.h. mehr als 3 Channels haben? Begründen Sie Ihre Antwort.</span>
<span id="cb14-248"><a href="#cb14-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-249"><a href="#cb14-249" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-250"><a href="#cb14-250" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Antwort anzeigen<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb14-251"><a href="#cb14-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-252"><a href="#cb14-252" aria-hidden="true" tabindex="-1"></a>**Ja, absolut!** CNNs können beliebig viele Input-Channels verarbeiten.</span>
<span id="cb14-253"><a href="#cb14-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-254"><a href="#cb14-254" aria-hidden="true" tabindex="-1"></a>**Begründung:**</span>
<span id="cb14-255"><a href="#cb14-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-256"><a href="#cb14-256" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Der erste Conv-Layer spezifiziert <span class="in">`in_channels`</span> - dies kann jede Zahl sein</span>
<span id="cb14-257"><a href="#cb14-257" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Satellitendaten haben oft 10+ Bänder (multispektral, hyperspektral)</span>
<span id="cb14-258"><a href="#cb14-258" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Medizinische Bilder können mehrere Modalitäten kombinieren (CT + MRI)</span>
<span id="cb14-259"><a href="#cb14-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-260"><a href="#cb14-260" aria-hidden="true" tabindex="-1"></a>**Beispiel:**</span>
<span id="cb14-261"><a href="#cb14-261" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb14-262"><a href="#cb14-262" aria-hidden="true" tabindex="-1"></a><span class="co"># Satellitendaten mit 12 Bändern</span></span>
<span id="cb14-263"><a href="#cb14-263" aria-hidden="true" tabindex="-1"></a>nn.Conv2d(in_channels<span class="op">=</span><span class="dv">12</span>, out_channels<span class="op">=</span><span class="dv">64</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-264"><a href="#cb14-264" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-265"><a href="#cb14-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-266"><a href="#cb14-266" aria-hidden="true" tabindex="-1"></a>**Vorteile:**</span>
<span id="cb14-267"><a href="#cb14-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-268"><a href="#cb14-268" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>CNNs lernen automatisch relevante Kombinationen der Channels</span>
<span id="cb14-269"><a href="#cb14-269" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Räumliche Struktur bleibt erhalten (2D Convolutions)</span>
<span id="cb14-270"><a href="#cb14-270" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Gleiche Architektur-Prinzipien wie bei RGB</span>
<span id="cb14-271"><a href="#cb14-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-272"><a href="#cb14-272" aria-hidden="true" tabindex="-1"></a>**Hinweis:** Bei Transfer Learning von ImageNet (3 Channels) muss der erste Layer angepasst werden!</span>
<span id="cb14-273"><a href="#cb14-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-274"><a href="#cb14-274" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-275"><a href="#cb14-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-276"><a href="#cb14-276" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>Sie möchten Bilder modellieren, die nicht quadratisch sind. Sie haben eine räumliche Auflösung von 800x400. Was ist bei einem CNN anders wenn es mit nicht quadratischen Bildern trainiert wird (im Vergleich zu quadratischen Bildern)?</span>
<span id="cb14-277"><a href="#cb14-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-278"><a href="#cb14-278" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-279"><a href="#cb14-279" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Antwort anzeigen<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb14-280"><a href="#cb14-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-281"><a href="#cb14-281" aria-hidden="true" tabindex="-1"></a>**Hauptsächlich nichts Fundamentales!** CNNs funktionieren auch mit nicht-quadratischen Bildern.</span>
<span id="cb14-282"><a href="#cb14-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-283"><a href="#cb14-283" aria-hidden="true" tabindex="-1"></a>**Unterschiede:**</span>
<span id="cb14-284"><a href="#cb14-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-285"><a href="#cb14-285" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Activation Maps bleiben nicht-quadratisch:**</span>
<span id="cb14-286"><a href="#cb14-286" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Input $800 \times 400$ → nach Conv mit stride=1, padding=1 immer noch $800 \times 400$</span>
<span id="cb14-287"><a href="#cb14-287" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Nach Pooling (2x2): $400 \times 200$</span>
<span id="cb14-288"><a href="#cb14-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-289"><a href="#cb14-289" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Fully Connected Layer:**</span>
<span id="cb14-290"><a href="#cb14-290" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>FC Layer müssen die genaue Auflösung kennen</span>
<span id="cb14-291"><a href="#cb14-291" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Bei $32 \times 128 \times 8 \times 4$: <span class="in">`nn.Linear(128*8*4, num_classes)`</span></span>
<span id="cb14-292"><a href="#cb14-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-293"><a href="#cb14-293" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Global Pooling vorteilhaft:**</span>
<span id="cb14-294"><a href="#cb14-294" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span><span class="in">`nn.AdaptiveAvgPool2d((1,1))`</span> funktioniert unabhängig von Auflösung</span>
<span id="cb14-295"><a href="#cb14-295" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Macht Modell flexibler für verschiedene Grössen</span>
<span id="cb14-296"><a href="#cb14-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-297"><a href="#cb14-297" aria-hidden="true" tabindex="-1"></a>**Kein Unterschied:**</span>
<span id="cb14-298"><a href="#cb14-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-299"><a href="#cb14-299" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Convolutions funktionieren identisch</span>
<span id="cb14-300"><a href="#cb14-300" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Receptive fields sind gleich</span>
<span id="cb14-301"><a href="#cb14-301" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Training-Algorithmus ist gleich</span>
<span id="cb14-302"><a href="#cb14-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-303"><a href="#cb14-303" aria-hidden="true" tabindex="-1"></a>**Best Practice:** Global Average Pooling verwenden statt FC-Layers mit fixer Grösse.</span>
<span id="cb14-304"><a href="#cb14-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-305"><a href="#cb14-305" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-306"><a href="#cb14-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-307"><a href="#cb14-307" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>Im obigen Beispiel mit Bildern mit einer räumlichen Auflösung von 800x400. Wenn Sie wirklich quadratische Activation Maps in Ihrem Modell haben wollten, wie könnten Sie dies mit dem ersten Convolutional Layer erreichen? Wie müssten Sie diesen parametrisieren? Konsultieren Sie bei Bedarf die PyTorch Dokumentation <span class="co">[</span><span class="ot">torch.nn.Conv2d</span><span class="co">](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)</span>.</span>
<span id="cb14-308"><a href="#cb14-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-309"><a href="#cb14-309" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-310"><a href="#cb14-310" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Antwort anzeigen<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb14-311"><a href="#cb14-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-312"><a href="#cb14-312" aria-hidden="true" tabindex="-1"></a>**Lösung: Asymmetrische Stride-Parameter verwenden**</span>
<span id="cb14-313"><a href="#cb14-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-314"><a href="#cb14-314" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb14-315"><a href="#cb14-315" aria-hidden="true" tabindex="-1"></a>nn.Conv2d(</span>
<span id="cb14-316"><a href="#cb14-316" aria-hidden="true" tabindex="-1"></a>    in_channels<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb14-317"><a href="#cb14-317" aria-hidden="true" tabindex="-1"></a>    out_channels<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb14-318"><a href="#cb14-318" aria-hidden="true" tabindex="-1"></a>    kernel_size<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb14-319"><a href="#cb14-319" aria-hidden="true" tabindex="-1"></a>    stride<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">1</span>),  <span class="co"># Höhe um Faktor 2 reduzieren, Breite beibehalten</span></span>
<span id="cb14-320"><a href="#cb14-320" aria-hidden="true" tabindex="-1"></a>    padding<span class="op">=</span><span class="dv">1</span></span>
<span id="cb14-321"><a href="#cb14-321" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-322"><a href="#cb14-322" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-323"><a href="#cb14-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-324"><a href="#cb14-324" aria-hidden="true" tabindex="-1"></a>**Berechnung:**</span>
<span id="cb14-325"><a href="#cb14-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-326"><a href="#cb14-326" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Input: $800 \times 400$</span>
<span id="cb14-327"><a href="#cb14-327" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Nach Conv mit <span class="in">`stride=(2, 1)`</span>: $400 \times 400$</span>
<span id="cb14-328"><a href="#cb14-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-329"><a href="#cb14-329" aria-hidden="true" tabindex="-1"></a>**Alternative mit Pooling:**</span>
<span id="cb14-330"><a href="#cb14-330" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb14-331"><a href="#cb14-331" aria-hidden="true" tabindex="-1"></a><span class="co"># Convolution mit stride=1</span></span>
<span id="cb14-332"><a href="#cb14-332" aria-hidden="true" tabindex="-1"></a>conv <span class="op">=</span> nn.Conv2d(<span class="dv">3</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-333"><a href="#cb14-333" aria-hidden="true" tabindex="-1"></a><span class="co"># Asymmetrisches Pooling</span></span>
<span id="cb14-334"><a href="#cb14-334" aria-hidden="true" tabindex="-1"></a>pool <span class="op">=</span> nn.MaxPool2d(kernel_size<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">1</span>), stride<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">1</span>))</span>
<span id="cb14-335"><a href="#cb14-335" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-336"><a href="#cb14-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-337"><a href="#cb14-337" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-338"><a href="#cb14-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-339"><a href="#cb14-339" aria-hidden="true" tabindex="-1"></a><span class="ss">7. </span>Wenn die Pixel Ihrer Input Bilder mit einer fixen Permutationsmatrix zufällig neu sortiert werden (jedes Bild wird identisch neu sortiert), bevorzugen Sie ein CNN oder ein MLP? Begründen Sie Ihre Antwort.</span>
<span id="cb14-340"><a href="#cb14-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-341"><a href="#cb14-341" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-342"><a href="#cb14-342" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Antwort anzeigen<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb14-343"><a href="#cb14-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-344"><a href="#cb14-344" aria-hidden="true" tabindex="-1"></a>**Antwort: MLP ist vorzuziehen!**</span>
<span id="cb14-345"><a href="#cb14-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-346"><a href="#cb14-346" aria-hidden="true" tabindex="-1"></a>**Begründung:**</span>
<span id="cb14-347"><a href="#cb14-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-348"><a href="#cb14-348" aria-hidden="true" tabindex="-1"></a>**CNNs basieren auf räumlicher Lokalität:**</span>
<span id="cb14-349"><a href="#cb14-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-350"><a href="#cb14-350" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Convolutions nutzen aus, dass benachbarte Pixel zusammenhängen</span>
<span id="cb14-351"><a href="#cb14-351" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Nach zufälliger Permutation ist diese Struktur zerstört</span>
<span id="cb14-352"><a href="#cb14-352" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Ein 3×3 Filter würde auf räumlich nicht-zusammenhängende Pixel angewendet</span>
<span id="cb14-353"><a href="#cb14-353" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Der Hauptvorteil von CNNs (lokale Feature-Extraktion) geht verloren</span>
<span id="cb14-354"><a href="#cb14-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-355"><a href="#cb14-355" aria-hidden="true" tabindex="-1"></a>**MLPs behandeln alle Positionen gleich:**</span>
<span id="cb14-356"><a href="#cb14-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-357"><a href="#cb14-357" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>MLPs haben keine Annahmen über räumliche Struktur</span>
<span id="cb14-358"><a href="#cb14-358" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Jedes Pixel wird unabhängig behandelt (volles weight sharing)</span>
<span id="cb14-359"><a href="#cb14-359" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Permutation ändert nur die Position im Eingangsvektor, nicht die Funktionsweise</span>
<span id="cb14-360"><a href="#cb14-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-361"><a href="#cb14-361" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-362"><a href="#cb14-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-363"><a href="#cb14-363" aria-hidden="true" tabindex="-1"></a><span class="ss">8.  </span>Schauen Sie sich das folgende CNN an. Können Sie die räumliche Auflösung der Bilder ableiten? Wenn ja, berechnen Sie diese. Wenn nein, begründen Sie.</span>
<span id="cb14-364"><a href="#cb14-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-365"><a href="#cb14-365" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-366"><a href="#cb14-366" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Antwort anzeigen<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb14-367"><a href="#cb14-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-368"><a href="#cb14-368" aria-hidden="true" tabindex="-1"></a>**Ja, die räumliche Auflösung kann berechnet werden!**</span>
<span id="cb14-369"><a href="#cb14-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-370"><a href="#cb14-370" aria-hidden="true" tabindex="-1"></a>**Rückwärts-Rechnung vom FC-Layer:**</span>
<span id="cb14-371"><a href="#cb14-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-372"><a href="#cb14-372" aria-hidden="true" tabindex="-1"></a>Der FC-Layer erwartet: <span class="in">`32 * 8 * 8 = 2048`</span> Eingänge</span>
<span id="cb14-373"><a href="#cb14-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-374"><a href="#cb14-374" aria-hidden="true" tabindex="-1"></a>Dies bedeutet nach 2× Pooling: $32 \times 8 \times 8$</span>
<span id="cb14-375"><a href="#cb14-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-376"><a href="#cb14-376" aria-hidden="true" tabindex="-1"></a>**Berechnung:**</span>
<span id="cb14-377"><a href="#cb14-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-378"><a href="#cb14-378" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Nach pool2: $32 \times 8 \times 8$</span>
<span id="cb14-379"><a href="#cb14-379" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Vor pool2 (nach conv2): $32 \times 16 \times 16$ (Pooling halbiert)</span>
<span id="cb14-380"><a href="#cb14-380" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Nach pool1: $16 \times 16 \times 16$</span>
<span id="cb14-381"><a href="#cb14-381" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Vor pool1 (nach conv1): $16 \times 32 \times 32$ (Pooling halbiert)</span>
<span id="cb14-382"><a href="#cb14-382" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Input:** $3 \times 32 \times 32$</span>
<span id="cb14-383"><a href="#cb14-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-384"><a href="#cb14-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-385"><a href="#cb14-385" aria-hidden="true" tabindex="-1"></a>**Antwort: Die Input-Bilder haben eine räumliche Auflösung von $32 \times 32$**</span>
<span id="cb14-386"><a href="#cb14-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-387"><a href="#cb14-387" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-388"><a href="#cb14-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-391"><a href="#cb14-391" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-392"><a href="#cb14-392" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb14-393"><a href="#cb14-393" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-394"><a href="#cb14-394" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CNN1(nn.Module):</span>
<span id="cb14-395"><a href="#cb14-395" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb14-396"><a href="#cb14-396" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(CNN1, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb14-397"><a href="#cb14-397" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(<span class="dv">3</span>, <span class="dv">16</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-398"><a href="#cb14-398" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(<span class="dv">16</span>, <span class="dv">32</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-399"><a href="#cb14-399" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu <span class="op">=</span> nn.ReLU()</span>
<span id="cb14-400"><a href="#cb14-400" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pool <span class="op">=</span> nn.MaxPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb14-401"><a href="#cb14-401" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc <span class="op">=</span> nn.Linear(<span class="dv">32</span> <span class="op">*</span> <span class="dv">8</span> <span class="op">*</span> <span class="dv">8</span>, <span class="dv">10</span>)</span>
<span id="cb14-402"><a href="#cb14-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-403"><a href="#cb14-403" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb14-404"><a href="#cb14-404" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.conv1(x))</span>
<span id="cb14-405"><a href="#cb14-405" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(x)</span>
<span id="cb14-406"><a href="#cb14-406" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.conv2(x))</span>
<span id="cb14-407"><a href="#cb14-407" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(x)</span>
<span id="cb14-408"><a href="#cb14-408" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(x.size(<span class="dv">0</span>), <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb14-409"><a href="#cb14-409" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc(x)</span>
<span id="cb14-410"><a href="#cb14-410" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb14-411"><a href="#cb14-411" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-412"><a href="#cb14-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-413"><a href="#cb14-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-414"><a href="#cb14-414" aria-hidden="true" tabindex="-1"></a><span class="ss">9. </span>Schauen Sie sich die folgenden beiden CNNs an. Was sind die wichtigsten praktischen Unterschiede?</span>
<span id="cb14-415"><a href="#cb14-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-418"><a href="#cb14-418" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-419"><a href="#cb14-419" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb14-420"><a href="#cb14-420" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-421"><a href="#cb14-421" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CNN1(nn.Module):</span>
<span id="cb14-422"><a href="#cb14-422" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb14-423"><a href="#cb14-423" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(CNN1, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb14-424"><a href="#cb14-424" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(<span class="dv">3</span>, <span class="dv">16</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-425"><a href="#cb14-425" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(<span class="dv">16</span>, <span class="dv">32</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-426"><a href="#cb14-426" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu <span class="op">=</span> nn.ReLU()</span>
<span id="cb14-427"><a href="#cb14-427" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pool <span class="op">=</span> nn.MaxPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb14-428"><a href="#cb14-428" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc <span class="op">=</span> nn.Linear(<span class="dv">32</span> <span class="op">*</span> <span class="dv">8</span> <span class="op">*</span> <span class="dv">8</span>, <span class="dv">10</span>)</span>
<span id="cb14-429"><a href="#cb14-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-430"><a href="#cb14-430" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb14-431"><a href="#cb14-431" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.conv1(x))</span>
<span id="cb14-432"><a href="#cb14-432" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(x)</span>
<span id="cb14-433"><a href="#cb14-433" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.conv2(x))</span>
<span id="cb14-434"><a href="#cb14-434" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(x)</span>
<span id="cb14-435"><a href="#cb14-435" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(x.size(<span class="dv">0</span>), <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb14-436"><a href="#cb14-436" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc(x)</span>
<span id="cb14-437"><a href="#cb14-437" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb14-438"><a href="#cb14-438" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-439"><a href="#cb14-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-442"><a href="#cb14-442" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb14-443"><a href="#cb14-443" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb14-444"><a href="#cb14-444" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: true</span></span>
<span id="cb14-445"><a href="#cb14-445" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CNN2(nn.Module):</span>
<span id="cb14-446"><a href="#cb14-446" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb14-447"><a href="#cb14-447" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(CNN2, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb14-448"><a href="#cb14-448" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(<span class="dv">3</span>, <span class="dv">16</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-449"><a href="#cb14-449" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(<span class="dv">16</span>, <span class="dv">32</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-450"><a href="#cb14-450" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu <span class="op">=</span> nn.ReLU()</span>
<span id="cb14-451"><a href="#cb14-451" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pool <span class="op">=</span> nn.MaxPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb14-452"><a href="#cb14-452" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.global_avg_pool <span class="op">=</span> nn.AdaptiveAvgPool2d((<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb14-453"><a href="#cb14-453" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc <span class="op">=</span> nn.Linear(<span class="dv">32</span>, <span class="dv">10</span>)</span>
<span id="cb14-454"><a href="#cb14-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-455"><a href="#cb14-455" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb14-456"><a href="#cb14-456" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.conv1(x))</span>
<span id="cb14-457"><a href="#cb14-457" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(x)</span>
<span id="cb14-458"><a href="#cb14-458" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu(<span class="va">self</span>.conv2(x))</span>
<span id="cb14-459"><a href="#cb14-459" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(x)</span>
<span id="cb14-460"><a href="#cb14-460" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.global_avg_pool(x)</span>
<span id="cb14-461"><a href="#cb14-461" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(x.size(<span class="dv">0</span>), <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb14-462"><a href="#cb14-462" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc(x)</span>
<span id="cb14-463"><a href="#cb14-463" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb14-464"><a href="#cb14-464" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-465"><a href="#cb14-465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-466"><a href="#cb14-466" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-467"><a href="#cb14-467" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Antwort anzeigen<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb14-468"><a href="#cb14-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-469"><a href="#cb14-469" aria-hidden="true" tabindex="-1"></a>**Hauptunterschied: Global Average Pooling vs Fixed-size FC Layer**</span>
<span id="cb14-470"><a href="#cb14-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-471"><a href="#cb14-471" aria-hidden="true" tabindex="-1"></a>**CNN1:**</span>
<span id="cb14-472"><a href="#cb14-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-473"><a href="#cb14-473" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Verwendet <span class="in">`nn.Linear(32 * 8 * 8, 10)`</span> - fest an Input-Grösse $32 \times 32$ gebunden</span>
<span id="cb14-474"><a href="#cb14-474" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Kann nur mit Bildern dieser Grösse umgehen</span>
<span id="cb14-475"><a href="#cb14-475" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Mehr Parameter im FC-Layer: $32 \times 8 \times 8 \times 10 = 20{,}480$ Parameter</span>
<span id="cb14-476"><a href="#cb14-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-477"><a href="#cb14-477" aria-hidden="true" tabindex="-1"></a>**CNN2:**</span>
<span id="cb14-478"><a href="#cb14-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-479"><a href="#cb14-479" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Verwendet <span class="in">`nn.AdaptiveAvgPool2d((1, 1))`</span> - reduziert jede feature map auf 1×1</span>
<span id="cb14-480"><a href="#cb14-480" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Funktioniert mit **beliebigen** Input-Grössen!</span>
<span id="cb14-481"><a href="#cb14-481" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Weniger Parameter: $32 \times 10 = 320$ Parameter</span>
<span id="cb14-482"><a href="#cb14-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-483"><a href="#cb14-483" aria-hidden="true" tabindex="-1"></a>**Praktische Implikationen:**</span>
<span id="cb14-484"><a href="#cb14-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-485"><a href="#cb14-485" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Flexibilität:** CNN2 kann $16\times16$, $64\times64$, $128\times256$ verarbeiten</span>
<span id="cb14-486"><a href="#cb14-486" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Effizienz:** CNN2 hat drastisch weniger Parameter</span>
<span id="cb14-487"><a href="#cb14-487" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Overfitting:** CNN2 ist robuster (weniger Parameter)</span>
<span id="cb14-488"><a href="#cb14-488" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Modern Best Practice:** Global Average Pooling wird in modernen Architekturen bevorzugt</span>
<span id="cb14-489"><a href="#cb14-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-490"><a href="#cb14-490" aria-hidden="true" tabindex="-1"></a>**Empfehlung: CNN2 (Global Average Pooling) ist deutlich besser!**</span>
<span id="cb14-491"><a href="#cb14-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-492"><a href="#cb14-492" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-493"><a href="#cb14-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-494"><a href="#cb14-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-495"><a href="#cb14-495" aria-hidden="true" tabindex="-1"></a><span class="ss">10. </span>Zeichnen Sie das Receptive Field vom mittleren Wert der Activation map ganz rechts (grau) in @fig-quiz-cnn-receptive-field bezüglich der Input Activation ganz links ein. Die Convs sind folgendermasen parametrisiert:</span>
<span id="cb14-496"><a href="#cb14-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-497"><a href="#cb14-497" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>CONV1: kernel_size = 5, stride = 1, dilation = 1, padding=2</span>
<span id="cb14-498"><a href="#cb14-498" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>CONV2: kernel_size = 3, stride = 1, dilation = 1, padding=1</span>
<span id="cb14-499"><a href="#cb14-499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-500"><a href="#cb14-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-501"><a href="#cb14-501" aria-hidden="true" tabindex="-1"></a>::: {#fig-quiz-cnn-receptive-field}</span>
<span id="cb14-502"><a href="#cb14-502" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}cnn_receptive_field.png)</span></span>
<span id="cb14-503"><a href="#cb14-503" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-504"><a href="#cb14-504" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-505"><a href="#cb14-505" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-506"><a href="#cb14-506" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Antwort anzeigen<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb14-507"><a href="#cb14-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-508"><a href="#cb14-508" aria-hidden="true" tabindex="-1"></a>**Berechnung des Receptive Fields:**</span>
<span id="cb14-509"><a href="#cb14-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-510"><a href="#cb14-510" aria-hidden="true" tabindex="-1"></a>**Rückwärts von Layer 2 zu Layer 1:**</span>
<span id="cb14-511"><a href="#cb14-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-512"><a href="#cb14-512" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Mittlerer Pixel in Layer 2 Activation</span>
<span id="cb14-513"><a href="#cb14-513" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>CONV2 hat kernel_size=3, stride=1</span>
<span id="cb14-514"><a href="#cb14-514" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Dieser Pixel "sieht" 3×3 Bereich in Layer 1 Activation</span>
<span id="cb14-515"><a href="#cb14-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-516"><a href="#cb14-516" aria-hidden="true" tabindex="-1"></a>**Rückwärts von Layer 1 zu Input:**</span>
<span id="cb14-517"><a href="#cb14-517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-518"><a href="#cb14-518" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Jeder dieser 3×3 Pixel in Layer 1 Activation</span>
<span id="cb14-519"><a href="#cb14-519" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>CONV1 hat kernel_size=5, stride=1</span>
<span id="cb14-520"><a href="#cb14-520" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Jeder "sieht" 5×5 Bereich im Input</span>
<span id="cb14-521"><a href="#cb14-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-522"><a href="#cb14-522" aria-hidden="true" tabindex="-1"></a>**Receptive Field Berechnung:**</span>
<span id="cb14-523"><a href="#cb14-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-524"><a href="#cb14-524" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Nach CONV1: 5×5</span>
<span id="cb14-525"><a href="#cb14-525" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Nach CONV2: $5 + 2 \times (3-1) = 5 + 4 = 9 \times 9$</span>
<span id="cb14-526"><a href="#cb14-526" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-527"><a href="#cb14-527" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-528"><a href="#cb14-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-529"><a href="#cb14-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-530"><a href="#cb14-530" aria-hidden="true" tabindex="-1"></a><span class="ss">11. </span>Berechnen Sie den fehlenden Wert in der Activation Map von @fig-quiz-cnn-calc-act.</span>
<span id="cb14-531"><a href="#cb14-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-532"><a href="#cb14-532" aria-hidden="true" tabindex="-1"></a>Die Convolution ist folgendermassen parametrisiert:</span>
<span id="cb14-533"><a href="#cb14-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-534"><a href="#cb14-534" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-535"><a href="#cb14-535" aria-hidden="true" tabindex="-1"></a><span class="in">stride=1</span></span>
<span id="cb14-536"><a href="#cb14-536" aria-hidden="true" tabindex="-1"></a><span class="in">padding=1</span></span>
<span id="cb14-537"><a href="#cb14-537" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-538"><a href="#cb14-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-539"><a href="#cb14-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-540"><a href="#cb14-540" aria-hidden="true" tabindex="-1"></a>::: {#fig-quiz-cnn-calc-act}</span>
<span id="cb14-541"><a href="#cb14-541" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}conv_calc.png)</span></span>
<span id="cb14-542"><a href="#cb14-542" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-543"><a href="#cb14-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-544"><a href="#cb14-544" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-545"><a href="#cb14-545" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Antwort anzeigen<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb14-546"><a href="#cb14-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-547"><a href="#cb14-547" aria-hidden="true" tabindex="-1"></a>todo</span>
<span id="cb14-548"><a href="#cb14-548" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-549"><a href="#cb14-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-550"><a href="#cb14-550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-551"><a href="#cb14-551" aria-hidden="true" tabindex="-1"></a><span class="ss">12. </span>Können CNNs verwendet werden um Objekte zu zählen? Schauen Sie sich @fig-quiz-cnn-counting an. Sie möchten zählen in wie vielen Quadranten ein Objekt vorkommt. Begründen Sie Ihre Antwort.</span>
<span id="cb14-552"><a href="#cb14-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-553"><a href="#cb14-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-554"><a href="#cb14-554" aria-hidden="true" tabindex="-1"></a>::: {#fig-quiz-cnn-counting}</span>
<span id="cb14-555"><a href="#cb14-555" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}counting.jpg)</span>{width=300}</span>
<span id="cb14-556"><a href="#cb14-556" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb14-557"><a href="#cb14-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-558"><a href="#cb14-558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-559"><a href="#cb14-559" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-560"><a href="#cb14-560" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Antwort anzeigen<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb14-561"><a href="#cb14-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-562"><a href="#cb14-562" aria-hidden="true" tabindex="-1"></a>Ja, CNNs können zum Zählen verwendet werden. Die Aktivation-Maps werden je nach Anzahl Objekten an verschiedenen Orten aktiv. Diese Aktivierung kann mit z.B. einer Regression (Output ist Anzahl Objekte) modelliert werden.</span>
<span id="cb14-563"><a href="#cb14-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-564"><a href="#cb14-564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-565"><a href="#cb14-565" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-566"><a href="#cb14-566" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-567"><a href="#cb14-567" aria-hidden="true" tabindex="-1"></a><span class="fu">## Image Classification</span></span>
<span id="cb14-568"><a href="#cb14-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-569"><a href="#cb14-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-570"><a href="#cb14-570" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Sie haben folgende logits <span class="in">`[3.0, 1.0, 1.0]`</span>.</span>
<span id="cb14-571"><a href="#cb14-571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-572"><a href="#cb14-572" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Wieviele Klassen hat ihr Klassifikationsproblem?</span>
<span id="cb14-573"><a href="#cb14-573" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Die Summe über die Logits ist 5.0. Was ist die Summe nach einer Transformation mit der Softmax Funktion? Warum ist das wichtig?</span>
<span id="cb14-574"><a href="#cb14-574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-575"><a href="#cb14-575" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-576"><a href="#cb14-576" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Antwort anzeigen<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb14-577"><a href="#cb14-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-578"><a href="#cb14-578" aria-hidden="true" tabindex="-1"></a>**Teil 1: Anzahl Klassen**</span>
<span id="cb14-579"><a href="#cb14-579" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**3 Klassen** (ein Logit pro Klasse)</span>
<span id="cb14-580"><a href="#cb14-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-581"><a href="#cb14-581" aria-hidden="true" tabindex="-1"></a>**Teil 2: Softmax Summe**</span>
<span id="cb14-582"><a href="#cb14-582" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Die Summe ist immer 1.0** nach Softmax!</span>
<span id="cb14-583"><a href="#cb14-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-584"><a href="#cb14-584" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-585"><a href="#cb14-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-586"><a href="#cb14-586" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Beschreiben Sie wie Image pre-processing das Training eines CNNs beeinflusst. Auf was muss man bezüglich Pre-Processing achten, wenn man ein vortrainiertes Modell verwenden möchte (Transfer-Learning)?</span>
<span id="cb14-587"><a href="#cb14-587" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-588"><a href="#cb14-588" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-589"><a href="#cb14-589" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Antwort anzeigen<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb14-590"><a href="#cb14-590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-591"><a href="#cb14-591" aria-hidden="true" tabindex="-1"></a>**Einfluss von Pre-Processing auf Training:**</span>
<span id="cb14-592"><a href="#cb14-592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-593"><a href="#cb14-593" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Normalisierung:** Beschleunigt Konvergenz, stabilisiert Training</span>
<span id="cb14-594"><a href="#cb14-594" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Data Augmentation:** Erhöht Generalisierung, reduziert Overfitting</span>
<span id="cb14-595"><a href="#cb14-595" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Resizing:** Beeinflusst sichtbare Details, Rechenaufwand</span>
<span id="cb14-596"><a href="#cb14-596" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-597"><a href="#cb14-597" aria-hidden="true" tabindex="-1"></a>**Bei Transfer Learning - KRITISCH:**</span>
<span id="cb14-598"><a href="#cb14-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-599"><a href="#cb14-599" aria-hidden="true" tabindex="-1"></a>**Must Do:**</span>
<span id="cb14-600"><a href="#cb14-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-601"><a href="#cb14-601" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Gleiche Normalisierung wie Vortraining verwenden!**</span>
<span id="cb14-602"><a href="#cb14-602" aria-hidden="true" tabindex="-1"></a>  <span class="in">```python</span></span>
<span id="cb14-603"><a href="#cb14-603" aria-hidden="true" tabindex="-1"></a>  <span class="co"># ImageNet Preprocessing</span></span>
<span id="cb14-604"><a href="#cb14-604" aria-hidden="true" tabindex="-1"></a>  transforms.Normalize(mean<span class="op">=</span>[<span class="fl">0.485</span>, <span class="fl">0.456</span>, <span class="fl">0.406</span>],</span>
<span id="cb14-605"><a href="#cb14-605" aria-hidden="true" tabindex="-1"></a>                       std<span class="op">=</span>[<span class="fl">0.229</span>, <span class="fl">0.224</span>, <span class="fl">0.225</span>])</span>
<span id="cb14-606"><a href="#cb14-606" aria-hidden="true" tabindex="-1"></a>  <span class="in">```</span></span>
<span id="cb14-607"><a href="#cb14-607" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Gleiche Input-Grösse:** Meist 224×224 für ImageNet-Modelle</span>
<span id="cb14-608"><a href="#cb14-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-609"><a href="#cb14-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-610"><a href="#cb14-610" aria-hidden="true" tabindex="-1"></a>**Warum wichtig?**</span>
<span id="cb14-611"><a href="#cb14-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-612"><a href="#cb14-612" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Vortrainiertes Modell erwartet spezifische Input-Distribution</span>
<span id="cb14-613"><a href="#cb14-613" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Falsche Normalisierung → Features "out of distribution" → schlechte Performance</span>
<span id="cb14-614"><a href="#cb14-614" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Dokumentation des Modells konsultieren!</span>
<span id="cb14-615"><a href="#cb14-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-616"><a href="#cb14-616" aria-hidden="true" tabindex="-1"></a>**Tipp:** Verwenden Sie die offiziellen Preprocessing-Pipelines des Modells (z.B. von timm oder Hugging Face).</span>
<span id="cb14-617"><a href="#cb14-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-618"><a href="#cb14-618" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-619"><a href="#cb14-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-620"><a href="#cb14-620" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Sie haben ein Modell trainiert um synthetische (fake) Bilder zu identifizieren. Das Modell ist ziemlich gut aber nicht perfekt. Wenn Sie das Modell deployen haben Sie die Möglichkeit einige der Bilder von einem Experten überprüfen zu lassen. Welche Bilder wählen Sie aus? Begründen Sie die Antwort.</span>
<span id="cb14-621"><a href="#cb14-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-622"><a href="#cb14-622" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-623"><a href="#cb14-623" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Antwort anzeigen<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb14-624"><a href="#cb14-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-625"><a href="#cb14-625" aria-hidden="true" tabindex="-1"></a>**Strategie**: Bilder mit **unsicheren Vorhersagen** auswählen</span>
<span id="cb14-626"><a href="#cb14-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-627"><a href="#cb14-627" aria-hidden="true" tabindex="-1"></a>**Konkret:**</span>
<span id="cb14-628"><a href="#cb14-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-629"><a href="#cb14-629" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Wählen Sie Bilder, bei denen die Confidence nahe bei 0.5 liegt</span>
<span id="cb14-630"><a href="#cb14-630" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Beispiel: P(fake) = 0.45 bis 0.55</span>
<span id="cb14-631"><a href="#cb14-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-632"><a href="#cb14-632" aria-hidden="true" tabindex="-1"></a>**Begründung:**</span>
<span id="cb14-633"><a href="#cb14-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-634"><a href="#cb14-634" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Maximaler Informationsgewinn:**</span>
<span id="cb14-635"><a href="#cb14-635" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Unsichere Predictions zeigen schwierige Fälle</span>
<span id="cb14-636"><a href="#cb14-636" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Experten-Labels für diese sind am wertvollsten</span>
<span id="cb14-637"><a href="#cb14-637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-638"><a href="#cb14-638" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Effiziente Ressourcennutzung:**</span>
<span id="cb14-639"><a href="#cb14-639" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Sehr sichere Predictions ($P &gt; 0.95$) sind wahrscheinlich korrekt</span>
<span id="cb14-640"><a href="#cb14-640" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Experte bestätigt nur Bekanntes → wenig Mehrwert</span>
<span id="cb14-641"><a href="#cb14-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-642"><a href="#cb14-642" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Active Learning Prinzip:**</span>
<span id="cb14-643"><a href="#cb14-643" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Modell lernt am meisten an der Entscheidungsgrenze</span>
<span id="cb14-644"><a href="#cb14-644" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Diese Bilder können fürs Re-Training genutzt werden</span>
<span id="cb14-645"><a href="#cb14-645" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-646"><a href="#cb14-646" aria-hidden="true" tabindex="-1"></a>Auch extreme Misclassifications (sehr sichere aber falsche) anschauen, um systematische Fehler zu finden.</span>
<span id="cb14-647"><a href="#cb14-647" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-648"><a href="#cb14-648" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-649"><a href="#cb14-649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-650"><a href="#cb14-650" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Sie möchten ein Modell trainieren um Bilder von Kamerafallen zu klassifizieren. Die Biologen möchten ein Modell, welches für ein Bild aussagt welche Spezies zu sehen ist (immer nur jeweils eine) und was die Tiere machen (jweils eine von 5 Aktivitäten, wie z.B. Essen, Ruhen, etc.) ist möglich. Was ist das für eine Problemstellung? Wie würden Sie diese modellieren?</span>
<span id="cb14-651"><a href="#cb14-651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-652"><a href="#cb14-652" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-653"><a href="#cb14-653" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Antwort anzeigen<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb14-654"><a href="#cb14-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-655"><a href="#cb14-655" aria-hidden="true" tabindex="-1"></a>**Problemstellung: Multi-Task Learning mit zwei Klassifikations-Aufgaben**</span>
<span id="cb14-656"><a href="#cb14-656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-657"><a href="#cb14-657" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Task 1:** Spezies-Klassifikation (single-label, N Klassen)</span>
<span id="cb14-658"><a href="#cb14-658" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Task 2:** Aktivitäts-Klassifikation (single-label, 5 Klassen)</span>
<span id="cb14-659"><a href="#cb14-659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-660"><a href="#cb14-660" aria-hidden="true" tabindex="-1"></a>**Modellierung:**</span>
<span id="cb14-661"><a href="#cb14-661" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-662"><a href="#cb14-662" aria-hidden="true" tabindex="-1"></a>Shared Backbone + Two Heads (empfohlen)</span>
<span id="cb14-663"><a href="#cb14-663" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb14-664"><a href="#cb14-664" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiTaskModel(nn.Module):</span>
<span id="cb14-665"><a href="#cb14-665" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_species, num_activities<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb14-666"><a href="#cb14-666" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb14-667"><a href="#cb14-667" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Shared feature extractor</span></span>
<span id="cb14-668"><a href="#cb14-668" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.backbone <span class="op">=</span> resnet50(pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-669"><a href="#cb14-669" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.backbone.fc <span class="op">=</span> nn.Identity()  <span class="co"># Remove final layer</span></span>
<span id="cb14-670"><a href="#cb14-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-671"><a href="#cb14-671" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Two separate classification heads</span></span>
<span id="cb14-672"><a href="#cb14-672" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.species_head <span class="op">=</span> nn.Linear(<span class="dv">2048</span>, num_species)</span>
<span id="cb14-673"><a href="#cb14-673" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activity_head <span class="op">=</span> nn.Linear(<span class="dv">2048</span>, num_activities)</span>
<span id="cb14-674"><a href="#cb14-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-675"><a href="#cb14-675" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb14-676"><a href="#cb14-676" aria-hidden="true" tabindex="-1"></a>        features <span class="op">=</span> <span class="va">self</span>.backbone(x)</span>
<span id="cb14-677"><a href="#cb14-677" aria-hidden="true" tabindex="-1"></a>        species_logits <span class="op">=</span> <span class="va">self</span>.species_head(features)</span>
<span id="cb14-678"><a href="#cb14-678" aria-hidden="true" tabindex="-1"></a>        activity_logits <span class="op">=</span> <span class="va">self</span>.activity_head(features)</span>
<span id="cb14-679"><a href="#cb14-679" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> species_logits, activity_logits</span>
<span id="cb14-680"><a href="#cb14-680" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-681"><a href="#cb14-681" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-682"><a href="#cb14-682" aria-hidden="true" tabindex="-1"></a>**Loss Function:**</span>
<span id="cb14-683"><a href="#cb14-683" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb14-684"><a href="#cb14-684" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> lambda_species <span class="op">*</span> CE_loss(species_logits, species_labels) <span class="op">+</span> <span class="op">\</span></span>
<span id="cb14-685"><a href="#cb14-685" aria-hidden="true" tabindex="-1"></a>       lambda_activity <span class="op">*</span> CE_loss(activity_logits, activity_labels)</span>
<span id="cb14-686"><a href="#cb14-686" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb14-687"><a href="#cb14-687" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-688"><a href="#cb14-688" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-689"><a href="#cb14-689" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-690"><a href="#cb14-690" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>Sie möchten Bilder von Kamerafallen klassifizieren. Insbesondere wollen Sie ein Modell, welches die Spezies bestimmt, welche in einem Bild sichtbar ist. Sie haben Bilder, die am Tag aufgenommen worden sind und solche die bei Nacht aufgenommen worden sind. Die Tagbilder sind RGB, die Nachtbilder sind graustufen Bilder. Erwähnen Sie 2 Optionen um Tag- und Nachtbilder zu modellieren. Wählen Sie eine aus und begründen Sie.</span>
<span id="cb14-691"><a href="#cb14-691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-692"><a href="#cb14-692" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-693"><a href="#cb14-693" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Antwort anzeigen<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb14-694"><a href="#cb14-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-695"><a href="#cb14-695" aria-hidden="true" tabindex="-1"></a>Graustufen zu RGB konvertieren. Umgekehrt gibt es einen Informationsverlust.</span>
<span id="cb14-696"><a href="#cb14-696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-697"><a href="#cb14-697" aria-hidden="true" tabindex="-1"></a>Vortrainierte Modelle  erwarten RGB!</span>
<span id="cb14-698"><a href="#cb14-698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-699"><a href="#cb14-699" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-700"><a href="#cb14-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-701"><a href="#cb14-701" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>Sind tiefere Netzwerke in der Regel besser als weniger tiefe? Auf was muss man achten? Begründe die Antwort.</span>
<span id="cb14-702"><a href="#cb14-702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-703"><a href="#cb14-703" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-704"><a href="#cb14-704" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Antwort anzeigen<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb14-705"><a href="#cb14-705" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-706"><a href="#cb14-706" aria-hidden="true" tabindex="-1"></a>Ja. Siehe Arbeit von @He2016 an ResNet. Overfitting und schlechte Konvergenz könnten Probleme bereiten.</span>
<span id="cb14-707"><a href="#cb14-707" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-708"><a href="#cb14-708" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-709"><a href="#cb14-709" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-710"><a href="#cb14-710" aria-hidden="true" tabindex="-1"></a><span class="ss">7. </span>Sie werden beauftragt ein Modell zu entwickeln mit dem man in Echtzeit Bilder von einem Kamerasystem klassifizieren kann. Was für Eigenschaften muss die Architektur haben, welche Sie auswählen?</span>
<span id="cb14-711"><a href="#cb14-711" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-712"><a href="#cb14-712" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-713"><a href="#cb14-713" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-714"><a href="#cb14-714" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Antwort anzeigen<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb14-715"><a href="#cb14-715" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-716"><a href="#cb14-716" aria-hidden="true" tabindex="-1"></a>Geschwindigkeit ist zentral. Die Architektur muss also wohl eher klein sein um die Anforderung "Echtzeit" zu erfüllen.</span>
<span id="cb14-717"><a href="#cb14-717" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-718"><a href="#cb14-718" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-719"><a href="#cb14-719" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-720"><a href="#cb14-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-721"><a href="#cb14-721" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-722"><a href="#cb14-722" aria-hidden="true" tabindex="-1"></a><span class="fu">## Representation Learning</span></span>
<span id="cb14-723"><a href="#cb14-723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-724"><a href="#cb14-724" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-725"><a href="#cb14-725" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Warum ist die Darstellung von Bildern als Pixel-Werte ineffizient? Wie hilft Representation Learning?</span>
<span id="cb14-726"><a href="#cb14-726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-727"><a href="#cb14-727" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-728"><a href="#cb14-728" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Antwort anzeigen<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb14-729"><a href="#cb14-729" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-730"><a href="#cb14-730" aria-hidden="true" tabindex="-1"></a>**Problem der Pixel-Repräsentation:**</span>
<span id="cb14-731"><a href="#cb14-731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-732"><a href="#cb14-732" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Der Raum aller möglichen Bilder ist astronomisch gross</span>
<span id="cb14-733"><a href="#cb14-733" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Für RGB-Bilder mit $224 \times 224$ Pixeln und 255 Intensitätswerten: $255^{224 \times 224 \times 3} \approx 255^{150{,}528}$ mögliche Bilder</span>
<span id="cb14-734"><a href="#cb14-734" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Natürliche (reale) Bilder machen nur einen winzigen Bruchteil dieses Raums aus</span>
<span id="cb14-735"><a href="#cb14-735" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Pixel-Repräsentation ist daher extrem ineffizient</span>
<span id="cb14-736"><a href="#cb14-736" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-737"><a href="#cb14-737" aria-hidden="true" tabindex="-1"></a>**Lösung durch Representation Learning:**</span>
<span id="cb14-738"><a href="#cb14-738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-739"><a href="#cb14-739" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Findet **kompakte Repräsentationen**, die viel kleiner sind als Raw-Pixel</span>
<span id="cb14-740"><a href="#cb14-740" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Behält dabei **relevante Information** für praktische Aufgaben</span>
<span id="cb14-741"><a href="#cb14-741" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-742"><a href="#cb14-742" aria-hidden="true" tabindex="-1"></a>**Vorteile:**</span>
<span id="cb14-743"><a href="#cb14-743" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-744"><a href="#cb14-744" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Effizienter für Downstream-Tasks (z.B. Klassifikation)</span>
<span id="cb14-745"><a href="#cb14-745" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Reduziert Dimensionalität drastisch</span>
<span id="cb14-746"><a href="#cb14-746" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Konzentriert sich auf semantisch bedeutsame Merkmale</span>
<span id="cb14-747"><a href="#cb14-747" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Ermöglicht bessere Generalisierung</span>
<span id="cb14-748"><a href="#cb14-748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-749"><a href="#cb14-749" aria-hidden="true" tabindex="-1"></a>**Intuition:** Natürliche Bilder haben Struktur und Regularität - Representation Learning nutzt dies aus, um einen besser geeigneten "Koordinatenraum" zu finden.</span>
<span id="cb14-750"><a href="#cb14-750" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-751"><a href="#cb14-751" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-752"><a href="#cb14-752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-753"><a href="#cb14-753" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Sie möchten eine App entwickeln, welche Bilder in "Screenshots" oder "Fotos" klassifiziert. Allerdings sollte es die Option geben, dass der Anwender zusätzliche Klassen hinzufügen kann. Welches Modell wählen Sie und warum?</span>
<span id="cb14-754"><a href="#cb14-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-755"><a href="#cb14-755" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-756"><a href="#cb14-756" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Antwort anzeigen<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb14-757"><a href="#cb14-757" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-758"><a href="#cb14-758" aria-hidden="true" tabindex="-1"></a>**Empfehlung: CLIP oder DINOvx (Foundation Models mit Feature Extraction)**</span>
<span id="cb14-759"><a href="#cb14-759" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-760"><a href="#cb14-760" aria-hidden="true" tabindex="-1"></a>**Begründung:**</span>
<span id="cb14-761"><a href="#cb14-761" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-762"><a href="#cb14-762" aria-hidden="true" tabindex="-1"></a>**CLIP (beste Wahl):**</span>
<span id="cb14-763"><a href="#cb14-763" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-764"><a href="#cb14-764" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Zero-shot fähig: Kann neue Klassen ohne Training hinzufügen</span>
<span id="cb14-765"><a href="#cb14-765" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Nutzer kann einfach Text-Labels definieren (z.B. "diagram", "meme")</span>
<span id="cb14-766"><a href="#cb14-766" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Bereits ausgezeichnet für "Screenshot" vs "Photo" Unterscheidung</span>
<span id="cb14-767"><a href="#cb14-767" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-768"><a href="#cb14-768" aria-hidden="true" tabindex="-1"></a>**DINOv2 (Alternative):**</span>
<span id="cb14-769"><a href="#cb14-769" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-770"><a href="#cb14-770" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Exzellente visuelle Features</span>
<span id="cb14-771"><a href="#cb14-771" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>k-NN classifier kann schnell erweitert werden</span>
<span id="cb14-772"><a href="#cb14-772" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Wenige Beispiele pro neuer Klasse nötig</span>
<span id="cb14-773"><a href="#cb14-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-774"><a href="#cb14-774" aria-hidden="true" tabindex="-1"></a>**Warum nicht supervised (ResNet)?**</span>
<span id="cb14-775"><a href="#cb14-775" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-776"><a href="#cb14-776" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Müsste für jede neue Klasse neu trainiert werden</span>
<span id="cb14-777"><a href="#cb14-777" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Benötigt viele Labels pro Klasse</span>
<span id="cb14-778"><a href="#cb14-778" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Nicht flexibel für user-defined Klassen</span>
<span id="cb14-779"><a href="#cb14-779" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-780"><a href="#cb14-780" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-781"><a href="#cb14-781" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-782"><a href="#cb14-782" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-783"><a href="#cb14-783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-784"><a href="#cb14-784" aria-hidden="true" tabindex="-1"></a><span class="fu">## Praktische Aspekte</span></span>
<span id="cb14-785"><a href="#cb14-785" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-786"><a href="#cb14-786" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Sie untersuchen die Rohdaten und stellen fest, dass die Labels in 5% aller Fälle inkorrekt sind. Welche Performance dürfen Sie maximal erwarten, wenn Sie ihr bestes Modell auf einem Testsplit auf diesen Rohdaten evaluieren?</span>
<span id="cb14-787"><a href="#cb14-787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-788"><a href="#cb14-788" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-789"><a href="#cb14-789" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Antwort anzeigen<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb14-790"><a href="#cb14-790" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-791"><a href="#cb14-791" aria-hidden="true" tabindex="-1"></a>**Maximal 95% Accuracy!**</span>
<span id="cb14-792"><a href="#cb14-792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-793"><a href="#cb14-793" aria-hidden="true" tabindex="-1"></a>**Begründung:**</span>
<span id="cb14-794"><a href="#cb14-794" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-795"><a href="#cb14-795" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>5% der Labels sind falsch (Label Noise)</span>
<span id="cb14-796"><a href="#cb14-796" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Selbst ein perfektes Modell, das die wahre Funktion lernt, wird diese 5% "falsch" klassifizieren</span>
<span id="cb14-797"><a href="#cb14-797" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Das Modell wird für korrekte Predictions bestraft, weil die Ground Truth falsch ist</span>
<span id="cb14-798"><a href="#cb14-798" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-799"><a href="#cb14-799" aria-hidden="true" tabindex="-1"></a>**Wichtige Implikationen:**</span>
<span id="cb14-800"><a href="#cb14-800" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-801"><a href="#cb14-801" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Human Baseline** ist maximal 95%</span>
<span id="cb14-802"><a href="#cb14-802" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Models können nicht besser sein als die Label-Qualität</span>
<span id="cb14-803"><a href="#cb14-803" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Bei Accuracy &gt; 95% liegt vermutlich Overfitting vor (Modell lernt Noise)</span>
<span id="cb14-804"><a href="#cb14-804" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-805"><a href="#cb14-805" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-806"><a href="#cb14-806" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-807"><a href="#cb14-807" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Sie möchten ein Bildklassifikationsproblem modellieren. Welche der folgenden Modelle probieren Sie zuerst aus nachdem Sie ihre Pipeline validiert und eine Baseline etabliert haben? Begründen Sie ihre Antwort.</span>
<span id="cb14-808"><a href="#cb14-808" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>A: ResNet50 vortrainiert auf ImageNet bei dem Sie einen linearen Layer hinzufügen</span>
<span id="cb14-809"><a href="#cb14-809" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>B: ResNet50 vortrainiert auf ImageNet, welches Sie mit einem Squeeze-and-Excite Block und einem linearen Layer erweitern</span>
<span id="cb14-810"><a href="#cb14-810" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>C: ResNet50 ohne Vortraining</span>
<span id="cb14-811"><a href="#cb14-811" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-812"><a href="#cb14-812" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-813"><a href="#cb14-813" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Antwort anzeigen<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb14-814"><a href="#cb14-814" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-815"><a href="#cb14-815" aria-hidden="true" tabindex="-1"></a>**Antwort: A (ResNet50 pretrained + Linear Layer)**</span>
<span id="cb14-816"><a href="#cb14-816" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-817"><a href="#cb14-817" aria-hidden="true" tabindex="-1"></a>**Begründung:**</span>
<span id="cb14-818"><a href="#cb14-818" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-819"><a href="#cb14-819" aria-hidden="true" tabindex="-1"></a>**Prinzip: "Don't be a hero"** (Karpathy)</span>
<span id="cb14-820"><a href="#cb14-820" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-821"><a href="#cb14-821" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Beginnen Sie mit einfachsten, etablierten Lösungen</span>
<span id="cb14-822"><a href="#cb14-822" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Fügen Sie Komplexität nur hinzu, wenn nötig</span>
<span id="cb14-823"><a href="#cb14-823" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-824"><a href="#cb14-824" aria-hidden="true" tabindex="-1"></a>**Warum A?**</span>
<span id="cb14-825"><a href="#cb14-825" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-826"><a href="#cb14-826" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Transfer Learning funktioniert:** ImageNet Features sind sehr allgemein</span>
<span id="cb14-827"><a href="#cb14-827" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Schnelles Training:** Nur Linear Layer trainieren (Feature Extraction)</span>
<span id="cb14-828"><a href="#cb14-828" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Gute Baseline:** Meist ~90% der finalen Performance</span>
<span id="cb14-829"><a href="#cb14-829" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Wenige Hyperparameter:** Reduziert Fehlerquellen</span>
<span id="cb14-830"><a href="#cb14-830" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-831"><a href="#cb14-831" aria-hidden="true" tabindex="-1"></a>**Warum nicht B?**</span>
<span id="cb14-832"><a href="#cb14-832" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Squeeze-and-Excite Blocks sind fancy, aber erhöhen Komplexität</span>
<span id="cb14-833"><a href="#cb14-833" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Erst hinzufügen, wenn A nicht gut genug ist</span>
<span id="cb14-834"><a href="#cb14-834" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Mehr Hyperparameter zum Tunen</span>
<span id="cb14-835"><a href="#cb14-835" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-836"><a href="#cb14-836" aria-hidden="true" tabindex="-1"></a>**Warum nicht C?**</span>
<span id="cb14-837"><a href="#cb14-837" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Training from scratch benötigt viel mehr Daten</span>
<span id="cb14-838"><a href="#cb14-838" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Länger Trainingszeit</span>
<span id="cb14-839"><a href="#cb14-839" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Meist schlechter als pretrained (ausser sehr domain-spezifisch)</span>
<span id="cb14-840"><a href="#cb14-840" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-841"><a href="#cb14-841" aria-hidden="true" tabindex="-1"></a>**Workflow:**</span>
<span id="cb14-842"><a href="#cb14-842" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>A: Pretrained + Linear → Baseline</span>
<span id="cb14-843"><a href="#cb14-843" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>A mit Fine-tuning → Wenn mehr Performance nötig</span>
<span id="cb14-844"><a href="#cb14-844" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>B: Custom Blocks → Nur bei speziellen Requirements</span>
<span id="cb14-845"><a href="#cb14-845" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-846"><a href="#cb14-846" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-847"><a href="#cb14-847" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-848"><a href="#cb14-848" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-849"><a href="#cb14-849" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Sie möchten Bilder von Kamerafallen im Wald klassifizieren. Diese Kamerafallen werden durch Bewegung ausgelöst und enthalten viele False Positives (kein Tier im Bild). In einem ersten Schritt möchten Sie leere Bilder (kein Tier) von nicht-leeren Bildern (hat Tier) unterscheiden. Sie stellen fest, dass einige Spezies (Eichhörnchen!) sehr klein sind. Was müssen Sie beachten?</span>
<span id="cb14-850"><a href="#cb14-850" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-851"><a href="#cb14-851" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">details</span><span class="dt">&gt;</span></span>
<span id="cb14-852"><a href="#cb14-852" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">summary</span><span class="dt">&gt;</span>Antwort anzeigen<span class="dt">&lt;/</span><span class="kw">summary</span><span class="dt">&gt;</span></span>
<span id="cb14-853"><a href="#cb14-853" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-854"><a href="#cb14-854" aria-hidden="true" tabindex="-1"></a>**Hauptherausforderung: Kleine Objekte im grossen Bild**</span>
<span id="cb14-855"><a href="#cb14-855" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-856"><a href="#cb14-856" aria-hidden="true" tabindex="-1"></a>**Kritische Aspekte:**</span>
<span id="cb14-857"><a href="#cb14-857" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-858"><a href="#cb14-858" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Räumliche Auflösung beibehalten:**</span>
<span id="cb14-859"><a href="#cb14-859" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Nicht zu aggressiv downsampling in frühen Layern</span>
<span id="cb14-860"><a href="#cb14-860" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Stride=1 oder kleinere Pooling-Operationen verwenden</span>
<span id="cb14-861"><a href="#cb14-861" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Eichhörnchen könnten nur 20×20 Pixel einnehmen</span>
<span id="cb14-862"><a href="#cb14-862" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-863"><a href="#cb14-863" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Input-Auflösung:**</span>
<span id="cb14-864"><a href="#cb14-864" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Höhere Input-Auflösung verwenden (z.B. 512×512 statt 224×224)</span>
<span id="cb14-865"><a href="#cb14-865" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Trade-off: Mehr Memory und Rechenzeit</span>
<span id="cb14-866"><a href="#cb14-866" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-867"><a href="#cb14-867" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Evaluation:**</span>
<span id="cb14-868"><a href="#cb14-868" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Recall für "animal"-Klasse ist wichtig (keine Tiere verpassen)</span>
<span id="cb14-869"><a href="#cb14-869" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Precision für "empty" weniger kritisch (False Positives OK)</span>
<span id="cb14-870"><a href="#cb14-870" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-871"><a href="#cb14-871" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">details</span><span class="dt">&gt;</span></span>
</code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© Copyright 2025, Marco Willi</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2025/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This page is built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>