<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Marco Willi">

<title>Project Ideas – CAS Deep Learning - Computer Vision</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../pages/misc/notation.html" rel="prev">
<link href="../..//assets/cv_logo_small.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-065a5179aebd64318d7ea99d77b64a9e.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-159896a7fbbc1299bfd73fe3af9cd852.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-97BR3FTSN1"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-97BR3FTSN1', { 'anonymize_ip': true});
</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../pages/misc/literature.html">Resources</a></li><li class="breadcrumb-item"><a href="../../pages/misc/student_projects.html">Project Ideas</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-center sidebar-header">
      <a href="../../index.html" class="sidebar-logo-link">
      <img src="../../assets/cv_logo_small.png" alt="" class="sidebar-logo light-content py-0 d-lg-inline d-none">
      <img src="../../assets/cv_logo_small.png" alt="" class="sidebar-logo dark-content py-0 d-lg-inline d-none">
      </a>
      <div class="sidebar-tools-main">
    <a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2025" title="GitHub organization" class="quarto-navigation-tool px-1" aria-label="GitHub organization"><i class="bi bi-github"></i></a>
    <a href="" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-code-square"></i></a>
</div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Background &amp; Preparation</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/background/machine_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1 - Machine Learning Basics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/background/neural_networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2 - Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/background/frameworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3 - Software &amp; Hardware for Deep Learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Lectures</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1 - Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/cnns.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2 - Convolutional Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3 - Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/representation_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">4 - Representation Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/practical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">5 - Practical</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/lectures/recent_advances.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">6 - Recent Advances</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Slides</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/cnns.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Convolutional Neural Networks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/image_classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Image Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/representation_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Representation Learning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/practical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Practical Considerations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/slides/recent_advances.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Recent Advances</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Resources</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/literature.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Books</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/links.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Helpful Links &amp; Resources</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Mathematical Notation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../pages/misc/student_projects.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Project Ideas</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#topic-list" id="toc-topic-list" class="nav-link active" data-scroll-target="#topic-list"><span class="header-section-number">1</span> Topic List</a>
  <ul class="collapse">
  <li><a href="#image-classification-eurosat-land-use-and-land-cover-classification" id="toc-image-classification-eurosat-land-use-and-land-cover-classification" class="nav-link" data-scroll-target="#image-classification-eurosat-land-use-and-land-cover-classification"><span class="header-section-number">1.1</span> Image Classification: EuroSAT Land Use and Land Cover Classification</a></li>
  <li><a href="#image-classification-food-images" id="toc-image-classification-food-images" class="nav-link" data-scroll-target="#image-classification-food-images"><span class="header-section-number">1.2</span> Image Classification: Food Images</a></li>
  <li><a href="#image-classification-synthetic-image-detection-with-foundation-models" id="toc-image-classification-synthetic-image-detection-with-foundation-models" class="nav-link" data-scroll-target="#image-classification-synthetic-image-detection-with-foundation-models"><span class="header-section-number">1.3</span> Image Classification: Synthetic Image Detection with Foundation Models</a></li>
  <li><a href="#counting-sea-turtles-from-drone-images" id="toc-counting-sea-turtles-from-drone-images" class="nav-link" data-scroll-target="#counting-sea-turtles-from-drone-images"><span class="header-section-number">1.4</span> Counting Sea Turtles from Drone Images</a></li>
  <li><a href="#semantic-segmentation-underwater-imagery" id="toc-semantic-segmentation-underwater-imagery" class="nav-link" data-scroll-target="#semantic-segmentation-underwater-imagery"><span class="header-section-number">1.5</span> Semantic Segmentation: Underwater Imagery</a></li>
  <li><a href="#semantic-segmentation-pothole-and-crack-detection" id="toc-semantic-segmentation-pothole-and-crack-detection" class="nav-link" data-scroll-target="#semantic-segmentation-pothole-and-crack-detection"><span class="header-section-number">1.6</span> Semantic Segmentation: Pothole and Crack Detection</a></li>
  <li><a href="#object-detection-polyp-detection" id="toc-object-detection-polyp-detection" class="nav-link" data-scroll-target="#object-detection-polyp-detection"><span class="header-section-number">1.7</span> Object Detection: Polyp Detection</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2025/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../pages/misc/literature.html">Resources</a></li><li class="breadcrumb-item"><a href="../../pages/misc/student_projects.html">Project Ideas</a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Project Ideas</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Marco Willi </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Institute for Data Science I4DS, FHNW
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  


</header>


<section id="topic-list" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="topic-list"><span class="header-section-number">1</span> Topic List</h2>
<p>You can choose your own topic!</p>
<p>Alternatively, feel free to pick one of the following topics.</p>
<section id="image-classification-eurosat-land-use-and-land-cover-classification" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="image-classification-eurosat-land-use-and-land-cover-classification"><span class="header-section-number">1.1</span> Image Classification: EuroSAT Land Use and Land Cover Classification</h3>
<p><strong>Goal</strong>: Develop a model to classify satellite images. You should consider RGB images and images with 13 spectral bands (see <a href="https://en.wikipedia.org/wiki/Sentinel-2">wiki</a>) in modeling. There are 10 classes and 27,000 images.</p>
<p><strong>Approach</strong>: Investigate various model architectures and decide what works best. Compare pre-trained models with those you train from scratch. Use appropriate data augmentation techniques. Since the dataset is relatively small, you should be careful of overfitting and robustly compare different models. Use RGB-only models for simplicity.</p>
<p><strong>Focus</strong>: Compare different pre-trained models (foundation models) and a model trained from scratch.</p>
<p><strong>Dataset</strong>: The two datasets can be found here: <a href="https://github.com/phelber/eurosat">https://github.com/phelber/eurosat</a>. There is a dataset in RGB format and a dataset with 13 spectral bands.</p>
<div id="fig-mp-eurosat" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mp-eurosat-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/student_projects/eurosat.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mp-eurosat-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Source: <a href="https://github.com/phelber/eurosat">Link</a>
</figcaption>
</figure>
</div>
<p><strong>Difficulty/Effort</strong>: Small - Medium</p>
</section>
<section id="image-classification-food-images" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="image-classification-food-images"><span class="header-section-number">1.2</span> Image Classification: Food Images</h3>
<p><strong>Goal</strong>: Develop a model to classify images of food. Pick 10 food classes (≈10k imgs)</p>
<p><strong>Approach</strong>: Use pre-trained (foundation) models.</p>
<p><strong>Focus</strong>: Compare different models, including CLIP zero-shot learning, and a DINO variant. Investigate how DINO improves with more labels.</p>
<p><strong>Dataset</strong>: The dataset can be found here: <a href="https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/">https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/</a>, see <span class="citation" data-cites="bossard_food-101_2014">Bossard, Guillaumin, and Van Gool (<a href="#ref-bossard_food-101_2014" role="doc-biblioref">2014</a>)</span>. It is 5 GB.</p>
<div id="fig-mp-eurosat" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mp-eurosat-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/student_projects/food101.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mp-eurosat-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Source: <a href="https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/">Link</a>
</figcaption>
</figure>
</div>
<p><strong>Difficulty/Effort</strong>: Small - Medium</p>
</section>
<section id="image-classification-synthetic-image-detection-with-foundation-models" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="image-classification-synthetic-image-detection-with-foundation-models"><span class="header-section-number">1.3</span> Image Classification: Synthetic Image Detection with Foundation Models</h3>
<p><strong>Goal</strong>: Develop a model to classify images into either synthetic or not (binary classification). You have to use pre-trained CLIP (<span class="citation" data-cites="radford_learning_2021">Radford et al. (<a href="#ref-radford_learning_2021" role="doc-biblioref">2021</a>)</span>) models and fine-tune them for this task.</p>
<p><strong>Approach</strong>: Choose a CLIP model (for example from here: <a href="https://huggingface.co/models">HuggingFace</a>). Understand how it works, and how the outputs of this model are organized. Use the pre-trained image encoder as a feature extractor and learn a small model on top of these features. Carefully split the dataset to train and evaluate it (image id, datasets, generative models).</p>
<p><strong>Focus</strong>: Focus on making the model work (establish a working baseline). Train your model on a train-split of the Synthbuster/RAISE1-K dataset and evaluate on a validation-split. Test your model on SynthwildX. Report overall and model-specific performances.</p>
<p><strong>Dataset</strong>: There are three ready-made datasets (Synthbuster, SynthWildX and <a href="http://loki.disi.unitn.it/RAISE/index.php">RAISE-1K</a>) in the <a href="https://huggingface.co/docs/datasets/en/index">dataset</a> format. It is important to read the documentation to efficiently use this format. The datasets consist of real images (RAISE-1K) and synthetic images (SynthwildX and Synthbuster) from different generative models. The Synthbuster dataset was created to match the RAISE-1K dataset: each real image was used to create a corresponding synthetic image (see <a href="#fig-mp-synthbuster" class="quarto-xref">Figure&nbsp;3</a>). The SynthwildX dataset was scraped from social media and is thus a good independent test set.</p>
<ul>
<li><a href="https://drive.google.com/file/d/1v3224djuHqMX2zbxoqPd6CDxA4Ali9qc/view?usp=drive_link">SynthwildX - 450 MB</a></li>
<li><a href="https://drive.google.com/file/d/1deG5gRNsHvnAA4nJzzIjL_iGpjHQqBY7/view?usp=drive_link">Synthbuster and RAISE-1K - 2.3 GB</a></li>
<li><a href="https://zenodo.org/records/10066460">(original link) Synthbuster</a></li>
<li><a href="http://loki.disi.unitn.it/RAISE/index.php">(original link) RAISE-1k</a></li>
</ul>
<div id="fig-mp-synthbuster" class="quarto-float quarto-figure quarto-figure-center anchored" width="800">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mp-synthbuster-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/student_projects/synth_detection.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mp-synthbuster-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Images from <span class="citation" data-cites="bammey_synthbuster_2023">(<a href="#ref-bammey_synthbuster_2023" role="doc-biblioref"><strong>bammey_synthbuster_2023?</strong></a>)</span> and <span class="citation" data-cites="dang-nguyen_raise_2015">(<a href="#ref-dang-nguyen_raise_2015" role="doc-biblioref"><strong>dang-nguyen_raise_2015?</strong></a>)</span>. The first row are real photographs that represent templates to generate synthetic images from (see second row onward).
</figcaption>
</figure>
</div>
<p><strong>Difficulty/Effort</strong>: HIGH!</p>
</section>
<section id="counting-sea-turtles-from-drone-images" class="level3" data-number="1.4">
<h3 data-number="1.4" class="anchored" data-anchor-id="counting-sea-turtles-from-drone-images"><span class="header-section-number">1.4</span> Counting Sea Turtles from Drone Images</h3>
<p><strong>Goal</strong>: Develop a model to process high-resolution images taken from drones to count sea turtles. The results can be used to survey marine turtles. The dataset includes approximately 1’000 images of high resolution (4,048 × 3,048 pixels).</p>
<p><strong>Approach</strong>: Read the paper <span class="citation" data-cites="gray_convolutional_2019">(<a href="#ref-gray_convolutional_2019" role="doc-biblioref"><strong>gray_convolutional_2019?</strong></a>)</span> as inspiration. It contains an easy way to break the problem down to an image classification task (see <a href="#fig-mp-turtle" class="quarto-xref">Figure&nbsp;4</a>). Implement your own CNN architecture and train a model. Use the train/validation splits as described in the paper. Compare different architectures and compare to the results reported in the paper. You can test different variations and techniques: CNN architectures, window-size, data augmentation and regularization.</p>
<p><strong>Dataset</strong>: Available here: <a href="https://zenodo.org/records/5004596#.ZChnr3ZBxD8">https://zenodo.org/records/5004596#.ZChnr3ZBxD8</a>.</p>
<div id="fig-mp-turtle" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mp-turtle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/student_projects/sea_turtles.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mp-turtle-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Source <span class="citation" data-cites="gray_convolutional_2019">(<a href="#ref-gray_convolutional_2019" role="doc-biblioref"><strong>gray_convolutional_2019?</strong></a>)</span>
</figcaption>
</figure>
</div>
<p><strong>Difficulty/Effort</strong>: Medium - High</p>
</section>
<section id="semantic-segmentation-underwater-imagery" class="level3" data-number="1.5">
<h3 data-number="1.5" class="anchored" data-anchor-id="semantic-segmentation-underwater-imagery"><span class="header-section-number">1.5</span> Semantic Segmentation: Underwater Imagery</h3>
<p><strong>Goal</strong>: Develop a segmentation model to segment underwater camera images. You should classify pixels into 8 classes on 1,500 images.</p>
<p><strong>Approach</strong>: Develop your architecture based on your intuition and knowledge from the course. Read the paper <span class="citation" data-cites="islam_semantic_2020">(<a href="#ref-islam_semantic_2020" role="doc-biblioref"><strong>islam_semantic_2020?</strong></a>)</span> and implement one of the architectures presented (SUIM-Net RSB or SUIM-Net VGG) and compare with your architecture. Since the dataset is relatively small, you should be careful of overfitting and robustly compare different models.</p>
<p><strong>Dataset</strong>: Available here: <a href="https://irvlab.cs.umn.edu/resources/suim-dataset">https://irvlab.cs.umn.edu/resources/suim-dataset</a>.</p>
<div id="fig-mp-suim" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mp-suim-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/student_projects/semantic_seg_underwater.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mp-suim-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Source: <a href="https://github.com/phelber/eurosat">Link</a>
</figcaption>
</figure>
</div>
<p><strong>Difficulty/Effort</strong>: Medium</p>
</section>
<section id="semantic-segmentation-pothole-and-crack-detection" class="level3" data-number="1.6">
<h3 data-number="1.6" class="anchored" data-anchor-id="semantic-segmentation-pothole-and-crack-detection"><span class="header-section-number">1.6</span> Semantic Segmentation: Pothole and Crack Detection</h3>
<p><strong>Goal</strong>: Segment images from street surfaces to detect regions with potholes and cracks. Classify each pixel into either normal, pothole or crack. The dataset consists of 4’340 images and masks.</p>
<p><strong>Approach</strong>. This dataset was used in a competition. Make sure to consult a description of the best methods as described in <span class="citation" data-cites="thompson_shrec_2022">(<a href="#ref-thompson_shrec_2022" role="doc-biblioref"><strong>thompson_shrec_2022?</strong></a>)</span>. Start with developing your architecture based on your intuition and knowledge from the course. One problem is the class imbalance, with many pixels being normal. Consider this when designing your loss function.</p>
<p><strong>Dataset</strong>: Available here: <a href="https://data.mendeley.com/datasets/kfth5g2xk3/2">https://data.mendeley.com/datasets/kfth5g2xk3/2</a>. See also <span class="citation" data-cites="andrea_ranieri_pothole_2022">(<a href="#ref-andrea_ranieri_pothole_2022" role="doc-biblioref"><strong>andrea_ranieri_pothole_2022?</strong></a>)</span>. This datset is actually a collection of different datasets. See <a href="#fig-mp-potholes" class="quarto-xref">Figure&nbsp;6</a> for examples.</p>
<p><strong>Difficulty/Effort</strong>: Medium</p>
<div id="fig-mp-potholes" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mp-potholes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/student_projects/potholes_n_cracks.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mp-potholes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Examples from different datasets. Red are cracks, blue potholes. From <span class="citation" data-cites="thompson_shrec_2022">(<a href="#ref-thompson_shrec_2022" role="doc-biblioref"><strong>thompson_shrec_2022?</strong></a>)</span>.
</figcaption>
</figure>
</div>
</section>
<section id="object-detection-polyp-detection" class="level3" data-number="1.7">
<h3 data-number="1.7" class="anchored" data-anchor-id="object-detection-polyp-detection"><span class="header-section-number">1.7</span> Object Detection: Polyp Detection</h3>
<p><strong>Goal</strong>: Develop an object detection model to detect polyps in images from endoscopic procedures. The goal is to detect each polyp with a bounding box. There are 1’000 images for training. See left and mid image of <a href="#fig-mp-polyp" class="quarto-xref">Figure&nbsp;7</a>.</p>
<p><strong>Approach</strong>: Develop <strong>your own architecture</strong> based on the lecture notes. Follow the best practices and implement your ML pipeline carefully from the ground up by gradually adding complexity. Compare a few architectures and hyper-parameters.</p>
<p><strong>Dataset</strong>: Available here: <a href="https://datasets.simula.no/kvasir-seg/">https://datasets.simula.no/kvasir-seg/</a>.</p>
<div id="fig-mp-polyp" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mp-polyp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../assets/images/student_projects/polyp_segmentation.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mp-polyp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Input image (left), semantic segmentation (middle) and object detection (right). From <span class="citation" data-cites="ro_kvasir-seg_2020">(<a href="#ref-ro_kvasir-seg_2020" role="doc-biblioref"><strong>ro_kvasir-seg_2020?</strong></a>)</span>.
</figcaption>
</figure>
</div>
<p><strong>Difficulty/Effort</strong>: High</p>
<!--


## Pet Breed Transfer Learning (small fine-tune)

**Task.** Fine-tune MobileNet/ResNet to classify 6–10 chosen breeds; visualize Grad-CAM on a few images.
**Dataset.** Oxford-IIIT Pet — <https://www.robots.ox.ac.uk/~vgg/data/pets/>
**Visual (pipeline).**
224×224 resize → pretrained backbone → small classification head → Grad-CAM on sample images



## Food-101 (Few-Class Mini-Challenge)

**Task.** Pick 10 food classes (≈10k imgs). Fine-tune a small net; discuss class imbalance & noisy labels.
**Dataset.** Food-101 — <https://www.vision.ee.ethz.ch/datasets_extra/food-101/>
**Visual (pipeline).**
food images → stratified split → transfer learning → metrics (Top-1, F1) → error gallery



## EuroSAT: Land-Cover from Space

**Task.** Classify land-use classes using EuroSAT RGB; compare training from scratch vs. ImageNet-pretrained; map most confused classes.
**Dataset.** EuroSAT — <https://github.com/phelber/EuroSAT>
**Visual (pipeline).**
Sentinel-2 patch (RGB) → preprocess → {ResNet18 scratch | ResNet18 pretrained} → compare metrics


## Industrial Anomaly Detection (one category)

**Task.** Choose one MVTec AD class (e.g., *bottle*). Train a simple autoencoder or kNN-embedding baseline on **only normal** images; detect defects on the test set.
**Dataset.** MVTec AD — <https://www.mvtec.com/company/research/datasets/mvtec-ad>
**Visual (pipeline).**
(normal train set) → {autoencoder | CNN embeddings + kNN} → fit “normality”
(test image) → reconstruct/embed → anomaly score → threshold → pass/fail


## Tiny Image Retrieval (CBIR) with Embeddings

**Task.** Compute image embeddings (pretrained CNN). Given a query image, return top-k similar images (cosine). Evaluate with precision@k on CIFAR-10 or a 10-class Food-101 subset.
**Dataset.** CIFAR-10 — <https://www.cs.toronto.edu/~kriz/cifar.html> or Food-101 subset
**Visual (pipeline).**
index images → embed with CNN → build vector index
query image → embed → nearest neighbors → precision@k


## Natural Scenes (Intel) — Quick Transfer Learn

**Task.** Classify 6 scene types (buildings/forest/glacier/mountain/sea/street). Great for a clean, single-evening fine-tune with tidy splits.
**Dataset.** Intel Image Classification (Kaggle) — <https://www.kaggle.com/datasets/puneet6060/intel-image-classification>
**Visual (pipeline).**
scene image → resize (~160) → MobileNetV3-Small → accuracy + ROC


## Suggested Time Split (per project)

- **Setup & data** (env, download, sanity checks): 2–3 h
- **Baseline model** (train once, save metrics): 4–5 h
- **One improvement** (augmentation / lr-schedule / transfer): 3–4 h
- **Evaluation & visuals** (confusion matrix, sample errors): 2–3 h
- **Short report** (1–2 pages with plots): 2 h


### Notes & Tips

- Favor **small backbones** and **few classes** to stay under the time budget.
- When datasets are large, **sample 2–10 classes** or **subset N images per class**.
- Track basic metrics (accuracy, F1), save a confusion matrix, and include 8–12 **failure cases** (thumbnails) in your report.
 -->


<!-- -->


</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-bossard_food-101_2014" class="csl-entry" role="listitem">
Bossard, Lukas, Matthieu Guillaumin, and Luc Van Gool. 2014. <span>“Food-101 – <span>Mining</span> <span>Discriminative</span> <span>Components</span> with <span>Random</span> <span>Forests</span>.”</span> In <em>European <span>Conference</span> on <span>Computer</span> <span>Vision</span></em>.
</div>
<div id="ref-radford_learning_2021" class="csl-entry" role="listitem">
Radford, Alec, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, et al. 2021. <span>“Learning <span>Transferable</span> <span>Visual</span> <span>Models</span> <span>From</span> <span>Natural</span> <span>Language</span> <span>Supervision</span>.”</span> <em>arXiv:2103.00020 [Cs]</em>, February. <a href="http://arxiv.org/abs/2103.00020">http://arxiv.org/abs/2103.00020</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/marco-willi\.github\.io\/cas-dl-compvis-lectures-hs2025\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../pages/misc/notation.html" class="pagination-link" aria-label="Mathematical Notation">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Mathematical Notation</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Project Ideas"</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="an">params:</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co">   images_path: "/assets/images/student_projects/"</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>::: {.content-hidden}</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>{{&lt; include /assets/_macros.tex &gt;}}</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="fu">## Topic List</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>You can choose your own topic!</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>Alternatively, feel free to pick one of the following topics.</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="fu">### Image Classification: EuroSAT Land Use and Land Cover Classification</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>**Goal**: Develop a model to classify satellite images. You should consider RGB images and images with 13 spectral bands (see <span class="co">[</span><span class="ot">wiki</span><span class="co">](https://en.wikipedia.org/wiki/Sentinel-2)</span>) in modeling. There are 10 classes and 27,000 images.</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>**Approach**: Investigate various model architectures and decide what works best. Compare pre-trained models with those you train from scratch. Use appropriate data augmentation techniques. Since the dataset is relatively small, you should be careful of overfitting and robustly compare different models. Use RGB-only models for simplicity.</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>**Focus**: Compare different pre-trained models (foundation models) and a model trained from scratch.</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>**Dataset**: The two datasets can be found here: <span class="co">[</span><span class="ot">https://github.com/phelber/eurosat</span><span class="co">](https://github.com/phelber/eurosat)</span>. There is a dataset in RGB format and a dataset with 13 spectral bands.</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>:::{#fig-mp-eurosat}</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}eurosat.png)</span>{width=600}</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>Source: <span class="co">[</span><span class="ot">Link</span><span class="co">](https://github.com/phelber/eurosat)</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>**Difficulty/Effort**: Small - Medium</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="fu">### Image Classification: Food Images</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>**Goal**: Develop a model to classify images of food. Pick 10 food classes (≈10k imgs)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>**Approach**: Use pre-trained (foundation) models.</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>**Focus**: Compare different models, including CLIP zero-shot learning, and a DINO variant. Investigate how DINO improves with more labels.</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>**Dataset**: The dataset can be found here: <span class="co">[</span><span class="ot">https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/</span><span class="co">](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/)</span>, see @bossard_food-101_2014. It is 5 GB.</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>:::{#fig-mp-eurosat}</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}food101.png)</span>{width=600}</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>Source: <span class="co">[</span><span class="ot">Link</span><span class="co">](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/)</span></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>**Difficulty/Effort**: Small - Medium</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a><span class="fu">### Image Classification: Synthetic Image Detection with Foundation Models</span></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>**Goal**: Develop a model to classify images into either synthetic or not (binary classification). You have to use pre-trained CLIP (@radford_learning_2021) models and fine-tune them for this task.</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>**Approach**: Choose a CLIP model (for example from here:  <span class="co">[</span><span class="ot">HuggingFace</span><span class="co">](https://huggingface.co/models)</span>). Understand how it works, and how the outputs of this model are organized. Use the pre-trained image encoder as a feature extractor and learn a small model on top of these features. Carefully split the dataset to train and evaluate it (image id, datasets, generative models).</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>**Focus**: Focus on making the model work (establish a working baseline). Train your model on a train-split of the Synthbuster/RAISE1-K dataset and evaluate on a validation-split. Test your model on SynthwildX. Report overall and model-specific performances.</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>**Dataset**: There are three ready-made datasets (Synthbuster, SynthWildX and <span class="co">[</span><span class="ot">RAISE-1K</span><span class="co">](http://loki.disi.unitn.it/RAISE/index.php)</span>) in the <span class="co">[</span><span class="ot">dataset</span><span class="co">](https://huggingface.co/docs/datasets/en/index)</span> format. It is important to read the documentation to efficiently use this format. The datasets consist of real images (RAISE-1K) and synthetic images (SynthwildX and Synthbuster) from different generative models. The Synthbuster dataset was created to match the RAISE-1K dataset: each real image was used to create a corresponding synthetic image (see @fig-mp-synthbuster). The SynthwildX dataset was scraped from social media and is thus a good independent test set.</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">SynthwildX - 450 MB</span><span class="co">](https://drive.google.com/file/d/1v3224djuHqMX2zbxoqPd6CDxA4Ali9qc/view?usp=drive_link)</span></span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Synthbuster and RAISE-1K - 2.3 GB</span><span class="co">](https://drive.google.com/file/d/1deG5gRNsHvnAA4nJzzIjL_iGpjHQqBY7/view?usp=drive_link)</span></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">(original link) Synthbuster</span><span class="co">](https://zenodo.org/records/10066460)</span></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">(original link) RAISE-1k</span><span class="co">](http://loki.disi.unitn.it/RAISE/index.php)</span></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>:::{#fig-mp-synthbuster width=800}</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}synth_detection.png)</span></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>Images from @bammey_synthbuster_2023 and @dang-nguyen_raise_2015. The first row are real photographs that represent templates to generate synthetic images from (see second row onward).</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>**Difficulty/Effort**: HIGH!</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a><span class="fu">### Counting Sea Turtles from Drone Images</span></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>**Goal**: Develop a model to process high-resolution images taken from drones to count sea turtles. The  results can be used to survey marine turtles. The dataset includes approximately 1'000 images of high resolution (4,048 × 3,048 pixels).</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>**Approach**: Read the paper @gray_convolutional_2019 as inspiration. It contains an easy way to break the problem down to an image classification task (see @fig-mp-turtle). Implement your own CNN architecture and train a model. Use the train/validation splits as described in the paper. Compare different architectures and compare to the results reported in the paper. You can test different variations and techniques: CNN architectures, window-size, data augmentation and regularization.</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>**Dataset**: Available here: <span class="co">[</span><span class="ot">https://zenodo.org/records/5004596#.ZChnr3ZBxD8</span><span class="co">](https://zenodo.org/records/5004596#.ZChnr3ZBxD8)</span>.</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>:::{#fig-mp-turtle}</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}sea_turtles.png)</span>{width=600}</span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>Source @gray_convolutional_2019</span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>**Difficulty/Effort**: Medium - High</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a><span class="fu">### Semantic Segmentation: Underwater Imagery</span></span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a>**Goal**: Develop a segmentation model to segment underwater camera images. You should classify pixels into 8 classes on 1,500 images.</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a>**Approach**: Develop your architecture based on your intuition and knowledge from the course. Read the paper @islam_semantic_2020 and implement one of the architectures presented (SUIM-Net RSB or SUIM-Net VGG) and compare with your architecture. Since the dataset is relatively small, you should be careful of overfitting and robustly compare different models.</span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a>**Dataset**: Available here: <span class="co">[</span><span class="ot">https://irvlab.cs.umn.edu/resources/suim-dataset</span><span class="co">](https://irvlab.cs.umn.edu/resources/suim-dataset)</span>.</span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a>:::{#fig-mp-suim}</span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}semantic_seg_underwater.png)</span>{width=600}</span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a>Source: <span class="co">[</span><span class="ot">Link</span><span class="co">](https://github.com/phelber/eurosat)</span></span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a>**Difficulty/Effort**: Medium</span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a><span class="fu">### Semantic Segmentation: Pothole and Crack Detection</span></span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a>**Goal**: Segment images from street surfaces to detect regions with potholes and cracks. Classify each pixel into either normal, pothole or crack. The dataset consists of 4'340 images and masks.</span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a>**Approach**. This dataset was used in a competition. Make sure to consult a description of the best methods as described in @thompson_shrec_2022. Start with developing your architecture based on your intuition and knowledge from the course. One problem is the class imbalance, with many pixels being normal. Consider this when designing your loss function.</span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a>**Dataset**: Available here: <span class="co">[</span><span class="ot">https://data.mendeley.com/datasets/kfth5g2xk3/2</span><span class="co">](https://data.mendeley.com/datasets/kfth5g2xk3/2)</span>. See also @andrea_ranieri_pothole_2022. This datset is actually a collection of different datasets. See @fig-mp-potholes for examples.</span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a>**Difficulty/Effort**: Medium</span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a>:::{#fig-mp-potholes}</span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}potholes_n_cracks.png)</span>{width=600}</span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a>Examples from different datasets. Red are cracks, blue potholes. From @thompson_shrec_2022.</span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a><span class="fu">### Object Detection: Polyp Detection</span></span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a>**Goal**: Develop an object detection model to detect polyps in images from endoscopic procedures. The goal is to detect each polyp with a bounding box. There are 1'000 images for training. See left and mid image of @fig-mp-polyp.</span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a>**Approach**: Develop **your own architecture** based on the lecture notes. Follow the best practices and implement your ML pipeline carefully from the ground up by gradually adding complexity. Compare a few architectures and hyper-parameters.</span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a>**Dataset**: Available here: <span class="co">[</span><span class="ot">https://datasets.simula.no/kvasir-seg/</span><span class="co">](https://datasets.simula.no/kvasir-seg/)</span>.</span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a>:::{#fig-mp-polyp}</span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a><span class="al">![]({{&lt; meta params.images_path &gt;}}polyp_segmentation.png)</span>{width=600}</span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a>Input image (left), semantic segmentation (middle) and object detection (right). From @ro_kvasir-seg_2020.</span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a>**Difficulty/Effort**: High</span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--</span></span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a><span class="co">## Pet Breed Transfer Learning (small fine-tune)</span></span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a><span class="co">**Task.** Fine-tune MobileNet/ResNet to classify 6–10 chosen breeds; visualize Grad-CAM on a few images.</span></span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a><span class="co">**Dataset.** Oxford-IIIT Pet — &lt;https://www.robots.ox.ac.uk/~vgg/data/pets/&gt;</span></span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a><span class="co">**Visual (pipeline).**</span></span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a><span class="co">224×224 resize → pretrained backbone → small classification head → Grad-CAM on sample images</span></span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a><span class="co">## Food-101 (Few-Class Mini-Challenge)</span></span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a><span class="co">**Task.** Pick 10 food classes (≈10k imgs). Fine-tune a small net; discuss class imbalance &amp; noisy labels.</span></span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a><span class="co">**Dataset.** Food-101 — &lt;https://www.vision.ee.ethz.ch/datasets_extra/food-101/&gt;</span></span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a><span class="co">**Visual (pipeline).**</span></span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a><span class="co">food images → stratified split → transfer learning → metrics (Top-1, F1) → error gallery</span></span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a><span class="co">## EuroSAT: Land-Cover from Space</span></span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a><span class="co">**Task.** Classify land-use classes using EuroSAT RGB; compare training from scratch vs. ImageNet-pretrained; map most confused classes.</span></span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a><span class="co">**Dataset.** EuroSAT — &lt;https://github.com/phelber/EuroSAT&gt;</span></span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a><span class="co">**Visual (pipeline).**</span></span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a><span class="co">Sentinel-2 patch (RGB) → preprocess → {ResNet18 scratch | ResNet18 pretrained} → compare metrics</span></span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a><span class="co">## Industrial Anomaly Detection (one category)</span></span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a><span class="co">**Task.** Choose one MVTec AD class (e.g., *bottle*). Train a simple autoencoder or kNN-embedding baseline on **only normal** images; detect defects on the test set.</span></span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a><span class="co">**Dataset.** MVTec AD — &lt;https://www.mvtec.com/company/research/datasets/mvtec-ad&gt;</span></span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a><span class="co">**Visual (pipeline).**</span></span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a><span class="co">(normal train set) → {autoencoder | CNN embeddings + kNN} → fit “normality”</span></span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a><span class="co">(test image) → reconstruct/embed → anomaly score → threshold → pass/fail</span></span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a><span class="co">## Tiny Image Retrieval (CBIR) with Embeddings</span></span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a><span class="co">**Task.** Compute image embeddings (pretrained CNN). Given a query image, return top-k similar images (cosine). Evaluate with precision@k on CIFAR-10 or a 10-class Food-101 subset.</span></span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a><span class="co">**Dataset.** CIFAR-10 — &lt;https://www.cs.toronto.edu/~kriz/cifar.html&gt; or Food-101 subset</span></span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a><span class="co">**Visual (pipeline).**</span></span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a><span class="co">index images → embed with CNN → build vector index</span></span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a><span class="co">query image → embed → nearest neighbors → precision@k</span></span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a><span class="co">## Natural Scenes (Intel) — Quick Transfer Learn</span></span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a><span class="co">**Task.** Classify 6 scene types (buildings/forest/glacier/mountain/sea/street). Great for a clean, single-evening fine-tune with tidy splits.</span></span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a><span class="co">**Dataset.** Intel Image Classification (Kaggle) — &lt;https://www.kaggle.com/datasets/puneet6060/intel-image-classification&gt;</span></span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a><span class="co">**Visual (pipeline).**</span></span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a><span class="co">scene image → resize (~160) → MobileNetV3-Small → accuracy + ROC</span></span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a><span class="co">## Suggested Time Split (per project)</span></span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a><span class="co">- **Setup &amp; data** (env, download, sanity checks): 2–3 h</span></span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a><span class="co">- **Baseline model** (train once, save metrics): 4–5 h</span></span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a><span class="co">- **One improvement** (augmentation / lr-schedule / transfer): 3–4 h</span></span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a><span class="co">- **Evaluation &amp; visuals** (confusion matrix, sample errors): 2–3 h</span></span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a><span class="co">- **Short report** (1–2 pages with plots): 2 h</span></span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a><span class="al">###</span><span class="co"> Notes &amp; Tips</span></span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a><span class="co">- Favor **small backbones** and **few classes** to stay under the time budget.</span></span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a><span class="co">- When datasets are large, **sample 2–10 classes** or **subset N images per class**.</span></span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a><span class="co">- Track basic metrics (accuracy, F1), save a confusion matrix, and include 8–12 **failure cases** (thumbnails) in your report.</span></span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a><span class="co"> --&gt;</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© Copyright 2025, Marco Willi</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/marco-willi/cas-dl-compvis-lectures-hs2025/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This page is built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>