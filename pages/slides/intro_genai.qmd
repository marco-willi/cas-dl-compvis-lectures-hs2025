---
title: "Intro to Generative AI"
params:
   images_path: "/assets/images/genai/"
---

## Overview

- Motivation & Applications
- What is Generative Modelling?
- Learning Generative Models
- Desirable Properties
- Comparing Approaches
- Evaluation


::: {.content-hidden}
$$
{{< include /assets/_macros.tex >}}
$$
:::


# Motivation

## Can You Tell This is Fake?

![]({{< meta params.images_path >}}intro_receipt.jpg){.r-stretch}

::: {.fragment}
This is a synthetic image generated by AI.
:::

## Image Generation: Midjourney

![]({{< meta params.images_path >}}intro_midjourney_images.png){.r-stretch}

::: {.fragment}
Creating highly creative and diverse images from text prompts
:::

## Video Generation: Midjourney

{{< video https://docs.midjourney.com/hc/article_attachments/37460844248077 >}}

::: {.fragment}
Recent advances enable short video generation
:::

## Consistent Multi-Modal Generation

::: {layout-ncol=2}

![]({{< meta params.images_path >}}intro_openai_1.webp)

![]({{< meta params.images_path >}}intro_openai_2.webp)

:::

::: {.fragment}
OpenAI's model generates consistent edits following instructions
:::

## Magic Editing

{{< video https://storage.googleapis.com/gweb-mobius-cdn/photos/uploads/6e54ed750f84538fd052b31818127f1e4df5711c.compressed.mp4 width=800 >}}

::: {.fragment}
Google Magic Editor: Is this how it really happened?
:::

## Super-Resolution

![]({{< meta params.images_path >}}intro_superres_images.png){.r-stretch}

::: {.fragment}
Generative models can enhance detail and up-sample images spatially
:::

## Gaming: NVIDIA DLSS

{{< video https://youtu.be/K8DddlREHEE?si=TvQIpHQUaLBf_M7Z&t=4 >}}

::: {.fragment}
Up-scaling using neural networks is cheaper than native high-res rendering
:::

## Ray Tracing Reconstruction

{{< video https://youtu.be/bTJYvtPRRcM?si=Cc_9T87nnNeOO3pH >}}

::: {.fragment}
Generative AI for expensive ray tracing operations
:::

## Medical Applications: MRI to CT

![]({{< meta params.images_path >}}mri_to_ct.png){.r-stretch}

::: {.fragment}
Convert MRI to synthetic CT scans without radiation exposure
:::

## Synthetic Training Data

![]({{< meta params.images_path >}}solar_flares.png){.r-stretch}

::: {.fragment}
Generate synthetic training data for solar flare prediction
:::

## Commercial: Virtual Staging

::: {layout-ncol=2}

![]({{< meta params.images_path >}}virtual_staging1.jpg)

![]({{< meta params.images_path >}}virtual_staging3.png)

:::

::: {.fragment}
Real estate: virtually furnish empty rooms
:::

## Evolution of Quality

![]({{< meta params.images_path >}}genai_evolution.png){.r-stretch}

::: {.fragment}
From 2014 to 2023: Dramatic quality improvements
:::


# What is Generative Modelling?

## Classification Task

![]({{< meta params.images_path >}}basics_classifier.png){.r-stretch}

::: {.fragment}
Classification: $f(\vect{x}) = \vect{y}$ (many-to-one mapping)
:::

## Generative Modelling

![]({{< meta params.images_path >}}basics_elephant.png){.r-stretch}

::: {.fragment}
Generation: $g(\vect{y}) = \vect{x}$ (one-to-many mapping)
:::

## Quiz: The Problem {.center}

::: {.r-fit-text}

If the input is only a label like "elephant," how can the model generate different elephants?

:::

## Quiz: The Problem {.center}

::: {.r-fit-text}

If the input is only a label like "elephant," how can the model generate different elephants?

**Answer**: Neural networks are deterministic. We need stochasticity!

:::

## The Diversity Challenge

![]({{< meta params.images_path >}}basics_elephants_probabilistic.png){.r-stretch}

::: {.fragment}
Need stochastic function to generate diverse outputs
:::

## Solution: Latent Variables

::: {.fragment}
Condition on stochastic input: $g(\vect{y}, \vect{z})$ where $\vect{z} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$
:::

::: {.fragment}
**Latent variable** $\vect{z}$: Unobserved characteristics

- Pose, size, number of individuals
- Time-of-day, surroundings, lighting
:::

::: {.fragment}
**Conditional generation**: $g(\vect{y}, \vect{z})$ with label $\vect{y}$

**Unconditional generation**: $g(\vect{z})$ without labels
:::

## Quiz: Latent Variables {.center}

![]({{< meta params.images_path >}}latent_variables_intuition1.png){width=50%}

::: {.fragment}
What could be the latent variables for this dataset?
:::

## Quiz: Latent Variables {.center}

![]({{< meta params.images_path >}}latent_variables_intuition1.png){width=50%}

**Answer**: Width and height (2 unobserved latent variables)

## Latent Space Example

![]({{< meta params.images_path >}}latent_variables_intuition2.png){.r-stretch}

::: {.fragment}
Mapping from 2D latent space to outputs
:::


# Learning Generative Models

## Core Objective

![]({{< meta params.images_path >}}synthetic_quality/synthetic_quality_evaluation.png){.r-stretch}

::: {.fragment}
**Goal**: Synthetic data $\hat{\vect{x}}$ should have high probability under real data distribution $p_{\text{data}}(\hat{\vect{x}})$
:::

## Two Approaches

::: {.fragment}
**1. Implicit Models (Direct)**

Learn function that directly generates samples: $g(\vect{z}) = \vect{x}$
:::

::: {.fragment}
**2. Density Models (Indirect)**

Learn probability density function $p_{\theta}(\vect{x})$
:::

## Density Models

::: {.fragment}
Learn $p_{\theta}: \vect{x} \to [0, \infty)$ with normalization:

$$\int_{\mathbf{x}} p_{\theta}(\mathbf{x}) d\mathbf{x} = 1$$
:::

::: {.fragment}
**Objective**: Maximize likelihood of training data

$$\underset{\theta}{\arg\max} \frac{1}{N} \sum_{i=1}^{N} \log p_{\theta}(\vect{x}^{(i)})$$
:::

## Learning a Density Model

::: {layout-ncol=2}

![Initial]({{< meta params.images_path >}}density_model_1d/initial_distribution.png)

![Improved]({{< meta params.images_path >}}density_model_1d/improved_distribution.png)

:::

::: {.fragment}
Distribution adapts to place higher probability where data points lie
:::


# Desirable Properties

## Property 1: Efficient Sampling

::: {.incremental}
- Generate new samples quickly
- Critical for real-time applications
- **Excel**: GANs, normalizing flows (single forward pass)
- **Struggle**: Autoregressive, diffusion models (sequential steps)
:::

## Property 2: High-Quality Sampling

::: {.incremental}
- Realistic, detailed samples
- Indistinguishable from real data
- **Excel**: Modern GANs, diffusion models, large autoregressive
- Often trades off with computational cost
:::

## Property 3: Coverage (Diversity)

::: {.incremental}
- Generate full diversity of training data
- Avoid mode collapse (limited variety)
- **Excel**: VAEs (good coverage)
- **Challenge**: GANs (risk mode collapse)
:::

## Property 4: Well-Behaved Latent Space

::: {.incremental}
- Smooth, meaningful structure
- Similar $\vect{z}$ → similar outputs
- Enables interpolation and controlled generation
- **Excel**: VAEs (explicit training objective)
:::

## Property 5: Disentangled Latent Space

::: {.incremental}
- Different dimensions control independent factors
- Example: separate controls for pose, lighting, color
- Enables fine-grained controllable generation
- Requires careful architectural design
:::

## Property 6: Efficient Likelihood Computation

::: {.incremental}
- Evaluate $p_{\theta}(\vect{x})$ for any sample
- Enables direct optimization, anomaly detection
- **Have**: Autoregressive, flows, VAEs (approx)
- **Lack**: GANs, many diffusion models
:::

## Quiz: No Perfect Model {.center}

::: {.r-fit-text}

Why is there no generative model that achieves all desirable properties?

:::

## Quiz: No Perfect Model {.center}

::: {.r-fit-text}

Why is there no generative model that achieves all desirable properties?

**Answer**: Fundamental tradeoffs exist between properties (e.g., fast sampling vs. exact likelihood, quality vs. coverage)

:::


# Comparing Approaches

## Model Comparison

| Method | Latent Variables | Density | Fast Sampling |
|--------|-----------------|----------------|---------------|
| **Autoregressive** | ❌ | ✅ | ❌ |
| **Diffusion** | ✅ | ⚠️ | ❌ |
| **GANs** | ✅ | ❌ | ✅ |
| **VAEs** | ✅ | ⚠️ | ✅ |

## Key Observations

::: {.incremental}
- **No method achieves all properties**
- **GANs**: Excellent quality & speed, no likelihood, mode collapse risk
- **VAEs**: Good coverage, approximate likelihood, sometimes blurry
- **Diffusion**: SOTA quality & coverage, slow sampling
- **Autoregressive**: Exact likelihood, very slow generation
:::


# Evaluation

## Challenge

::: {.fragment}
Evaluating quality and diversity is not trivial
:::

::: {.incremental}
- **Likelihood**: Only for models with $p_{\theta}(\vect{x})$
- **Human studies**: Cumbersome, not scalable
- **Automated metrics**: Various scores proposed
:::

## Inception Score

![]({{< meta params.images_path >}}evaluating_genai.png){.r-stretch}

::: {.fragment}
Uses classifier to evaluate quality and diversity
:::

## Inception Score Intuition

::: {.incremental}
- Good model generates images classifier predicts with high confidence
- Marginal distribution should be uniform (all classes represented)
- Only feasible for datasets with known classes (e.g., ImageNet)
:::


# Summary

## Key Takeaways

::: {.incremental}
- **Generative AI** creates synthetic data resembling real data
- **One-to-many mapping** requires latent variables for diversity
- **Core objective**: Maximize $p_{\text{data}}(\hat{\vect{x}})$
- **Tradeoffs**: Different approaches excel at different properties
- **No perfect model**: Choose based on application requirements
:::

## Major Model Families

::: {.incremental}
- **Autoregressive**: Sequential generation, exact likelihood
- **Diffusion**: State-of-the-art quality, iterative process
- **GANs**: Fast generation, adversarial training
- **VAEs**: Structured latent space, probabilistic framework
:::


# References

::: {style="font-size: 50%;"}

::: {#refs}
:::

:::
