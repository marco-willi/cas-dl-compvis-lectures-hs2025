<!DOCTYPE html>
<html lang="en"><head>
<link href="../..//assets/cv_logo_small.png" rel="icon" type="image/png">
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/tabby.min.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-bb1cd4028f63a369ab283e072cb7d572.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.8.25">

  <meta name="author" content="Marco Willi">
  <title>CAS Deep Learning - Computer Vision – Foundation Models</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #767676;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #767676;  padding-left: 4px; }
    div.sourceCode
      { color: #545454; background-color: #fefefe; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #545454; } /* Normal */
    code span.al { color: #7928a1; } /* Alert */
    code span.an { color: #696969; } /* Annotation */
    code span.at { color: #a55a00; } /* Attribute */
    code span.bn { color: #7928a1; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #d91e18; } /* ControlFlow */
    code span.ch { color: #008000; } /* Char */
    code span.cn { color: #d91e18; } /* Constant */
    code span.co { color: #696969; } /* Comment */
    code span.cv { color: #696969; font-style: italic; } /* CommentVar */
    code span.do { color: #696969; font-style: italic; } /* Documentation */
    code span.dt { color: #7928a1; } /* DataType */
    code span.dv { color: #7928a1; } /* DecVal */
    code span.er { color: #7928a1; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #a55a00; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #696969; } /* Information */
    code span.kw { color: #d91e18; } /* Keyword */
    code span.op { color: #00769e; } /* Operator */
    code span.ot { color: #d91e18; } /* Other */
    code span.pp { color: #7928a1; } /* Preprocessor */
    code span.sc { color: #00769e; } /* SpecialChar */
    code span.ss { color: #008000; } /* SpecialString */
    code span.st { color: #008000; } /* String */
    code span.va { color: #a55a00; } /* Variable */
    code span.vs { color: #008000; } /* VerbatimString */
    code span.wa { color: #696969; font-style: italic; } /* Warning */
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/theme/quarto-b593fe19bbc53d5665e08ad100a1fb50.css">
  <script async="" src="https://www.googletagmanager.com/gtag/js?id=G-97BR3FTSN1"></script>

  <script type="text/javascript">

  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-97BR3FTSN1', { 'anonymize_ip': true});
  </script>
  <link href="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" data-background-image="../../assets/fhnw_bg.png" data-background-opacity="1.0" data-background-size="contain" class="quarto-title-block center">
  <h1 class="title">Foundation Models</h1>
  <p class="subtitle">CAS Deep Learning - Computer Vision (Part1)</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Marco Willi 
</div>
        <p class="quarto-title-affiliation">
            Institute for Data Science I4DS, FHNW
          </p>
    </div>
</div>

</section>
<section id="what-are-they" class="slide level2">
<h2>What are they?</h2>
<p>Foundation models are large-scale machine learning models trained on vast amounts of data that can be fine-tuned for various downstream tasks. These models have demonstrated remarkable capabilities in natural language processing, computer vision, and other fields.</p>
</section>
<section id="characteristics-of-foundation-models" class="slide level2">
<h2>Characteristics of Foundation Models</h2>
<ul>
<li><strong>Large-scale Pre-training</strong>: Foundation models are pre-trained on extensive datasets, enabling them to capture a wide range of knowledge.</li>
<li><strong>Transfer Learning</strong>: These models can be fine-tuned on specific tasks with relatively small datasets, making them versatile and efficient.</li>
<li><strong>Multimodal Capabilities</strong>: Some foundation models can process and integrate multiple types of data, such as text and images.</li>
</ul>
</section>
<section id="clip-a-foundation-model-example" class="slide level2">
<h2>CLIP: A Foundation Model Example</h2>
<p>CLIP (Contrastive Language-Image Pre-training, <span class="citation" data-cites="radford_learning_2021">Radford et al. (<a href="#/references" role="doc-biblioref" onclick="">2021</a>)</span>) is a foundation model developed by OpenAI. It is designed to understand images and text jointly, making it capable of tasks like zero-shot image classification.</p>
</section>
<section id="how-clip-works" class="slide level2">
<h2>How CLIP Works</h2>

<img data-src="../../assets/images/foundation_models/clip_part1.jpg" class="r-stretch quarto-figure-center"><p class="caption">Source: <span class="citation" data-cites="radford_learning_2021">Radford et al. (<a href="#/references" role="doc-biblioref" onclick="">2021</a>)</span></p><p>CLIP is pre-trained on a diverse dataset of images and their corresponding textual descriptions. It learns to associate images with their textual descriptions using a contrastive learning approach, which maximizes the similarity between correct image-text pairs and minimizes the similarity between incorrect pairs.</p>
</section>
<section id="how-clip-can-be-applied" class="slide level2">
<h2>How CLIP can be applied</h2>

<img data-src="../../assets/images/foundation_models/clip_part2.jpg" class="r-stretch quarto-figure-center"><p class="caption">Source: <span class="citation" data-cites="radford_learning_2021">Radford et al. (<a href="#/references" role="doc-biblioref" onclick="">2021</a>)</span></p></section>
<section id="applications-of-clip" class="slide level2">
<h2>Applications of CLIP</h2>
<ul>
<li><strong>Zero-Shot Classification</strong>: CLIP can classify images into categories it has not explicitly been trained on by leveraging its understanding of language.</li>
<li><strong>Image Search</strong>: By inputting a textual description, CLIP can retrieve relevant images from a database.</li>
<li><strong>(near) Duplicate Detection</strong>: CLIP can assist in identifying near duplicate images.</li>
</ul>
</section>
<section id="example" class="slide level2">
<h2>Example</h2>
<p>Here’s a simple example of using CLIP for zero-shot image classification:</p>
<div id="38989435" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a></a><span class="im">import</span> requests</span>
<span id="cb1-2"><a></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb1-3"><a></a><span class="im">from</span> transformers <span class="im">import</span> CLIPModel, CLIPProcessor</span>
<span id="cb1-4"><a></a></span>
<span id="cb1-5"><a></a><span class="co"># 1) Get Models and Data Pre-Processors</span></span>
<span id="cb1-6"><a></a>model <span class="op">=</span> CLIPModel.from_pretrained(<span class="st">"openai/clip-vit-base-patch32"</span>)</span>
<span id="cb1-7"><a></a>processor <span class="op">=</span> CLIPProcessor.from_pretrained(<span class="st">"openai/clip-vit-base-patch32"</span>)</span>
<span id="cb1-8"><a></a></span>
<span id="cb1-9"><a></a><span class="co"># 2) Define Input Data: Image and Texts</span></span>
<span id="cb1-10"><a></a>url <span class="op">=</span> <span class="st">"http://images.cocodataset.org/val2017/000000039769.jpg"</span></span>
<span id="cb1-11"><a></a>image <span class="op">=</span> Image.<span class="bu">open</span>(requests.get(url, stream<span class="op">=</span><span class="va">True</span>).raw)</span>
<span id="cb1-12"><a></a></span>
<span id="cb1-13"><a></a>prompt_template <span class="op">=</span> <span class="st">"a photo of a </span><span class="sc">{}</span><span class="st">"</span></span>
<span id="cb1-14"><a></a>classes <span class="op">=</span> [<span class="st">"cat"</span>, <span class="st">"dog"</span>, <span class="st">"car"</span>, <span class="st">"tree"</span>]</span>
<span id="cb1-15"><a></a>prompts <span class="op">=</span> [prompt_template.<span class="bu">format</span>(class_name) <span class="cf">for</span> class_name <span class="kw">in</span> classes]</span>
<span id="cb1-16"><a></a></span>
<span id="cb1-17"><a></a><span class="co"># 3) Pre-Process Inputs</span></span>
<span id="cb1-18"><a></a>inputs <span class="op">=</span> processor(</span>
<span id="cb1-19"><a></a>    text<span class="op">=</span>prompts,</span>
<span id="cb1-20"><a></a>    images<span class="op">=</span>image,</span>
<span id="cb1-21"><a></a>    return_tensors<span class="op">=</span><span class="st">"pt"</span>,</span>
<span id="cb1-22"><a></a>    padding<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-23"><a></a>)</span>
<span id="cb1-24"><a></a></span>
<span id="cb1-25"><a></a><span class="co"># 4) Forward-Pass</span></span>
<span id="cb1-26"><a></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb1-27"><a></a>    outputs <span class="op">=</span> model(<span class="op">**</span>inputs)</span>
<span id="cb1-28"><a></a>logits_per_image <span class="op">=</span> outputs.logits_per_image  <span class="co"># this is the image-text similarity score</span></span>
<span id="cb1-29"><a></a></span>
<span id="cb1-30"><a></a><span class="co"># 5) Find most likely label</span></span>
<span id="cb1-31"><a></a>probs <span class="op">=</span> logits_per_image.softmax(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-32"><a></a><span class="bu">print</span>(<span class="st">"Label:"</span>, classes[probs.argmax().item()])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
</section>
<section id="fine-tuning-clip" class="slide level2">
<h2>Fine-Tuning CLIP</h2>
<p>If CLIP does not perform well with zero-shot learning, it can be fine-tuned with various techniques. This is a powerful mechanism because the features that CLIP has learned are very powerful and might be useful for your target task.</p>
</section>
<section>
<section id="multi-modal-models" class="title-slide slide level1 center">
<h1>Multi-Modal Models</h1>

</section>
<section id="visual-question-anwering" class="slide level2">
<h2>Visual Question Anwering</h2>
<p>Multi-modal models extend the capabilities of foundation models by integrating and processing multiple types of data simultaneously. One notable example of a multi-modal model is a Visual Question Answering (VQA) system, which can understand and answer questions about images. <a href="#/fig-foundation-blip2" class="quarto-xref">Figure&nbsp;1</a> shows an example.</p>

<img data-src="../../assets/images/foundation_models/blip2_examples.jpg" class="r-stretch quarto-figure-center" id="fig-foundation-blip2"><p class="caption">Source: <span class="citation" data-cites="li_blip-2_2023">Li et al. (<a href="#/references" role="doc-biblioref" onclick="">2023</a>)</span></p></section>
<section id="how-vqa-works" class="slide level2">
<h2>How VQA Works</h2>
<p>VQA models combine visual data (images) with textual data (questions) to generate accurate answers. These models are typically pre-trained on large datasets containing images, questions about those images, and the corresponding answers. <a href="#/fig-foundation-flamengo" class="quarto-xref">Figure&nbsp;2</a> shows training data examples that were used in the Flamingo model <span class="citation" data-cites="alayrac_flamingo_2022">Alayrac et al. (<a href="#/references" role="doc-biblioref" onclick="">2022</a>)</span>.</p>

<img data-src="../../assets/images/foundation_models/flamengo_training_data.jpg" class="r-stretch quarto-figure-center" id="fig-foundation-flamengo"><p class="caption">Source: <span class="citation" data-cites="alayrac_flamingo_2022">Alayrac et al. (<a href="#/references" role="doc-biblioref" onclick="">2022</a>)</span></p></section>
<section id="applications-of-vqa" class="slide level2">
<h2>Applications of VQA</h2>
<ul>
<li><strong>Accessibility</strong>: VQA can help visually impaired users by answering questions about their surroundings based on images captured by a camera.</li>
<li><strong>Chat Bots with Visual Understanding</strong>: VQA systems can be used in chat bots to incorporate visual inputs.</li>
<li><strong>Customer Support</strong>: VQA can enhance customer support by allowing users to submit images and ask questions about products or services.</li>
</ul>
</section>
<section id="conclusion" class="slide level2">
<h2>Conclusion</h2>
<p>Foundation models like CLIP and multi-modal models such as VQA represent significant advancements in machine learning, offering powerful capabilities across various tasks. Their ability to learn from large datasets and generalize to new tasks makes them valuable tools in the AI landscape.</p>
</section></section>
<section id="references" class="title-slide slide level1 smaller scrollable">
<h1>References</h1>
<div style="font-size: 50%;">
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-alayrac_flamingo_2022" class="csl-entry" role="listitem">
Alayrac, Jean-Baptiste, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, et al. 2022. <span>“Flamingo: A <span>Visual</span> <span>Language</span> <span>Model</span> for <span>Few</span>-<span>Shot</span> <span>Learning</span>.”</span> arXiv. <a href="http://arxiv.org/abs/2204.14198">http://arxiv.org/abs/2204.14198</a>.
</div>
<div id="ref-li_blip-2_2023" class="csl-entry" role="listitem">
Li, Junnan, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. <span>“<span>BLIP</span>-2: <span>Bootstrapping</span> <span>Language</span>-<span>Image</span> <span>Pre</span>-Training with <span>Frozen</span> <span>Image</span> <span>Encoders</span> and <span>Large</span> <span>Language</span> <span>Models</span>.”</span> arXiv. <a href="http://arxiv.org/abs/2301.12597">http://arxiv.org/abs/2301.12597</a>.
</div>
<div id="ref-radford_learning_2021" class="csl-entry" role="listitem">
Radford, Alec, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, et al. 2021. <span>“Learning <span>Transferable</span> <span>Visual</span> <span>Models</span> <span>From</span> <span>Natural</span> <span>Language</span> <span>Supervision</span>.”</span> <em>arXiv:2103.00020 [Cs]</em>, February. <a href="http://arxiv.org/abs/2103.00020">http://arxiv.org/abs/2103.00020</a>.
</div>
</div>
</div>


</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<p><img src="../../assets/fhnw_logo.png" class="slide-logo"></p>
<div class="footer footer-default">
<p><a href="https://marco-willi.github.io/cas-dl-compvis-lectures-hs2025">https://marco-willi.github.io/cas-dl-compvis-lectures-hs2025/</a></p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1920,

        height: 1080,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
          const outerScaffold = trigger.parentElement.cloneNode(true);
          const codeEl = outerScaffold.querySelector('code');
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/marco-willi\.github\.io\/cas-dl-compvis-lectures-hs2025\/");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>