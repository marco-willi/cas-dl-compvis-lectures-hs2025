
---
title: "Convolutional Neural Networks"
params:
   images_path: "/assets/images/cnns/"
---

<!--
NOTE: For Quarto Reveal.js syntax and advanced slide features,
refer to QUARTO.md in the project root.
Key features used in this file:
- Fragments (.fragment) for incremental reveals
- Incremental lists (.incremental)
- Columns layout (.columns)
- Auto-animate for transitions
See QUARTO.md sections:
- "Creating Reveal.js Slides" for basics
- "Advanced Reveal.js Features" for fragments, animations, layouts
-->

::: {.content-hidden}
$$
{{< include /assets/_macros.tex >}}
$$
:::

## Overview

- Introduction & Motivation
- Convolutional Layers
- Properties
- Variants and Layers
- Visualizations and Architectures


# Introduction & Motivation

## Properties of Image Data


![[Image Source](https://unsplash.com/de/fotos/braune-schafe-auf-grunem-grasfeld-tagsuber-olonUwUrmQk)]({{< meta params.images_path >}}cows2_small.png){width=100%}

::: {.incremental}
- **High-Dimensional**: An RGB image of size $224 \times 224$ has $224 \times 224 \times 3 = 150{,}528$ values
- **Locality**: Nearby pixels are statistically related (edges, textures, objects)
- **Stability under transformations**: A cat should be recognized regardless of position
- **Hierarchical Structure**: Features at multiple scales (pixels â†’ edges â†’ textures â†’ objects)
:::



## Multilayer-Perceptron and Images


![ ]({{< meta params.images_path >}}mlp_images_example.png)


::: {.fragment}
MLPs deal with flat inputs: The spatial structure gets lost.
:::



## MLPs and High-Dimensional Inputs

![ ]({{< meta params.images_path >}}mlp_images_example_weights.png)

::: {.fragment}
Weight matrix $\m{W} \in \mathbb{R}^{d \times k}$ scales with input size ($d$ = input dimensionality, $k$ = number of neurons)
:::

::: {.fragment}
Columns of $\m{W}$ can be visualized as images to show what patterns each neuron learns
:::


<!-- $k$ must be sufficiently large to learn the necessary patterns. This can lead to practical problems regarding memory and compute. -->

<!-- For each neuron in the first hidden layer, the model must learn as many weights as there are input neurons. For an RGB image of size $224 \times 224$  (height, width) each neuron connects to each input via $150'528$ weights. To learn many and varying patterns a large number of neurons are often required, leading to practical problems (memory and compute) when training an MLP on such inputs. -->

<!--
## Multilayer-Perceptrons and Pattern Learning

::: {#fig-cnns-mlp-example3}

![MLP on example image.]({{< meta params.images_path >}}mlp_images_example_weights_as_images.png)

:::

The columns of the weight matrix $\m{W}$ (in the first hidden layer) can be visualized to learn what kind pattern a specific neuron has learned by reshaping them to the dimensionality of the input images $\vect{x} \in \mathbb{R}^{h \times w}$. -->


<!--
## Multilayer-Perceptrons and Pattern Learning

::: {#fig-cnns-mlp-example4}

![MLP on example image.]({{< meta params.images_path >}}mlp_images_example_pattern_matching.png)

:::


## Multilayer-Perceptrons and Pattern Learning

::: {#fig-cnns-mlp-example-permutation}

![MLP on example image.]({{< meta params.images_path >}}mlp_images_examples_permutation.png)

:::

::: {.fragment .highlight-red}
Can an MLP successfully learn patterns in images $\vect{x}$ that are permuted with a permutation matrix $\m{P}$, i.e. $f(P(\vect{x}))$?

:::

## Multilayer-Perceptrons and Pattern Learning

::: {#fig-cnns-mlp-example-permutation}

![MLP on example image.]({{< meta params.images_path >}}mlp_images_examples_permuted_weights.png)

:::

Yes! An MLP has no notion of distance and treats every connection between every input equally. -->


## MLPs and Translations

![MLP and pattern shifts]({{< meta params.images_path >}}mlp_images_example_pattern_shifts.png)

::: {.fragment}
Often patterns appear at different positions (not stationary)
:::

::: {.fragment}
Ideally we want to recognize them under translations: $g(\vect{x})$ where $g()$ is a spatial translation
:::

::: {.fragment}

::: {.fragment .highlight-red}
**How do MLPs deal with this?**
:::

:::

## Multilayer-Perceptrons and Pattern Learning


![MLP on example image.]({{< meta params.images_path >}}mlp_images_examples_permutation.png)

::: {.fragment .highlight-red}
Can an MLP successfully learn patterns in images $\vect{x}$ that are permuted with a permutation matrix $\m{P}$, i.e. $f(P(\vect{x}))$?
:::


## Problems: MLPs and Images

::: {.incremental}
- **High-Dimensional**: MLP size scales with input dimensionality â†’ memory/compute issues
- **No Locality**: MLPs can't exploit spatial relationships in images
- **Inefficient Translation Handling**: Must learn position-dependent patterns separately
:::

::: {.fragment}
**Interactive Demo**: [https://adamharley.com/nn_vis/mlp/2d.html](https://adamharley.com/nn_vis/mlp/2d.html)
:::

::: {.fragment}
**Solution?** â†’ Need architecture with **inductive biases** for images!
:::


## Invariance and Equivariance

For many tasks small variations in the input should either **not** change the model output (invariance) or should change the output **in tandem** with the input changes (equivariance).

::: {.fragment}
A function $f(\vect{x})$ is **invariant** to a transformation $g(\vect{x})$ if:

\begin{align}
f(g(\vect{x})) = f(\vect{x})
\end{align}
:::


::: {.fragment}

A function $f(\vect{x})$ (such as a layer in a neural network) of an image $\vect{x}$ is **equivariant** with respect to a transformation $g(\vect{\vect{x}})$ if:

\begin{align}
f(g(\vect{x})) = g(f(\vect{x}))
\end{align}
:::



## Example Invariance

Example where invariance is required:


::: {#fig-cnns-invariance-example}

![When objects in the input translate spatially, the output (in this case the classification of the images) does not change. The model  $f(\vect{x})$ is thus invariant to spatial translations. ]({{< meta params.images_path >}}cows2_small_invariance_collage.png)

:::


## Example Equivariance

Example where equivariance is required:

::: {#fig-cnns-equivariance-example}

![When objects in the input (first row) translate spatially, the detections (bounding boxes) change accordingly (bottom row). The model  $f(\vect{x})$ that produces the bounding boxes is thus equivariant with respect to spatial translations.]({{< meta params.images_path >}}cows2_small_detection_collage.png)

:::


# Convolutional Layers


## History: Experiments on Cats

::: {#fig-cnns-huber-wiesel}

![Illustration [Source](https://link.springer.com/chapter/10.1007/978-3-030-28954-6_4/figures/1)]({{< meta params.images_path >}}huber_wiesel_cat_experiment.jpg)

:::



## Visual Cortex

::: {#fig-cnns-visual-cortex}

![Representation of transformations in the visual cortex. Source: @kubilius_ventral_2017]({{< meta params.images_path >}}ventralvisualstream_v2.png)

:::


## Convolutional Neural Networks

::: {#fig-cnns-intro-example}

![The activations of a ConvNet architecture. The input image is on the left, and the predictions are on the right. Source: @li_cs231n_2022]({{< meta params.images_path >}}convnet.jpeg)

:::


## Convolution on RGB Images

::: {#fig-cnns-intro-example}

![Source:  @prince_understanding_2023]({{< meta params.images_path >}}convolution_prince_rgb.png)

:::

::: {.fragment}
\begin{equation}
S(i, j) = (K * I)(i, j) = \sum_m \sum_n I(i + m, j + n) K(m, n)
\end{equation}

- $I$: Input (e.g., image)
- $K$: Kernel (typically smaller than $I$)
- Bias term $b$ added to weighted sum
:::


## Convolutional Layers

![Source: @johnson_eecs_2019]({{< meta params.images_path >}}cnn_conv_one_number.jpg)


## Convolutional Layers

![Source: @johnson_eecs_2019]({{< meta params.images_path >}}conv_activation_map.jpg)



## Convolutional Layers

![Source: @johnson_eecs_2019]({{< meta params.images_path >}}conv_activation_map2.jpg)



## Convolutional Layers

![Source: @johnson_eecs_2019]({{< meta params.images_path >}}conv_activation_map3.jpg)



## Convolutional Layers

![Source: @johnson_eecs_2019]({{< meta params.images_path >}}conv_activation_map4.jpg)


<!--
## Convolution on RGB Images

::: {#fig-cnns-intro-example}

![A 2D convolution applied to an image. Illustrated is the element-wise multiplication of the kernel weights $\Omega$ with the corresponding RGB input values at a specific position. Source:  @prince_understanding_2023]({{< meta params.images_path >}}convolution_prince_rgb.png)

::: -->


## Convolution Hyperparameters

Key parameters to configure a convolutional layer:

::: {.incremental}
- **Padding** ($p$): Zero-padding added to input borders
- **Stride** ($s$): Step size when moving the kernel
- **Kernel Size** ($k$): Spatial extent (typically $3\times3$ or $5\times5$)
- **Dilation** ($d$): Spacing between kernel elements
:::


## Padding

::: {.fragment}
**Question**: Output size when convolving with $3\times3$ kernel?
:::

:::: {.columns}

::: {.column width="50%"}
::: {.fragment}
**Without padding:**

![ ]({{< meta params.images_path >}}padding_input_activations_5x5.jpg)
:::
:::

::: {.column width="50%"}
::: {.fragment}
**With zero-padding:**

![ ]({{< meta params.images_path >}}padding_input_activations_5x5_with_zero_padding.jpg)
:::
:::

::::

## Padding Effect

![Left: Input (Yellow) with Zero-Padding (white border), Middle: Filter, Right: Output.]({{< meta params.images_path >}}padding_example_result.jpg)

::: {.fragment}
**Key Use**: Preserve spatial dimensions â†’ essential for segmentation!
:::


## Stride = 1

![Convolution with stride $(1, 1)$]({{< meta params.images_path >}}stride1_example.jpg)

::: {.fragment}
Kernel moves **1 pixel** at a time â†’ **preserves spatial resolution**
:::

## Stride = 2

![Convolution with stride $(2, 2)$]({{< meta params.images_path >}}stride2_example.jpg)

::: {.fragment}
Kernel moves **2 pixels** at a time â†’ **downsamples** by factor of 2
:::

::: {.fragment}
**Advantage**: Learned downsampling (vs. fixed pooling)
:::


## Kernel-Size



![ ]({{< meta params.images_path >}}kernel_size_example_3x3.jpg)

## Kernel-Size

![ ]({{< meta params.images_path >}}kernel_size_example_5x5.jpg)

::: {.fragment}
What is the difference between using 3x3 vs 5x5 kernels? What is the same?
:::



## Dilation

![ ]({{< meta params.images_path >}}dilation_example.jpg)

::: {.fragment}
Why would we use dilation $d \gt 1$?
:::

## Calculations: Spatial Dimensionality of Activation Maps

You can calculate the dimensionality of the activation maps with the following formulas:

- $i$: Side length of the input activations (assumption: square inputs)
- $k$: Kernel size (assumption: square kernel)
- $o$: Side length of the output activation maps
- $s$: Stride (assumption: same stride along spatial dimensions)
- $p$: Number of paddings on each side (assumption: same number of paddings along spatial dimensions)
- $d$: Dilation rate.


::: {.fragment}
\begin{equation}
o = \left\lfloor \frac{i + 2p - k-(k-1)(d-1)}{s} \right\rfloor + 1
\end{equation}

Note: If dilation is 1 $(k-1)(d-1)$ equals 0.
:::


## Calculations: Number of Weights in a Convolutional Layer

You can calculate the total number of weights in a convolutional layer with the following formula:

- $k$: Kernel size (assumption: square kernel, so kernel has dimensions $k \times k$)
- $C_{in}$: Number of input channels
- $C_{out}$: Number of output channels

::: {.fragment}
\begin{equation}
\text{Total Weights} = (k \times k) \times C_{in} \times C_{out}
\end{equation}
:::

::: {.fragment}
Each filter has a size of $k \times k \times C_{in}$, and there are $C_{out}$ filters in total, resulting in $(k \times k \times C_{in}) \times C_{out}$ weights.
:::

::: {.fragment}
**Biases:**
If each output channel has a bias term, add an additional $C_{out}$ weights for the biases.


\begin{equation}
\text{Total Parameters} = (k \times k \times C_{in} \times C_{out}) + C_{out}
\end{equation}
:::


## Quiz: Output Shape & Parameters

**Scenario:**

- Input: $3 \times 32 \times 32$ (C, H, W)
- Convolution: 10 filters, kernel $5 \times 5$, stride=1, pad=2

::: {.fragment}
**Questions:**

1. What is the size of the output activation map?
2. How many parameters does this operation have?
:::

## Quiz: Output Shape & Parameters

**Formulas:**

::: {.fragment}
\begin{equation}
o = \left\lfloor \frac{i + 2p - d\cdot(k-1) - 1}{s} + 1 \right\rfloor
\end{equation}
:::

::: {.fragment}
\begin{equation}
\text{Total Parameters} = (k \times k \times C_{in} \times C_{out}) + C_{out}
\end{equation}
:::

## Quiz: Solutions

::: {.fragment}
**1. Output size:**

$o = \lfloor \frac{32 + 2(2) - 1(5-1) - 1}{1} + 1 \rfloor = \lfloor \frac{32 + 4 - 4 - 1}{1} + 1 \rfloor = 32$
:::

::: {.fragment}
Output shape: $10 \times 32 \times 32$
:::

::: {.fragment}
**2. Parameters:**

$(5 \times 5 \times 3 \times 10) + 10 = 750 + 10 = 760$ parameters
:::

::: {.fragment}
**Key Insight:** Padding=2 preserves spatial dimensions with kernel=5!
:::


# Properties

## CNN Key Properties

::: {.fragment}
**1. Local (Sparse) Connectivity**

Neurons only connect to small spatial regions
:::

::: {.fragment}
**2. Parameter Sharing**

Same weights applied across entire input (translation equivariance!)
:::

## Think About It

::: {.incremental}
- Can we recognize cats with a one-layer CNN?
- Can we recognize cats **anywhere** in an image with a CNN?
- Is parameter sharing **always** useful?
- What about images with position-dependent features?
:::


## Translation Invariance / Equivariance

Given a translation $g()$, which spatially shifts inputs:

- Translation invariance: $f(g(x))=f(x)$
- Translation equivariance: $f(g(x))=g(f(x))$

::: {.fragment}
Are convolutions translation invariant, equivariant, or both?
:::

::: {.fragment}
{{< video https://www.youtube.com/embed/qoWAFBYOtoU?start=50 width=800 height=400 >}}
:::

## Stacking Convolutions

![Source: @johnson_eecs_2019]({{< meta params.images_path >}}conv_stacking.jpg){width=100% height=70%}


## Receptive Field


![Using a 5x5 kernel]({{< meta params.images_path >}}receptive_field1.jpg)



## Receptive Field


![Using a 5x5 kernel and two layers]({{< meta params.images_path >}}receptive_field2.jpg)



## Receptive Field


![ ]({{< meta params.images_path >}}receptive_field3.jpg)



## Receptive Field


![ ]({{< meta params.images_path >}}receptive_field4.jpg)

## Receptive Field: Formula

The receptive field of a CNN $r_0$:

::: {.fragment}
\begin{equation}
r_0 = \sum_{l=1}^L \left( (k_l - 1) \prod_{i=1}^{l-1} s_i \right) + 1
\end{equation}
:::

::: {.fragment}
- $L$: Total number of layers
- $k_l$: Kernel size at layer $l$
- $s_i$: Stride at layer $i$, from layer 1 up to $l-1$
:::

::: {.fragment}
**Special case**: If all strides $s_i=1$, receptive field = $\sum (k_l - 1) + 1$
:::

## Quiz: Receptive Field

**Network Architecture:**

- Layer 1: $k_1 = 3$, $s_1 = 1$
- Layer 2: $k_2 = 3$, $s_2 = 1$
- Layer 3: $k_3 = 3$, $s_3 = 1$

::: {.fragment}
**Question**: What is the receptive field $r_0$?
:::

## Quiz: Receptive Field Solution

::: {.fragment}
Using formula: $r_0 = \sum_{l=1}^L \left( (k_l - 1) \prod_{i=1}^{l-1} s_i \right) + 1$
:::

::: {.fragment}
Since all $s_i = 1$:
$$r_0 = (3-1) + (3-1) + (3-1) + 1 = 2 + 2 + 2 + 1 = 7$$
:::

::: {.fragment}
**Key Insight**: Stacking three $3\times3$ convs gives $7\times7$ receptive field!
:::

::: {.fragment}
**Efficiency**: Three $3\times3$ = $3 \times 9 = 27$ params vs one $7\times7$ = 49 params
:::


## Architecture

Sequence of layers and their hyper-parameters defines an architecture.

::: {.fragment}
![VGG - Source: @prince_understanding_2023]({{< meta params.images_path >}}vgg_illustration.jpg)
:::

## Learned Filters

![Source: @krizhevsky_imagenet_2012]({{< meta params.images_path >}}learned_filters.png)

## Let's test it!

Let's train a CNN.

Here is an interesting demo: [https://poloclub.github.io/cnn-explainer/](https://poloclub.github.io/cnn-explainer/)


# More Layers & Operations

## Downsampling

::: {.fragment}
Spatial downsampling reduces resolution (HÃ—W â†’ H'Ã—W')
:::

::: {.fragment}
**Options:**

1. Stride $> 1$ (learned downsampling)
2. Pooling layers (fixed operation)
:::

::: {.fragment}
![a) Sub-Sampling, b) Max Pooling, c) Average Pooling. Source: @prince_understanding_2023]({{< meta params.images_path >}}downsampling_with_pooling.jpg){width=80%}
:::

## Upsampling

::: {.fragment}
Need to **increase** spatial dimensions (HÃ—W â†’ H'Ã—W')
:::

::: {.fragment}
**Example use case?**
:::

::: {.fragment}
**Image Segmentation** - need pixel-level predictions!
:::

## Upsampling Methods

![a) Duplicate, b) Max-Unpooling, c) Bilinear Interpolation. Source: @prince_understanding_2023]({{< meta params.images_path >}}upsampling_options.jpg){width=80%}

::: {.fragment}
Also: **Transposed Convolutions** (learnable upsampling)
:::

## Upsampling with Transposed Convolutions


![ ]({{< meta params.images_path >}}transposed_convolution.jpg)

More details in image segmentation!


## 1Ã—1 Convolutions

::: {.fragment}
**Purpose**: Change number of channels without spatial mixing
:::

::: {.fragment}
$(C_{in} \times H \times W) \rightarrow (C_{out} \times H \times W)$
:::

::: {.fragment}
**Use cases:**

- Dimensionality reduction (bottleneck layers)
- Increase model depth without parameters explosion
- Cross-channel information mixing
:::
<!-- ::: {#fig-cnn-1x1}

![Source: @johnson_eecs_2019]({{< meta params.images_path >}}1x1_conv.jpg){width=100% height=70%}

::: -->


::: {#fig-cnn-1x1}

![Source: @prince_understanding_2023]({{< meta params.images_path >}}1x1_conv_example.jpg)

:::




## Global Average Pooling

Global Average Pooling is often an important component. It computes the average of the activations along the depth dimension, reducing activation maps from (C x H x W) to (C x 1 x 1). This is useful for directly modeling logits in a classification problem with C classes, enabling architectures that completely eliminate fully-connected layers.

::: {#fig-cnn-global-avg-pooling}

![Global Average pooling, input (left) and output (right).]({{< meta params.images_path >}}global_average_pooling.jpg)
:::


## Other Layers

::: {.fragment}
Many other layer types exist:
:::

::: {.incremental}
- Depthwise separable convolutions (efficiency)
- Dilated convolutions (large receptive fields)
- Grouped convolutions (parameter reduction)
- Batch normalization (training stability)
- Dropout (regularization)
:::

## Quiz: MLP vs CNN Efficiency

**CIFAR-10 Images**: $32 \times 32 \times 3$

::: {.fragment}
**MLP**: 2 hidden layers (256, 128 neurons)

How many parameters in first layer?
:::

::: {.fragment}
**CNN**: 2 conv layers (16 filters, $3\times3$, then 32 filters, $3\times3$)

How many parameters in first layer?
:::

## Quiz: Solutions

::: {.fragment}
**MLP first layer:**

$(32 \times 32 \times 3) \times 256 = 3{,}072 \times 256 = 786{,}432$ parameters
:::

::: {.fragment}
**CNN first layer:**

$(3 \times 3 \times 3 \times 16) + 16 = 432 + 16 = 448$ parameters
:::

::: {.fragment}
**CNN uses 1,755Ã— fewer parameters!** ðŸš€
:::

::: {.fragment}
**Why?** Local connectivity + parameter sharing = massive efficiency!
:::



# References

::: {style="font-size: 50%;"}

::: {#refs}
:::

:::
