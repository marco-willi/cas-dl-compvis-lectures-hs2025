---
title: "Representation Learning"
params:
   images_path: "/assets/images/representation_learning/"
---

<!--
NOTE: For Quarto Reveal.js syntax and advanced slide features,
refer to QUARTO.md in the project root.
Key features used in this file:
- Fragments (.fragment) for incremental reveals
- Incremental lists (.incremental)
- Columns layout (.columns)
- Tabsets for comparisons
See QUARTO.md sections:
- "Creating Reveal.js Slides" for basics
- "Advanced Reveal.js Features" for fragments, animations, layouts
-->

::: {.content-hidden}
$$
{{< include /assets/_macros.tex >}}
$$
:::

## Overview

- Motivation & Introduction
- What is Representation Learning?
- Deep Image Representations
- Comparing Representations
- Practical Applications
- Summary


# Motivation

## The Challenge

![Example Image [Source](https://unsplash.com/de/fotos/elefantenwanderung-tagsuber-QJbyG6O0ick)]({{< meta params.images_path >}}elephant.jpg){width=70%}

::: {.fragment}
**Human description**: "An elephant facing the camera with large tusks, standing in a savannah, bright daylight, cloudy sky"
:::

::: {.fragment}
**Key insight**: Massive compression while preserving semantically relevant information!
:::

## Information Content Comparison

::: {.fragment}
**Raw RGB Image**: $640 \times 360 \times 3 \times 8 = 5{,}529{,}600$ bits ≈ **675 KB**
:::

::: {.fragment}
**Natural Language**: ~1,400 bits ≈ **0.17 KB**
:::

::: {.fragment}
**Compression Factor**: ~3,950×!
:::

::: {.fragment}
But: Much relevant information preserved for practical tasks
:::

## The Image Space Problem

![Illustrated are the set of all possible images and the subset of natural images]({{< meta params.images_path >}}set_of_images.png){width=60%}

::: {.fragment}
Total possible $256\times256$ RGB images: $255^{256 \times 256 \times 3}$ ≈ **infinite**
:::

::: {.fragment}
Natural images: **tiny subset** of this space
:::

::: {.fragment}
**Implication**: Pixel space is extremely inefficient for representing real images!
:::


# What is Representation Learning?

## Definition

::: {.fragment}
A **representation** is the output $\vect{z} = f(\vect{x})$ produced by an encoder $f$ applied to input $\vect{x}$
:::

::: {.fragment}
- $\vect{z}$ retains **task-relevant structure**
- Discards **nuisance variation**
- Encoder $f$ is the mapping, $\vect{z}$ is the result
:::

::: {.fragment}
We focus on **vector-valued** (distributed) representations, not symbolic descriptions
:::

## Intuition: Synthetic Examples

::: {layout-nrow=2}


![Elephant - Day 1]({{< meta params.images_path >}}examples/elephant_day_01.png){fig-alt="Elephant in daylight savanna"}

![Elephant - Day 1]({{< meta params.images_path >}}augmented/elephant_flip_horizontal.png){fig-alt="Elephant in daylight savanna"}

![Elephant - Day 1]({{< meta params.images_path >}}augmented/elephant_brightness_1.8x.png){fig-alt="Elephant in daylight savanna"}

![Elephant - Day 2]({{< meta params.images_path >}}examples/elephant_day_02.png){fig-alt="Elephant by watering hole in daylight"}

![Giraffe - Day]({{< meta params.images_path >}}examples/giraffe_day_01.png){fig-alt="Giraffe in daylight grassland"}

![Rhino - Night]({{< meta params.images_path >}}examples/rhino_night_01.png){fig-alt="Rhinoceros at nighttime"}

![Elephant - Night]({{< meta params.images_path >}}examples/elephant_night_01.png){fig-alt="Elephant at night with moonlight"}

![Car - Day]({{< meta params.images_path >}}examples/car_day_01.png){fig-alt="Modern car in bright daylight"}

:::


## Exercise: 2D Representation

![Empty 2-dimensional representation space for mapping image similarity]({{< meta params.images_path >}}representation_space.png){width=40%}

::: {.fragment}
**Your task**: How would you map these images to 2D space?
:::

::: {.fragment}
**Hint**: Similar images should be close, dissimilar far apart
:::

## Data Space → Representation Space

![**Data Space vs Representation Space**: The encoder $f$ transforms high-dimensional input samples $\vect{x}_i$ into compact representations $\vect{z}_i$ where semantic relationships become explicit]({{< meta params.images_path >}}data_and_representation_space.png){width=80%}

## What Makes Good Representations?

::: {.fragment}
**1. Invariance & Selectivity Balance**

- Invariant to nuisance transforms (lighting, pose)
- Selective for semantic differences
:::

::: {.fragment}
**2. Geometric Structure Preservation**

- Similar concepts cluster together
- Smooth manifold structure
:::

::: {.fragment}
**3. Transfer Efficiency**

- Few examples needed for new tasks
- Broad applicability across domains
:::

::: {.fragment}
**4. Computational Practicality**

- Compact dimensionality: $\dim(\vect{z}) \ll \dim(\vect{x})$
- Fast forward pass
:::

## The Invariance-Selectivity Trade-off

::: {.fragment}
**Invariance**: $\|f(\vect{x}) - f(g(\vect{x}))\|_2 < \epsilon$

(Similar output for nuisance transforms)
:::

::: {.fragment}
**Selectivity**: $\|f(\vect{x}_i) - f(\vect{x}_j)\|_2 > \delta$

(Different outputs for semantic differences)
:::

::: {.fragment}
**Challenge**: Too much invariance → loss of discriminative details!
:::

::: {.fragment}
**Sweet spot**: Preserve task-relevant variations, discard nuisance factors
:::


# Deep Image Representations

## Three Learning Paradigms

::: {.incremental}
1. **Supervised Learning**: Learn from human labels (ImageNet)
2. **Weakly Supervised**: Learn from noisy/partial labels (image-text pairs)
3. **Self-Supervised**: Learn from data structure (no labels needed)
:::

## Supervised Learning

![**Supervised Pre-training**: Encoder + classifier trained with labeled data]({{< meta params.images_path >}}supervised_learning_representations.png){width=70%}

::: {.fragment}
- Train on labeled data (e.g., ImageNet 1000 classes)
- Cross-entropy loss on human-provided labels
- Penultimate layer embeddings = transferable features
:::

## Weakly Supervised: CLIP

![**CLIP Contrastive Pre-training**: Dual encoders align images with text captions]({{< meta params.images_path >}}clip_contrastive_pre_training.png){width=70%}

::: {.fragment}
Uses image-text pairs (web-scale data)
:::

::: {.fragment}
Contrastive loss: maximize diagonal, minimize off-diagonal
:::

## CLIP: Key Properties

::: {.fragment}
**Zero-shot Classification**

Text prompt: "a photo of a {class}" → class prototype
:::

::: {.fragment}
**Scalability**

Learns from billions of noisy web pairs
:::

::: {.fragment}
**Vision-Language Alignment**

Shared embedding space for images and text
:::

## Self-Supervised Learning

::: {.fragment}
**Create pretext tasks from raw data** (no human labels!)
:::

::: {.fragment}
Two dominant families:

1. **Imputation/Masked Modeling** (MAE)
2. **Contrastive/Self-Distillation** (SimCLR, DINO)
:::

## Masked Autoencoding (MAE)

![**MAE Pipeline**: Mask patches → Encode visible → Decode all → Reconstruct masked]({{< meta params.images_path >}}masked_auto_encoder.png){width=90%}

## MAE: How It Works

::: {.fragment}
1. **Mask** 75% of image patches randomly
:::

::: {.fragment}
2. **Encode** only visible patches (efficient!)
:::

::: {.fragment}
3. **Decode** with mask tokens inserted
:::

::: {.fragment}
4. **Reconstruct** original pixels at masked positions
:::

::: {.fragment}
**Key insight**: To fill in gaps, model must learn global structure
:::

## Contrastive Learning: SimCLR

::: {layout-ncol=2}

![]({{< meta params.images_path >}}self_supervised_contrastive1.png)

![]({{< meta params.images_path >}}self_supervised_contrastive2.png)

:::


## Self-Distillation: DINO

![]({{< meta params.images_path >}}dino_principle.png)

## DINO: Key Advantages

::: {.incremental}
- **No explicit negatives** needed (unlike SimCLR)
- Works with **smaller batches**
- Emergent semantic clustering
- Produces high-quality **local features** (patch tokens)
:::

::: {.fragment}
**Result**: State-of-the-art transfer performance with simpler training
:::

## DINOv3: Local Features

![**DINOv3 Patch Features**: Spatially-resolved embeddings preserve structure]({{< meta params.images_path >}}dinov3_local_features_grid.png){width=80%}

::: {.fragment}
Each patch = semantic feature vector (unlike global pooling)
:::


# Comparing Representations

## Test Images Overview

![]({{< meta params.images_path >}}images_overview.png){.r-stretch}

## Comparisons

![]({{< meta params.images_path >}}images_overview.png){.r-stretch}

::: {.panel-tabset #fig-similarity-comparison-models}

### Pixel Space

::: {#fig-rl-similarity-pixel-space}

![]({{< meta params.images_path >}}similarity_pixel_space.png){fig-alt="Pixel-space similarity matrix" width=600}

Raw pixel-level similarity fails to capture semantic relationships.
:::

### ResNet-50

::: {#fig-rl-similarity-resnet}

![]({{< meta params.images_path >}}similarity_resnet50.png){fig-alt="ResNet-50 supervised representation similarity" width=600}

Supervised CNN representations group semantically similar images.
:::

### CLIP

::: {#fig-rl-similarity-clip}

![]({{< meta params.images_path >}}similarity_clip.png){fig-alt="CLIP multimodal representation similarity" width=600}

Language-aligned embeddings from CLIP show text-driven semantic structure.
:::

### DINOv2

::: {#fig-rl-similarity-dinov2}

![]({{< meta params.images_path >}}similarity_dinov2.png){fig-alt="DINOv2 self-supervised representation similarity" width=600}

Self-distilled DINOv2 features reveal fine-grained semantic clusters.
:::

:::

## Pixel Space Similarity

![]({{< meta params.images_path >}}similarity_pixel_space.png)

::: {.fragment}
Problem: Lighting variations dominate, semantic relationships hidden!
:::

## ResNet (Supervised) Similarity

![]({{< meta params.images_path >}}similarity_resnet50.png)

::: {.fragment}
Better semantic grouping, but still affected by brightness
:::

## CLIP Similarity

![]({{< meta params.images_path >}}similarity_clip.png)

::: {.fragment}
Strong vision-language alignment, robust to appearance changes
:::

## DINOv2 Similarity

![]({{< meta params.images_path >}}similarity_dinov2.png)

::: {.fragment}
Best semantic discrimination, excellent invariance to nuisance factors
:::

## Comparison Summary

::: {.fragment}
| Model | Semantic | Invariance | Use Case |
|-------|----------|------------|----------|
| **Pixel** | ❌ Poor | ❌ None | Baseline |
| **ResNet** | ⚠️ Moderate | ⚠️ Limited | Fast inference |
| **CLIP** | ✅ Good | ✅ Strong | Zero-shot, V+L |
| **DINOv2** | ✅ Excellent | ✅ Excellent | Few-shot, dense tasks |
:::


# Practical Applications

## Best Practices

::: {.incremental}
- ✅ Start with k-NN baseline (diagnostic value)
- ✅ Use linear probe before fine-tuning
- ✅ Match invariances to task requirements
- ✅ Choose global/local features based on task
- ✅ Full fine-tuning is often overkill
:::


## Model Selection Guide

::: {.fragment}
**DINOv3**: Best for discriminative vision tasks, few-shot learning
:::

::: {.fragment}
**CLIP**: Zero-shot classification, vision-language tasks
:::

::: {.fragment}
**Supervised CNNs**: When computational efficiency is critical
:::

## Global vs Local Representations

::: {.fragment}
**Global Embeddings** (e.g., [CLS] token)

- Single vector per image
- Use for: Classification, retrieval, zero-shot
:::

::: {.fragment}
**Local Embeddings** (patch tokens)

- Grid of spatially-resolved features
- Use for: Segmentation, detection, dense prediction
:::

::: {.fragment}
**Modern models provide both!**
:::


## The Adaptation Hierarchy

::: {.fragment}
**Always start simple, add complexity only when needed!**
:::

::: {.incremental}
1. **k-NN baseline** (0 training) → Quick reality check
2. **Linear probe** (freeze encoder) → Test feature quality
3. **LoRA/Adapters** (efficient tuning) → Parameter-efficient
4. **Partial fine-tune** (top layers) → Balanced approach
5. **Full fine-tune** (all weights) → Last resort
:::

## Strategy 1: k-NN Baseline

::: {.fragment}
**No training required!**
:::

```python
# Extract features with frozen pretrained model
features_train = model.encode(train_images)
features_test = model.encode(test_images)

# Classify by nearest neighbors
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(features_train, train_labels)
predictions = knn.predict(features_test)
```

::: {.fragment}
✅ Instant results, reveals representation quality
:::

## Strategy 2: Linear Probe

::: {.fragment}
**Freeze encoder, train only classification head**
:::

```python
# Freeze pretrained model
for param in model.parameters():
    param.requires_grad = False

# Add trainable head
head = nn.Linear(embed_dim, num_classes)

# Train only the head
optimizer = AdamW(head.parameters(), lr=5e-4)
```

::: {.fragment}
✅ Fast, stable, minimal overfitting risk
:::

## Strategy 3: LoRA/Adapters

::: {.fragment}
**Add small adapter modules, train only those**
:::

```python
from peft import LoraConfig, get_peft_model

config = LoraConfig(
    r=16, lora_alpha=32,
    target_modules=["query","key","value"],
)
model = get_peft_model(model, config)
```

::: {.fragment}
✅ Much cheaper than full fine-tuning, adapts to task
:::

## Strategy 4: Partial Fine-tune

::: {.fragment}
**Unfreeze top layers, keep early layers frozen**
:::

```python
# Freeze all parameters
for param in model.parameters():
    param.requires_grad = False

# Unfreeze last 2 transformer blocks
for param in model.blocks[-2:].parameters():
    param.requires_grad = True

# Replace classification head
model.head = nn.Linear(embed_dim, num_classes)
```

::: {.fragment}
✅ Adjusts high-level features while keeping general features
:::

## Strategy 5: Full Fine-tune

::: {.fragment}
**Train all model weights - use as last resort!**
:::

```python
# Unfreeze everything
for param in model.parameters():
    param.requires_grad = True

# Use small learning rate to avoid catastrophic forgetting
optimizer = AdamW(model.parameters(), lr=5e-5)
```

::: {.fragment .highlight-red}
⚠️ Risk: Overfitting with limited data, expensive compute
:::



# Summary

## Key Takeaways

::: {.incremental}
1. **Representation learning** transforms raw pixels into semantic embeddings
2. **Three paradigms**: Supervised, weakly supervised, self-supervised
3. **Trade-off**: Balance invariance (robustness) with selectivity (discrimination)
4. **Model selection**: DINOv3 for vision, CLIP for V+L, MAE for reconstruction
5. **Start simple**: k-NN → Linear → Adapters → Fine-tune
:::

## Best Practices

::: {.incremental}
- ✅ Start with k-NN baseline (diagnostic value)
- ✅ Use linear probe before fine-tuning
- ✅ Match invariances to task requirements
- ✅ Choose global/local features based on task
- ✅ Full fine-tuning is often overkill
:::




# References

::: {style="font-size: 50%;"}

::: {#refs}
:::

:::
